{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcGcwnZ6nKXXOs8Z535XAm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/03_2_3_2025_1250.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "b6152cf8-7a3b-452e-cfad-33e243dd6a08",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.2\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_2_1-2025_1823\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script using:\n",
        " - Chunk-based Parallel Processing for Speed\n",
        " - Anchor-suppression to avoid individual's own ID\n",
        " - **FP-Growth**-based Shared Segment Detection (instead of Apriori),\n",
        "   which avoids repeated passes over the entire dataset\n",
        "\n",
        "Exports final CSV/HTML sorted by \"Yates DNA Ancestral Line\".\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "import logging\n",
        "import functools\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# NEW: MLxtend library for FP-Growth\n",
        "# Make sure you've done: pip install mlxtend\n",
        "from mlxtend.frequent_patterns import fpgrowth\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# Global Variables\n",
        "###############################################################################\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "###############################################################################\n",
        "# Utility: chunk generator\n",
        "###############################################################################\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "###############################################################################\n",
        "# GedcomDataset\n",
        "###############################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        \"\"\"\n",
        "        Extract cM from NPFX field. If NPFX has a format like \"175&someSort**someYDNA\",\n",
        "        the cM is '175'. If it doesn't parse cleanly, returns blank.\n",
        "        \"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        \"\"\"\n",
        "        If NPFX has \"xxx&sortVal**ydnaVal\", returns sortVal. If not found, blank.\n",
        "        \"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        \"\"\"\n",
        "        If NPFX has something like \"...**ydnaVal\", return ydnaVal. If not found, blank.\n",
        "        \"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "###############################################################################\n",
        "# Gedcom Class\n",
        "###############################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        # Optional second-level filter\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                self.filter_pool = [\n",
        "                    d for d in self.filter_pool if d.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "###############################################################################\n",
        "# quick_extract_name\n",
        "###############################################################################\n",
        "def quick_extract_name(full_text):\n",
        "    \"\"\"\n",
        "    Minimal function to extract a short name from a GEDCOM chunk.\n",
        "    \"\"\"\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start)\n",
        "    if end == -1:\n",
        "        end = len(full_text)\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "###############################################################################\n",
        "# Parents, Ancestors\n",
        "###############################################################################\n",
        "def find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def find_distant_ancestors(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(find_distant_ancestors(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(find_distant_ancestors(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "###############################################################################\n",
        "# filter_ancestral_line\n",
        "###############################################################################\n",
        "def filter_ancestral_line(winning_path_ids, generation_table_local, names_map):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table_local:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    matching_table.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for gen, pair in matching_table:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(f\"{name_pair[0]}&{name_pair[1]}\")\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "###############################################################################\n",
        "# process_record_wrapper (parallel)\n",
        "###############################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, parents_map)\n",
        "    distant_anc_paths = find_distant_ancestors(individual_id, parents_map)\n",
        "\n",
        "    best_score = None\n",
        "    best_path = None\n",
        "    for path in distant_anc_paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = 0\n",
        "        for idx, nm in enumerate(name_path):\n",
        "            if 'Yates' in nm:\n",
        "                score += (idx + 1)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_path = path\n",
        "\n",
        "    if not best_path:\n",
        "        best_path = []\n",
        "\n",
        "    # remove individual's own ID\n",
        "    best_path_cleaned = [pid for pid in best_path if pid != individual_id]\n",
        "\n",
        "    line_str = filter_ancestral_line(set(best_path_cleaned), generation_table, names_map)\n",
        "\n",
        "    cm_value = ''\n",
        "    sort_value = ''\n",
        "    ydna_value = ''\n",
        "    anchor_name = ''\n",
        "    for ds in gedcom_instance.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            anchor_name = ds.get_anchor_gen1()\n",
        "            break\n",
        "\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    # Return columns: ID#, Match to, Name, cM, Yates DNA Ancestral Line\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str]\n",
        "\n",
        "###############################################################################\n",
        "# FP-Growth approach for frequent itemsets\n",
        "###############################################################################\n",
        "def fp_growth_itemsets(transactions, min_support_count=2, min_size=2):\n",
        "    \"\"\"\n",
        "    Replaces the old Apriori function with FP-Growth from mlxtend.\n",
        "\n",
        "    :param transactions: list of lists (each a list of items)\n",
        "    :param min_support_count: e.g., 2 means itemsets must appear at least twice\n",
        "    :param min_size: smallest itemset size you want to keep\n",
        "    :return: dict {tuple_of_items: frequency}, in no particular order\n",
        "    \"\"\"\n",
        "    from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "    n_transactions = len(transactions)\n",
        "    if n_transactions == 0:\n",
        "        return {}\n",
        "\n",
        "    # The support fraction needed so itemsets appear at least \"min_support_count\" times\n",
        "    required_support = min_support_count / n_transactions\n",
        "\n",
        "    # 1) Convert list-of-lists into one-hot encoded DataFrame\n",
        "    te = TransactionEncoder()\n",
        "    te_ary = te.fit_transform(transactions)\n",
        "    df_bool = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "    # 2) Run fpgrowth\n",
        "    logger.info(\"Running FP-Growth with support >= %.4f ...\", required_support)\n",
        "    results = fpgrowth(df_bool, min_support=required_support, use_colnames=True)\n",
        "\n",
        "    # 3) Filter out itemsets smaller than min_size\n",
        "    freq_dict = {}\n",
        "    for idx, row in results.iterrows():\n",
        "        itemset = row[\"itemsets\"]\n",
        "        if len(itemset) >= min_size:\n",
        "            sup_frac = row[\"support\"]\n",
        "            freq = int(round(sup_frac * n_transactions))\n",
        "            freq_dict[tuple(sorted(itemset))] = freq\n",
        "\n",
        "    logger.info(\"FP-Growth found %d itemsets of size >= %d.\", len(freq_dict), min_size)\n",
        "    return freq_dict\n",
        "\n",
        "###############################################################################\n",
        "# compute_value_for_line with caching\n",
        "###############################################################################\n",
        "@functools.lru_cache(maxsize=5000)\n",
        "def compute_value_for_line_cached(line_str, sorted_segments_hashable):\n",
        "    if pd.isna(line_str) or not line_str.strip():\n",
        "        return 0\n",
        "    sorted_segments = dict(sorted_segments_hashable)\n",
        "    lines = [s.strip() for s in line_str.split(\"~~~\") if s.strip()]\n",
        "    value = 0\n",
        "    lines_copy = lines.copy()\n",
        "\n",
        "    for seg, freq in sorted_segments.items():\n",
        "        c_seg = Counter(seg)\n",
        "        c_lines = Counter(lines_copy)\n",
        "        if all(c_lines[k] >= c_seg[k] for k in c_seg):\n",
        "            value += freq\n",
        "            for k, ccount in c_seg.items():\n",
        "                for _ in range(ccount):\n",
        "                    lines_copy.remove(k)\n",
        "    value += len(lines_copy)\n",
        "    return value\n",
        "\n",
        "###############################################################################\n",
        "# main()\n",
        "###############################################################################\n",
        "def main():\n",
        "    def select_gedcom():\n",
        "        files = glob.glob(\"*.ged\")\n",
        "        if not files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom()\n",
        "    if not gedcom_file_path:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    # 1) Parse GEDCOM\n",
        "    ged = Gedcom(gedcom_file_path)\n",
        "    ged.parse_gedcom()\n",
        "    filter_count = len(ged.filter_pool)\n",
        "    print(\"Records tagged and filtered by NPFX:\", filter_count)\n",
        "\n",
        "    # 2) Build parents_map, names_map from raw GEDCOM\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    blocks = raw_data.split('\\n0 ')\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find('\\n')\n",
        "        if flend == -1:\n",
        "            flend = len(blk)\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            start = first_line.find('@') + 1\n",
        "            end = first_line.find('@', start)\n",
        "            rec_id = first_line[start:end].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    names_map = {}\n",
        "\n",
        "    for rec_id, txt in all_records.items():\n",
        "        nm = quick_extract_name(\"\\n\" + txt)\n",
        "        names_map[rec_id] = nm\n",
        "\n",
        "    # gather families\n",
        "    families = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            if father_idx != -1:\n",
        "                start = father_idx + len('1 HUSB @')\n",
        "                end = txt.find('@', start)\n",
        "                husb_id = txt[start:end]\n",
        "            else:\n",
        "                husb_id = None\n",
        "\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            if wife_idx != -1:\n",
        "                start = wife_idx + len('1 WIFE @')\n",
        "                end = txt.find('@', start)\n",
        "                wife_id = txt[start:end]\n",
        "            else:\n",
        "                wife_id = None\n",
        "\n",
        "            kids = []\n",
        "            lines_ = txt.split('\\n')\n",
        "            for ln in lines_:\n",
        "                if ln.strip().startswith('1 CHIL @'):\n",
        "                    s2 = ln.strip().split('1 CHIL @')[1]\n",
        "                    kid_id = s2.split('@')[0]\n",
        "                    kids.append(kid_id)\n",
        "\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    # 3) gather ID list\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(f\"Processing {len(individual_ids)} individuals with chunk-based parallel...\")\n",
        "\n",
        "    # 4) chunk-based parallel to build lines\n",
        "    combined_rows = []\n",
        "    chunk_size = 20\n",
        "    max_workers = 10\n",
        "    logger.info(\"Starting chunk-based parallel processing with %d workers.\", max_workers)\n",
        "\n",
        "\n",
        "    total_records = len(individual_ids)\n",
        "    from functools import partial\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor, \\\n",
        "            tqdm(total=total_records, desc=\"Processing individuals\") as pbar:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            func = partial(\n",
        "                process_record_wrapper,\n",
        "                gedcom_instance=ged,\n",
        "                parents_map=parents_map,\n",
        "                names_map=names_map\n",
        "            )\n",
        "            results = list(executor.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    # At this point, combined_rows has 5 columns:\n",
        "    # [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "\n",
        "    def remove_prefix(row):\n",
        "        prefix = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "        line = row[\"Yates DNA Ancestral Line\"]\n",
        "        if line.startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = line[len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(remove_prefix, axis=1)\n",
        "    df.index += 1\n",
        "\n",
        "    # 5) Instead of Apriori, use FP-Growth\n",
        "    logger.info(\"Building transaction list for FP-Growth.\")\n",
        "    transactions = []\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            transactions.append([])\n",
        "        else:\n",
        "            items = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "            transactions.append(items)\n",
        "\n",
        "    logger.info(\"Running FP-Growth with min_support_count=2, min_size=2...\")\n",
        "    freq_itemsets_dict = fp_growth_itemsets(\n",
        "        transactions,\n",
        "        min_support_count=2,   # itemsets must appear at least 2 times\n",
        "        min_size=2             # only itemsets of size >= 2\n",
        "    )\n",
        "\n",
        "    # Convert to a sorted structure so we can pass it to compute_value_for_line\n",
        "    sorted_itemsets = dict(\n",
        "        sorted(freq_itemsets_dict.items(), key=lambda x: (len(x[0]), x[1]), reverse=True)\n",
        "    )\n",
        "    shared_segments_hashable = frozenset(sorted_itemsets.items())\n",
        "\n",
        "    # 6) Compute \"Value\"\n",
        "    logger.info(\"Computing 'Value' for each row...\")\n",
        "    values = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Computing Values\"):\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        val = compute_value_for_line_cached(line_str, shared_segments_hashable)\n",
        "        values.append(val)\n",
        "\n",
        "    df[\"Value\"] = values\n",
        "\n",
        "    # sort final by \"Yates DNA Ancestral Line\"\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], ascending=True, inplace=True)\n",
        "\n",
        "    # 7) Z-Score, Robust Z-Score, etc.\n",
        "    df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors=\"coerce\")\n",
        "    mean_v = df[\"Value\"].mean()\n",
        "    std_v = df[\"Value\"].std()\n",
        "    if std_v == 0:\n",
        "        df[\"Z-Score\"] = 0\n",
        "    else:\n",
        "        df[\"Z-Score\"] = (df[\"Value\"] - mean_v) / std_v\n",
        "\n",
        "    median_v = df[\"Value\"].median()\n",
        "    mad_v = np.median(np.abs(df[\"Value\"] - median_v))\n",
        "    if mad_v == 0:\n",
        "        df[\"Robust Z-Score\"] = 0\n",
        "    else:\n",
        "        df[\"Robust Z-Score\"] = (df[\"Value\"] - median_v) / (mad_v * 1.4826)\n",
        "\n",
        "    df[\"Percentile Rank\"] = df[\"Value\"].rank(pct=True) * 100\n",
        "\n",
        "    df[\"Composite Score\"] = (\n",
        "        df[\"Z-Score\"].abs() +\n",
        "        df[\"Robust Z-Score\"].abs() +\n",
        "        (df[\"Percentile Rank\"] / 100)\n",
        "    ) / 3\n",
        "    df[\"Composite Score\"] = df[\"Composite Score\"].round(2)\n",
        "\n",
        "    def assign_desc(s):\n",
        "        if s >= 2.0:\n",
        "            return \"High\"\n",
        "        elif s >= 1.5:\n",
        "            return \"Moderate\"\n",
        "        elif s >= 1.0:\n",
        "            return \"Medium\"\n",
        "        elif s >= 0.5:\n",
        "            return \"Low\"\n",
        "        else:\n",
        "            return \"Very Low\"\n",
        "\n",
        "    df[\"Composite Significance\"] = df[\"Composite Score\"].apply(assign_desc)\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # FINAL COLUMNS:\n",
        "    # We keep \"cM\" but drop \"Name\" from the final export:\n",
        "    # ------------------------------------------------\n",
        "    final_cols = [\n",
        "        \"ID#\",\n",
        "        \"Match to\",\n",
        "        \"cM\",\n",
        "        \"Value\",\n",
        "        \"Composite Score\",\n",
        "        \"Composite Significance\",\n",
        "        \"Yates DNA Ancestral Line\"\n",
        "    ]\n",
        "    df = df[final_cols]\n",
        "\n",
        "    logger.info(\"Final DataFrame columns: %s\", df.columns.tolist())\n",
        "    print(df.head(10))\n",
        "\n",
        "    # 8) Export\n",
        "    csv_name = \"final_combined_df_with_composite_scores.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    logger.info(\"Exported final DataFrame to '%s'.\", csv_name)\n",
        "\n",
        "    html_name = \"HTML_combined_df_with_composite_scores.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table {\n",
        "      width: 100%;\n",
        "      border-collapse: collapse;\n",
        "      margin: 20px 0;\n",
        "    }\n",
        "    table, th, td {\n",
        "      border: 1px solid #333;\n",
        "    }\n",
        "    th, td {\n",
        "      padding: 8px 12px;\n",
        "      text-align: center;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "    }\n",
        "    /* Left-align the last column */\n",
        "    td:nth-child(7) {\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    html_content = css_style + df.to_html(\n",
        "        index=False,\n",
        "        columns=final_cols,\n",
        "        escape=False\n",
        "    )\n",
        "    with open(html_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Exported HTML to '%s'.\", html_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "# 78 1 min 20 s\n",
        "# 100_1 min 33s\n",
        "# 250_4 min 9 s\n",
        "# 250_2 min 52 s\n",
        "# 250_2 min 50 s\n",
        "# 500_10 min 24 s\n",
        "# 500_ 10 min 30 s\n",
        "# 750_49 min  19s\n",
        "# 1,000_1 hr 47 min  49s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6K2lze31_pD",
        "outputId": "5b42c250-0197-4826-95d7-18e7f1e6b56b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58272 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 90\n",
            "Autosomal matches: 1211\n",
            "After manual filter, total records: 78\n",
            "Records tagged and filtered by NPFX: 78\n",
            "Processing 78 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing individuals: 100%|██████████| 78/78 [00:54<00:00,  1.43it/s]\n",
            "Computing Values: 100%|██████████| 78/78 [00:14<00:00,  5.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ID#      Match to   cM  Value  Composite Score Composite Significance  \\\n",
            "55  I56154        hatpat   10      7             0.77                    Low   \n",
            "15  I55749        hatpat  187      4             0.78                    Low   \n",
            "69  I56904       hatpatm   13     76             0.17               Very Low   \n",
            "72  I57027        hatpat   40     80             0.20               Very Low   \n",
            "68  I56895       hatpatm   12     75             0.17               Very Low   \n",
            "9   I54336        hatpat   20     79             0.18               Very Low   \n",
            "8   I53819        hatpat   20     76             0.17               Very Low   \n",
            "58  I56188        hatpat   15    131             0.79                    Low   \n",
            "10  I54530  yates,ronald   16     76             0.17               Very Low   \n",
            "11  I54602  yates,ronald   15     76             0.17               Very Low   \n",
            "\n",
            "                             Yates DNA Ancestral Line  \n",
            "55  SherrillJohnathan&YatesSarah~~~SherrillJonatho...  \n",
            "15  ShortridgeAndesonSc&YatesElizabeth~~~Shortridg...  \n",
            "69  YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "72  YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "68  YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "9   YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "8   YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "58  YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "10  YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n",
            "11  YatesFrancis&TichborneJane~~~YatesThomas&Steph...  \n"
          ]
        }
      ]
    }
  ]
}