{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb7KqmN7APNE3kKdathe62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/Gold__1_%26_2_%26_3_20251029.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "1f94182b-535f-4027-8138-06bbfe64d455",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: python-gedcom in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "ERROR: unknown command \"caas_jupyter_tools\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n",
        "!pip caas_jupyter_tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credentials\n",
        "\n",
        "import os\n",
        "\n",
        "# Gmail SMTP creds\n",
        "os.environ['GMAIL_USER']         = 'yatesvilleron@gmail.com'\n",
        "os.environ['GMAIL_APP_PASSWORD'] = 'qtziwiblytgrlzvx'\n",
        "\n",
        "# FTPS upload creds — make sure FTP_PASS is exactly your password, no < or >\n",
        "os.environ['FTP_HOST']       = 'ftp.one-name.net'\n",
        "os.environ['FTP_PORT']       = '21'\n",
        "os.environ['FTP_USER']       = 'admin@yates.one-name.net'\n",
        "os.environ['FTP_PASS']       = 'v(i83lfQB@dB'\n"
      ],
      "metadata": {
        "id": "971jlPTnBVfk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 20250513-cell2 is good to use; adding more lineage functionality next\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script using:\n",
        " - Chunk-based Parallel Processing for Speed (Stage 1: genealogical line creation)\n",
        " - A Trie-based approach, then final \"Value\" = 5 * (number of couples with node.count >=2) + (total couples)\n",
        "\n",
        "For ancestral lines where none of the couples are repeated (a one-off line), the Value is still computed.\n",
        "Now, instead of composite scoring, two new columns are added:\n",
        "  - Value Range (the numeric bracket)\n",
        "  - Value Label (a descriptive label)\n",
        "\n",
        "Exports final CSV/HTML sorted by \"Yates DNA Ancestral Line\", including a 'haplogroup' column.\n",
        "\"\"\"\n",
        "import csv\n",
        "import glob\n",
        "import logging\n",
        "import functools\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# Global Variables\n",
        "###############################################################################\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "###############################################################################\n",
        "# Trie Data Structure\n",
        "###############################################################################\n",
        "class TrieNode:\n",
        "    \"\"\"A simple Trie node for storing a couple and counting how many lines pass here.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.children = {}\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert_line(self, couples_list):\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple not in current.children:\n",
        "                current.children[couple] = TrieNode()\n",
        "            current = current.children[couple]\n",
        "            current.count += 1\n",
        "\n",
        "    def get_couple_count(self, couples_list):\n",
        "        counts = []\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple in current.children:\n",
        "                current = current.children[couple]\n",
        "                counts.append(current.count)\n",
        "            else:\n",
        "                counts.append(0)\n",
        "                break\n",
        "        return counts\n",
        "\n",
        "###############################################################################\n",
        "# Utility: chunk generator\n",
        "###############################################################################\n",
        "def chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "###############################################################################\n",
        "# GedcomDataset\n",
        "###############################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "###############################################################################\n",
        "# Gedcom Class\n",
        "###############################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "        return autosomal_count\n",
        "\n",
        "###############################################################################\n",
        "# quick_extract_name\n",
        "###############################################################################\n",
        "def quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start)\n",
        "    if end == -1:\n",
        "        end = len(full_text)\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "###############################################################################\n",
        "# Parents & Ancestors\n",
        "###############################################################################\n",
        "def find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def find_distant_ancestors(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(find_distant_ancestors(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(find_distant_ancestors(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "###############################################################################\n",
        "# filter_ancestral_line\n",
        "###############################################################################\n",
        "def filter_ancestral_line(winning_path_ids, generation_table_local, names_map):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table_local:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    matching_table.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for gen, pair in matching_table:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(f\"{name_pair[0]}&{name_pair[1]}\")\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "###############################################################################\n",
        "# process_record_wrapper (parallel) - STAGE 1\n",
        "###############################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, parents_map)\n",
        "    distant_anc_paths = find_distant_ancestors(individual_id, parents_map)\n",
        "\n",
        "    best_score = None\n",
        "    best_path = None\n",
        "    for path in distant_anc_paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx+1) for idx, nm in enumerate(name_path) if 'Yates' in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_path = path\n",
        "\n",
        "    if not best_path:\n",
        "        best_path = []\n",
        "\n",
        "    best_path_cleaned = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str = filter_ancestral_line(set(best_path_cleaned), generation_table, names_map)\n",
        "\n",
        "    cm_value = ''\n",
        "    sort_value = ''\n",
        "    ydna_value = ''\n",
        "    for ds in gedcom_instance.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    # Return columns: ID#, Match to, Name, cM, Yates DNA Ancestral Line, haplogroup\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "###############################################################################\n",
        "# main()\n",
        "###############################################################################\n",
        "def main():\n",
        "    def select_gedcom():\n",
        "        files = glob.glob(\"*.ged\")\n",
        "        if not files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom()\n",
        "    if not gedcom_file_path:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    ged = Gedcom(gedcom_file_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "    filter_count = len(ged.filter_pool)\n",
        "\n",
        "    with open(\"autosomal_count.txt\", \"w\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "\n",
        "    print(\"Records tagged and filtered by NPFX:\", filter_count)\n",
        "\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    blocks = raw_data.split('\\n0 ')\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find('\\n')\n",
        "        if flend == -1:\n",
        "            flend = len(blk)\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            start = first_line.find('@') + 1\n",
        "            end = first_line.find('@', start)\n",
        "            rec_id = first_line[start:end].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    names_map = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        nm = quick_extract_name(\"\\n\" + txt)\n",
        "        names_map[rec_id] = nm\n",
        "\n",
        "    families = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            husb_id = txt[father_idx+len('1 HUSB @'):txt.find('@', father_idx+len('1 HUSB @'))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            wife_id = txt[wife_idx+len('1 WIFE @'):txt.find('@', wife_idx+len('1 WIFE @'))] if wife_idx != -1 else None\n",
        "            kids = [ln.split('@')[1] for ln in txt.split('\\n') if ln.strip().startswith('1 CHIL @')]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(f\"Processing {len(individual_ids)} individuals with chunk-based parallel...\")\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Starting chunk-based parallel processing with %d workers.\", max_workers)\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            func = functools.partial(process_record_wrapper, gedcom_instance=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(executor.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def remove_specific_prefix(row):\n",
        "        prefix = \"YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&FauconerElizabeth~~~\"\n",
        "        if row[\"Yates DNA Ancestral Line\"].startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = row[\"Yates DNA Ancestral Line\"][len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(remove_specific_prefix, axis=1)\n",
        "\n",
        "    logger.info(\"Building Trie from reversed lines...\")\n",
        "    trie = Trie()\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.notna(line_str) and line_str.strip():\n",
        "            trie.insert_line([x.strip() for x in line_str.split(\"~~~\") if x.strip()])\n",
        "\n",
        "    values, prefix_counts = [], []\n",
        "    logger.info(\"Computing 'Value' = 5*(#couples with node.count >=2) + (total couples) ...\")\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            values.append(0)\n",
        "            prefix_counts.append(0)\n",
        "        else:\n",
        "            couples_list = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "            node_counts = trie.get_couple_count(couples_list)\n",
        "            prefix_count = sum(1 for c in node_counts if c >= 2)\n",
        "            values.append(5 * prefix_count + len(couples_list))\n",
        "            prefix_counts.append(prefix_count)\n",
        "\n",
        "    df[\"Value\"], df[\"PrefixCount\"] = values, prefix_counts\n",
        "\n",
        "    def assign_value_range_label(val):\n",
        "        try:\n",
        "            v = float(val)\n",
        "        except:\n",
        "            return \"\", \"\"\n",
        "        if v >= 60: return \">=60\", \"1-likely correct\"\n",
        "        if 47 <= v <= 59: return \"59~47\", \"2-lines forming\"\n",
        "        if 34 <= v <= 46: return \"46~34\", \"3-patterns emerging\"\n",
        "        if 21 <= v <= 33: return \"33~21\", \"4-notable patterns\"\n",
        "        if 8 <= v <= 20: return \"20~8\", \"5-patterns stable\"\n",
        "        if 1 <= v <= 7:  return f\"{v:.0f}\", \"6-need research\"\n",
        "        return f\"{v:.0f}\", \"0-uncategorized\"\n",
        "\n",
        "    ranges, labels = zip(*(assign_value_range_label(v) for v in df[\"Value\"]))\n",
        "    df[\"Value Range\"], df[\"Value Label\"] = ranges, labels\n",
        "\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "    df.drop(\"PrefixCount\", axis=1, inplace=True)\n",
        "\n",
        "    csv_name = \"final_combined_df_with_value_labels.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    logger.info(\"Exported final DataFrame to '%s'.\", csv_name)\n",
        "\n",
        "    html_name = \"HTML_combined_df_with_value_labels.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n",
        "    table, th, td { border: 1px solid #333; }\n",
        "    th, td { padding: 8px 12px; text-align: center; }\n",
        "    th { background-color: #f2f2f2; }\n",
        "    /* Left-align the last column */\n",
        "    td:nth-child(7) { text-align: left; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Value Range\", \"Value Label\", \"Yates DNA Ancestral Line\"]\n",
        "    html_content = css_style + df.to_html(index=False, columns=final_cols, escape=False)\n",
        "    with open(html_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Exported HTML to '%s'.\", html_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    try:\n",
        "        display(Javascript('alert(\"✅ GEDCOM processing (and HTML export) is complete!\");'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import smtplib, ssl\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "def send_email(subject, body, to_addr):\n",
        "    smtp_server = 'smtp.gmail.com'\n",
        "    port = 465\n",
        "    sender = os.environ['GMAIL_USER']\n",
        "    password = os.environ['GMAIL_APP_PASSWORD']\n",
        "    msg = MIMEText(body)\n",
        "    msg['Subject'] = subject\n",
        "    msg['From'] = sender\n",
        "    msg['To'] = to_addr\n",
        "    context = ssl.create_default_context()\n",
        "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
        "        server.login(sender, password)\n",
        "        server.send_message(msg)\n",
        "\n",
        "# Email summary (only total lines)\n",
        "df_summary = pd.read_csv(\"final_combined_df_with_value_labels.csv\")\n",
        "total = len(df_summary)\n",
        "summary = f\"GEDCOM processing complete!\\n\\nTotal lines: {total}\"\n",
        "\n",
        "send_email(\n",
        "    subject=\"✅ Cell #1 Report Ready\",\n",
        "    body=summary,\n",
        "    to_addr=os.environ['GMAIL_USER']\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Qh13Q-WUmVu3",
        "outputId": "b3cfb99c-54fa-4a86-fa4a-d6e799aecf1e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 62184 total records\n",
            "Records tagged and filtered by NPFX: 1556\n",
            "Records with YDNA information: 1\n",
            "Autosomal matches: 1555\n",
            "After manual filter, total records: 7\n",
            "Records tagged and filtered by NPFX: 7\n",
            "Processing 7 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 7/7 [00:04<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "alert(\"✅ GEDCOM processing (and HTML export) is complete!\");"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell2\n",
        "\n",
        "# ==============================================================================\n",
        "# == Gold 2 Refactored - Single Cell Version (COMPLETE & FINAL v2)\n",
        "# ==============================================================================\n",
        "#\n",
        "# == Instructions:\n",
        "# 1. Add your FTP credentials to the Colab \"Secrets\" tab (🔑 icon):\n",
        "#    - FTP_HOST\n",
        "#    - FTP_USER\n",
        "#    - FTP_PASS\n",
        "#    - FTP_DIR (e.g., \"gengen\" or leave blank for root - this is optional)\n",
        "# 2. Upload your \"final_combined_df_with_value_labels.csv\" file.\n",
        "# 3. Upload your \"autosomal_count.txt\" file.\n",
        "# 4. Run this single cell.\n",
        "#\n",
        "# ==============================================================================\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [0/7] SECURE CREDENTIAL LOADING\n",
        "# ------------------------------------------------------------------------------\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- This is the new, secure way ---\n",
        "# It loads secrets from the 🔑 menu and puts them into os.environ\n",
        "try:\n",
        "    # Load required secrets\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"❌ ERROR: Critical secrets (FTP_HOST, FTP_USER, FTP_PASS) were not found.\")\n",
        "    print(\"Please check the '🔑 Secrets' panel on the left and ensure they are spelled correctly.\")\n",
        "    # We don't raise here, to allow 'local save only' mode\n",
        "\n",
        "# Load optional FTP_DIR secret\n",
        "try:\n",
        "    os.environ['FTP_DIR'] = userdata.get('FTP_DIR')\n",
        "except userdata.SecretNotFoundError:\n",
        "    os.environ['FTP_DIR'] = '' # Default to empty string if not set\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [1/7] IMPORTS (All modules loaded at the top)\n",
        "# ------------------------------------------------------------------------------\n",
        "import re, json, time, io, posixpath, socket\n",
        "import pandas as pd\n",
        "from ftplib import FTP_TLS\n",
        "import urllib.parse as _u\n",
        "import html as _html\n",
        "import traceback\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [2/7] CONFIG + FTP + CONSTANTS + RULES\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# Column name defaults used across sections\n",
        "subject_code_col = \"Match to\"\n",
        "path_col = \"Yates DNA Ancestral Line\"\n",
        "LINEAGE_HEADER = path_col  # used by Excel builder and HTML table\n",
        "\n",
        "# Layout\n",
        "TABLE_WIDTH_PX = 3150\n",
        "COL_A_PX = 1100  # width for \"Match Summary\" column in HTML\n",
        "\n",
        "# Data / Output\n",
        "CSV_PATH = \"final_combined_df_with_value_labels.csv\"  # default input\n",
        "LOCAL_NAME = \"ons_yates_dna_register.htm\"             # main page filename (local)\n",
        "REMOTE_NAME = \"ons_yates_dna_register.htm\"            # remote filename (same name)\n",
        "LINEAGE_HEADER = \"Lineage (Starting with oldest ancestor, the line is:)\"\n",
        "ARROW_ENTITY = \"&rarr;\"\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "# Local exports\n",
        "MATCH_COUSINS_CSV = \"the_match_cousins.csv\"  # Column A (what’s shown)\n",
        "\n",
        "# Remote I/O toggles (same creds/dir as HTML upload)\n",
        "REMOTE_READ = True      # Pull resolver CSV from server before processing\n",
        "UPLOAD_COLUMN_A = True  # Push the_match_cousins.csv to server after build\n",
        "\n",
        "# TNG link pieces for matchee hotlink\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "HOME_URL = \"https://yates.one-name.net/ons_yates_dna_register.htm\"\n",
        "\n",
        "# FTP settings (hard timeouts)\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip()  # e.g., \"gengen\" or \"\"\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))  # connect + socket timeout\n",
        "FTP_PASSIVE = True  # passive safest behind NAT\n",
        "\n",
        "# ---------- Autosomal Count (Colab → Website) ----------\n",
        "LOCAL_COUNT_FILE = \"/content/autosomal_count.txt\"  # produced in Colab\n",
        "REMOTE_COUNT_NAME = \"autosomal_count.txt\"          # destination on host\n",
        "COUNT_PUBLIC_URL = f\"/{FTP_DIR}/{REMOTE_COUNT_NAME}\" if FTP_DIR else f\"/{REMOTE_COUNT_NAME}\"\n",
        "\n",
        "# ---------- Resolver (single source: website) ----------\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"  # temp file we download to\n",
        "\n",
        "# ---------- FTP helpers (with timeouts & cleanup) ----------\n",
        "def ftp_connect():\n",
        "    # Secrets are already loaded in os.environ from the top of the script\n",
        "    FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip()\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ['FTP_HOST'], int(os.environ.get('FTP_PORT', 21)))\n",
        "    ftps.login(os.environ['FTP_USER'], os.environ['FTP_PASS'])\n",
        "    try:\n",
        "        ftps.prot_p()  # secure data channel if supported\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        try:\n",
        "            ftps.cwd(FTP_DIR)\n",
        "        except Exception:\n",
        "            parts = [p for p in FTP_DIR.split(\"/\") if p]\n",
        "            for p in parts:\n",
        "                try:\n",
        "                    ftps.mkd(p)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip()\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ftp_download_if_exists(ftps: FTP_TLS, remote_name: str, local_name: str) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(f\"RETR \" + remote_name, f.write)\n",
        "        print(f\"⬇️ Pulled remote file: {remote_name} → {os.path.abspath(local_name)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ℹ️ Remote not found or unreadable: {remote_name} ({e})\")\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps: FTP_TLS, local_path: str, remote_name: str):\n",
        "    try:\n",
        "        with open(local_path, \"rb\") as fh:\n",
        "            ftps.storbinary(f\"STOR {remote_name}\", fh)\n",
        "        print(f\"⬆️ Uploaded: {local_path} → {remote_name}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Upload failed for {local_path} → {remote_name}: {e}\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [3/7] RESOLVER + NAME HELPERS\n",
        "# ------------------------------------------------------------------------------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(f\"Unable to read mapping CSV {path}: {last}\")\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV is empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    try:\n",
        "        with ftp_connect() as ftps:\n",
        "            ok = ftp_download_if_exists(\n",
        "                ftps,\n",
        "                _remote_path(SERVER_MAPPING_REMOTE),\n",
        "                SERVER_MAPPING_LOCAL_CACHE\n",
        "            )\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "    except Exception as e:\n",
        "         raise RuntimeError(\n",
        "            f\"FTP connection failed while trying to load resolver. Check FTP_HOST/USER/PASS env vars. Error: {e}\"\n",
        "         )\n",
        "\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            f\"Resolver not found on server: /{_remote_path(SERVER_MAPPING_REMOTE)}. \"\n",
        "            f\"Upload {SERVER_MAPPING_BASENAME} to /{SERVER_PARTIALS_DIR}/ and re-run.\"\n",
        "        )\n",
        "\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(f\"✅ Loaded resolver from server: {len(df_map)} codes\")\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))  # single source of truth\n",
        "\n",
        "# Global resolver dict\n",
        "MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:→|&rarr;|\\u2192|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s): return []\n",
        "    if not isinstance(s, str): s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r'~+', ' ', str(text))\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1)+m.group(2).upper()+m.group(3)+m.group(4).upper(),\n",
        "        token.lower()\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"+m.group(1).upper(),  token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\"+m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i>0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    if not token:\n",
        "        return (token,)\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i; break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i; break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]; given = token[idx:]\n",
        "    given_spaced = re.sub(r'(?<!^)([A-Z])', r' \\1', given)\n",
        "    return (f\"{given_spaced.strip()} {surname.strip()}\",)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = f\"{first} {last}\"\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def truncate_first(name: str, n: int = 4) -> str:\n",
        "    name = name.strip()\n",
        "    if not name: return name\n",
        "    parts = name.split()\n",
        "    return parts[0][:n] if len(parts) == 1 else f\"{parts[0][:n]} {parts[-1]}\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return f\"{greats}\\u00d7-great-grandparents\"\n",
        "\n",
        "def build_header(subject_name, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = f\"{int(round(float(cm_val)))}\"\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        f\"{subject_name} is a {cm_str} cM cousin match to {matchee_name_html}, whose\",\n",
        "        f\"{degree_label} (back {gens} Gens)\",\n",
        "        \"are\",\n",
        "        f\"{husband} & {wife}.\"\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r'\\.\\s*$', '', s)\n",
        "    return s\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str): return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(str(code).strip().lower(), str(code))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [4/7] CSV LOAD + COLUMN DETECTION + ROW HELPERS\n",
        "# ------------------------------------------------------------------------------\n",
        "CSV_IN = os.environ.get(\"CSV_IN\", CSV_PATH)\n",
        "\n",
        "_encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "_last_err = None\n",
        "df = None\n",
        "for _e in _encs:\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_IN, encoding=_e, dtype=str, keep_default_na=False)\n",
        "        break\n",
        "    except Exception as _ex:\n",
        "        _last_err = _ex\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise RuntimeError(f\"Unable to read CSV: {CSV_IN} ({_last_err})\")\n",
        "print(f\"✅ Loaded CSV — {len(df)} rows, {len(df.columns)} columns from {os.path.abspath(CSV_IN)}\")\n",
        "\n",
        "id_col        = find_col(df, [r'^(id#|personid)$'], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_to_col  = find_col(df, [r'^match\\s*to$'], [\"Match to\",\"Match\"])\n",
        "name_col      = find_col(df, [r'^name$'], [\"Name\"])\n",
        "cm_col        = find_col(df, [r'^(c\\s*:?m|cm)$', r'centi.?morgan'], [\"cM\",\"cm\"])\n",
        "path_col      = find_col(df, [r'(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)'],\n",
        "                         [\"Yates DNA Ancestral Line\",\"Ancestral Line\",\"Lineage\"])\n",
        "\n",
        "if not id_col:       raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "if not match_to_col: raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "if not name_col:     raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col:       raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not path_col:     raise ValueError(\"CSV missing lineage/path column.\")\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def _truncate_alpha(s: str, n: int) -> str:\n",
        "    return re.sub(r\"[^A-Za-z]\", \"\", s)[:n]\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        given = parts[0]\n",
        "        surname = parts[-1]\n",
        "        return f\"{_truncate_alpha(given, 7)} {surname}\".strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)  # drop leading initials\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return f\"{_truncate_alpha(ps[0], 7)} {ps[-1]}\".strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    given = smart_titlecase(given_candidates[0])\n",
        "    return f\"{_truncate_alpha(given, 7)} {surname}\".strip()\n",
        "\n",
        "print(\"✅ Columns:\", {\"ID\": id_col, \"Match to\": match_to_col, \"Name\": name_col, \"cM\": cm_col, \"Lineage\": path_col})\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [5/7] TRANSFORM & COLUMN A\n",
        "# ------------------------------------------------------------------------------\n",
        "headers  = []\n",
        "lineages = []\n",
        "findcol  = []\n",
        "\n",
        "REMOTE_NAME_ABS = \"/\" + REMOTE_NAME\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw  = row.get(match_to_col, \"\")\n",
        "    subject_name = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_b = f\"<strong>{subject_name}</strong>\" if subject_name else subject_name\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            f'<a href=\"{TNG_BASE}/verticalchart.php?personID={pid}&tree={TNG_TREE}&parentset=0&display=vertical&generations=15\" '\n",
        "            f'target=\"_blank\">{matchee_name}</a>'\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = matchee_name\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "\n",
        "    tokens     = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "    tokens_disp = tokens[:7]  # show first 7 pairs\n",
        "\n",
        "    if \"common_husband\" in df.columns and \"common_wife\" in df.columns:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw    = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_b,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        truncate_first(husband_raw, 7) if husband_raw else \"\",\n",
        "        truncate_first(wife_raw, 7) if wife_raw else \"\"\n",
        "    )\n",
        "\n",
        "    if tokens_disp:\n",
        "        tokens_disp[0] = f\"<strong>{tokens_disp[0]}</strong>\"\n",
        "    sep = f\" {ARROW_ENTITY} \"\n",
        "    lineage_text = sep.join(tokens_disp) if tokens_disp else \"\"\n",
        "\n",
        "    q = _u.quote(subject_name)\n",
        "    quick = (f'<a class=\"find-btn\" href=\"{REMOTE_NAME_ABS}?q={q}\" target=\"_blank\" rel=\"noopener\" '\n",
        "             f'title=\"Open a filtered view for {subject_name}\">Find</a>')\n",
        "\n",
        "    headers.append(header_html)\n",
        "    lineages.append(lineage_text)\n",
        "    findcol.append(quick)\n",
        "\n",
        "LINEAGE_HEADER_SAFE = LINEAGE_HEADER\n",
        "\n",
        "df[\"Match Summary\"]             = headers\n",
        "df[LINEAGE_HEADER_SAFE]         = lineages\n",
        "df[\"Find\"]                      = findcol\n",
        "display_df = df[[\"Find\", \"Match Summary\", LINEAGE_HEADER_SAFE]]\n",
        "\n",
        "display_df[[\"Match Summary\"]].to_csv(MATCH_COUSINS_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "print(\"✅ Wrote local CSV (Column A):\", os.path.abspath(MATCH_COUSINS_CSV))\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [6/7] HTML (TABLE + CSS + JS)\n",
        "# ------------------------------------------------------------------------------\n",
        "display_for_html = display_df\n",
        "\n",
        "html_table = display_for_html[[\"Find\", \"Match Summary\", LINEAGE_HEADER_SAFE]].to_html(\n",
        "    index=False, escape=False, classes=\"sortable\"\n",
        ")\n",
        "\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">', 1\n",
        ")\n",
        "html_table = html_table.replace('<tbody>\\n<tr>', '<tbody>\\n<tr id=\"first-row\">', 1)\n",
        "\n",
        "html_table = html_table.replace(\n",
        "    \"<th>Match Summary</th>\",\n",
        "    \"<th>Match Summary&ndash;click to sort</th>\",\n",
        "    1\n",
        ")\n",
        "\n",
        "html_table = html_table.replace(\n",
        "    f\"<th>{_html.escape(LINEAGE_HEADER_SAFE)}</th>\",\n",
        "    \"<th>Lineage (Starting with oldest ancestor&ndash;click to sort)</th>\",\n",
        "    1\n",
        ")\n",
        "\n",
        "FIND_PX = 118  # width of Find column (checkbox + Email + Find)\n",
        "colgroup_html = (\n",
        "    \"<colgroup>\\n\"\n",
        "    f\"  <col style=\\\"width:{FIND_PX}px;\\\" />\\n\"\n",
        "    f\"  <col style=\\\"width:{COL_A_PX}px;\\\" />\\n\"\n",
        "    \"  <col />\\n\"\n",
        "    \"</colgroup>\\n\"\n",
        ")\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n' + colgroup_html, 1\n",
        ")\n",
        "\n",
        "html_table = html_table.replace(\n",
        "    \"<th>Find</th>\",\n",
        "    \"<th>Find&nbsp;<input type=\\\"checkbox\\\" id=\\\"sel-all\\\" title=\\\"Select all visible\\\" /></th>\",\n",
        "    1\n",
        ")\n",
        "\n",
        "html_table_scrolling = '<div class=\"table-scroll\">\\n' + html_table + '\\n</div>'\n",
        "\n",
        "TABLE_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \"  html { scroll-behavior: smooth; }\\n\"\n",
        "    \"  body { font-family: 'Times New Roman', Georgia, serif; font-size:100%; background:#ffffff; color:#222; margin:0; padding:0; line-height:1.5; }\\n\"\n",
        "    f\"  .wrap {{ max-width:{TABLE_WIDTH_PX}px; margin:0 auto; background:#ffffff; padding:20px; padding-bottom:48px; }}\\n\"\n",
        "    \"  a { color:#154b8b; text-decoration:none; } a:hover { text-decoration:underline; }\\n\"\n",
        "    \"  h1 { margin:0 0 6px 0; font-size:26px; line-height:1.2; text-align:center; }\\n\"\n",
        "    \"  .updated { font-size:12px; color:#555; text-align:center; margin:2px 0 10px 0; }\\n\"\n",
        "    \"  .sortbar { margin:6px 0 10px 0; font-size:13px; background:#ffffff; padding:6px 8px; border-radius:6px;\\n\"\n",
        "    \"             display:flex; flex-wrap:wrap; gap:5px; align-items:center; border:1px solid #ddd; }\\n\"\n",
        "    \"  .btn { display:inline-block; border:1px solid #5b79b8; background:#5b79b8; color:#fff; padding:4px 9px;\\n\"\n",
        "    \"         text-decoration:none; cursor:pointer; border-radius:5px; line-height:1.2; transition:background 0.2s, transform 0.1s; user-select:none; }\\n\"\n",
        "    \"  .btn:hover { background:#4668aa; transform:translateY(-1px); }\\n\"\n",
        "    \"  input.btn.search { background:#fff; color:#111; border-color:#bbb; }\\n\"\n",
        "    \"  .btn-mini { font-size:12px; padding:2px 6px; line-height:1.1; margin-left:6px; }\\n\"\n",
        "    \"  .find-cell { white-space:nowrap; }\\n\"\n",
        "    \"  .selbox { margin-right:6px; vertical-align:middle; }\\n\"\n",
        "    \"  .table-scroll { max-height:70vh; overflow-y:auto; overflow-x:auto; border:1px solid #ddd; }\\n\"\n",
        "    f\"  table.sortable {{ border-collapse:collapse; width:{TABLE_WIDTH_PX}px; table-layout:fixed; }}\\n\"\n",
        "    \"  table.sortable th, table.sortable td { border:1px solid #ddd; padding:6px 8px; vertical-align:top; }\\n\"\n",
        "    \"  table.sortable th { background:#e3eaf8; text-align:left; position:sticky; top:0; z-index:2; box-shadow:0 1px 0 #ccc; cursor:pointer; }\\n\"\n",
        "    \"  table.sortable td { word-wrap:break-word; overflow-wrap:break-word; }\\n\"\n",
        "    \"  #first-row td { border-top:2px solid #999; }\\n\"\n",
        "    \"  .back-to-top { position:fixed; right:16px; bottom:16px; padding:6px 10px; border:1px solid #3e5a97; background:#5b79b8; color:#fff;\\n\"\n",
        "    \"                 cursor:pointer; border-radius:6px; font-size:12px; display:none; z-index:9999; }\\n\"\n",
        "    \"  .back-to-top:hover { background:#4668aa; }\\n\"\n",
        "    \"  #dynamicContent { margin:10px 0 14px 0; }\\n\"\n",
        "    \"  @media screen and (max-width: 820px) { .wrap { padding:12px; } h1 { font-size:22px; } }\\n\"\n",
        "    \"</style>\\n\"\n",
        ")\n",
        "\n",
        "DYNAMIC_BLOCK = (\n",
        "    \"<div class=\\\"sortbar\\\">\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\\\" target=\\\"_blank\\\">Study Details</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\\\" target=\\\"_blank\\\">Theory in Action</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"gengen/images/cousin-calculator.jpg\\\" target=\\\"_blank\\\">Cousin Connection</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"gengen/images/Shared_cM_Project_v4.jpg\\\" target=\\\"_blank\\\">Cousin by DNA</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"partials/match_count.htm\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Match Count</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"partials/lineage_count.htm\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Lineage Count</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"/partials/cousin_list_print.htm\\\" target=\\\"_blank\\\">Cousin List</a>\\n\"\n",
        "    \"  <span class=\\\"btn\\\" id=\\\"email-selected\\\">Email Selected</span>\\n\"\n",
        "    \"  <span class=\\\"btn\\\" id=\\\"clear-selected\\\">Clear</span>\\n\"\n",
        "    \"  <input type=\\\"text\\\" id=\\\"search-box\\\" class=\\\"btn search\\\" size=\\\"24\\\" value=\\\"\\\" placeholder=\\\"Search&hellip;\\\" \"\n",
        "    \"         autocomplete=\\\"off\\\" autocapitalize=\\\"off\\\" spellcheck=\\\"false\\\" inputmode=\\\"search\\\" enterkeyhint=\\\"search\\\" />\\n\"\n",
        "    \"</div>\\n\"\n",
        "    \"<div id=\\\"dynamicContent\\\"></div>\\n\"\n",
        ")\n",
        "\n",
        "JS_COUNT_URL = COUNT_PUBLIC_URL.replace(\"'\", \"%27\")\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    \"<div class=\\\"updated\\\">\"\n",
        "    f\"<a href=\\\"{HOME_URL}\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Home</a>\"\n",
        "    \" &nbsp;|&nbsp; Last updated: <span id=\\\"last-updated\\\"></span>\"\n",
        "    \" &nbsp;|&nbsp; Autosomal matches: <span id=\\\"auto-count\\\" class=\\\"js-count\\\"></span>\"\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ==============================================================================\n",
        "# ==  🛑 SYNTAX ERROR FIX:\n",
        "# ==  The entire <script> block below now uses {{ and }} (double braces)\n",
        "# ==  to correctly escape the f-string.\n",
        "# ==============================================================================\n",
        "template_html = f\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "{TABLE_CSS}\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1>ONS Yates Study Autosomal DNA Register</h1>\n",
        "  {UPDATED_BLOCK}\n",
        "  {DYNAMIC_BLOCK}\n",
        "{html_table_scrolling}\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){{\n",
        "  function textOf(cell){{ return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase(); }}\n",
        "  function sortTable(tbl, colIndex, dir){{\n",
        "    var tb=tbl && tbl.tBodies ? tbl.tBodies[0] : null; if(!tb) return;\n",
        "    var rows=[].slice.call(tb.rows||[]);\n",
        "    var asc=(dir==='asc');\n",
        "    rows.sort(function(a,b){{\n",
        "      var A=textOf(a.cells[colIndex]), B=textOf(b.cells[colIndex]);\n",
        "      if(A<B) return asc?-1:1; if(A>B) return asc?1:-1; return 0;\n",
        "    }});\n",
        "    var frag=document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateSelAll();\n",
        "  }}\n",
        "\n",
        "  function bindHeaderSort(){{\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return; var ths=tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){{\n",
        "      var th = ths[idx];\n",
        "      // Skip checkbox header\n",
        "      if (th.querySelector('input[type=checkbox]')) return;\n",
        "      var dir='asc';\n",
        "      th.addEventListener('click',function(){{\n",
        "        dir=(dir==='asc')?'desc':'asc';\n",
        "        // Clear arrows\n",
        "        for (var j = 0; j < ths.length; j++) {{\n",
        "          ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+[↑↓]/, '');\n",
        "        }}\n",
        "        // Add new arrow\n",
        "        th.innerHTML += (dir === 'asc' ? ' &uarr;' : ' &darr;');\n",
        "        sortTable(tbl,idx,dir);\n",
        "      }},false);\n",
        "    }})(i);\n",
        "  }}\n",
        "\n",
        "  var PARTIAL_BASES=['/partials/','partials/','gengen/partials/','/gengen/partials/'];\n",
        "  function tryFetchSequential(urls,onOK,onFail){{\n",
        "    if(!urls.length) return onFail('No valid locations'); var url=urls.shift();\n",
        "    fetch(url,{{cache:'no-store'}}).then(function(r){{ if(!r.ok) throw new Error('HTTP '+r.status); return r.text();}})\n",
        "      .then(onOK).catch(function(){{ tryFetchSequential(urls,onOK,onFail);}});\n",
        "  }}\n",
        "  function bindPartials(){{\n",
        "    var bar=document.querySelector('.sortbar'); if(!bar) return;\n",
        "    bar.addEventListener('click',function(e){{\n",
        "      var btn=e.target && e.target.closest ? e.target.closest('.btn') : null; if(!btn) return;\n",
        "      var rel=btn.getAttribute('data-load-partial'); if(!rel) return;\n",
        "      var c=document.getElementById('dynamicContent'); if(!c) return; c.innerHTML='<p><em>Loading latest data&hellip;</em></p>';\n",
        "      var bust=encodeURIComponent(document.lastModified||(new Date()).toUTCString());\n",
        "      var bases=PARTIAL_BASES.slice(); var candidates=bases.map(function(b){{return b+rel+'?v='+bust;}});\n",
        "      tryFetchSequential(candidates.slice(), function(html){{ c.innerHTML=html; }}, function(){{ c.innerHTML='<p style=\\\\\"color:#a00;\\\\\">Could not load content.</p>'; }});\n",
        "    }});\n",
        "  }}\n",
        "\n",
        "  function stampLastUpdated(){{\n",
        "    var el=document.getElementById('last-updated'); if(!el) return;\n",
        "    var d=new Date(document.lastModified||new Date());\n",
        "    function z(n){{return(n<10?'0':'')+n;}}\n",
        "    el.innerHTML=d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes());\n",
        "  }}\n",
        "  function formatWithCommas(n){{\n",
        "    try{{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }}catch(e){{ return String(n||''); }}\n",
        "  }}\n",
        "  function loadAutoCount(){{\n",
        "    var el=document.getElementById('auto-count'); if(!el) return;\n",
        "    var url='{JS_COUNT_URL}';\n",
        "    try{{\n",
        "      var xhr=new XMLHttpRequest(); xhr.open('GET', url+(url.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true);\n",
        "      xhr.onreadystatechange=function(){{ if(xhr.readyState===4){{ if(xhr.status>=200&&xhr.status<300){{\n",
        "        var m=(xhr.responseText||'').match(/(\\\\d+)/); var num=m?m[1]:'';\n",
        "        el.textContent = formatWithCommas(num) || '(unavailable)';\n",
        "      }} else {{ el.textContent='(unavailable)'; }} }} }};\n",
        "      xhr.send(null);\n",
        "    }}catch(e){{ el.textContent='(unavailable)'; }}\n",
        "  }}\n",
        "\n",
        "  function getParam(name){{ var m=location.search.match(new RegExp('[?&]'+name+'=([^&]+)')); return m?decodeURIComponent(m[1].replace(/\\\\+/g,' ')):''; }}\n",
        "  function bindSearch(){{\n",
        "    var box=document.getElementById('search-box'); var tbl=document.getElementById('refactor-table'); if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return; var tb=tbl.tBodies[0];\n",
        "    function norm(s){{ return String(s||'').replace(/\\\\s+/g,' ').toLowerCase(); }}\n",
        "    function rowText(tr){{ var t=''; for(var i=1;i<tr.cells.length;i++){{ t+= ' ' + (tr.cells[i].textContent||tr.cells[i].innerText||''); }} return norm(t); }} // Start at 1 to skip 'Find'\n",
        "    var cached=[]; (function seed(){{ var rows=tb.rows; cached=[]; for(var i=0;i<rows.length;i++){{ cached.push({{el:rows[i], txt:rowText(rows[i])}}); }} }})();\n",
        "    function apply(q){{\n",
        "      q=norm(q);\n",
        "      for(var i=0;i<cached.length;i++){{\n",
        "        var hit = !q || cached[i].txt.indexOf(q)>-1;\n",
        "        cached[i].el.style.display = hit? '' : 'none';\n",
        "      }}\n",
        "      updateSelAll();\n",
        "    }}\n",
        "    var to=null; function onInput(){{ if(to) clearTimeout(to); to=setTimeout(function(){{ apply(box.value); }}, 60); }}\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false); // for 'x' clear button\n",
        "    var q0=getParam('q');\n",
        "    if(q0){{ box.value=q0; apply(q0); try{{ history.replaceState(null, '', location.pathname); }}catch(e){{}} }}\n",
        "    else {{{{ box.value=''; apply(''); setTimeout(function(){{ if(!getParam('q')){{ box.value=''; apply(''); }} }}, 0); }}}}\n",
        "  }}\n",
        "\n",
        "  function visibleRowCheckboxes(){{\n",
        "    var tbl=document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return [];\n",
        "    var tb=tbl.tBodies[0], out=[];\n",
        "    for(var i=0;i<tb.rows.length;i++){{\n",
        "      var tr=tb.rows[i];\n",
        "      if(tr.style.display !== 'none'){{\n",
        "        var cb=tr.querySelector('.selbox');\n",
        "        if(cb) out.push(cb);\n",
        "      }}\n",
        "    }}\n",
        "    return out;\n",
        "  }}\n",
        "  function updateSelAll(){{\n",
        "    var sa=document.getElementById('sel-all'); if(!sa) return;\n",
        "    var cbs=visibleRowCheckboxes();\n",
        "    var all_vis=cbs.length > 0;\n",
        "    var all_checked=all_vis;\n",
        "    for(var i=0;i<cbs.length;i++){{\n",
        "      if(!cbs[i].checked) all_checked=false;\n",
        "    }}\n",
        "    sa.checked = all_vis && all_checked;\n",
        "    sa.indeterminate = all_vis && !all_checked && cbs.some(function(cb){{return cb.checked;}});\n",
        "  }}\n",
        "  function bindSelectAll(){{\n",
        "    var sa=document.getElementById('sel-all'); if(!sa) return;\n",
        "    sa.addEventListener('click', function(){{\n",
        "      var cbs=visibleRowCheckboxes();\n",
        "      for(var i=0;i<cbs.length;i++) cbs[i].checked = sa.checked;\n",
        "    }}, false);\n",
        "    var tb=document.getElementById('refactor-table'); if(!tb) return;\n",
        "    tb.addEventListener('click', function(e){{\n",
        "      if(e.target && e.target.classList.contains('selbox')){{\n",
        "        updateSelAll();\n",
        "      }}\n",
        "    }}, false);\n",
        "  }}\n",
        "\n",
        "  function bindEmail(){{\n",
        "    var btn=document.getElementById('email-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(){{\n",
        "      var cbs=visibleRowCheckboxes();\n",
        "      var names=[];\n",
        "      for(var i=0;i<cbs.length;i++){{\n",
        "        if(cbs[i].checked){{\n",
        "          var name=cbs[i].getAttribute('data-name');\n",
        "          if(name) names.push(name);\n",
        "        }}\n",
        "      }}\n",
        "      if(!names.length) return alert('No rows selected.');\n",
        "      var subj='ONS Yates DNA Register';\n",
        "      var body = 'Selected cousins:\\\\n\\\\n' + names.join('\\\\n') + '\\\\n\\\\n';\n",
        "      var href='mailto:?subject='+encodeURIComponent(subj)+'&body='+encodeURIComponent(body);\n",
        "      if(href.length > 2000) href = 'mailto:?subject='+encodeURIComponent(subj);\n",
        "      window.location.href = href;\n",
        "    }}, false);\n",
        "  }}\n",
        "  function bindClear(){{\n",
        "    var btn=document.getElementById('clear-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(){{\n",
        "      var cbs=visibleRowCheckboxes();\n",
        "      for(var i=0;i<cbs.length;i++) cbs[i].checked = false;\n",
        "      updateSelAll();\n",
        "    }}, false);\n",
        "  }}\n",
        "\n",
        "  function bindBackToTop(){{\n",
        "    var btn=document.getElementById('back-to-top'); if(!btn) return;\n",
        "    window.addEventListener('scroll', function(){{\n",
        "      btn.style.display = (window.scrollY > 200) ? 'block' : 'none';\n",
        "    }}, {{passive:true}});\n",
        "    btn.addEventListener('click', function(){{\n",
        "      window.scrollTo(0,0);\n",
        "    }}, false);\n",
        "  }}\n",
        "\n",
        "  function addCheckboxes(){{\n",
        "    var tbl=document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb=tbl.tBodies[0];\n",
        "    for(var i=0;i<tb.rows.length;i++){{\n",
        "      var tr=tb.rows[i];\n",
        "      var cell=tr.cells[0]; // 'Find' column\n",
        "      var findBtn=cell ? cell.querySelector('.find-btn') : null;\n",
        "      var name = findBtn ? (findBtn.getAttribute('title')||'').replace('Open a filtered view for ','') : ('Row '+(i+1));\n",
        "      if(cell){{\n",
        "        cell.classList.add('find-cell');\n",
        "        cell.innerHTML = '<input type=\\\"checkbox\\\" class=\\\"selbox\\\" title=\\\"Select this row\\\" data-name=\\\"'+name.replace(/\"/g,'&quot;')+'\\\" />' + cell.innerHTML;\n",
        "      }}\n",
        "    }}\n",
        "  }}\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){{\n",
        "    addCheckboxes();\n",
        "    stampLastUpdated();\n",
        "    loadAutoCount();\n",
        "    bindHeaderSort();\n",
        "    bindPartials();\n",
        "    bindSearch();\n",
        "    bindSelectAll();\n",
        "    bindEmail();\n",
        "    bindClear();\n",
        "    bindBackToTop();\n",
        "  }});\n",
        "\n",
        "}})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "print(\"✅ HTML template created.\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- [7/7] GENERATE PARTIALS, SAVE, & UPLOAD\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# ====== [NEW FUNCTION: GENERATE PARTIALS - v9 (Ancestor Pair Count)] =====\n",
        "#\n",
        "#    ===> REPLACE your old 'generate_partial_files' function with this one <===\n",
        "#\n",
        "# ==============================================================================\n",
        "def generate_partial_files():\n",
        "    \"\"\"\n",
        "    Generates STYLED match_count.htm (3 columns) and lineage_count.htm\n",
        "    (counts oldest ancestor pair, 3 columns: Count, %, Name).\n",
        "    Saves them to /content/\n",
        "    \"\"\"\n",
        "    print(\"--- 📊 Generating partial count files (with styling) ---\")\n",
        "\n",
        "    if 'df' not in globals() or df.empty:\n",
        "        print(\"❌ Could not generate partials: Main 'df' is not loaded.\")\n",
        "        return\n",
        "\n",
        "    # --- Define a *simpler* nav bar for the partial pages ---\n",
        "    PARTIAL_DYNAMIC_BLOCK = (\n",
        "        f\"<div class=\\\"sortbar\\\">\\n\"\n",
        "        f\"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\\\" target=\\\"_blank\\\">Study Details</a>\\n\"\n",
        "        f\"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\\\" target=\\\"_blank\\\">Theory in Action</a>\\n\"\n",
        "        f\"  <a class=\\\"btn\\\" href=\\\"gengen/images/cousin-calculator.jpg\\\" target=\\\"_blank\\\">Cousin Connection</a>\\n\"\n",
        "        f\"  <a class=\\\"btn\\\" href=\\\"gengen/images/Shared_cM_Project_v4.jpg\\\" target=\\\"_blank\\\">Cousin by DNA</a>\\n\"\n",
        "        f\"  <a class=\\\"btn\\\" href=\\\"/partials/cousin_list_print.htm\\\" target=\\\"_blank\\\">Cousin List</a>\\n\"\n",
        "        f\"  <a class=\\\"btn\\\" href=\\\"{HOME_URL}\\\" style=\\\"background:#3a6b3a; border-color:#3a6b3a;\\\">&larr; Back to Main Register</a>\\n\"\n",
        "        f\"</div>\\n\"\n",
        "    )\n",
        "\n",
        "    # --- Define a *simpler* updated block for partials ---\n",
        "    PARTIAL_UPDATED_BLOCK = (\n",
        "        \"<div class=\\\"updated\\\">\"\n",
        "        f\"<a href=\\\"{HOME_URL}\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Home</a>\"\n",
        "        \" &nbsp;|&nbsp; Last updated: <span id=\\\"last-updated\\\"></span>\"\n",
        "        \"</div>\"\n",
        "    )\n",
        "\n",
        "    # --- Define the shared HTML template for partial pages ---\n",
        "    PARTIAL_CSS = TABLE_CSS.replace(\n",
        "        f\"table.sortable {{ border-collapse:collapse; width:{TABLE_WIDTH_PX}px; table-layout:fixed; }}\",\n",
        "        \"table.sortable { border-collapse:collapse; width:100%; table-layout:auto; margin:0; }\"\n",
        "    ).replace(\n",
        "        f\"  .wrap {{ max-width:{TABLE_WIDTH_PX}px; margin:0 auto; background:#ffffff; padding:20px; padding-bottom:48px; }}\\n\",\n",
        "        \"  .wrap { max-width:1200px; margin:0 auto; background:#ffffff; padding:20px; padding-bottom:48px; }\\n\"\n",
        "    ).replace(\n",
        "        \".table-scroll { max-height:70vh; overflow-y:auto; overflow-x:auto; border:1px solid #ddd; }\",\n",
        "        \".table-scroll { max-height:none; overflow-y:visible; overflow-x:auto; border:1px solid #ddd; }\"\n",
        "    )\n",
        "    PARTIAL_CSS = PARTIAL_CSS.replace(\n",
        "        \"</style>\",\n",
        "        \"  table.sortable th, table.sortable td { padding: 4px 6px; } /* Narrower padding */\\n</style>\"\n",
        "    )\n",
        "\n",
        "\n",
        "    # Function to generate the full page HTML using f-string directly\n",
        "    def create_partial_page(page_title, table_html):\n",
        "        return f\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>{page_title}</title>\n",
        "{PARTIAL_CSS}\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1>{page_title}</h1>\n",
        "  {PARTIAL_UPDATED_BLOCK}\n",
        "  {PARTIAL_DYNAMIC_BLOCK}\n",
        "\n",
        "  <div class=\"table-scroll\">\n",
        "    {table_html}\n",
        "    <a href=\"{HOME_URL}\" style=\"margin-top:12px; display:inline-block; font-weight:bold;\">&larr; Back to Main Register</a>\n",
        "  </div>\n",
        "\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){{{{\n",
        "  function stampLastUpdated(){{{{\n",
        "    var el=document.getElementById('last-updated'); if(!el) return;\n",
        "    var d=new Date(document.lastModified||new Date());\n",
        "    function z(n){{{{return(n<10?'0':'')+n;}}}}\n",
        "    el.innerHTML=d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes());\n",
        "  }}}}\n",
        "\n",
        "  function bindBackToTop(){{{{\n",
        "    var btn=document.getElementById('back-to-top');\n",
        "    if (btn) {{\n",
        "        window.addEventListener('scroll', function(){{{{\n",
        "        btn.style.display = (window.scrollY > 200) ? 'block' : 'none';\n",
        "        }}}}, {{{{passive:true}}}});\n",
        "        btn.addEventListener('click', function(){{{{\n",
        "        window.scrollTo(0,0);\n",
        "        }}}}, false);\n",
        "    }}\n",
        "  }}}}\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){{{{\n",
        "    stampLastUpdated();\n",
        "    bindBackToTop();\n",
        "  }}}});\n",
        "}})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # --- 1. Match Count (3 Columns - Unchanged) ---\n",
        "        match_data = df[[match_to_col]].copy()\n",
        "        match_data['Unmasked Name'] = df[match_to_col].apply(resolve_match_to).apply(normalize_person_name)\n",
        "        match_data.rename(columns={match_to_col: 'Masked Name'}, inplace=True)\n",
        "        match_counts_df = match_data.groupby(['Masked Name', 'Unmasked Name']).size().reset_index(name='Count')\n",
        "        match_counts_df = match_counts_df.sort_values(by=['Count', 'Unmasked Name'], ascending=[False, True])\n",
        "\n",
        "        match_table_html = match_counts_df.to_html(index=False, border=1, classes=\"sortable match-count-table\")\n",
        "        colgroup_match = \"\"\"\n",
        "<colgroup>\n",
        "    <col style=\"width: 150px;\">\n",
        "    <col style=\"width: 300px;\">\n",
        "    <col style=\"width: 80px; text-align: right;\">\n",
        "</colgroup>\n",
        "\"\"\"\n",
        "        match_table_html = match_table_html.replace(\n",
        "            '<table border=\"1\" class=\"dataframe sortable match-count-table\">',\n",
        "            '<table border=\"1\" class=\"dataframe sortable match-count-table\">' + colgroup_match, 1\n",
        "        )\n",
        "        match_html_full = create_partial_page(\"ONS Yates Study - Match Count\", match_table_html)\n",
        "\n",
        "        with open('/content/match_count.htm', 'w', encoding='utf-8') as f:\n",
        "            f.write(match_html_full)\n",
        "        print(f\"✅ Generated local file: /content/match_count.htm ({len(match_counts_df)} rows)\")\n",
        "\n",
        "\n",
        "        # --- 2. Lineage Count (Oldest Ancestor Pair - Modified) ---\n",
        "        total_rows = len(df)\n",
        "        if total_rows > 0:\n",
        "            # Get the first token (oldest pair) from each lineage\n",
        "            first_pairs_raw = df[path_col].apply(lambda x: split_tokens(x)[0] if split_tokens(x) else '')\n",
        "\n",
        "            # Normalize the names in the first pair\n",
        "            normalized_pairs = first_pairs_raw.apply(lambda x: \" and \".join(derive_common_from_first_token([x])) if x else \"Unknown\")\n",
        "\n",
        "            # Count the occurrences of each normalized pair\n",
        "            ancestor_counts = normalized_pairs.value_counts().reset_index()\n",
        "            ancestor_counts.columns = ['Ancestor Pair', 'Count']\n",
        "\n",
        "            # Calculate Percentage\n",
        "            ancestor_counts['Percentage'] = (ancestor_counts['Count'] / total_rows * 100)\n",
        "\n",
        "            # Format Percentage column\n",
        "            ancestor_counts['Percentage'] = ancestor_counts['Percentage'].map('{:.2f}%'.format)\n",
        "\n",
        "            # Reorder and rename columns for final table\n",
        "            ancestor_counts_final = ancestor_counts[['Count', 'Percentage', 'Ancestor Pair']]\n",
        "\n",
        "            # Sort by Count descending, then Ancestor Pair ascending\n",
        "            ancestor_counts_final = ancestor_counts_final.sort_values(by=['Count', 'Ancestor Pair'], ascending=[False, True])\n",
        "\n",
        "            # Generate HTML table\n",
        "            lineage_table_html = ancestor_counts_final.to_html(index=False, border=1, classes=\"sortable lineage-count-table\")\n",
        "\n",
        "            # Inject <colgroup> for styling (right-align count and percentage)\n",
        "            colgroup_lineage = \"\"\"\n",
        "<colgroup>\n",
        "    <col style=\"width: 80px; text-align: right;\">\n",
        "    <col style=\"width: 100px; text-align: right;\">\n",
        "    <col style=\"width: auto;\">\n",
        "</colgroup>\n",
        "\"\"\"\n",
        "            lineage_table_html = lineage_table_html.replace(\n",
        "                 '<table border=\"1\" class=\"dataframe sortable lineage-count-table\">',\n",
        "                 '<table border=\"1\" class=\"dataframe sortable lineage-count-table\">' + colgroup_lineage, 1\n",
        "            )\n",
        "            lineage_row_count = len(ancestor_counts_final)\n",
        "        else:\n",
        "            lineage_table_html = \"<p>No lineage data found to generate counts.</p>\"\n",
        "            lineage_row_count = 0\n",
        "\n",
        "        # Create full page HTML\n",
        "        lineage_html_full = create_partial_page(\"ONS Yates Study - Oldest Ancestor Pair Count\", lineage_table_html) # Updated title\n",
        "\n",
        "        with open('/content/lineage_count.htm', 'w', encoding='utf-8') as f:\n",
        "            f.write(lineage_html_full)\n",
        "        print(f\"✅ Generated local file: /content/lineage_count.htm ({lineage_row_count} rows)\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error while generating partial files: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ====== [END NEW FUNCTION] ==============================================\n",
        "\n",
        "# ====== [FUNCTION: SAVE] =======================================================\n",
        "def save_and_upload_all():\n",
        "    \"\"\"\n",
        "    Generates partials, saves HTML locally, then uploads all 5 files.\n",
        "    \"\"\"\n",
        "    print(f\"--- 💾 Starting Save & Upload ---\")\n",
        "\n",
        "    # 1. Generate partial files\n",
        "    generate_partial_files()\n",
        "\n",
        "    # 2. Save main HTML file locally\n",
        "    try:\n",
        "        with open(LOCAL_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "            f.write(template_html)\n",
        "        print(f\"✅ Saved main HTML locally: {os.path.abspath(LOCAL_NAME)}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to save local HTML file {LOCAL_NAME}: {e}\")\n",
        "\n",
        "    # 3. Connect to FTP\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        print(f\"✅ Connected to FTP host.\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ FTP connect failed: {e}\")\n",
        "        print(\"Skipping all uploads. Check FTP_HOST/USER/PASS environment variables.\")\n",
        "        return\n",
        "\n",
        "    # 4. Upload main HTML\n",
        "    try:\n",
        "        ftp_upload_overwrite(ftps, LOCAL_NAME, REMOTE_NAME)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Main HTML upload failed: {e}\")\n",
        "\n",
        "    # 5. Upload Column A CSV\n",
        "    if UPLOAD_COLUMN_A:\n",
        "        if os.path.exists(MATCH_COUSINS_CSV):\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, MATCH_COUSINS_CSV, MATCH_COUSINS_CSV)\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Column A CSV upload failed: {e}\")\n",
        "        else:\n",
        "            print(f\"ℹ️ Skipping Column A upload: File not found at {MATCH_COUSINS_CSV}\")\n",
        "\n",
        "    # 6. Upload Autosomal Count\n",
        "    if os.path.exists(LOCAL_COUNT_FILE):\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_COUNT_FILE, REMOTE_COUNT_NAME)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Autosomal count upload failed: {e}\")\n",
        "\n",
        "    # 7. Upload match_count.htm\n",
        "    if os.path.exists('/content/match_count.htm'):\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, '/content/match_count.htm', 'partials/match_count.htm')\n",
        "        except Exception as e:\n",
        "            print(f\"❌ match_count.htm upload failed: {e}\")\n",
        "    else:\n",
        "        print(f\"ℹ️ Skipping match_count.htm upload: File not found.\")\n",
        "\n",
        "    # 8. Upload lineage_count.htm\n",
        "    if os.path.exists('/content/lineage_count.htm'):\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, '/content/lineage_count.htm', 'partials/lineage_count.htm')\n",
        "        except Exception as e:\n",
        "            print(f\"❌ lineage_count.htm upload failed: {e}\")\n",
        "    else:\n",
        "        print(f\"ℹ️ Skipping lineage_count.htm upload: File not found.\")\n",
        "\n",
        "    # 9. Disconnect\n",
        "    try:\n",
        "        ftps.quit()\n",
        "        print(f\"✅ Disconnected from FTP.\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    print(f\"--- 🎉 Process Complete ---\")\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# -- 🚀 EXECUTION BLOCK\n",
        "# ------------------------------------------------------------------------------\n",
        "print(\"--- 🚀 Starting Gold 2 Refactor Build (Single Cell) ---\")\n",
        "try:\n",
        "    # 1. Check for necessary env vars (which were loaded at the top)\n",
        "    if not all(os.environ.get(k) for k in ['FTP_HOST', 'FTP_USER', 'FTP_PASS']):\n",
        "        print(\"❌ Critical FTP environment variables (FTP_HOST, FTP_USER, FTP_PASS) are not set.\")\n",
        "        print(\"Running in 'local save only' mode.\")\n",
        "\n",
        "        # We must at least generate the partials for local save\n",
        "        generate_partial_files()\n",
        "\n",
        "        # Re-run local save logic\n",
        "        with open(LOCAL_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "            f.write(template_html)\n",
        "        print(f\"✅ Saved main HTML locally: {os.path.abspath(LOCAL_NAME)}\")\n",
        "    else:\n",
        "        # 2. Call the main function\n",
        "        save_and_upload_all()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ❌ BUILD FAILED ---\")\n",
        "    print(\"An error occurred during the build process:\")\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PifS-lUCLvS5",
        "outputId": "32a4845d-dd41-4721-b4b4-d5320891e4b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Pulled remote file: partials/match_to_unmasked.csv → /content/match_to_unmasked.server.csv\n",
            "✅ Loaded resolver from server: 79 codes\n",
            "✅ Loaded CSV — 7 rows, 9 columns from /content/final_combined_df_with_value_labels.csv\n",
            "✅ Columns: {'ID': 'ID#', 'Match to': 'Match to', 'Name': 'Name', 'cM': 'cM', 'Lineage': 'Yates DNA Ancestral Line'}\n",
            "✅ Wrote local CSV (Column A): /content/the_match_cousins.csv\n",
            "✅ HTML template created.\n",
            "--- 🚀 Starting Gold 2 Refactor Build (Single Cell) ---\n",
            "--- 💾 Starting Save & Upload ---\n",
            "--- 📊 Generating partial count files (with styling) ---\n",
            "✅ Generated local file: /content/match_count.htm (1 rows)\n",
            "✅ Generated local file: /content/lineage_count.htm (2 rows)\n",
            "✅ Saved main HTML locally: /content/ons_yates_dna_register.htm\n",
            "✅ Connected to FTP host.\n",
            "⬆️ Uploaded: ons_yates_dna_register.htm → ons_yates_dna_register.htm\n",
            "⬆️ Uploaded: the_match_cousins.csv → the_match_cousins.csv\n",
            "⬆️ Uploaded: /content/autosomal_count.txt → autosomal_count.txt\n",
            "⬆️ Uploaded: /content/match_count.htm → partials/match_count.htm\n",
            "⬆️ Uploaded: /content/lineage_count.htm → partials/lineage_count.htm\n",
            "✅ Disconnected from FTP.\n",
            "--- 🎉 Process Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gold 3 Ancestor Register (mobile-friendly, sortable)\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "import os\n",
        "\n",
        "# ————— Load Data —————\n",
        "df = pd.read_csv(\"final_combined_df_with_value_labels.csv\")\n",
        "\n",
        "# ————— Blank out any NaN haplogroups —————\n",
        "df['haplogroup'] = df['haplogroup'].fillna('')\n",
        "\n",
        "# ————— Hyperlink various IDs —————\n",
        "hap_base      = \"gengen/haplogroup/\"\n",
        "ydna_overview = \"gengen/Y-designation-overview.htm\"\n",
        "dar_base      = \"https://services.dar.org/Public/DAR_Research/search/?Keyword=\"\n",
        "sar_base      = \"https://sarpatriots.sar.org/patriot/search?searchText=\"\n",
        "\n",
        "def link_value(x):\n",
        "    if x.startswith(\"Y-\"):\n",
        "        return f'<a href=\"{ydna_overview}\" target=\"_blank\">{x}</a>'\n",
        "    if x.startswith(\"dar-A-\"):\n",
        "        return f'<a href=\"{dar_base}{x}\" target=\"_blank\">{x}</a>'\n",
        "    if x.startswith(\"sar-P-\"):\n",
        "        return f'<a href=\"{sar_base}{x}\" target=\"_blank\">{x}</a>'\n",
        "    if x:\n",
        "        return f'<a href=\"{hap_base}{x}.htm\" target=\"_blank\">{x}</a>'\n",
        "    return ''\n",
        "\n",
        "df['haplogroup'] = df['haplogroup'].apply(link_value)\n",
        "\n",
        "# ————— Load autosomal counts —————\n",
        "try:\n",
        "    with open(\"autosomal_count.txt\", \"r\") as f:\n",
        "        autosomal_count = int(f.read().strip())\n",
        "except:\n",
        "    autosomal_count = None\n",
        "\n",
        "prev_count = None\n",
        "additional_str = \"\"\n",
        "if os.path.exists(\"autosomal_count_prev.txt\"):\n",
        "    try:\n",
        "        with open(\"autosomal_count_prev.txt\", \"r\") as f:\n",
        "            prev_count = int(f.read().strip())\n",
        "        if autosomal_count is not None and prev_count is not None:\n",
        "            diff = autosomal_count - prev_count\n",
        "            additional_str = f\" (+{diff} since last run)\"\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# ————— Timestamp —————\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "updated_str = now.strftime(\"%d %B %Y at %-I:%M %p EDT\")\n",
        "\n",
        "# ————— Insert Action column —————\n",
        "df.insert(6, 'Action', '→')\n",
        "\n",
        "# ————— XHTML Template with search box, sticky header, first two columns —————\n",
        "full_html_template = \"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\"/>\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"/>\n",
        "  <title>Ancestor Register</title>\n",
        "  <script src=\"../sorttable.js\" type=\"text/javascript\"></script>\n",
        "\n",
        "  <script>\n",
        "    // Filter table rows based on search box input\n",
        "    function filterTable() {\n",
        "      const query = document.getElementById('searchBox').value.toLowerCase();\n",
        "      const rows = document.querySelectorAll('#table-container table.sortable tbody tr');\n",
        "      rows.forEach(row => {\n",
        "        const cells = row.querySelectorAll('td');\n",
        "        let match = false;\n",
        "        cells.forEach(cell => {\n",
        "          if (cell.textContent.toLowerCase().includes(query)) {\n",
        "            match = true;\n",
        "          }\n",
        "        });\n",
        "        // Explicitly set display to \"table-row\" when matching, \"none\" otherwise\n",
        "        row.style.display = match ? \"table-row\" : \"none\";\n",
        "      });\n",
        "    }\n",
        "\n",
        "    // Scroll to top of table container\n",
        "    function scrollToTop() {\n",
        "      const container = document.getElementById('table-container');\n",
        "      if (!container) return;\n",
        "      container.scrollTo({ top: 0, behavior: 'smooth' });\n",
        "    }\n",
        "  </script>\n",
        "\n",
        "  <style>\n",
        "    body { margin: 0; padding: 0; font-family: Arial,Helvetica,sans-serif; background: #faf9d3; font-size: 14px; }\n",
        "    .intro { padding: 20px; text-align: center; }\n",
        "    .intro h2 { margin: 0 0 10px; }\n",
        "    .intro p { margin: 0.5em 0; }\n",
        "    .meta  { font-size: 0.9em; margin-bottom: 15px; display: inline-block; }\n",
        "    /* Search box styling */\n",
        "    #searchBox {\n",
        "      margin-left: 10px;\n",
        "      padding: 4px 8px;\n",
        "      font-size: 0.9em;\n",
        "      border: 1px solid #333;\n",
        "      border-radius: 4px;\n",
        "    }\n",
        "\n",
        "    .output-table {\n",
        "      max-height: 75vh;\n",
        "      overflow: auto;\n",
        "      -webkit-overflow-scrolling: touch;\n",
        "      border: 1px solid #333;\n",
        "      position: relative;\n",
        "      margin: 0 20px;\n",
        "    }\n",
        "\n",
        "    table.sortable {\n",
        "      width: 100%;\n",
        "      border-collapse: collapse;\n",
        "      min-width: 600px;\n",
        "    }\n",
        "    th, td {\n",
        "      border: 1px solid #333;\n",
        "      padding: 5px 8px;\n",
        "      background: #faf9d3;\n",
        "      white-space: nowrap;\n",
        "    }\n",
        "\n",
        "    /* 1) Sticky horizontal header */\n",
        "    th {\n",
        "      position: sticky;\n",
        "      top: 0;\n",
        "      background: #ffffcc;\n",
        "      z-index: 2;\n",
        "      text-align: center;\n",
        "    }\n",
        "    th:hover { background: #ffeb99; }\n",
        "\n",
        "    /* 2) Sticky first two columns (including headers) */\n",
        "    th:nth-child(1), td:nth-child(1) {\n",
        "      position: sticky;\n",
        "      left: 0;\n",
        "      background: #ffffcc;\n",
        "      z-index: 3;\n",
        "    }\n",
        "    th:nth-child(2), td:nth-child(2) {\n",
        "      position: sticky;\n",
        "      left: 80px; /* match column 1 width */\n",
        "      background: #ffffcc;\n",
        "      z-index: 3;\n",
        "    }\n",
        "    th:nth-child(1), th:nth-child(2) {\n",
        "      z-index: 4; /* ensure header cells sit above others */\n",
        "    }\n",
        "\n",
        "    /* adjust min-widths if needed */\n",
        "    th:nth-child(1), td:nth-child(1) { min-width: 80px; }\n",
        "    th:nth-child(2), td:nth-child(2) { min-width: 100px; }\n",
        "\n",
        "    /* other columns normal */\n",
        "    th:nth-child(7), td:nth-child(7) { width: 40px; }\n",
        "    th:nth-child(8), td:nth-child(8) { text-align: left; }\n",
        "\n",
        "    .match { background: #fff; }\n",
        "    .blank { background: #ccc; color: #ccc; }\n",
        "\n",
        "    .back-to-top {\n",
        "      position: fixed;\n",
        "      bottom: 20px;\n",
        "      right: 20px;\n",
        "      background: #333;\n",
        "      color: #fff;\n",
        "      padding: 8px 12px;\n",
        "      border-radius: 4px;\n",
        "      font-size: 12px;\n",
        "      opacity: 0.7;\n",
        "      cursor: pointer;\n",
        "      z-index: 1000;\n",
        "    }\n",
        "    .back-to-top:hover { opacity: 1; }\n",
        "\n",
        "    @media (max-width: 600px) {\n",
        "      body { font-size: 12px; }\n",
        "      table.sortable { min-width: 480px; }\n",
        "      th, td { padding: 4px 6px; }\n",
        "    }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <div id=\"top\"></div>\n",
        "  <div class=\"intro\">\n",
        "    <h2>Ancestor Register</h2>\n",
        "    <div class=\"meta\">\n",
        "      Return to <a href=\"https://yates.one-name.net/ons_yates_dna_register.htm\">DNA Register</a> |\n",
        "      Autosomal matches: {autosomal_count}{additional_str} |\n",
        "      Updated: {updated_str}\n",
        "    </div>\n",
        "<p>\n",
        "  <input type=\"text\" id=\"searchBox\" placeholder=\"Search this page...\" oninput=\"filterTable()\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "  <b><i>Click on the header to sort any column</i></b>\n",
        "\n",
        "</p>\n",
        "\n",
        "  </div>\n",
        "\n",
        "  <div class=\"output-table\" id=\"table-container\">\n",
        "    <!-- TABLE_PLACEHOLDER -->\n",
        "  </div>\n",
        "\n",
        "  <div class=\"back-to-top\" onclick=\"scrollToTop()\">\n",
        "    Back to Top ↑\n",
        "  </div>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "# ————— Build and inject sortable table —————\n",
        "final_cols = [\"ID#\", \"Match to\", \"cM\", \"haplogroup\", \"Value Range\", \"Value Label\", \"Action\", \"Yates DNA Ancestral Line\"]\n",
        "html_table = df.to_html(index=False, columns=final_cols, escape=False, classes=\"sortable\")\n",
        "html_table = html_table.replace('<tbody>\\n<tr>', '<tbody>\\n<tr id=\"first-row\">', 1)\n",
        "\n",
        "final_html = (full_html_template\n",
        "  .replace(\"{autosomal_count}\", str(autosomal_count or \"Unknown\"))\n",
        "  .replace(\"{additional_str}\", additional_str)\n",
        "  .replace(\"{updated_str}\", updated_str)\n",
        "  .replace(\"<!-- TABLE_PLACEHOLDER -->\", html_table)\n",
        ")\n",
        "\n",
        "# ————— Save & upload —————\n",
        "with open(\"yates_ancestor_register.htm\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_html)\n",
        "with FTP_TLS() as ftps:\n",
        "    ftps.connect(os.environ['FTP_HOST'], int(os.environ.get('FTP_PORT', 21)))\n",
        "    ftps.login(os.environ['FTP_USER'], os.environ['FTP_PASS'])\n",
        "    try:\n",
        "        ftps.delete(\"yates_ancestor_register.htm\")\n",
        "    except:\n",
        "        pass\n",
        "    ftps.storbinary(\"STOR yates_ancestor_register.htm\", open(\"yates_ancestor_register.htm\", \"rb\"))\n",
        "\n",
        "# ————— Persist count —————\n",
        "if autosomal_count is not None:\n",
        "    with open(\"autosomal_count_prev.txt\", \"w\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "\n",
        "print(\"✅ Full DNA Report Card with improved search box, sticky header, and sticky columns uploaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx7TFp0x47dv",
        "outputId": "2ac23a3b-6c7e-4e6c-9e78-e2420bfa54b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Full DNA Report Card with improved search box, sticky header, and sticky columns uploaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gold Cell 3 for Y-DNA Grid with Auto-Adjusting Column Widths\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ── PATHS ─────────────────────────────────────────────────────────────────\n",
        "combo_csv  = \"/content/y_dna_user_detail_combo.csv\"\n",
        "output_csv = \"/content/y_dna_grid.csv\"\n",
        "output_htm = \"/content/y_dna_grid.htm\"\n",
        "\n",
        "# ── 1) Load vertical data ─────────────────────────────────────────────────\n",
        "df = pd.read_csv(combo_csv)\n",
        "\n",
        "# Rename “Date” → “Era”\n",
        "if \"Date\" in df.columns:\n",
        "    df.rename(columns={\"Date\": \"Era\"}, inplace=True)\n",
        "\n",
        "# ── 2) Insert Action *after* Era ──────────────────────────────────────────\n",
        "# Era is at index 1, so Action goes at index 2\n",
        "df.insert(2, \"Action\", [\"→\"] * len(df))\n",
        "\n",
        "# ── 3) Save vertical CSV ─────────────────────────────────────────────────\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"✅ Saved vertical grid CSV to {output_csv}\")\n",
        "\n",
        "# ── 4) Build HTML ─────────────────────────────────────────────────────────\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "ts  = now.strftime(\"%-m/%-d/%y, %-I:%M %p EDT\")\n",
        "cols = df.columns.tolist()\n",
        "\n",
        "html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head><meta charset=\"UTF-8\"><title>Yates Y-DNA Grid</title>\n",
        "<style>\n",
        "body {{\n",
        "  background: #faf9d3;\n",
        "  font-family: Arial, sans-serif;\n",
        "  font-size: 14px;\n",
        "  margin: 0;\n",
        "  padding: 0;\n",
        "}}\n",
        ".container {{\n",
        "  padding: 10px;\n",
        "}}\n",
        ".table-container {{\n",
        "  overflow-x: auto;\n",
        "  max-height: 80vh;\n",
        "}}\n",
        "table {{\n",
        "  border: 2px solid #333;\n",
        "  border-collapse: collapse;\n",
        "  margin: 0 auto;\n",
        "}}\n",
        "table.mainsection {{\n",
        "  /* allows CSS targeting of blank under “Year” */\n",
        "}}\n",
        "thead {{\n",
        "  display: table-header-group;\n",
        "}}\n",
        "thead th {{\n",
        "  position: sticky;\n",
        "  top: 0;\n",
        "  background: #333;\n",
        "  color: #fff;\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  z-index: 3;\n",
        "}}\n",
        "a {{\n",
        "  color: #fff;\n",
        "  text-decoration: underline;\n",
        "}}\n",
        ".era {{\n",
        "  background: #666;\n",
        "  color: #eee;\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  font-size: 0.9em;\n",
        "}}\n",
        ".action {{\n",
        "  background: #fff;\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  text-align: center;\n",
        "}}\n",
        "td {{\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  text-align: center;\n",
        "}}\n",
        "th:nth-child(n+4),\n",
        "td:nth-child(n+4) {{\n",
        "  border: 1px solid #333;\n",
        "}}\n",
        ".match {{\n",
        "  background: #fff;\n",
        "}}\n",
        ".blank {{\n",
        "  background: #ccc;\n",
        "  color: #ccc;\n",
        "}}\n",
        "/* make the blank under the “Year” header match the era-cell background */\n",
        "table.mainsection td.blank:nth-child(2) {{\n",
        "  background-color: #fdfcd0;\n",
        "}}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"container\">\n",
        "    <h1 style=\"text-align:center\">Yates Y-DNA Grid</h1>\n",
        "    <p style=\"text-align:center;font-size:0.9em\">Updated: {ts}</p>\n",
        "    <p style=\"text-align:center;margin-bottom:12px\">\n",
        "      <a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\">\n",
        "        Return to DNA Cousin Surname Study\n",
        "      </a>\n",
        "    </p>\n",
        "    <div class=\"table-container\">\n",
        "      <table class=\"mainsection\">\n",
        "        <thead>\n",
        "          <tr>\"\"\"\n",
        "\n",
        "# Header row\n",
        "for i, c in enumerate(cols):\n",
        "    if i == 0:\n",
        "        html += \"<th>SNP</th>\"\n",
        "    elif i == 1:\n",
        "        html += \"<th>Year</th>\"\n",
        "    elif i == 2:\n",
        "        html += \"<th>Action</th>\"\n",
        "    else:\n",
        "        pid = c.split(\"-\")[0].upper()\n",
        "        html += (\n",
        "          '<th>'\n",
        "          f'<a href=\"https://yates.one-name.net/tng/verticalchart.php?'\n",
        "          f'personID={pid}&tree=tree1&parentset=0&display=vertical&generations=15\">{c}</a>'\n",
        "          '</th>'\n",
        "        )\n",
        "\n",
        "html += \"\"\"\n",
        "          </tr>\n",
        "        </thead>\n",
        "        <tbody>\"\"\"\n",
        "\n",
        "# Data rows\n",
        "for _, row in df.iterrows():\n",
        "    html += \"<tr>\"\n",
        "    for i, c in enumerate(cols):\n",
        "        v = row[c]\n",
        "        if i == 0:\n",
        "            html += f\"<td>{v}</td>\"\n",
        "        elif i == 1:\n",
        "            html += '<td class=\"blank\">–</td>' if pd.isna(v) or not str(v).strip() else f'<td class=\"era\">{v}</td>'\n",
        "        elif i == 2:\n",
        "            html += '<td class=\"blank\">–</td>' if pd.isna(v) or not str(v).strip() else f'<td class=\"action\">{v}</td>'\n",
        "        else:\n",
        "            html += '<td class=\"blank\">–</td>' if pd.isna(v) or not str(v).strip() else f'<td class=\"match\">{v}</td>'\n",
        "    html += \"</tr>\"\n",
        "\n",
        "html += \"\"\"\n",
        "        </tbody>\n",
        "      </table>\n",
        "    </div>\n",
        "  </div>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "with open(output_htm, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html)\n",
        "print(f\"✅ Saved vertical XHTML to {output_htm}\")\n",
        "\n",
        "# ── 5) FTP upload ────────────────────────────────────────────────────────\n",
        "ftp = FTP_TLS()\n",
        "ftp.connect(os.environ[\"FTP_HOST\"], int(os.environ[\"FTP_PORT\"]))\n",
        "ftp.login(os.environ[\"FTP_USER\"], os.environ[\"FTP_PASS\"])\n",
        "ftp.prot_p()\n",
        "for path in (output_csv, output_htm):\n",
        "    fn = os.path.basename(path)\n",
        "    try:\n",
        "        ftp.delete(fn)\n",
        "    except:\n",
        "        pass\n",
        "    with open(path, \"rb\") as fp:\n",
        "        ftp.storbinary(f\"STOR {fn}\", fp)\n",
        "ftp.quit()\n",
        "print(\"✅ Uploaded CSV & HTML to server\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xecfLjgt4h-y",
        "outputId": "3cb35ada-e4ec-4b5a-ef03-12e5ede80527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved vertical grid CSV to /content/y_dna_grid.csv\n",
            "✅ Saved vertical XHTML to /content/y_dna_grid.htm\n",
            "✅ Uploaded CSV & HTML to server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ── CONFIG ───────────────────────────────────────────────────────────────\n",
        "info_csv   = \"/content/haplogroup_info.csv\"\n",
        "user_csv   = \"/content/y_dna_user_detail.csv\"\n",
        "output_csv = \"/content/y_dna_grid.csv\"\n",
        "output_htm = \"/content/y_dna_grid.htm\"\n",
        "\n",
        "# ── 1) Load & prepare haplogroup info ───────────────────────────────────\n",
        "df_info = pd.read_csv(info_csv)\n",
        "if \"Date\" in df_info.columns:\n",
        "    df_info.rename(columns={\"Date\": \"Era\"}, inplace=True)\n",
        "df_info = df_info.loc[df_info[\"Haplogroup\"].drop_duplicates().index]\n",
        "hap_order = df_info[\"Haplogroup\"].tolist()\n",
        "era_map   = dict(zip(df_info[\"Haplogroup\"], df_info.get(\"Era\", [\"\"] * len(df_info))))\n",
        "\n",
        "# ── 2) Load user detail table ───────────────────────────────────────────\n",
        "df_users = pd.read_csv(user_csv)\n",
        "if \"User_ID\" not in df_users.columns:\n",
        "    df_users.rename(columns={df_users.columns[0]: \"User_ID\"}, inplace=True)\n",
        "user_chains = [\n",
        "    [str(v) for v in row.drop(labels=[\"User_ID\"]).tolist() if pd.notna(v) and str(v).strip()]\n",
        "    for _, row in df_users.iterrows()\n",
        "]\n",
        "\n",
        "# ── 3) Insert new SNPs after parent ──────────────────────────────────────\n",
        "for chain in user_chains:\n",
        "    prev = None\n",
        "    for h in chain:\n",
        "        if prev and h not in hap_order:\n",
        "            idx = hap_order.index(prev)\n",
        "            hap_order.insert(idx + 1, h)\n",
        "        prev = h\n",
        "# Build final eras list\n",
        "eras = [era_map.get(h, \"\") for h in hap_order]\n",
        "\n",
        "# ── 4) Build horizontal grid DataFrame ───────────────────────────────────\n",
        "for h in hap_order:\n",
        "    if h not in df_users.columns:\n",
        "        df_users[h] = \"\"\n",
        "df_grid_h = df_users[[\"User_ID\"] + hap_order]\n",
        "\n",
        "# ── 5) Transform to vertical layout ─────────────────────────────────────\n",
        "df_vert = df_grid_h.set_index(\"User_ID\").T\n",
        "# Insert Era as first column\n",
        "df_vert.insert(0, 'Era', eras)\n",
        "df_vert.index.name = 'SNP'\n",
        "df_grid = df_vert.reset_index()\n",
        "\n",
        "# ── 6) Save vertical CSV ─────────────────────────────────────────────────\n",
        "df_grid.to_csv(output_csv, index=False)\n",
        "print(f\"✅ Vertical grid CSV saved to {output_csv}\")\n",
        "\n",
        "# ── 7) Generate XHTML (vertical) ────────────────────────────────────────\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "ts  = now.strftime(\"%-m/%-d/%y, %-I:%M %p EDT\")\n",
        "\n",
        "template = '''<!DOCTYPE html>\n",
        "<html><head><meta charset=\"UTF-8\"><title>Yates Y-DNA Grid</title>\n",
        "<style>\n",
        "  body { background:#faf9d3; font-family:Arial,Helvetica,sans-serif; font-size:14px; }\n",
        "  table { width:100%; border:1px solid #333; border-collapse:collapse; table-layout:auto; }\n",
        "  th { background:#333; color:#fff; padding:6px; border:1px solid #999; }\n",
        "  .era { background:#666; color:#eee; padding:6px; border:1px solid #999; font-size:0.9em; }\n",
        "  td { padding:6px; border:1px solid #999; text-align:center; white-space:nowrap; }\n",
        "  .match { background:#fff; }\n",
        "  .blank { background:#ccc; color:#ccc; }\n",
        "</style>\n",
        "</head><body>\n",
        "  <h1 style=\"text-align:center;\">Yates Y-DNA Grid</h1>\n",
        "  <table>\n",
        "'''  # end template\n",
        "\n",
        "# Build header row\n",
        "cols = df_grid.columns.tolist()\n",
        "header_html = '<tr><th>SNP</th><th>Era</th>' + ''.join(f'<th>{u}</th>' for u in cols[2:]) + '</tr>'\n",
        "\n",
        "# Build data rows\n",
        "rows_html = []\n",
        "for _, row in df_grid.iterrows():\n",
        "    cells = []\n",
        "    for u in cols[2:]:\n",
        "        v = row[u]\n",
        "        if pd.isna(v) or not str(v).strip():\n",
        "            cells.append('<td class=\"blank\">–</td>')\n",
        "        else:\n",
        "            cells.append(f'<td class=\"match\">{v}</td>')\n",
        "    rows_html.append(f'<tr><td>{row[\"SNP\"]}</td><td class=\"era\">{row[\"Era\"]}</td>' + ''.join(cells) + '</tr>')\n",
        "\n",
        "# Combine and save HTML\n",
        "html = template + header_html + '\\n' + '\\n'.join(rows_html) + f'''\n",
        "  </table>\n",
        "  <p style=\"text-align:right;font-size:0.9em;\">Updated: {ts}</p>\n",
        "</body>\n",
        "</html>'''\n",
        "with open(output_htm, 'w', encoding='utf-8') as f:\n",
        "    f.write(html)\n",
        "print(f\"✅ Vertical XHTML Grid saved to {output_htm}\")\n",
        "\n",
        "# ── 8) FTP Upload ───────────────────────────────────────────────────────\n",
        "ftp = FTP_TLS()\n",
        "ftp.connect(os.environ['FTP_HOST'], int(os.environ.get('FTP_PORT',21)))\n",
        "ftp.login(os.environ['FTP_USER'], os.environ['FTP_PASS'])\n",
        "ftp.prot_p()\n",
        "for path in [output_csv, output_htm]:\n",
        "    name = os.path.basename(path)\n",
        "    try: ftp.delete(name)\n",
        "    except: pass\n",
        "    with open(path,'rb') as fp:\n",
        "        ftp.storbinary(f\"STOR {name}\", fp)\n",
        "ftp.quit()\n",
        "print(\"✅ Uploaded to server.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFqM0kUliAqX",
        "outputId": "d6d54e4c-5b1c-497b-f896-757477e43e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vertical grid CSV saved to /content/y_dna_grid.csv\n",
            "✅ Vertical XHTML Grid saved to /content/y_dna_grid.htm\n",
            "✅ Uploaded to server.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Y-DNA cell 1\n",
        "\n",
        "# === Cell 1: New user settings ===\n",
        "USER_ID       = 'I56217'  # the new column header\n",
        "PATH_STRING   = (      # the SNP chain for this user\n",
        "    \"R-M207 > R-M173 > R-M343 > R-M269 > R-FT266064 > R-FT266579 > R-FTF17042\"\n",
        ")\n",
        "INSERT_MISSING = True       # if True, adds any SNPs from PATH_STRING that aren't yet rows\n",
        "MASTER_CSV     = '/content/y_dna_user_detail_combo.csv'\n",
        "UPDATED_CSV    = '/content/y_dna_user_detail_combo_updated.csv'\n"
      ],
      "metadata": {
        "id": "0bO8B-Gnls49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load → Append User → Save\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load the existing master CSV\n",
        "df = pd.read_csv(MASTER_CSV)\n",
        "\n",
        "# 2) Normalize the first column name to 'SNP' for easy matching\n",
        "first_col = df.columns[0]\n",
        "if first_col != 'SNP':\n",
        "    df.rename(columns={first_col: 'SNP'}, inplace=True)\n",
        "\n",
        "# 3) Parse the new user's SNP chain\n",
        "chain = PATH_STRING.split('>')\n",
        "\n",
        "# 4) Optionally insert any SNPs not yet present (appends at bottom)\n",
        "if INSERT_MISSING:\n",
        "    missing = [s for s in chain if s not in df['SNP'].values]\n",
        "    if missing:\n",
        "        df = pd.concat([df, pd.DataFrame([{'SNP': s} for s in missing])],\n",
        "                       ignore_index=True)\n",
        "\n",
        "# 5) Create the new user column in the next free position\n",
        "df[USER_ID] = ''\n",
        "\n",
        "# 6) Populate: copy the SNP value into that column where it matches the chain\n",
        "df.loc[df['SNP'].isin(chain), USER_ID] = df['SNP']\n",
        "\n",
        "# 7) Save the updated CSV back to /content\n",
        "df.to_csv(UPDATED_CSV, index=False)\n",
        "print(f\"✅ Updated CSV saved to {UPDATED_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjd2-uEdmJKR",
        "outputId": "b6746fb3-cc40-4e7e-d787-d1a5ec09ad00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated CSV saved to /content/y_dna_user_detail_combo_updated.csv\n"
          ]
        }
      ]
    }
  ]
}