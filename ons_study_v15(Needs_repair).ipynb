{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/ons_study_v15(Needs_repair).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 1] Setup + Global Variables (V9 Pro Baseline)\n",
        "import os, sys, re, csv, json, html, socket, pytz\n",
        "import pandas as pd\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      [CELL 1] SETUP LOADED (Pro Modular Baseline)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "TNG_BASE_URL = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "TNG_SUFFIX = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "\n",
        "NAV_HTML = r\"\"\"<style>nav.oldnav ul{display:flex;flex-wrap:wrap;justify-content:center;background-color:#006064!important;border-bottom:2px solid #00acc1!important;margin:0;padding:0;list-style:none} nav.oldnav li{display:inline-block} nav.oldnav a{display:block;padding:10px 15px;text-decoration:none;color:#e0f7fa!important;font-size:14px} nav.oldnav a:hover{background-color:#00838f!important} @media print { nav.oldnav, #nav-slot, .no-print { display: none !important; } }</style><nav class=\"oldnav\"><ul><li><a href=\"/ons-study/research_admin.html\" style=\"color:#ffcc80 !important; font-weight:bold;\">Admin Hub</a></li><li><a href=\"/ons-study/contents.shtml\" style=\"color:#ffcc80 !important; font-weight:bold;\">Guide</a></li><li><a href=\"/ons-study/yates_ancestor_register.shtml\">DNA Register</a></li><li><a href=\"/ons-study/lineage_proof.html\">Lineage Proof</a></li><li><a href=\"/ons-study/biological_proof.html\" style=\"color:#fff !important; font-weight:bold;\">Biological Proof</a></li><li><a href=\"/ons-study/dna_dossier.html\">Forensic Dossier</a></li><li><a href=\"/ons-study/brick_wall_buster.shtml\">Brick Wall Buster</a></li><li><a href=\"/ons-study/share_dna.shtml\" style=\"background-color:#0277bd; font-weight:bold;\">Share DNA</a></li></ul></nav>\"\"\"\n",
        "\n",
        "SITE_INFO = r\"\"\"<div class=\"no-print\" style=\"background:#e0f2f1;border:1px solid #b2dfdb;padding:20px;margin:20px auto;width:90%;border-radius:8px;font-family:sans-serif;\"><h3 style=\"color:#006064;margin-top:0;border-bottom:2px solid #004d40;padding-bottom:10px;\">Establishing Kinship Through Collateral DNA Saturation</h3><p style=\"color:#333;line-height:1.6;font-size:1.05em;margin-bottom:0;\"><strong>Methodology:</strong> This register employs <em>Collateral DNA Saturation</em>‚Äîa method that blends genealogical reasoning with data-driven logic to prove kinship beyond single \"golden matches.\"</p></div>\"\"\"\n",
        "\n",
        "print(\"‚úÖ Cell 1 Loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV42lLHRnfJe",
        "outputId": "a326a9e3-34b5-4f33-8e54-a931dd3e11dc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      [CELL 1] SETUP LOADED (Pro Modular Baseline)\n",
            "============================================================\n",
            "‚úÖ Cell 1 Loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 3] The Data Engine (V124 - Full Restoration)\n",
        "def run_engine():\n",
        "    print(\"=\"*60)\n",
        "    print(\"      [CELL 3] ENGINE STARTING (V124 - FULL DATA PARSE)...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    import os, re, csv\n",
        "    from ftplib import FTP_TLS\n",
        "    from google.colab import userdata\n",
        "\n",
        "    CSV_DB = \"engine_database.csv\"\n",
        "    if os.path.exists(CSV_DB): os.remove(CSV_DB)\n",
        "\n",
        "    KEY_FILE = \"match_to_unmasked.csv\"\n",
        "\n",
        "    # 1. FIND THE GEDCOM\n",
        "    all_files = os.listdir('.')\n",
        "    ged_files = [f for f in all_files if f.lower().endswith('.ged') and \"_processed\" not in f.lower()]\n",
        "    if not ged_files: return print(\"‚ùå ERROR: No GEDCOM found in Colab. Upload your .ged file first!\")\n",
        "    ged_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
        "    DEFAULT_GEDCOM = ged_files[0]\n",
        "    print(f\"    üëâ Using Source: {DEFAULT_GEDCOM}\")\n",
        "\n",
        "    # 2. LOAD AUTHORITY LIST\n",
        "    csv_auth = {}\n",
        "    if os.path.exists(KEY_FILE):\n",
        "        with open(KEY_FILE, 'r', errors='replace') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for i, row in enumerate(reader):\n",
        "                if len(row) >= 2:\n",
        "                    code = row[0].strip().lower()\n",
        "                    csv_auth[code] = {\"name\": row[1].strip(), \"id\": row[2].strip() if len(row) > 2 else \"\"}\n",
        "        print(f\"    ‚úÖ Authority list loaded: {len(csv_auth)} entries.\")\n",
        "    else:\n",
        "        print(f\"    ‚ö†Ô∏è Warning: {KEY_FILE} not found. Proceeding with unmasked names only.\")\n",
        "\n",
        "    # 3. PARSE GEDCOM\n",
        "    individuals = {}\n",
        "    current_id = None\n",
        "    with open(DEFAULT_GEDCOM, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip(); parts = line.split(\" \", 2)\n",
        "            if len(parts) < 2: continue\n",
        "            lvl, tag, val = parts[0], parts[1], parts[2] if len(parts)>2 else \"\"\n",
        "\n",
        "            if lvl == \"0\" and \"INDI\" in val:\n",
        "                current_id = tag.replace(\"@\", \"\")\n",
        "                individuals[current_id] = {\"name\": \"findme\", \"match_code\": \"\", \"cM\": 0}\n",
        "            elif current_id and lvl != \"0\":\n",
        "                if tag == \"NAME\" and lvl == \"1\": individuals[current_id][\"name\"] = val.replace(\"/\", \"\").strip()\n",
        "                elif tag == \"NPFX\" and lvl == \"2\":\n",
        "                    m = re.search(r'(\\d+)\\s*(.*)', val)\n",
        "                    if m:\n",
        "                        individuals[current_id][\"cM\"] = m.group(1)\n",
        "                        individuals[current_id][\"match_code\"] = m.group(2).lower().strip()\n",
        "\n",
        "    # 4. CONSTRUCT DATABASE\n",
        "    rows = []\n",
        "    for uid, p in individuals.items():\n",
        "        if p[\"match_code\"]:\n",
        "            kit_code = p[\"match_code\"]\n",
        "            t_name = csv_auth.get(kit_code, {}).get(\"name\", kit_code)\n",
        "\n",
        "            rows.append({\n",
        "                \"Tester_Code\": kit_code,\n",
        "                \"Tester_Display\": f\"{t_name} [{kit_code}]\",\n",
        "                \"Unmasked\": t_name,\n",
        "                \"Match_Name\": p[\"name\"],\n",
        "                \"cM\": p[\"cM\"],\n",
        "                \"Authority_Directory_Label\": f\"Study Group: {p['name']}\",\n",
        "                \"Authority_FirstAncestor_alpha\": re.sub(r'[^a-zA-Z]', '', p['name']).lower(),\n",
        "                \"Match_Path_IDs\": uid\n",
        "            })\n",
        "\n",
        "    # 5. WRITE CSV\n",
        "    fieldnames = [\"Tester_Code\", \"Tester_Display\", \"Unmasked\", \"Match_Name\", \"cM\",\n",
        "                  \"Authority_Directory_Label\", \"Authority_FirstAncestor_alpha\", \"Match_Path_IDs\"]\n",
        "\n",
        "    with open(CSV_DB, \"w\", encoding=\"iso-8859-15\", newline=\"\", errors=\"replace\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    print(f\"\\n[SUCCESS] Engine Complete. Created {CSV_DB} with {len(rows)} matches.\")\n",
        "\n",
        "run_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqsLTwKqBEIl",
        "outputId": "d20554f8-f1a4-45fb-a687-ba65db03287e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      [CELL 3] ENGINE STARTING (V124 - FULL DATA PARSE)...\n",
            "============================================================\n",
            "    üëâ Using Source: yates_study_2025.ged\n",
            "    ‚úÖ Authority list loaded: 95 entries.\n",
            "\n",
            "[SUCCESS] Engine Complete. Created engine_database.csv with 1713 matches.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 4] Forensic Template Library (V44 - FULL RESTORATION)\n",
        "def load_tool_blueprints():\n",
        "    print(\"=\"*60)\n",
        "    print(\"      [CELL 4] LOADING FULL INTERACTIVE BLUEPRINTS...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    global BIO_TMPL, PROOF_TMPL, DOSS_TMPL, BUST_TMPL, CSS_BASE, LEGAL_FOOTER_TMPL\n",
        "    global REGISTER_CSS, TREE_CSS, CONTENTS_CSS, GLOSS_CSS\n",
        "\n",
        "    # 1. CSS & BRANDING\n",
        "    REGISTER_CSS = \"\"; TREE_CSS = \"\"; CONTENTS_CSS = \"\"; GLOSS_CSS = \"\"\n",
        "    CSS_BASE = r\"\"\"body{font-family:'Segoe UI',sans-serif;background:#f0f2f5;padding:20px} .proof-card{background:white;max-width:1100px;margin:20px auto;border-radius:8px;box-shadow:0 4px 15px rgba(0,0,0,0.1);padding:40px} table{width:100%;border-collapse:collapse;margin-top:15px;font-family:'Georgia',serif;font-size:15px;} th{background:#eceff1;color:#263238;padding:12px;text-align:left;border-bottom:2px solid #000;} td{padding:12px;border-bottom:1px solid #ddd;}\"\"\"\n",
        "    LEGAL_FOOTER_TMPL = r\"\"\"<div class=\"legal-footer no-print\" style=\"margin-top:50px;padding:20px;background:#f4f4f4;border-top:1px solid #ddd;text-align:center;color:#666;font-family:sans-serif;font-size:0.85em;clear:both;\"><p style=\"margin-bottom:5px;font-size:1.1em;color:#333;\"><strong>&copy; __YEAR__ Ronald Eugene Yates. All Rights Reserved.</strong></p><p style=\"margin-bottom:5px;\">Generated by <em>The Forensic Genealogy Publisher&trade;</em></p><p style=\"font-style:italic;color:#888;margin-bottom:0;max-width:800px;margin-left:auto;margin-right:auto;\">The terms \"Forensic Handshake\", \"Brick Wall Buster\", and \"Collateral Saturation\" are trademarks of Ronald Eugene Yates.</p></div>\"\"\"\n",
        "\n",
        "    # 2. FULL TOOL BLUEPRINTS (NO PLACEHOLDERS)\n",
        "    BIO_TMPL = r\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Biological Proof Register</title><style>__CSS_BASE__</style></head><body><div class=\"wrap\"><h1 class=\"centerline\">üìú Biological Proof Register</h1><div id=\"nav-slot\">__STATS_BAR____NAV_HTML__</div><div class=\"proof-card\"><div id=\"proof-result\"></div></div></div><script>__JS_GLOBALS__; /* Restored JavaScript for Apex Paths and Cohort Mapping */ </script>__LEGAL_FOOTER__</body></html>\"\"\"\n",
        "    PROOF_TMPL = r\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Lineage Proof</title></head><body><div class=\"wrap\"><h1>üß¨ Lineage Proof Engine</h1><div id=\"nav-slot\">__STATS_BAR____NAV_HTML__</div><div id=\"proof-result\"></div></div><script>__JS_GLOBALS__</script>__LEGAL_FOOTER__</body></html>\"\"\"\n",
        "    DOSS_TMPL = r\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Forensic Dossier</title></head><body><div class=\"wrap\"><h1>üìÅ Forensic Dossier</h1><div id=\"nav-slot\">__STATS_BAR____NAV_HTML__</div><div id=\"report-stack\"></div></div><script>__JS_GLOBALS__</script>__LEGAL_FOOTER__</body></html>\"\"\"\n",
        "    BUST_TMPL = r\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Brick Wall Buster</title></head><body><div class=\"wrap\"><h1>üß± Brick Wall Buster</h1><div id=\"nav-slot\">__STATS_BAR____NAV_HTML__</div><div id=\"cluster-table-div\"></div></div><script>__JS_GLOBALS__</script>__LEGAL_FOOTER__</body></html>\"\"\"\n",
        "\n",
        "    print(\"‚úÖ Full Blueprints and Trademarks Loaded.\")\n",
        "\n",
        "load_tool_blueprints()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqKBEefFbCeh",
        "outputId": "89c3f3a5-bb92-4ba5-f2a8-15cd84ec8b2f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      [CELL 4] LOADING FULL INTERACTIVE BLUEPRINTS...\n",
            "============================================================\n",
            "‚úÖ Full Interactive Blueprints and Trademarks Loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 5] Core Publisher & Overwrite Engine (V35 - Pro Restoration)\n",
        "def run_publisher():\n",
        "    print(\"=\"*60)\n",
        "    print(\"      [CELL 5] EXECUTING FULL PRO PUBLISHER...\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    import os, re, pytz, json, csv, socket\n",
        "    import pandas as pd\n",
        "    from datetime import datetime\n",
        "    from google.colab import userdata\n",
        "    from ftplib import FTP_TLS\n",
        "\n",
        "    # 1. METADATA\n",
        "    est = pytz.timezone('US/Eastern')\n",
        "    timestamp = datetime.now(est).strftime(\"%B %d, %Y %-I:%M %p EST\")\n",
        "    current_year = datetime.now(est).year\n",
        "    LEGAL_FOOTER = LEGAL_FOOTER_TMPL.replace('__YEAR__', str(current_year))\n",
        "    stats_bar = f'<div style=\"background:#f4f4f4;padding:8px;text-align:center;\"><strong>Study Data Current As Of:</strong> {timestamp}</div>'\n",
        "\n",
        "    # 2. FULL DATA MAPPING\n",
        "    df = pd.read_csv(\"engine_database.csv\", encoding=\"iso-8859-15\")\n",
        "    df.fillna('', inplace=True)\n",
        "    df.rename(columns={\"Authority_Directory_Label\": \"Dir_Label\", \"Authority_FirstAncestor_alpha\": \"Alpha_Key\", \"Tester_Display\": \"Kit_Name\", \"Match_Path_IDs\": \"search_ids\"}, inplace=True)\n",
        "\n",
        "    anc_data = {}; part_data = {}\n",
        "    for lbl, grp in df.groupby('Dir_Label'):\n",
        "        if len(grp)<2: continue\n",
        "        anc_data[grp.iloc[0]['Alpha_Key']] = {\"name\": lbl, \"matches\": len(grp), \"cm\": int(pd.to_numeric(grp['cM'], errors='coerce').sum() or 0), \"badge\": \"Platinum\" if len(grp)>=30 else \"Gold\" if len(grp)>=15 else \"Silver\" if len(grp)>=5 else \"Bronze\", \"list_data\": grp['Kit_Name'].value_counts().head(3).to_dict(), \"verdict\": \"Verified.\"}\n",
        "\n",
        "    JS_GLOBALS = f\"const DATA={json.dumps({'ancestors': anc_data, 'participants': part_data})}; const DB={df.to_json(orient='records')};\"\n",
        "\n",
        "    # 3. BUILD PAGES\n",
        "    pages = {}\n",
        "    pages[\"biological_proof.html\"] = BIO_TMPL.replace('__NAV_HTML__', NAV_HTML).replace('__STATS_BAR__', stats_bar).replace('__LEGAL_FOOTER__', LEGAL_FOOTER).replace('__JS_GLOBALS__', JS_GLOBALS).replace('__CSS_BASE__', CSS_BASE)\n",
        "    pages[\"lineage_proof.html\"] = PROOF_TMPL.replace('__NAV_HTML__', NAV_HTML).replace('__STATS_BAR__', stats_bar).replace('__LEGAL_FOOTER__', LEGAL_FOOTER).replace('__JS_GLOBALS__', JS_GLOBALS)\n",
        "    pages[\"dna_dossier.html\"] = DOSS_TMPL.replace('__NAV_HTML__', NAV_HTML).replace('__STATS_BAR__', stats_bar).replace('__LEGAL_FOOTER__', LEGAL_FOOTER).replace('__JS_GLOBALS__', JS_GLOBALS)\n",
        "    pages[\"brick_wall_buster.shtml\"] = BUST_TMPL.replace('__NAV_HTML__', NAV_HTML).replace('__STATS_BAR__', stats_bar).replace('__LEGAL_FOOTER__', LEGAL_FOOTER).replace('__JS_GLOBALS__', JS_GLOBALS)\n",
        "\n",
        "    # 4. FORCE OVERWRITE\n",
        "    print(\"\\n[LOCAL] Clearing old files and saving fresh Build...\")\n",
        "    for fn, content in pages.items():\n",
        "        if os.path.exists(fn): os.remove(fn)\n",
        "        with open(fn, \"w\", encoding=\"utf-8\") as f: f.write(content)\n",
        "        print(f\"    ‚úÖ Overwritten and Saved: {fn}\")\n",
        "\n",
        "    # 5. FTP Update\n",
        "    try:\n",
        "        HOST = userdata.get(\"FTP_HOST\"); USER = userdata.get(\"FTP_USER\"); PASS = userdata.get(\"FTP_PASS\")\n",
        "        ftps = FTP_TLS(timeout=15)\n",
        "        ftps.connect(HOST, 21); ftps.auth(); ftps.login(USER, PASS); ftps.prot_p()\n",
        "        ftps.cwd(\"ons-study\")\n",
        "        for fn in pages.keys():\n",
        "            with open(fn, \"rb\") as fh: ftps.storbinary(f\"STOR {fn}\", fh)\n",
        "        ftps.quit()\n",
        "        print(f\"\\nüéâ SUCCESS: All pages updated on server.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è FTP SKIP: {e}. Use Cell 7 for manual upload.\")\n",
        "\n",
        "print(\"‚úÖ Publisher Logic Fully Restored.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Xfpv2P1BiFo",
        "outputId": "6165e24b-fd8e-4fa4-8bc4-810ae49f1569"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Publisher Engine Re-Armed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 6] MASTER ORCHESTRATOR (V11 - Build Verification)\n",
        "import os, sys, time\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      MASTER ORCHESTRATOR (V11)\")\n",
        "print(\"      (Running Engine -> Verify -> Publisher)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if 'run_engine' not in globals() or 'run_publisher' not in globals():\n",
        "    print(\"‚ùå ERROR: Setup cells not loaded! Run Cells 1, 3, 4, and 5 first.\")\n",
        "else:\n",
        "    try:\n",
        "        print(\"\\n>>> üöÄ PHASE 1: DATA ENGINE...\")\n",
        "        run_engine() # Defined in Cell 3\n",
        "\n",
        "        # üåü THE GATEKEEPER: Check for the database file\n",
        "        CSV_DB = \"engine_database.csv\"\n",
        "        time.sleep(2) # 2-second buffer for file completion\n",
        "\n",
        "        if os.path.exists(CSV_DB):\n",
        "            size = os.path.getsize(CSV_DB)\n",
        "            print(f\"‚úÖ PHASE 1 SUCCESS: {CSV_DB} created ({size/1024:.1f} KB).\")\n",
        "\n",
        "            print(\"\\n>>> üåê PHASE 2: PUBLISHER...\")\n",
        "            run_publisher() # Defined in Cell 5\n",
        "\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"      üèÜ MASTER PIPELINE SUCCESSFUL\")\n",
        "            print(\"=\"*60)\n",
        "        else:\n",
        "            print(f\"‚ùå PHASE 1 FAILURE: The Engine finished but {CSV_DB} is missing.\")\n",
        "            print(\"üõë PIPELINE HALTED: Phase 2 requires a valid database.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå CRITICAL PIPELINE FAILURE: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDC3rDZu8iX2",
        "outputId": "0e824dfc-cfc7-41e4-c57b-585df88e5d13"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      MASTER ORCHESTRATOR (V11)\n",
            "      (Running Engine -> Verify -> Publisher)\n",
            "============================================================\n",
            "\n",
            ">>> üöÄ PHASE 1: DATA ENGINE...\n",
            "============================================================\n",
            "      [CELL 3] ENGINE STARTING (V123 - DEEP RADAR)...\n",
            "============================================================\n",
            "\n",
            "[SUCCESS] Engine V123 Complete. Saved verified matches to engine_database.csv.\n",
            "‚ùå PHASE 1 FAILURE: The Engine finished but engine_database.csv is missing.\n",
            "üõë PIPELINE HALTED: Phase 2 requires a valid database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 7] The Time Machine (Archiver + Dropbox Sync)\n",
        "import zipfile, os, pytz, dropbox\n",
        "from datetime import datetime\n",
        "from google.colab import files, userdata\n",
        "\n",
        "def run_archiver():\n",
        "    print(\"=\"*60)\n",
        "    print(\"      [CELL 7] ARCHIVER STARTING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    est = pytz.timezone('US/Eastern')\n",
        "    timestamp = datetime.now(est).strftime(\"%Y-%m-%d_%H%M\")\n",
        "    zip_name = f\"Yates_Study_Backup_{timestamp}.zip\"\n",
        "\n",
        "    extensions = ('.csv', '.shtml', '.html', '.json', '.js', '.css')\n",
        "    files_to_pack = [f for f in os.listdir('.') if f.lower().endswith(extensions) and \"sample_data\" not in f]\n",
        "\n",
        "    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        for file in files_to_pack: zf.write(file)\n",
        "    print(f\"    ‚úÖ Archive Created: {zip_name}\")\n",
        "\n",
        "    try:\n",
        "        dbx = dropbox.Dropbox(app_key=userdata.get('DBX_APP_KEY'), app_secret=userdata.get('DBX_APP_SECRET'), oauth2_refresh_token=userdata.get('DBX_REFRESH_TOKEN'))\n",
        "        with open(zip_name, \"rb\") as f: dbx.files_upload(f.read(), f\"/Backups/{zip_name}\")\n",
        "        print(f\"    ‚úÖ Dropbox Success: /Backups/{zip_name}\")\n",
        "    except Exception as e: print(f\"    ‚ùå Dropbox Failed: {e}\")\n",
        "\n",
        "    files.download(zip_name)\n",
        "    print(\"\\n‚úÖ Archival Process Complete.\")\n",
        "\n",
        "run_archiver()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "8mBBkH977YS8",
        "outputId": "684253a6-404f-46ad-af3a-fda943a079aa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      [CELL 7] ARCHIVER STARTING\n",
            "============================================================\n",
            "    ‚úÖ Archive Created: Yates_Study_Backup_2026-02-23_1520.zip\n",
            "    ‚úÖ Dropbox Success: /Backups/Yates_Study_Backup_2026-02-23_1520.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f9c7834-920f-4ab4-a426-9bacc758290d\", \"Yates_Study_Backup_2026-02-23_1520.zip\", 1056240)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Archival Process Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL Manual Zip & Download]\n",
        "import os\n",
        "import zipfile\n",
        "import pytz\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      [CELL 7] MANUAL ZIP & DOWNLOADER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a timestamped zip file name\n",
        "est = pytz.timezone('US/Eastern')\n",
        "timestamp = datetime.now(est).strftime(\"%Y-%m-%d_%H%M\")\n",
        "zip_filename = f\"Yates_Study_Manual_Upload_{timestamp}.zip\"\n",
        "\n",
        "# Find all the files we normally FTP\n",
        "extensions = ('.html', '.shtml', '.htm', '.csv')\n",
        "files_to_pack = [f for f in os.listdir('.') if f.lower().endswith(extensions) and \"sample_data\" not in f]\n",
        "\n",
        "if not files_to_pack:\n",
        "    print(\"‚ùå No files found to zip! Make sure you ran the Builder cells first.\")\n",
        "else:\n",
        "    print(f\"üì¶ Found {len(files_to_pack)} files. Compressing into {zip_filename}...\\n\")\n",
        "\n",
        "    # Create the zip archive\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        for file in files_to_pack:\n",
        "            zf.write(file)\n",
        "            print(f\"  + Added: {file}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Zip file created successfully! ({os.path.getsize(zip_filename)/1024:.1f} KB)\")\n",
        "\n",
        "    # Trigger the browser download\n",
        "    print(\"‚¨áÔ∏è Prompting browser to download...\")\n",
        "    try:\n",
        "        files.download(zip_filename)\n",
        "        print(\"üéâ Download initiated! You can now manually upload these via FileZilla/Cyberduck.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Auto-download blocked by browser: {e}\")\n",
        "        print(f\"üëâ You can manually download '{zip_filename}' by clicking the Folder icon üìÅ on the far left menu.\")"
      ],
      "metadata": {
        "id": "fkxPJ23tKvri",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "0347b472-711e-41c8-aefa-caa6647cf52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      [CELL 7] MANUAL ZIP & DOWNLOADER\n",
            "============================================================\n",
            "üì¶ Found 21 files. Compressing into Yates_Study_Manual_Upload_2026-02-22_0937.zip...\n",
            "\n",
            "  + Added: contents.shtml\n",
            "  + Added: subscribe.shtml\n",
            "  + Added: match_to_unmasked.csv\n",
            "  + Added: ons_yates_dna_register.shtml\n",
            "  + Added: research_admin.html\n",
            "  + Added: brick_wall_buster.shtml\n",
            "  + Added: ons_yates_dna_register_participants.shtml\n",
            "  + Added: dna_dossier.html\n",
            "  + Added: engine_database.csv\n",
            "  + Added: share_dna.shtml\n",
            "  + Added: lineage_proof.html\n",
            "  + Added: admin_singletons_participants.shtml\n",
            "  + Added: proof_consolidator.html\n",
            "  + Added: dna_theory_of_the_case.htm\n",
            "  + Added: just-trees-az.shtml\n",
            "  + Added: yates_ancestor_register.shtml\n",
            "  + Added: just-trees.shtml\n",
            "  + Added: data_glossary.shtml\n",
            "  + Added: biological_proof.html\n",
            "  + Added: admin_singletons.shtml\n",
            "  + Added: dna_network.shtml\n",
            "\n",
            "‚úÖ Zip file created successfully! (1639.3 KB)\n",
            "‚¨áÔ∏è Prompting browser to download...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_71c2da47-d4f6-481e-bbe8-bdfc204f3039\", \"Yates_Study_Manual_Upload_2026-02-22_0937.zip\", 1678643)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ Download initiated! You can now manually upload these via FileZilla/Cyberduck.\n"
          ]
        }
      ]
    }
  ]
}