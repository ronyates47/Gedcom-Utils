{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsw5SYDfkpnjhemKMYb+Nr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/REFACTOR_Gold__1_%26_2_%26_3_20251014.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "61c2219a-b05d-481b-aa8d-4c4ef2bcffb0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: python-gedcom in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.2)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "ERROR: unknown command \"caas_jupyter_tools\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n",
        "!pip caas_jupyter_tools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credentials\n",
        "\n",
        "import os\n",
        "\n",
        "# Gmail SMTP creds\n",
        "os.environ['GMAIL_USER']         = 'yatesvilleron@gmail.com'\n",
        "os.environ['GMAIL_APP_PASSWORD'] = 'qtziwiblytgrlzvx'\n",
        "\n",
        "# FTPS upload creds — make sure FTP_PASS is exactly your password, no < or >\n",
        "os.environ['FTP_HOST']       = 'ftp.one-name.net'\n",
        "os.environ['FTP_PORT']       = '21'\n",
        "os.environ['FTP_USER']       = 'admin@yates.one-name.net'\n",
        "os.environ['FTP_PASS']       = 'v(i83lfQB@dB'\n"
      ],
      "metadata": {
        "id": "971jlPTnBVfk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 20250513\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script using:\n",
        " - Chunk-based Parallel Processing for Speed (Stage 1: genealogical line creation)\n",
        " - A Trie-based approach, then final \"Value\" = 5 * (number of couples with node.count >=2) + (total couples)\n",
        "\n",
        "For ancestral lines where none of the couples are repeated (a one-off line), the Value is still computed.\n",
        "Now, instead of composite scoring, two new columns are added:\n",
        "  - Value Range (the numeric bracket)\n",
        "  - Value Label (a descriptive label)\n",
        "\n",
        "Exports final CSV/HTML sorted by \"Yates DNA Ancestral Line\", including a 'haplogroup' column.\n",
        "\"\"\"\n",
        "import csv\n",
        "import glob\n",
        "import logging\n",
        "import functools\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# Global Variables\n",
        "###############################################################################\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "###############################################################################\n",
        "# Trie Data Structure\n",
        "###############################################################################\n",
        "class TrieNode:\n",
        "    \"\"\"A simple Trie node for storing a couple and counting how many lines pass here.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.children = {}\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert_line(self, couples_list):\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple not in current.children:\n",
        "                current.children[couple] = TrieNode()\n",
        "            current = current.children[couple]\n",
        "            current.count += 1\n",
        "\n",
        "    def get_couple_count(self, couples_list):\n",
        "        counts = []\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple in current.children:\n",
        "                current = current.children[couple]\n",
        "                counts.append(current.count)\n",
        "            else:\n",
        "                counts.append(0)\n",
        "                break\n",
        "        return counts\n",
        "\n",
        "###############################################################################\n",
        "# Utility: chunk generator\n",
        "###############################################################################\n",
        "def chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "###############################################################################\n",
        "# GedcomDataset\n",
        "###############################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "###############################################################################\n",
        "# Gedcom Class\n",
        "###############################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "        return autosomal_count\n",
        "\n",
        "###############################################################################\n",
        "# quick_extract_name\n",
        "###############################################################################\n",
        "def quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start)\n",
        "    if end == -1:\n",
        "        end = len(full_text)\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "###############################################################################\n",
        "# Parents & Ancestors\n",
        "###############################################################################\n",
        "def find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def find_distant_ancestors(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(find_distant_ancestors(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(find_distant_ancestors(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "###############################################################################\n",
        "# filter_ancestral_line\n",
        "###############################################################################\n",
        "def filter_ancestral_line(winning_path_ids, generation_table_local, names_map):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table_local:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    matching_table.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for gen, pair in matching_table:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(f\"{name_pair[0]}&{name_pair[1]}\")\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "###############################################################################\n",
        "# process_record_wrapper (parallel) - STAGE 1\n",
        "###############################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, parents_map)\n",
        "    distant_anc_paths = find_distant_ancestors(individual_id, parents_map)\n",
        "\n",
        "    best_score = None\n",
        "    best_path = None\n",
        "    for path in distant_anc_paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx+1) for idx, nm in enumerate(name_path) if 'Yates' in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_path = path\n",
        "\n",
        "    if not best_path:\n",
        "        best_path = []\n",
        "\n",
        "    best_path_cleaned = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str = filter_ancestral_line(set(best_path_cleaned), generation_table, names_map)\n",
        "\n",
        "    cm_value = ''\n",
        "    sort_value = ''\n",
        "    ydna_value = ''\n",
        "    for ds in gedcom_instance.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    # Return columns: ID#, Match to, Name, cM, Yates DNA Ancestral Line, haplogroup\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "###############################################################################\n",
        "# main()\n",
        "###############################################################################\n",
        "def main():\n",
        "    def select_gedcom():\n",
        "        files = glob.glob(\"*.ged\")\n",
        "        if not files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom()\n",
        "    if not gedcom_file_path:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    ged = Gedcom(gedcom_file_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "    filter_count = len(ged.filter_pool)\n",
        "\n",
        "    with open(\"autosomal_count.txt\", \"w\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "\n",
        "    print(\"Records tagged and filtered by NPFX:\", filter_count)\n",
        "\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    blocks = raw_data.split('\\n0 ')\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find('\\n')\n",
        "        if flend == -1:\n",
        "            flend = len(blk)\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            start = first_line.find('@') + 1\n",
        "            end = first_line.find('@', start)\n",
        "            rec_id = first_line[start:end].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    names_map = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        nm = quick_extract_name(\"\\n\" + txt)\n",
        "        names_map[rec_id] = nm\n",
        "\n",
        "    families = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            husb_id = txt[father_idx+len('1 HUSB @'):txt.find('@', father_idx+len('1 HUSB @'))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            wife_id = txt[wife_idx+len('1 WIFE @'):txt.find('@', wife_idx+len('1 WIFE @'))] if wife_idx != -1 else None\n",
        "            kids = [ln.split('@')[1] for ln in txt.split('\\n') if ln.strip().startswith('1 CHIL @')]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(f\"Processing {len(individual_ids)} individuals with chunk-based parallel...\")\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Starting chunk-based parallel processing with %d workers.\", max_workers)\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            func = functools.partial(process_record_wrapper, gedcom_instance=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(executor.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def remove_specific_prefix(row):\n",
        "        prefix = \"YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&FauconerElizabeth~~~\"\n",
        "        if row[\"Yates DNA Ancestral Line\"].startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = row[\"Yates DNA Ancestral Line\"][len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(remove_specific_prefix, axis=1)\n",
        "\n",
        "    logger.info(\"Building Trie from reversed lines...\")\n",
        "    trie = Trie()\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.notna(line_str) and line_str.strip():\n",
        "            trie.insert_line([x.strip() for x in line_str.split(\"~~~\") if x.strip()])\n",
        "\n",
        "    values, prefix_counts = [], []\n",
        "    logger.info(\"Computing 'Value' = 5*(#couples with node.count >=2) + (total couples) ...\")\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            values.append(0)\n",
        "            prefix_counts.append(0)\n",
        "        else:\n",
        "            couples_list = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "            node_counts = trie.get_couple_count(couples_list)\n",
        "            prefix_count = sum(1 for c in node_counts if c >= 2)\n",
        "            values.append(5 * prefix_count + len(couples_list))\n",
        "            prefix_counts.append(prefix_count)\n",
        "\n",
        "    df[\"Value\"], df[\"PrefixCount\"] = values, prefix_counts\n",
        "\n",
        "    def assign_value_range_label(val):\n",
        "        try:\n",
        "            v = float(val)\n",
        "        except:\n",
        "            return \"\", \"\"\n",
        "        if v >= 60: return \">=60\", \"1-likely correct\"\n",
        "        if 47 <= v <= 59: return \"59~47\", \"2-lines forming\"\n",
        "        if 34 <= v <= 46: return \"46~34\", \"3-patterns emerging\"\n",
        "        if 21 <= v <= 33: return \"33~21\", \"4-notable patterns\"\n",
        "        if 8 <= v <= 20: return \"20~8\", \"5-patterns stable\"\n",
        "        if 1 <= v <= 7:  return f\"{v:.0f}\", \"6-need research\"\n",
        "        return f\"{v:.0f}\", \"0-uncategorized\"\n",
        "\n",
        "    ranges, labels = zip(*(assign_value_range_label(v) for v in df[\"Value\"]))\n",
        "    df[\"Value Range\"], df[\"Value Label\"] = ranges, labels\n",
        "\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "    df.drop(\"PrefixCount\", axis=1, inplace=True)\n",
        "\n",
        "    csv_name = \"final_combined_df_with_value_labels.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    logger.info(\"Exported final DataFrame to '%s'.\", csv_name)\n",
        "\n",
        "    html_name = \"HTML_combined_df_with_value_labels.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n",
        "    table, th, td { border: 1px solid #333; }\n",
        "    th, td { padding: 8px 12px; text-align: center; }\n",
        "    th { background-color: #f2f2f2; }\n",
        "    /* Left-align the last column */\n",
        "    td:nth-child(7) { text-align: left; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Value Range\", \"Value Label\", \"Yates DNA Ancestral Line\"]\n",
        "    html_content = css_style + df.to_html(index=False, columns=final_cols, escape=False)\n",
        "    with open(html_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Exported HTML to '%s'.\", html_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    try:\n",
        "        display(Javascript('alert(\"✅ GEDCOM processing (and HTML export) is complete!\");'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import smtplib, ssl\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "def send_email(subject, body, to_addr):\n",
        "    smtp_server = 'smtp.gmail.com'\n",
        "    port = 465\n",
        "    sender = os.environ['GMAIL_USER']\n",
        "    password = os.environ['GMAIL_APP_PASSWORD']\n",
        "    msg = MIMEText(body)\n",
        "    msg['Subject'] = subject\n",
        "    msg['From'] = sender\n",
        "    msg['To'] = to_addr\n",
        "    context = ssl.create_default_context()\n",
        "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
        "        server.login(sender, password)\n",
        "        server.send_message(msg)\n",
        "\n",
        "# Email summary (only total lines)\n",
        "df_summary = pd.read_csv(\"final_combined_df_with_value_labels.csv\")\n",
        "total = len(df_summary)\n",
        "summary = f\"GEDCOM processing complete!\\n\\nTotal lines: {total}\"\n",
        "\n",
        "send_email(\n",
        "    subject=\"✅ Cell #1 Report Ready\",\n",
        "    body=summary,\n",
        "    to_addr=os.environ['GMAIL_USER']\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Qh13Q-WUmVu3",
        "outputId": "6785ff8c-b47f-4785-dbba-17cec47c11db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:filtered_ids.xlsx not found. Skipping second-level manual filter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEDCOM contained 62030 total records\n",
            "Records tagged and filtered by NPFX: 1623\n",
            "Records with YDNA information: 90\n",
            "Autosomal matches: 1533\n",
            "Records tagged and filtered by NPFX: 1623\n",
            "Processing 1623 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 1623/1623 [12:00<00:00,  2.25it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "alert(\"✅ GEDCOM processing (and HTML export) is complete!\");"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REFACTOR-Gold 2 — create the Report Card (mobile-friendly, sortable)\n",
        "# Gold 2 create the Report Card (mobile-friendly, sortable)\n",
        "# -*- coding: utf-8 -*-\n",
        "# DNA Cousin Surname REFACTOR (Stokes Display, 2 columns)\n",
        "# Resolver precedence:\n",
        "#   1) match_to_unmasked.csv (fresh upload)  [remote fetch supported]\n",
        "#   2) match_to_unmasked.cache.json (last good snapshot)\n",
        "#   3) built-in DEFAULT_MATCH_TO_UNMASKED\n",
        "# Features kept: \"(back N Gens)\", matchee first-name truncated to 4, Column A CSV, sticky headers, back-to-top.\n",
        "# NEW: Optional remote read for inputs + remote upload for outputs (same FTP as HTML).\n",
        "\n",
        "# ========= CUT START [1/3] Config + Resolver + Helpers ===========================================\n",
        "import os, re, json, time, io, posixpath\n",
        "import pandas as pd\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ========= LAYOUT =========\n",
        "TABLE_WIDTH_PX = 6000\n",
        "COL_A_PX       = 1220\n",
        "\n",
        "# ========= DATA / OUTPUT =========\n",
        "CSV_PATH            = \"final_combined_df_with_value_labels.csv\"   # main input CSV (will be pulled from server if REMOTE_READ)\n",
        "LOCAL_NAME          = \"dna_cousin_surname_REFACTOR.htm\"\n",
        "REMOTE_NAME         = \"dna_cousin_surname_REFACTOR.htm\"\n",
        "LINEAGE_HEADER      = \"Lineage (Starting with oldest ancestor, the line is:)\"\n",
        "ARROW_ENTITY        = \"&rarr;\"\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "# Local exports\n",
        "MATCH_COUSINS_CSV   = \"the_match_cousins.csv\"            # Column A\n",
        "RESOLVER_CSV        = \"match_to_unmasked.csv\"            # fresh upload (optional)\n",
        "RESOLVER_CACHE_JSON = \"match_to_unmasked.cache.json\"     # last good snapshot (auto-written)\n",
        "\n",
        "# ========= Remote I/O toggles (use same creds/dir as HTML upload) =========\n",
        "REMOTE_READ        = True   # Pull input CSV (and resolver CSV if present) from server before processing\n",
        "UPLOAD_COLUMN_A    = True   # Push the_match_cousins.csv to server after build\n",
        "UPLOAD_CACHE_JSON  = False  # If you want to mirror the cache JSON on server, set True\n",
        "\n",
        "# ========= Optional remote subdirectory (same place you host the HTML) =========\n",
        "# If your files live in a subfolder (e.g., /public_html/gengen), set FTP_DIR in env.\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip()  # e.g., \"gengen\" or \"public_html/gengen\"\n",
        "\n",
        "# ========= Built-in fallback resolver (only used if neither CSV nor cache is available) =========\n",
        "DEFAULT_MATCH_TO_UNMASKED = {\n",
        "    \"1200am\":\"Cheryl Midnight\",\"adamssarah\":\"Sarah Adams\",\"addison,david\":\"Dave Addison\",\"amanic\":\"Amanda Radnage\",\n",
        "    \"beardali\":\"Alice Beard\",\"birdwelljac\":\"Jacalyn Yates\",\"bucb\":\"Beth Buckley\",\"camry\":\"Cami Crockett\",\"camry_jy\":\"Jamie Yates\",\n",
        "    \"cagilaba,leigh\":\"Leigh Yates\",\"evansdei\":\"Deidre Evans\",\"franch,mike\":\"Mike Franch\",\"fridine\":\"Nadine Brown\",\n",
        "    \"girtain,alma\":\"Alma Girtain\",\"girtain,andy\":\"Andy Girtain\",\"girtain,kathryn\":\"Kathryn Girtain\",\"girtain,theresa\":\"Theresa Girtain\",\n",
        "    \"girtja\":\"Josh Girtain\",\"handmer\":\"Meredith Aronson\",\"hatpat\":\"Pat Hatfield\",\"hatpatm\":\"Virginia Looney\",\"hell-bry\":\"Bobby Yates\",\n",
        "    \"hellopt\":\"Pat Thomas\",\"hendricksjas\":\"Jim Hendricks\",\"henryche\":\"Cheryl Henry\",\"husainir\":\"Rebecca Husaini\",\"klingal\":\"Albert Kling\",\n",
        "    \"kuhlmanj?\":\"Steve Kuhlman\",\"leedon\":\"Donna Lee\",\"lewiscla\":\"Claudia Lewis\",\"littleil\":\"Ilene Little\",\"lovewalk1\":\"Linda Lovett\",\n",
        "    \"marma\":\"Mary Marshall\",\"mccollummike\":\"Mike McCollum\",\"milocan\":\"Candy Milovich\",\"powers,kath\":\"Kathy Powers\",\n",
        "    \"rophy\":\"Phyllis Rounsevell\",\"rophy_rd\":\"Robyn Billinghurst\",\"sarpri\":\"Sarah Price\",\"smittybec\":\"Rebecca Smith\",\n",
        "    \"solyons\":\"Stephanie Yates\",\"stetlerkar\":\"Karen Stetler\",\"sudie\":\"Wanda Tabor\",\"walclif\":\"Ray Walton\",\"weeksjerri\":\"Jerri Weeks\",\n",
        "    \"wishardglen\":\"Glen Wishard\",\"yates,andreal\":\"Andrea Yates\",\"yates,patricial\":\"Pat Yates\",\"yates,robertd\":\"Robert Yates\",\n",
        "    \"yates,ronald\":\"Ron Yates\",\"yates,timothyb\":\"Tim Yates\",\"yates,timothyj\":\"Tim Joe Yates\",\"yatescmartin\":\"Charles Yates\",\n",
        "    \"yatesjamesrob\":\"James Yates\",\"yatesjohnrob\":\"John Yates\",\"yates_nj-a\":\"Arthur Yates\",\"yates_nj-h\":\"Howard Yates\",\n",
        "    \"yeatesd_gn\":\"Gillian Yates\",\"yeatesd_mb\":\"Margaret Yates\",\"yeatesd_ws\":\"Will Yeates\",\n",
        "}\n",
        "\n",
        "# ---------- FTP helpers ----------\n",
        "def ftp_connect():\n",
        "    ftps = FTP_TLS()\n",
        "    ftps.connect(os.environ['FTP_HOST'], int(os.environ.get('FTP_PORT', 21)))\n",
        "    ftps.login(os.environ['FTP_USER'], os.environ['FTP_PASS'])\n",
        "    try:\n",
        "        ftps.prot_p()  # secure data connection\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        try:\n",
        "            ftps.cwd(FTP_DIR)\n",
        "        except Exception:\n",
        "            # Try to walk nested directories (e.g., \"a/b/c\")\n",
        "            parts = [p for p in FTP_DIR.split(\"/\") if p]\n",
        "            for p in parts:\n",
        "                try:\n",
        "                    ftps.mkd(p)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    # Ensure POSIX-style join regardless of OS\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ftp_download_if_exists(ftps: FTP_TLS, remote_name: str, local_name: str) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(f\"RETR {remote_name}\", f.write)\n",
        "        print(f\"⬇️  Pulled remote file: {remote_name} → {os.path.abspath(local_name)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"ℹ️  Remote not found or unreadable: {remote_name} ({e})\")\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps: FTP_TLS, local_path: str, remote_name: str):\n",
        "    try:\n",
        "        with open(local_path, \"rb\") as fh:\n",
        "            ftps.storbinary(f\"STOR {remote_name}\", fh)\n",
        "        print(f\"⬆️  Uploaded: {local_path} → {remote_name}\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Upload failed for {local_path} → {remote_name}: {e}\")\n",
        "\n",
        "# ---------- resolver loading ----------\n",
        "def _read_csv_flexible(path):\n",
        "    \"\"\"Read 2-column resolver CSV with flexible casing of headers.\n",
        "       Expected headers (case-insensitive): 'Match to', 'Unmasked'.\n",
        "       If headers not found, use the first two columns.\"\"\"\n",
        "    encodings = (\"utf-8-sig\",\"utf-8\",\"cp1252\",\"iso-8859-15\",\"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise last_err if last_err else ValueError(\"CSV read failed\")\n",
        "\n",
        "    if df.empty:\n",
        "        raise ValueError(\"CSV empty\")\n",
        "\n",
        "    cols_map = {str(c).strip().lower(): c for c in df.columns}\n",
        "    code_col = None\n",
        "    name_col = None\n",
        "    for k, c in cols_map.items():\n",
        "        if k.replace(\"_\",\" \").startswith(\"match to\"):\n",
        "            code_col = c\n",
        "        if k.startswith(\"unmasked\"):\n",
        "            name_col = c\n",
        "    if code_col is None or name_col is None:\n",
        "        # fallback to first 2 columns\n",
        "        if len(df.columns) < 2:\n",
        "            raise ValueError(\"CSV must have at least two columns\")\n",
        "        code_col, name_col = df.columns[0], df.columns[1]\n",
        "\n",
        "    sub = df[[code_col, name_col]].copy()\n",
        "    sub[code_col] = sub[code_col].astype(str).str.strip()\n",
        "    sub[name_col] = sub[name_col].astype(str).str.strip()\n",
        "    sub = sub[sub[code_col] != \"\"]\n",
        "    if sub.empty:\n",
        "        raise ValueError(\"CSV has no usable rows\")\n",
        "\n",
        "    # build dict (lowercase keys)\n",
        "    out = {}\n",
        "    for _, r in sub.iterrows():\n",
        "        k = r[code_col].strip().lower()\n",
        "        v = r[name_col].strip()\n",
        "        if k:\n",
        "            out[k] = v\n",
        "    return out\n",
        "\n",
        "def load_resolver():\n",
        "    # If remote read is enabled, try to pull resolver CSV from server first\n",
        "    if REMOTE_READ:\n",
        "        try:\n",
        "            ftps = ftp_connect()\n",
        "            ftp_download_if_exists(ftps, _remote_path(RESOLVER_CSV), RESOLVER_CSV)\n",
        "            ftps.quit()\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Remote resolver fetch skipped: {e}\")\n",
        "\n",
        "    # 1) fresh CSV\n",
        "    if os.path.exists(RESOLVER_CSV):\n",
        "        try:\n",
        "            mapping = _read_csv_flexible(RESOLVER_CSV)\n",
        "            # cache snapshot for next runs\n",
        "            with open(RESOLVER_CACHE_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(mapping, f, ensure_ascii=False, indent=0)\n",
        "            print(f\"Resolver: CSV ({len(mapping)}), cached\")\n",
        "            return mapping\n",
        "        except Exception:\n",
        "            pass\n",
        "    # 2) cache\n",
        "    if os.path.exists(RESOLVER_CACHE_JSON):\n",
        "        try:\n",
        "            with open(RESOLVER_CACHE_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "                mapping = json.load(f)\n",
        "            # ensure keys are lowercased\n",
        "            mapping = {str(k).strip().lower(): str(v) for k, v in mapping.items()}\n",
        "            print(f\"Resolver: CACHE ({len(mapping)})\")\n",
        "            return mapping\n",
        "        except Exception:\n",
        "            pass\n",
        "    # 3) built-in\n",
        "    print(f\"Resolver: BUILT-IN ({len(DEFAULT_MATCH_TO_UNMASKED)})\")\n",
        "    return DEFAULT_MATCH_TO_UNMASKED.copy()\n",
        "\n",
        "MATCH_TO_UNMASKED = load_resolver()\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns: return name\n",
        "            if name.lower() in lowmap: return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c): return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:→|&rarr;|\\u2192|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s): return []\n",
        "    if not isinstance(s, str): s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r'~+', ' ', str(text))\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token: return token\n",
        "    token = re.sub(r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "                   lambda m: m.group(1)+m.group(2).upper()+m.group(3)+m.group(4).upper(),\n",
        "                   token.lower())\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"+m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\"+m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name: return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i>0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    if not token: return (token,)\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper(): idx = i; break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper(): idx = i; break\n",
        "    if idx is None: return (token,)\n",
        "    surname = token[:idx]; given = token[idx:]\n",
        "    given_spaced = re.sub(r'(?<!^)([A-Z])', r' \\1', given)\n",
        "    return (f\"{given_spaced.strip()} {surname.strip()}\",)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = f\"{first} {last}\"\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def truncate_first(name: str, n: int = 4) -> str:\n",
        "    name = name.strip()\n",
        "    if not name: return name\n",
        "    parts = name.split()\n",
        "    return parts[0][:n] if len(parts) == 1 else f\"{parts[0][:n]} {parts[-1]}\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens: return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2: return (\"\", \"\")\n",
        "    def _norm(s): return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return f\"{greats}\\u00d7-great-grandparents\"\n",
        "\n",
        "\n",
        "def build_header(subject_name, cm_val, matchee_name, gens, husband, wife):\n",
        "    matchee_display = truncate_first(matchee_name, 4)\n",
        "    try:\n",
        "        cm_str = f\"{int(round(float(cm_val)))}\"\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        f\"{subject_name} is a {cm_str} cM cousin match to {matchee_display}, whose\",\n",
        "        f\"{degree_label} (back {gens} Gens)\",\n",
        "        \"are\", f\"{husband} & {wife}.\"\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r'\\.\\s*$', '', s)\n",
        "    return s\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str): return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(str(code).strip().lower(), str(code))\n",
        "# ========= CUT END   [1/3] ========================================================================\n",
        "\n",
        "\n",
        "# ========= CUT START [2/3] Transform & Column A ================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from io import StringIO\n",
        "\n",
        "# ---------- Input HTML path ----------\n",
        "HTML_PATH = \"/content/HTML_combined_df_with_value_labels.html\"\n",
        "\n",
        "# ---------- Load data table from HTML ----------\n",
        "if not os.path.exists(HTML_PATH):\n",
        "    raise FileNotFoundError(f\"HTML source file not found: {HTML_PATH}\")\n",
        "\n",
        "with open(HTML_PATH, \"r\", encoding=\"iso-8859-15\", errors=\"ignore\") as f:\n",
        "    soup = BeautifulSoup(f, \"html.parser\")\n",
        "\n",
        "tables = soup.find_all(\"table\")\n",
        "if not tables:\n",
        "    raise ValueError(\"No <table> found in HTML file.\")\n",
        "\n",
        "# Parse all tables safely (avoid FutureWarning by wrapping in StringIO)\n",
        "dfs = []\n",
        "for t in tables:\n",
        "    try:\n",
        "        html_str = str(t)\n",
        "        df_try = pd.read_html(StringIO(html_str))[0]\n",
        "        dfs.append(df_try)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if not dfs:\n",
        "    raise ValueError(\"Could not parse any HTML table into a DataFrame.\")\n",
        "\n",
        "# Heuristic: choose the table with the most “hits” on expected columns\n",
        "def _score(df):\n",
        "    cols = [str(c).lower() for c in df.columns]\n",
        "    targets = [\n",
        "        \"match to\", \"match\", \"name\",\n",
        "        \"cm\", \"c m\", \"centimorgan\", \"centimorgans\",\n",
        "        \"lineage\", \"path\", \"yates dna ancestral line\",\n",
        "        \"id#\",\"id\"\n",
        "    ]\n",
        "    return sum(any(t in c for t in targets) for c in cols)\n",
        "\n",
        "df = max(dfs, key=_score)\n",
        "print(f\"✅ Loaded HTML table — {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "# ---------- Detect columns using existing helper (from [1/3]) ----------\n",
        "subject_code_col = find_col(df, [r'^match\\s*to$'], [\"Match to\",\"Match\"])\n",
        "# Matchee: prefer Name; if missing, allow ID#/ID as fallback\n",
        "matchee_col      = find_col(df, [r'^name$', r'^id\\#?$', r'^id$'], [\"Name\",\"ID#\",\"ID\"])\n",
        "cm_col           = find_col(df, [r'\\bc[\\s\\-_]*m\\b', r'centi.?morgan'], [\"cM\",\"cm\",\"Centimorgans\"])\n",
        "path_col         = find_col(df, [r'\\blineage\\b', r'\\bpath', r'ancestral\\s*line', r'yates dna ancestral line'],\n",
        "                            [\"Yates DNA Ancestral Line\",\"path_tokens\"])\n",
        "husb_col         = find_col(df, [r'\\bhusband\\b', r'common.*husb'], [\"common_husband\",\"Common Husband\"])\n",
        "wife_col         = find_col(df, [r'\\bwife\\b', r'common.*wife'], [\"common_wife\",\"Common Wife\"])\n",
        "\n",
        "missing_core = [name for name, col in {\n",
        "    \"Match to\":subject_code_col, \"cM\":cm_col, \"path\":path_col\n",
        "}.items() if not col]\n",
        "if missing_core:\n",
        "    raise ValueError(f\"Missing core columns in HTML table: {missing_core}\\nAvailable: {list(df.columns)}\")\n",
        "\n",
        "if not matchee_col:\n",
        "    # Fallback: if we truly have no matchee-like column, use subject code (resolved) as a stand-in\n",
        "    # (We’ll also log this so you know Name/ID# was absent.)\n",
        "    matchee_col = subject_code_col\n",
        "    print(\"ℹ️  Matchee column not found; using 'Match to' (resolved) as matchee display.\")\n",
        "\n",
        "# ---------- Build display (same semantics as CSV pipeline) ----------\n",
        "headers, lineages = [], []\n",
        "for _, row in df.iterrows():\n",
        "    subject_name = normalize_person_name(resolve_match_to(row[subject_code_col]))\n",
        "    matchee_raw  = row.get(matchee_col)\n",
        "    matchee_name = normalize_person_name(matchee_raw) if pd.notna(matchee_raw) else \"\"\n",
        "    cm_val  = row[cm_col]\n",
        "    tokens  = split_tokens(row[path_col])\n",
        "    gens    = len(tokens)\n",
        "    if husb_col and wife_col and pd.notna(row.get(husb_col)) and pd.notna(row.get(wife_col)):\n",
        "        husband, wife = smart_titlecase(str(row[husb_col])), smart_titlecase(str(row[wife_col]))\n",
        "    else:\n",
        "        husband, wife = derive_common_from_first_token(tokens)\n",
        "    header_text  = build_header(subject_name, cm_val, matchee_name, gens, husband, wife)\n",
        "    sep = \" %s \" % ARROW_ENTITY\n",
        "    lineage_text = sep.join(tokens) if tokens else \"\"\n",
        "    headers.append(header_text); lineages.append(lineage_text)\n",
        "\n",
        "LINEAGE_HEADER_SAFE = LINEAGE_HEADER  # from [1/3]\n",
        "df[\"Match Summary\"] = headers\n",
        "df[LINEAGE_HEADER_SAFE]  = lineages\n",
        "display_df = df[[\"Match Summary\", LINEAGE_HEADER_SAFE]]\n",
        "\n",
        "# ---------- export Column A ----------\n",
        "display_df[[\"Match Summary\"]].to_csv(MATCH_COUSINS_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "print(\"✅ Wrote local CSV (Column A):\", os.path.abspath(MATCH_COUSINS_CSV))\n",
        "# ========= CUT END   [2/3] ================================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ========= CUT START [3/3] HTML + Upload + Report ================================================\n",
        "# ---------- HTML table ----------\n",
        "html_table = display_df.to_html(index=False, escape=False, classes=\"sortable\")\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">', 1)\n",
        "html_table = html_table.replace('<tbody>\\n<tr>', '<tbody>\\n<tr id=\"first-row\">', 1)\n",
        "html_table = html_table.replace('<th>Match Summary</th>', '<th>Match Summary</th>', 1)\n",
        "colgroup_html = (\"<colgroup>\\n  <col style=\\\"width:%dpx;\\\" />\\n  <col />\\n</colgroup>\\n\") % (COL_A_PX)\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n' + colgroup_html,\n",
        "    1\n",
        ")\n",
        "html_table_scrolling = '<div class=\"table-scroll\">\\n' + html_table + '\\n</div>'\n",
        "\n",
        "# ---------- CSS ----------\n",
        "TABLE_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \"  html { scroll-behavior: smooth; }\\n\"\n",
        "    \"  body { font-family: Georgia, \\\"Times New Roman\\\", serif; background:#ffffff; color:#222; margin:0; padding:0; line-height:1.5; }\\n\"\n",
        "    \"  .wrap { max-width:%dpx; margin:0 auto; background:#ffffff; padding:20px; padding-bottom:48px; }\\n\"\n",
        "    \"  a { color:#154b8b; text-decoration:none; } a:hover { text-decoration:underline; }\\n\"\n",
        "    \"  h1 { margin:0 0 6px 0; font-size:26px; line-height:1.2; }\\n\"\n",
        "    \"  .topbar { display:flex; justify-content:space-between; align-items:flex-start; gap:10px; margin-bottom:6px; }\\n\"\n",
        "    \"  .topbar .left, .topbar .right { font-size:12px; }\\n\"\n",
        "    \"  .updated { font-size:11px; color:#555; }\\n\"\n",
        "    \"  .sortbar { margin:6px 0 10px 0; font-size:13px; background:#b4c3e3; padding:6px 8px; border-radius:6px; display:flex; flex-wrap:wrap; gap:5px; align-items:center; }\\n\"\n",
        "    \"  .btn { display:inline-block; border:1px solid #3e5a97; background:#5b79b8; color:#fff; padding:4px 9px; text-decoration:none; cursor:pointer; border-radius:5px; line-height:1.2; }\\n\"\n",
        "    \"  .btn:hover { background:#4668aa; }\\n\"\n",
        "    \"  .table-scroll { max-height:70vh; overflow-y:auto; overflow-x:auto; border:1px solid #ddd; }\\n\"\n",
        "    \"  table.sortable { border-collapse:collapse; width:%dpx; table-layout:fixed; }\\n\"\n",
        "    \"  table.sortable th, table.sortable td { border:1px solid #ddd; padding:6px 8px; vertical-align:top; }\\n\"\n",
        "    \"  table.sortable th { background:#e3eaf8; text-align:left; position:sticky; top:0; z-index:2; box-shadow:0 1px 0 #ccc; }\\n\"\n",
        "    \"  table.sortable td { word-wrap:break-word; overflow-wrap:break-word; }\\n\"\n",
        "    \"  #first-row td { border-top:2px solid #999; }\\n\"\n",
        "    \"  .back-to-top { position:fixed; right:16px; bottom:16px; padding:6px 10px; border:1px solid #3e5a97; background:#5b79b8; color:#fff; cursor:pointer; border-radius:6px; font-size:12px; display:none; z-index:9999; }\\n\"\n",
        "    \"  .back-to-top:hover { background:#4668aa; }\\n\"\n",
        "    \"</style>\\n\"\n",
        ") % (TABLE_WIDTH_PX, TABLE_WIDTH_PX)\n",
        "\n",
        "# ---------- HTML ----------\n",
        "FULL_HTML = (\n",
        "    \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "    \"  \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "    \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n\"\n",
        "    \"<head>\\n\"\n",
        "    \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "    \"<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\" />\\n\"\n",
        "    \"<title>DNA Cousin Surname &mdash; REFACTOR (Stokes Display)</title>\\n\"\n",
        "    \"%s\"\n",
        "    \"</head>\\n\"\n",
        "    \"<body id=\\\"top\\\">\\n\"\n",
        "    \"<div class=\\\"wrap\\\">\\n\"\n",
        "    \"  <div class=\\\"topbar\\\">\\n\"\n",
        "    \"    <div class=\\\"left\\\">&laquo; <a href=\\\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\\\">Return to Study Home</a></div>\\n\"\n",
        "    \"    <div class=\\\"right\\\">\\n\"\n",
        "    \"      <div class=\\\"updated\\\" id=\\\"last-updated\\\"></div>\\n\"\n",
        "    \"    </div>\\n\"\n",
        "    \"  </div>\\n\"\n",
        "    \"  <h1>DNA Cousin Surname &mdash; REFACTOR (Stokes Display)</h1>\\n\"\n",
        "    \"  <div class=\\\"sortbar\\\">\\n\"\n",
        "    \"    <span class=\\\"btn\\\" data-sort-col=\\\"0\\\" data-sort-dir=\\\"asc\\\">Sort Match Summary &uarr;</span>\\n\"\n",
        "    \"    <span class=\\\"btn\\\" data-sort-col=\\\"0\\\" data-sort-dir=\\\"desc\\\">Sort Match Summary &darr;</span>\\n\"\n",
        "    \"    <span class=\\\"btn\\\" data-sort-col=\\\"1\\\" data-sort-dir=\\\"asc\\\">Sort Lineage &uarr;</span>\\n\"\n",
        "    \"    <span class=\\\"btn\\\" data-sort-col=\\\"1\\\" data-sort-dir=\\\"desc\\\">Sort Lineage &darr;</span>\\n\"\n",
        "    \"    <a class=\\\"btn\\\" href=\\\"gengen/images/cousin-calculator.jpg\\\" target=\\\"_blank\\\">Cousin Connection</a>\\n\"\n",
        "    \"    <a class=\\\"btn\\\" href=\\\"gengen/images/Shared_cM_Project_v4.jpg\\\" target=\\\"_blank\\\">Cousin by DNA</a>\\n\"\n",
        "    \"  </div>\\n\"\n",
        "    \"  %s\\n\"\n",
        "    \"</div>\\n\"\n",
        "    \"<button id=\\\"back-to-top\\\" class=\\\"back-to-top\\\" aria-label=\\\"Back to top\\\">&#9650; Top</button>\\n\"\n",
        "    \"<script type=\\\"text/javascript\\\">\\n\"\n",
        "    \"//<![CDATA[\\n\"\n",
        "    \"(function () {\\n\"\n",
        "    \"  function textOf(cell) { return (cell.textContent || cell.innerText || '').trim().toLowerCase(); }\\n\"\n",
        "    \"  function sortTable(tbl, colIndex, dir) {\\n\"\n",
        "    \"    var tbody = tbl.tBodies[0]; if (!tbody) return;\\n\"\n",
        "    \"    var rows = Array.prototype.slice.call(tbody.rows);\\n\"\n",
        "    \"    rows.sort(function(a, b) {\\n\"\n",
        "    \"      var A = textOf(a.cells[colIndex] || {}), B = textOf(b.cells[colIndex] || {});\\n\"\n",
        "    \"      if (A < B) return dir === 'asc' ? -1 : 1;\\n\"\n",
        "    \"      if (A > B) return dir === 'asc' ? 1 : -1; return 0;\\n\"\n",
        "    \"    });\\n\"\n",
        "    \"    var frag = document.createDocumentFragment();\\n\"\n",
        "    \"    for (var i=0; i<rows.length; i++) frag.appendChild(rows[i]);\\n\"\n",
        "    \"    tbody.appendChild(frag);\\n\"\n",
        "    \"  }\\n\"\n",
        "    \"  var tbl = document.getElementById('refactor-table'); if (!tbl) return;\\n\"\n",
        "    \"  var bar = document.querySelector('.sortbar');\\n\"\n",
        "    \"  if (bar) { bar.addEventListener('click', function(e) {\\n\"\n",
        "    \"    var btn = e.target.closest('.btn'); if (!btn) return;\\n\"\n",
        "    \"    var col = parseInt(btn.getAttribute('data-sort-col'), 10);\\n\"\n",
        "    \"    var dir = btn.getAttribute('data-sort-dir') || 'asc'; sortTable(tbl, col, dir);\\n\"\n",
        "    \"  }, false); }\\n\"\n",
        "    \"  if (tbl.tHead && tbl.tHead.rows.length) {\\n\"\n",
        "    \"    var ths = tbl.tHead.rows[0].cells;\\n\"\n",
        "    \"    for (var i=0; i<ths.length; i++) (function(idx){\\n\"\n",
        "    \"      var dir = 'asc'; ths[idx].addEventListener('click', function(){ dir = (dir === 'asc') ? 'desc' : 'asc'; sortTable(tbl, idx, dir); }, false);\\n\"\n",
        "    \"    })(i);\\n\"\n",
        "    \"  }\\n\"\n",
        "    \"  // Initial sort\\n\"\n",
        "    \"  sortTable(tbl, 1, 'asc');\\n\"\n",
        "    \"  // Last updated (topbar)\\n\"\n",
        "    \"  var el = document.getElementById('last-updated'); if (el) {\\n\"\n",
        "    \"    var d = new Date(document.lastModified || new Date());\\n\"\n",
        "    \"    function z(n){return (n<10?'0':'')+n;}\\n\"\n",
        "    \"    el.innerHTML = 'Last updated: ' + d.getFullYear() + '-' + z(d.getMonth()+1) + '-' + z(d.getDate()) + ' ' + z(d.getHours()) + ':' + z(d.getMinutes());\\n\"\n",
        "    \"  }\\n\"\n",
        "    \"  // Back to top listens to both window and .table-scroll\\n\"\n",
        "    \"  var btt = document.getElementById('back-to-top'); var container = document.querySelector('.table-scroll');\\n\"\n",
        "    \"  function onAnyScroll(){ var y=(window.scrollY||window.pageYOffset||0); var cy=container?container.scrollTop:0; btt.style.display = (y>200||cy>200)?'block':'none'; }\\n\"\n",
        "    \"  window.addEventListener('scroll', onAnyScroll, {passive:true}); if (container) container.addEventListener('scroll', onAnyScroll, {passive:true}); onAnyScroll();\\n\"\n",
        "    \"  btt.addEventListener('click', function(){ if (container) container.scrollTo({top:0, behavior:'smooth'}); window.scrollTo({top:0, behavior:'smooth'}); });\\n\"\n",
        "    \"})();\\n\"\n",
        "    \"//]]>\\n\"\n",
        "    \"</script>\\n\"\n",
        "    \"</body>\\n\"\n",
        "    \"</html>\\n\"\n",
        ") % (TABLE_CSS, html_table_scrolling)\n",
        "\n",
        "# ---------- Save & Upload ----------\n",
        "with open(LOCAL_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(FULL_HTML)\n",
        "\n",
        "with ftp_connect() as ftps:\n",
        "    try: ftps.delete(_remote_path(REMOTE_NAME))\n",
        "    except Exception: pass\n",
        "    ftp_upload_overwrite(ftps, LOCAL_NAME, _remote_path(REMOTE_NAME))\n",
        "\n",
        "    if UPLOAD_COLUMN_A and os.path.exists(MATCH_COUSINS_CSV):\n",
        "        ftp_upload_overwrite(ftps, MATCH_COUSINS_CSV, _remote_path(MATCH_COUSINS_CSV))\n",
        "\n",
        "    if UPLOAD_CACHE_JSON and os.path.exists(RESOLVER_CACHE_JSON):\n",
        "        ftp_upload_overwrite(ftps, RESOLVER_CACHE_JSON, _remote_path(RESOLVER_CACHE_JSON))\n",
        "\n",
        "# ---------- Report: Resolver usage counts (CSV + console preview) ----------\n",
        "from collections import Counter\n",
        "\n",
        "# Normalize subject codes from the dataset\n",
        "used_series = df[subject_code_col].astype(str).map(lambda x: str(x).strip().lower())\n",
        "counts = Counter([c for c in used_series if c and c != \"nan\"])\n",
        "\n",
        "rows = []\n",
        "all_keys = set(MATCH_TO_UNMASKED.keys())\n",
        "\n",
        "# 1) Every resolver entry with a count (zeros included)\n",
        "for code in sorted(all_keys):\n",
        "    rows.append((code, MATCH_TO_UNMASKED.get(code, \"\"), counts.get(code, 0)))\n",
        "\n",
        "# 2) Any codes used in data that aren't in resolver (flag as unmapped)\n",
        "extra_codes = sorted(set(counts.keys()) - all_keys)\n",
        "for code in extra_codes:\n",
        "    rows.append((code, \"(unmapped)\", counts.get(code, 0)))\n",
        "\n",
        "usage_df = pd.DataFrame(rows, columns=[\"Match to (code)\", \"Unmasked\", \"Count\"])\n",
        "usage_df.sort_values([\"Count\", \"Match to (code)\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "# Save CSV locally and upload\n",
        "RESOLVER_USAGE_CSV = \"resolver_usage_report.csv\"\n",
        "usage_df.to_csv(RESOLVER_USAGE_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "print(\"✅ Wrote resolver usage CSV:\", os.path.abspath(RESOLVER_USAGE_CSV))\n",
        "\n",
        "try:\n",
        "    with ftp_connect() as ftps:\n",
        "        ftp_upload_overwrite(ftps, RESOLVER_USAGE_CSV, _remote_path(RESOLVER_USAGE_CSV))\n",
        "    print(f\"✅ Uploaded resolver usage CSV: https://yates.one-name.net/{RESOLVER_USAGE_CSV}\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️  Upload of resolver usage CSV failed: {e}\")\n",
        "\n",
        "# Console preview\n",
        "print(f\"📊 Resolver usage (top 30 of {len(usage_df)} rows):\")\n",
        "for _, r in usage_df.head(30).iterrows():\n",
        "    code = str(r['Match to (code)'])\n",
        "    unm  = str(r['Unmasked'])\n",
        "    cnt  = int(r['Count'])\n",
        "    print(f\"   {code:<20s} → {unm[:28]:<28s} : {cnt}\")\n",
        "# ========= CUT END   [3/3] =======================================================================\n"
      ],
      "metadata": {
        "id": "Ipkws6LmIBAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2474a5dd-bcb4-4728-9e43-0533f496a3fc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Pulled remote file: match_to_unmasked.csv → /content/match_to_unmasked.csv\n",
            "Resolver: CSV (77), cached\n",
            "✅ Loaded HTML table — 1623 rows, 7 columns\n",
            "✅ Wrote local CSV (Column A): /content/the_match_cousins.csv\n",
            "⬆️  Uploaded: dna_cousin_surname_REFACTOR.htm → dna_cousin_surname_REFACTOR.htm\n",
            "⬆️  Uploaded: the_match_cousins.csv → the_match_cousins.csv\n",
            "✅ Wrote resolver usage CSV: /content/resolver_usage_report.csv\n",
            "⬆️  Uploaded: resolver_usage_report.csv → resolver_usage_report.csv\n",
            "✅ Uploaded resolver usage CSV: https://yates.one-name.net/resolver_usage_report.csv\n",
            "📊 Resolver usage (top 30 of 89 rows):\n",
            "   yates,ronald         → Ron Yates                    : 135\n",
            "   marmar               → Mary Marshall                : 69\n",
            "   y-dna                → (unmapped)                   : 68\n",
            "   hatpat               → Pat Hatfield                 : 64\n",
            "   wishardglen          → Glen Wishard                 : 60\n",
            "   yates,patricial      → Pat Yates                    : 54\n",
            "   yates,johnh          → J Harry Yates                : 47\n",
            "   klingal              → Al Kling                     : 46\n",
            "   weeksjerri           → Jerr Weeks                   : 45\n",
            "   yates,timothyb       → T Brian Yates                : 42\n",
            "   fridine              → Nadi Brown                   : 40\n",
            "   yatesjamesrob        → Jame Yates                   : 39\n",
            "   birdwelljac          → Jaca Yates                   : 34\n",
            "   girtain,alma         → Alma Girtain                 : 34\n",
            "   smittybec            → Reb Smith                    : 31\n",
            "   cagilaba,leigh       → Leigh Yates                  : 30\n",
            "   yates,andreal        → Andi Yates                   : 30\n",
            "   bucb                 → Beth Buckley                 : 29\n",
            "   yatesjd              → J Dav Yates                  : 28\n",
            "   hendricksjas         → Jim Hendricks                : 27\n",
            "   yates_nj-a           → Arth Yates                   : 26\n",
            "   girtain,kathryn      → Kath Girtain                 : 24\n",
            "   littleil             → Ilen Little                  : 24\n",
            "   girtain,andy         → Andy Girtain                 : 23\n",
            "   hellopt              → Pat Thomas                   : 23\n",
            "   yeatesd_tm           → Tim Yeates                   : 23\n",
            "   cherylod             → Cheryl Yates                 : 22\n",
            "   leedon               → Donna Lee                    : 21\n",
            "   yates,timothyj       → T Joe Yates                  : 21\n",
            "   harper,mable         → (unmapped)                   : 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gold Cell 3 for Y-DNA Grid with Auto-Adjusting Column Widths\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ── PATHS ─────────────────────────────────────────────────────────────────\n",
        "combo_csv  = \"/content/y_dna_user_detail_combo.csv\"\n",
        "output_csv = \"/content/y_dna_grid.csv\"\n",
        "output_htm = \"/content/y_dna_grid.htm\"\n",
        "\n",
        "# ── 1) Load vertical data ─────────────────────────────────────────────────\n",
        "df = pd.read_csv(combo_csv)\n",
        "\n",
        "# Rename “Date” → “Era”\n",
        "if \"Date\" in df.columns:\n",
        "    df.rename(columns={\"Date\": \"Era\"}, inplace=True)\n",
        "\n",
        "# ── 2) Insert Action *after* Era ──────────────────────────────────────────\n",
        "# Era is at index 1, so Action goes at index 2\n",
        "df.insert(2, \"Action\", [\"→\"] * len(df))\n",
        "\n",
        "# ── 3) Save vertical CSV ─────────────────────────────────────────────────\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"✅ Saved vertical grid CSV to {output_csv}\")\n",
        "\n",
        "# ── 4) Build HTML ─────────────────────────────────────────────────────────\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "ts  = now.strftime(\"%-m/%-d/%y, %-I:%M %p EDT\")\n",
        "cols = df.columns.tolist()\n",
        "\n",
        "html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head><meta charset=\"UTF-8\"><title>Yates Y-DNA Grid</title>\n",
        "<style>\n",
        "body {{\n",
        "  background: #faf9d3;\n",
        "  font-family: Arial, sans-serif;\n",
        "  font-size: 14px;\n",
        "  margin: 0;\n",
        "  padding: 0;\n",
        "}}\n",
        ".container {{\n",
        "  padding: 10px;\n",
        "}}\n",
        ".table-container {{\n",
        "  overflow-x: auto;\n",
        "  max-height: 80vh;\n",
        "}}\n",
        "table {{\n",
        "  border: 2px solid #333;\n",
        "  border-collapse: collapse;\n",
        "  margin: 0 auto;\n",
        "}}\n",
        "table.mainsection {{\n",
        "  /* allows CSS targeting of blank under “Year” */\n",
        "}}\n",
        "thead {{\n",
        "  display: table-header-group;\n",
        "}}\n",
        "thead th {{\n",
        "  position: sticky;\n",
        "  top: 0;\n",
        "  background: #333;\n",
        "  color: #fff;\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  z-index: 3;\n",
        "}}\n",
        "a {{\n",
        "  color: #fff;\n",
        "  text-decoration: underline;\n",
        "}}\n",
        ".era {{\n",
        "  background: #666;\n",
        "  color: #eee;\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  font-size: 0.9em;\n",
        "}}\n",
        ".action {{\n",
        "  background: #fff;\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  text-align: center;\n",
        "}}\n",
        "td {{\n",
        "  padding: 6px;\n",
        "  border: 1px solid #999;\n",
        "  text-align: center;\n",
        "}}\n",
        "th:nth-child(n+4),\n",
        "td:nth-child(n+4) {{\n",
        "  border: 1px solid #333;\n",
        "}}\n",
        ".match {{\n",
        "  background: #fff;\n",
        "}}\n",
        ".blank {{\n",
        "  background: #ccc;\n",
        "  color: #ccc;\n",
        "}}\n",
        "/* make the blank under the “Year” header match the era-cell background */\n",
        "table.mainsection td.blank:nth-child(2) {{\n",
        "  background-color: #fdfcd0;\n",
        "}}\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"container\">\n",
        "    <h1 style=\"text-align:center\">Yates Y-DNA Grid</h1>\n",
        "    <p style=\"text-align:center;font-size:0.9em\">Updated: {ts}</p>\n",
        "    <p style=\"text-align:center;margin-bottom:12px\">\n",
        "      <a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\">\n",
        "        Return to DNA Cousin Surname Study\n",
        "      </a>\n",
        "    </p>\n",
        "    <div class=\"table-container\">\n",
        "      <table class=\"mainsection\">\n",
        "        <thead>\n",
        "          <tr>\"\"\"\n",
        "\n",
        "# Header row\n",
        "for i, c in enumerate(cols):\n",
        "    if i == 0:\n",
        "        html += \"<th>SNP</th>\"\n",
        "    elif i == 1:\n",
        "        html += \"<th>Year</th>\"\n",
        "    elif i == 2:\n",
        "        html += \"<th>Action</th>\"\n",
        "    else:\n",
        "        pid = c.split(\"-\")[0].upper()\n",
        "        html += (\n",
        "          '<th>'\n",
        "          f'<a href=\"https://yates.one-name.net/tng/verticalchart.php?'\n",
        "          f'personID={pid}&tree=tree1&parentset=0&display=vertical&generations=15\">{c}</a>'\n",
        "          '</th>'\n",
        "        )\n",
        "\n",
        "html += \"\"\"\n",
        "          </tr>\n",
        "        </thead>\n",
        "        <tbody>\"\"\"\n",
        "\n",
        "# Data rows\n",
        "for _, row in df.iterrows():\n",
        "    html += \"<tr>\"\n",
        "    for i, c in enumerate(cols):\n",
        "        v = row[c]\n",
        "        if i == 0:\n",
        "            html += f\"<td>{v}</td>\"\n",
        "        elif i == 1:\n",
        "            html += '<td class=\"blank\">–</td>' if pd.isna(v) or not str(v).strip() else f'<td class=\"era\">{v}</td>'\n",
        "        elif i == 2:\n",
        "            html += '<td class=\"blank\">–</td>' if pd.isna(v) or not str(v).strip() else f'<td class=\"action\">{v}</td>'\n",
        "        else:\n",
        "            html += '<td class=\"blank\">–</td>' if pd.isna(v) or not str(v).strip() else f'<td class=\"match\">{v}</td>'\n",
        "    html += \"</tr>\"\n",
        "\n",
        "html += \"\"\"\n",
        "        </tbody>\n",
        "      </table>\n",
        "    </div>\n",
        "  </div>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "with open(output_htm, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html)\n",
        "print(f\"✅ Saved vertical XHTML to {output_htm}\")\n",
        "\n",
        "# ── 5) FTP upload ────────────────────────────────────────────────────────\n",
        "ftp = FTP_TLS()\n",
        "ftp.connect(os.environ[\"FTP_HOST\"], int(os.environ[\"FTP_PORT\"]))\n",
        "ftp.login(os.environ[\"FTP_USER\"], os.environ[\"FTP_PASS\"])\n",
        "ftp.prot_p()\n",
        "for path in (output_csv, output_htm):\n",
        "    fn = os.path.basename(path)\n",
        "    try:\n",
        "        ftp.delete(fn)\n",
        "    except:\n",
        "        pass\n",
        "    with open(path, \"rb\") as fp:\n",
        "        ftp.storbinary(f\"STOR {fn}\", fp)\n",
        "ftp.quit()\n",
        "print(\"✅ Uploaded CSV & HTML to server\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xecfLjgt4h-y",
        "outputId": "3cb35ada-e4ec-4b5a-ef03-12e5ede80527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved vertical grid CSV to /content/y_dna_grid.csv\n",
            "✅ Saved vertical XHTML to /content/y_dna_grid.htm\n",
            "✅ Uploaded CSV & HTML to server\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ── CONFIG ───────────────────────────────────────────────────────────────\n",
        "info_csv   = \"/content/haplogroup_info.csv\"\n",
        "user_csv   = \"/content/y_dna_user_detail.csv\"\n",
        "output_csv = \"/content/y_dna_grid.csv\"\n",
        "output_htm = \"/content/y_dna_grid.htm\"\n",
        "\n",
        "# ── 1) Load & prepare haplogroup info ───────────────────────────────────\n",
        "df_info = pd.read_csv(info_csv)\n",
        "if \"Date\" in df_info.columns:\n",
        "    df_info.rename(columns={\"Date\": \"Era\"}, inplace=True)\n",
        "df_info = df_info.loc[df_info[\"Haplogroup\"].drop_duplicates().index]\n",
        "hap_order = df_info[\"Haplogroup\"].tolist()\n",
        "era_map   = dict(zip(df_info[\"Haplogroup\"], df_info.get(\"Era\", [\"\"] * len(df_info))))\n",
        "\n",
        "# ── 2) Load user detail table ───────────────────────────────────────────\n",
        "df_users = pd.read_csv(user_csv)\n",
        "if \"User_ID\" not in df_users.columns:\n",
        "    df_users.rename(columns={df_users.columns[0]: \"User_ID\"}, inplace=True)\n",
        "user_chains = [\n",
        "    [str(v) for v in row.drop(labels=[\"User_ID\"]).tolist() if pd.notna(v) and str(v).strip()]\n",
        "    for _, row in df_users.iterrows()\n",
        "]\n",
        "\n",
        "# ── 3) Insert new SNPs after parent ──────────────────────────────────────\n",
        "for chain in user_chains:\n",
        "    prev = None\n",
        "    for h in chain:\n",
        "        if prev and h not in hap_order:\n",
        "            idx = hap_order.index(prev)\n",
        "            hap_order.insert(idx + 1, h)\n",
        "        prev = h\n",
        "# Build final eras list\n",
        "eras = [era_map.get(h, \"\") for h in hap_order]\n",
        "\n",
        "# ── 4) Build horizontal grid DataFrame ───────────────────────────────────\n",
        "for h in hap_order:\n",
        "    if h not in df_users.columns:\n",
        "        df_users[h] = \"\"\n",
        "df_grid_h = df_users[[\"User_ID\"] + hap_order]\n",
        "\n",
        "# ── 5) Transform to vertical layout ─────────────────────────────────────\n",
        "df_vert = df_grid_h.set_index(\"User_ID\").T\n",
        "# Insert Era as first column\n",
        "df_vert.insert(0, 'Era', eras)\n",
        "df_vert.index.name = 'SNP'\n",
        "df_grid = df_vert.reset_index()\n",
        "\n",
        "# ── 6) Save vertical CSV ─────────────────────────────────────────────────\n",
        "df_grid.to_csv(output_csv, index=False)\n",
        "print(f\"✅ Vertical grid CSV saved to {output_csv}\")\n",
        "\n",
        "# ── 7) Generate XHTML (vertical) ────────────────────────────────────────\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "ts  = now.strftime(\"%-m/%-d/%y, %-I:%M %p EDT\")\n",
        "\n",
        "template = '''<!DOCTYPE html>\n",
        "<html><head><meta charset=\"UTF-8\"><title>Yates Y-DNA Grid</title>\n",
        "<style>\n",
        "  body { background:#faf9d3; font-family:Arial,Helvetica,sans-serif; font-size:14px; }\n",
        "  table { width:100%; border:1px solid #333; border-collapse:collapse; table-layout:auto; }\n",
        "  th { background:#333; color:#fff; padding:6px; border:1px solid #999; }\n",
        "  .era { background:#666; color:#eee; padding:6px; border:1px solid #999; font-size:0.9em; }\n",
        "  td { padding:6px; border:1px solid #999; text-align:center; white-space:nowrap; }\n",
        "  .match { background:#fff; }\n",
        "  .blank { background:#ccc; color:#ccc; }\n",
        "</style>\n",
        "</head><body>\n",
        "  <h1 style=\"text-align:center;\">Yates Y-DNA Grid</h1>\n",
        "  <table>\n",
        "'''  # end template\n",
        "\n",
        "# Build header row\n",
        "cols = df_grid.columns.tolist()\n",
        "header_html = '<tr><th>SNP</th><th>Era</th>' + ''.join(f'<th>{u}</th>' for u in cols[2:]) + '</tr>'\n",
        "\n",
        "# Build data rows\n",
        "rows_html = []\n",
        "for _, row in df_grid.iterrows():\n",
        "    cells = []\n",
        "    for u in cols[2:]:\n",
        "        v = row[u]\n",
        "        if pd.isna(v) or not str(v).strip():\n",
        "            cells.append('<td class=\"blank\">–</td>')\n",
        "        else:\n",
        "            cells.append(f'<td class=\"match\">{v}</td>')\n",
        "    rows_html.append(f'<tr><td>{row[\"SNP\"]}</td><td class=\"era\">{row[\"Era\"]}</td>' + ''.join(cells) + '</tr>')\n",
        "\n",
        "# Combine and save HTML\n",
        "html = template + header_html + '\\n' + '\\n'.join(rows_html) + f'''\n",
        "  </table>\n",
        "  <p style=\"text-align:right;font-size:0.9em;\">Updated: {ts}</p>\n",
        "</body>\n",
        "</html>'''\n",
        "with open(output_htm, 'w', encoding='utf-8') as f:\n",
        "    f.write(html)\n",
        "print(f\"✅ Vertical XHTML Grid saved to {output_htm}\")\n",
        "\n",
        "# ── 8) FTP Upload ───────────────────────────────────────────────────────\n",
        "ftp = FTP_TLS()\n",
        "ftp.connect(os.environ['FTP_HOST'], int(os.environ.get('FTP_PORT',21)))\n",
        "ftp.login(os.environ['FTP_USER'], os.environ['FTP_PASS'])\n",
        "ftp.prot_p()\n",
        "for path in [output_csv, output_htm]:\n",
        "    name = os.path.basename(path)\n",
        "    try: ftp.delete(name)\n",
        "    except: pass\n",
        "    with open(path,'rb') as fp:\n",
        "        ftp.storbinary(f\"STOR {name}\", fp)\n",
        "ftp.quit()\n",
        "print(\"✅ Uploaded to server.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFqM0kUliAqX",
        "outputId": "d6d54e4c-5b1c-497b-f896-757477e43e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vertical grid CSV saved to /content/y_dna_grid.csv\n",
            "✅ Vertical XHTML Grid saved to /content/y_dna_grid.htm\n",
            "✅ Uploaded to server.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Y-DNA cell 1\n",
        "\n",
        "# === Cell 1: New user settings ===\n",
        "USER_ID       = 'I56217'  # the new column header\n",
        "PATH_STRING   = (      # the SNP chain for this user\n",
        "    \"R-M207 > R-M173 > R-M343 > R-M269 > R-FT266064 > R-FT266579 > R-FTF17042\"\n",
        ")\n",
        "INSERT_MISSING = True       # if True, adds any SNPs from PATH_STRING that aren't yet rows\n",
        "MASTER_CSV     = '/content/y_dna_user_detail_combo.csv'\n",
        "UPDATED_CSV    = '/content/y_dna_user_detail_combo_updated.csv'\n"
      ],
      "metadata": {
        "id": "0bO8B-Gnls49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load → Append User → Save\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load the existing master CSV\n",
        "df = pd.read_csv(MASTER_CSV)\n",
        "\n",
        "# 2) Normalize the first column name to 'SNP' for easy matching\n",
        "first_col = df.columns[0]\n",
        "if first_col != 'SNP':\n",
        "    df.rename(columns={first_col: 'SNP'}, inplace=True)\n",
        "\n",
        "# 3) Parse the new user's SNP chain\n",
        "chain = PATH_STRING.split('>')\n",
        "\n",
        "# 4) Optionally insert any SNPs not yet present (appends at bottom)\n",
        "if INSERT_MISSING:\n",
        "    missing = [s for s in chain if s not in df['SNP'].values]\n",
        "    if missing:\n",
        "        df = pd.concat([df, pd.DataFrame([{'SNP': s} for s in missing])],\n",
        "                       ignore_index=True)\n",
        "\n",
        "# 5) Create the new user column in the next free position\n",
        "df[USER_ID] = ''\n",
        "\n",
        "# 6) Populate: copy the SNP value into that column where it matches the chain\n",
        "df.loc[df['SNP'].isin(chain), USER_ID] = df['SNP']\n",
        "\n",
        "# 7) Save the updated CSV back to /content\n",
        "df.to_csv(UPDATED_CSV, index=False)\n",
        "print(f\"✅ Updated CSV saved to {UPDATED_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjd2-uEdmJKR",
        "outputId": "b6746fb3-cc40-4e7e-d787-d1a5ec09ad00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated CSV saved to /content/y_dna_user_detail_combo_updated.csv\n"
          ]
        }
      ]
    }
  ]
}