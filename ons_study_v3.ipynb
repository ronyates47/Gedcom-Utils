{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1RROHlBgtXAYWOqDWQkIX8NCciZBqPcV3",
      "authorship_tag": "ABX9TyMWB3vf4Tf7YgVAzqq3pHt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/ons_study_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIP"
      ],
      "metadata": {
        "id": "XtvXRl-lcavJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "721a7104-0754-482e-fe2c-d1078c23a1bb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 0] Auto-Download Support Files (Master Edition)\n",
        "# Logic:\n",
        "#   1. Always downloads 'match_to_unmasked.csv' from the server (Web/HTTP).\n",
        "#   2. Intelligent GEDCOM Handling:\n",
        "#      - CHECKS LOCAL: If you uploaded a .ged to Colab, it uses that (prioritizing the newest).\n",
        "#      - FALLBACK TO SERVER: If no local .ged exists, it connects via FTP and pulls the latest one.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import requests\n",
        "import traceback\n",
        "import socket\n",
        "from datetime import datetime\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      AUTO-DOWNLOADER & GEDCOM MANAGER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION & CREDENTIALS\n",
        "# ==============================================================================\n",
        "CSV_URL           = \"https://yates.one-name.net/ons-study/match_to_unmasked.csv\"\n",
        "LOCAL_CSV         = \"match_to_unmasked.csv\"\n",
        "GEDCOM_REMOTE_DIR = \"/tng/gedcom\"\n",
        "LOCAL_GED_DIR     = \"/content\"\n",
        "\n",
        "# Load Credentials (Env or Secrets)\n",
        "HOST = os.environ.get(\"FTP_HOST\", \"\").strip()\n",
        "USER = os.environ.get(\"FTP_USER\", \"\").strip()\n",
        "PASS = os.environ.get(\"FTP_PASS\", \"\").strip()\n",
        "try: PORT = int(os.environ.get(\"FTP_PORT\", 21))\n",
        "except: PORT = 21\n",
        "\n",
        "if not HOST:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HOST = userdata.get(\"FTP_HOST\")\n",
        "        USER = userdata.get(\"FTP_USER\")\n",
        "        PASS = userdata.get(\"FTP_PASS\")\n",
        "    except: pass\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CSV DOWNLOADER (HTTP)\n",
        "# ==============================================================================\n",
        "def download_csv():\n",
        "    print(f\"\\n[1] Support File: {LOCAL_CSV}\")\n",
        "    print(f\"    - Downloading from: {CSV_URL}\")\n",
        "    try:\n",
        "        r = requests.get(CSV_URL)\n",
        "        r.raise_for_status()\n",
        "        with open(LOCAL_CSV, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        print(f\"    - [SUCCESS] Saved {len(r.content)} bytes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"    - [CRITICAL ERROR] Could not download CSV: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. FTP HELPER FUNCTIONS\n",
        "# ==============================================================================\n",
        "def _ftps_connect():\n",
        "    if not HOST or not USER or not PASS:\n",
        "        raise RuntimeError(\"Missing FTP Credentials in Secrets/Env.\")\n",
        "    ftps = FTP_TLS(timeout=60)\n",
        "    ftps.connect(HOST, PORT)\n",
        "    ftps.auth()\n",
        "    ftps.login(USER, PASS)\n",
        "    try: ftps.prot_p(); ftps.set_pasv(True)\n",
        "    except: pass\n",
        "    return ftps\n",
        "\n",
        "def _safe_nlst(ftps):\n",
        "    try:\n",
        "        return ftps.nlst()\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. INTELLIGENT GEDCOM LOGIC\n",
        "# ==============================================================================\n",
        "def _pick_local_gedcom_if_present():\n",
        "    \"\"\"\n",
        "    Prefer any *.ged already in /content.\n",
        "    Deterministic: newest mtime, tie-breaker lexicographic.\n",
        "    \"\"\"\n",
        "    # Use absolute path to ensure we scan the right place\n",
        "    search_path = os.path.join(LOCAL_GED_DIR, \"*.ged\")\n",
        "    geds = glob.glob(search_path)\n",
        "\n",
        "    if not geds:\n",
        "        return None\n",
        "\n",
        "    def _key(p):\n",
        "        try: return (float(os.path.getmtime(p)), str(p))\n",
        "        except: return (0.0, str(p))\n",
        "\n",
        "    geds.sort(key=_key, reverse=True)\n",
        "    chosen = geds[0]\n",
        "\n",
        "    try:\n",
        "        ts = datetime.fromtimestamp(os.path.getmtime(chosen)).isoformat(sep=\" \", timespec=\"seconds\")\n",
        "    except: ts = \"unknown\"\n",
        "\n",
        "    print(f\"    - [LOCAL FOUND] Using manual upload: {os.path.basename(chosen)} (mtime={ts})\")\n",
        "    return chosen\n",
        "\n",
        "def _choose_latest_gedcom(ftps, names):\n",
        "    \"\"\"Pick the newest *.ged using MDTM if available; fallback = last alphabetically.\"\"\"\n",
        "    ged_files = [n for n in names if n.lower().endswith(\".ged\")]\n",
        "    if not ged_files: return None\n",
        "\n",
        "    latest_name = None\n",
        "    latest_ts = None\n",
        "\n",
        "    for nm in ged_files:\n",
        "        ts = None\n",
        "        try:\n",
        "            resp = ftps.sendcmd(\"MDTM \" + nm)  # '213 YYYYMMDDhhmmss'\n",
        "            parts = resp.strip().split()\n",
        "            if len(parts) == 2 and parts[0] == \"213\":\n",
        "                ts = parts[1]\n",
        "        except: ts = None\n",
        "\n",
        "        # Fallback if server doesn't support MDTM\n",
        "        if ts is None: ts = \"00000000000000\" + nm\n",
        "\n",
        "        if latest_ts is None or ts > latest_ts:\n",
        "            latest_ts = ts\n",
        "            latest_name = nm\n",
        "    return latest_name\n",
        "\n",
        "def pull_latest_gedcom_if_needed():\n",
        "    print(f\"\\n[2] GEDCOM Manager\")\n",
        "\n",
        "    # 1. Check Local First\n",
        "    local = _pick_local_gedcom_if_present()\n",
        "    if local:\n",
        "        print(\"    - [SKIP] Skipping server pull (Local file takes priority).\")\n",
        "        return\n",
        "\n",
        "    # 2. Pull from Server\n",
        "    print(f\"    - Connecting to {HOST} :: {GEDCOM_REMOTE_DIR} ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "\n",
        "        # Navigate\n",
        "        try: ftps.cwd(\"/\")\n",
        "        except: pass\n",
        "        for seg in [p for p in GEDCOM_REMOTE_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "        # List files\n",
        "        names = _safe_nlst(ftps)\n",
        "        latest = _choose_latest_gedcom(ftps, names)\n",
        "\n",
        "        if not latest:\n",
        "            print(f\"    - [WARN] No .ged files found in remote folder.\")\n",
        "            ftps.quit()\n",
        "            return\n",
        "\n",
        "        print(f\"    - [FOUND] Latest on server: {latest}\")\n",
        "\n",
        "        # Clean old local GEDCOMs to prevent confusion\n",
        "        for fname in os.listdir(LOCAL_GED_DIR):\n",
        "            if fname.lower().endswith(\".ged\"):\n",
        "                try: os.remove(os.path.join(LOCAL_GED_DIR, fname))\n",
        "                except: pass\n",
        "\n",
        "        # Download\n",
        "        local_path = os.path.join(LOCAL_GED_DIR, latest)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR \" + latest, f.write)\n",
        "\n",
        "        sz = os.path.getsize(local_path)\n",
        "        print(f\"    - [DOWNLOADED] Saved to {local_path} ({sz} bytes)\")\n",
        "\n",
        "        try: ftps.quit()\n",
        "        except: pass\n",
        "\n",
        "    except Exception:\n",
        "        print(\"    - [WARN] GEDCOM pull failed.\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. EXECUTION\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Step A: Get the CSV Map\n",
        "    download_csv()\n",
        "\n",
        "    # Step B: Get the GEDCOM (Local or Remote)\n",
        "    pull_latest_gedcom_if_needed()\n",
        "\n",
        "    # Final Status\n",
        "    print(\"\\n[DONE] Workspace Ready.\")\n",
        "    geds = glob.glob(os.path.join(LOCAL_GED_DIR, \"*.ged\"))\n",
        "    if geds:\n",
        "        print(f\"       Active GEDCOM: {os.path.basename(geds[0])}\")\n",
        "        print(f\"       (Make sure CELL 1 uses: TARGET_GEDCOM = \\\"{os.path.basename(geds[0])}\\\")\")\n",
        "    else:\n",
        "        print(\"       [WARNING] No GEDCOM found in workspace!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q48fATHZT21M",
        "outputId": "14f9ef67-aaa7-410a-882c-0f3842e6cfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      AUTO-DOWNLOADER & GEDCOM MANAGER\n",
            "============================================================\n",
            "\n",
            "[1] Support File: match_to_unmasked.csv\n",
            "    - Downloading from: https://yates.one-name.net/ons-study/match_to_unmasked.csv\n",
            "    - [SUCCESS] Saved 2785 bytes.\n",
            "\n",
            "[2] GEDCOM Manager\n",
            "    - Connecting to ftp.one-name.net :: /tng/gedcom ...\n",
            "    - [FOUND] Latest on server: yates_study_2025.ged\n",
            "    - [DOWNLOADED] Saved to /content/yates_study_2025.ged (39255900 bytes)\n",
            "\n",
            "[DONE] Workspace Ready.\n",
            "       Active GEDCOM: yates_study_2025.ged\n",
            "       (Make sure CELL 1 uses: TARGET_GEDCOM = \"yates_study_2025.ged\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 3] GEDCOM Patcher (Permanent Unmasking)\n",
        "# Logic: Hard-codes the real names into the GEDCOM file so we stop relying on lookups.\n",
        "#        - TARGETS only '2 NPFX' tags to be safe.\n",
        "#        - REPLACES codes with Real Names globally.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      GEDCOM UNMASKING PATCHER\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# CONFIGURATION\n",
        "INPUT_GEDCOM  = \"yates_study_2025.ged\"       # Your current file\n",
        "OUTPUT_GEDCOM = \"yates_study_2025_UNMASKED.ged\" # The new clean file\n",
        "KEY_FILE      = \"match_to_unmasked.csv\"\n",
        "\n",
        "# 1. LOAD THE KEYS (Robust Reader)\n",
        "unmask_map = {}\n",
        "print(f\"[1] Reading {KEY_FILE}...\")\n",
        "\n",
        "if os.path.exists(KEY_FILE):\n",
        "    try:\n",
        "        with open(KEY_FILE, mode='r', encoding='utf-8-sig', errors='replace') as f:\n",
        "            # Sniff format just in case\n",
        "            try: dialect = csv.Sniffer().sniff(f.read(1024)); f.seek(0)\n",
        "            except: dialect = 'excel'; f.seek(0)\n",
        "\n",
        "            reader = csv.reader(f, dialect)\n",
        "            for row in reader:\n",
        "                if len(row) < 2: continue\n",
        "\n",
        "                # CLEANING: Strip spaces, handle \"yates,tim\" quotes automatically\n",
        "                code = row[0].strip()\n",
        "                name = row[1].strip()\n",
        "\n",
        "                # Skip header\n",
        "                if code.lower() == \"code\": continue\n",
        "\n",
        "                if code and name:\n",
        "                    # Map CODE -> NAME\n",
        "                    unmask_map[code] = name\n",
        "\n",
        "        print(f\"    - Loaded {len(unmask_map)} replacements.\")\n",
        "        # Debug check\n",
        "        if \"girtjosh\" in unmask_map:\n",
        "            print(f\"    - [CHECK] Will replace 'girtjosh' -> '{unmask_map['girtjosh']}'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    - [ERR] CSV Error: {e}\")\n",
        "else:\n",
        "    print(f\"    - [ERR] {KEY_FILE} not found.\")\n",
        "\n",
        "# 2. PATCH THE GEDCOM\n",
        "print(f\"\\n[2] Patching GEDCOM...\")\n",
        "if not os.path.exists(INPUT_GEDCOM):\n",
        "    print(f\"    - [ERR] {INPUT_GEDCOM} not found.\")\n",
        "else:\n",
        "    replacements_made = 0\n",
        "\n",
        "    with open(INPUT_GEDCOM, 'r', encoding='utf-8-sig', errors='ignore') as f_in:\n",
        "        with open(OUTPUT_GEDCOM, 'w', encoding='utf-8') as f_out:\n",
        "\n",
        "            for line in f_in:\n",
        "                # OPTIMIZATION: Only touch lines with \"NPFX\" and \"&\"\n",
        "                if \"2 NPFX\" in line and \"&\" in line:\n",
        "                    # Line looks like: \"2 NPFX 15 cM & girtjosh\"\n",
        "                    # We want to replace the code at the end\n",
        "\n",
        "                    original_line = line\n",
        "                    processed = False\n",
        "\n",
        "                    # Sort keys by length (longest first) to avoid replacing \"rob\" inside \"robert\"\n",
        "                    sorted_codes = sorted(unmask_map.keys(), key=len, reverse=True)\n",
        "\n",
        "                    for code in sorted_codes:\n",
        "                        # Check if the code is in the line (Case Insensitive Check, Exact Replacement)\n",
        "                        # We look for \"& code\" or \"&code\" or \"code\" at end of line\n",
        "\n",
        "                        # Logic: Search for the code, replace with name\n",
        "                        # We use a case-insensitive check but replace the text we found\n",
        "                        lower_line = line.lower()\n",
        "                        lower_code = code.lower()\n",
        "\n",
        "                        if lower_code in lower_line:\n",
        "                            # Verify it's actually the code part (after the &)\n",
        "                            parts = line.split('&')\n",
        "                            if len(parts) > 1:\n",
        "                                current_suffix = parts[1].strip()\n",
        "                                # Check if the suffix *starts* with our code (ignoring case)\n",
        "                                if current_suffix.lower().startswith(lower_code):\n",
        "                                    # REPLACE\n",
        "                                    # Reconstruct the line: Prefix + & + Real Name + Newline\n",
        "                                    new_line = f\"{parts[0]}& {unmask_map[code]}\\n\"\n",
        "                                    f_out.write(new_line)\n",
        "                                    replacements_made += 1\n",
        "                                    processed = True\n",
        "                                    break\n",
        "\n",
        "                    if not processed:\n",
        "                        f_out.write(line) # No code found, write original\n",
        "                else:\n",
        "                    f_out.write(line) # Not a match line, write original\n",
        "\n",
        "    print(f\"    - Complete. Replaced {replacements_made} codes.\")\n",
        "    print(f\"    - Created: {OUTPUT_GEDCOM}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"NEXT STEP: Go to [CELL 1] and change:\")\n",
        "    print(f'TARGET_GEDCOM = \"{OUTPUT_GEDCOM}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLUf9Ps_RJjv",
        "outputId": "48ce06f3-5a66-403d-e988-fe98ad5a8e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      GEDCOM UNMASKING PATCHER\n",
            "============================================================\n",
            "[1] Reading match_to_unmasked.csv...\n",
            "    - Loaded 94 replacements.\n",
            "    - [CHECK] Will replace 'girtjosh' -> 'Josh Girtain'\n",
            "\n",
            "[2] Patching GEDCOM...\n",
            "    - Complete. Replaced 1700 codes.\n",
            "    - Created: yates_study_2025_UNMASKED.ged\n",
            "------------------------------------------------------------\n",
            "NEXT STEP: Go to [CELL 1] and change:\n",
            "TARGET_GEDCOM = \"yates_study_2025_UNMASKED.ged\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 1] Master Engine (TARGET_GEDCOM = \"yates_study_2025_UNMASKED.ged)\n",
        "# Version: 2026.02.04-RICH-SLUG (Production)\n",
        "# Logic: Parses '2 NPFX', traces 'Yates' lines, formats 'Name (Years) & Spouse', and sorts via hidden slug.\n",
        "\n",
        "import os, re, glob, logging, pickle\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      CELL 1: THE ENGINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TARGET_GEDCOM = \"yates_study_2025.ged\"\n",
        "NAME_KEY_FILE = \"match_to_unmasked.csv\"\n",
        "CSV_OUT       = \"engine_database.csv\"\n",
        "VITALS_OUT    = \"dna_vitals.csv\"\n",
        "\n",
        "# --- UTILITIES ---\n",
        "def _slugify_last_first(name_str):\n",
        "    if not name_str or name_str.lower() == \"unknown\": return \"unknown\"\n",
        "    clean = name_str.replace(\"/\", \"\").strip()\n",
        "    parts = clean.split()\n",
        "    if not parts: return \"unknown\"\n",
        "    return re.sub(r\"[^a-z0-9]\", \"\", (parts[-1] + \"\".join(parts[:-1])).lower())\n",
        "\n",
        "def _pretty_name(display_name):\n",
        "    s = (display_name or \"\").replace(\"/\", \"\").strip()\n",
        "    return s if s and s.lower() != \"unknown\" else \"Unknown\"\n",
        "\n",
        "def _extract_years(txt):\n",
        "    b = re.search(r\"1 BIRT.*?2 DATE.*?(\\d{4})\", txt, re.S)\n",
        "    d = re.search(r\"1 DEAT.*?2 DATE.*?(\\d{4})\", txt, re.S)\n",
        "    return f\"{b.group(1) if b else ''}-{d.group(1) if d else ''}\".strip('-')\n",
        "\n",
        "# --- LOGIC CORE ---\n",
        "class StrictLineageTracer:\n",
        "    def __init__(self, parents_map, names_map, years_map):\n",
        "        self.parents = parents_map\n",
        "        self.names = names_map\n",
        "        self.years = years_map\n",
        "        self.target = \"yates\"\n",
        "\n",
        "    def get_lineage(self, start_id):\n",
        "        lineage = []\n",
        "        curr = start_id\n",
        "        for _ in range(50):\n",
        "            lineage.append({'id': curr, 'name': self.names.get(curr, \"Unknown\"), 'years': self.years.get(curr, \"\")})\n",
        "            f, m = self.parents.get(curr, (None, None))\n",
        "            if not f and not m: break\n",
        "\n",
        "            # Follow Yates\n",
        "            fn, mn = self.names.get(f, \"\").lower(), self.names.get(m, \"\").lower()\n",
        "            if self.target in fn: curr = f\n",
        "            elif self.target in mn: curr = m\n",
        "            elif f: curr = f\n",
        "            elif m: curr = m\n",
        "            else: break\n",
        "        return lineage\n",
        "\n",
        "def process_record(pkg):\n",
        "    rid, (pmap, nmap, ymap, smap), meta = pkg\n",
        "    tracer = StrictLineageTracer(pmap, nmap, ymap)\n",
        "\n",
        "    # 1. Trace & Reverse (Oldest -> Newest)\n",
        "    lineage = tracer.get_lineage(rid)\n",
        "    lineage.reverse()\n",
        "\n",
        "    # 2. Identify First Ancestor Pair\n",
        "    apex = lineage[0] if lineage else {'name': 'Unknown', 'id': None, 'years': ''}\n",
        "    spouses = smap.get(apex['id'], [])\n",
        "    spouse_name = nmap.get(spouses[0], \"Unknown\") if spouses else \"Unknown\"\n",
        "\n",
        "    # 3. Format Rich Header: \"William Yates (1750-1830) & Mary\"\n",
        "    header = f\"{apex['name']}\"\n",
        "    if apex['years']: header += f\" ({apex['years']})\"\n",
        "    if spouse_name != \"Unknown\": header += f\" & {spouse_name}\"\n",
        "\n",
        "    # 4. Inject Header into Lineage String\n",
        "    lineage_names = [x['name'] for x in lineage]\n",
        "    if lineage_names: lineage_names[0] = header\n",
        "\n",
        "    return {\n",
        "        \"ID#\": rid,\n",
        "        \"Match to\": meta.get(\"code\", \"\"),\n",
        "        \"Name\": meta.get(\"real_name\", \"Unknown\"),\n",
        "        \"cM\": meta.get(\"cm\", \"\"),\n",
        "        \"Yates DNA Ancestral Line\": \" -> \".join(lineage_names),\n",
        "        \"fa_1 masked\": _slugify_last_first(apex['name']),\n",
        "        \"FirstAncestor_pair cojoined\": header,\n",
        "        \"Authority_FirstAncestor\": _slugify_last_first(apex['name']) # Sort Key\n",
        "    }\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "def main():\n",
        "    if not os.path.exists(TARGET_GEDCOM):\n",
        "        print(f\"[ERR] {TARGET_GEDCOM} not found.\"); return\n",
        "\n",
        "    print(f\"[1] Parsing {TARGET_GEDCOM}...\")\n",
        "    with open(TARGET_GEDCOM, 'r', encoding='utf-8-sig', errors='ignore') as f: raw = f.read()\n",
        "\n",
        "    # Maps\n",
        "    pmap, nmap, ymap, smap, meta_map = {}, {}, {}, {}, {}\n",
        "    name_key = {}\n",
        "\n",
        "    if os.path.exists(NAME_KEY_FILE):\n",
        "        ndf = pd.read_csv(NAME_KEY_FILE, header=None)\n",
        "        name_key = dict(zip(ndf[0].str.strip().str.lower(), ndf[1].str.strip()))\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    total_recs = 0\n",
        "\n",
        "    for blk in blocks:\n",
        "        lines = blk.splitlines()\n",
        "        if \" INDI\" in lines[0]:\n",
        "            total_recs += 1\n",
        "            rid = lines[0].split(\"@\")[1]\n",
        "            name, npfx = \"\", \"\"\n",
        "            for l in lines:\n",
        "                if \"1 NAME\" in l: name = _pretty_name(l.split(\"NAME\")[1])\n",
        "                if \"2 NPFX\" in l: npfx = l.split(\"NPFX\")[1].strip()\n",
        "\n",
        "            nmap[rid] = name\n",
        "            ymap[rid] = _extract_years(blk)\n",
        "\n",
        "            if \"&\" in npfx:\n",
        "                try:\n",
        "                    parts = npfx.replace(\"(\", \"\").replace(\")\", \"\").split(\"&\")\n",
        "                    meta_map[rid] = {'cm': parts[0].strip(), 'code': parts[1].strip(), 'real_name': name_key.get(parts[1].strip().lower(), name)}\n",
        "                except: pass\n",
        "\n",
        "        if \" FAM\" in lines[0]:\n",
        "            h = re.search(r\"1 HUSB @(.*?)@\", blk)\n",
        "            w = re.search(r\"1 WIFE @(.*?)@\", blk)\n",
        "            kids = re.findall(r\"1 CHIL @(.*?)@\", blk)\n",
        "            hid, wid = (h.group(1) if h else None), (w.group(1) if w else None)\n",
        "            if hid and wid:\n",
        "                smap.setdefault(hid, []).append(wid)\n",
        "                smap.setdefault(wid, []).append(hid)\n",
        "            for k in kids: pmap[k] = (hid, wid)\n",
        "\n",
        "    # Processing\n",
        "    queue = [(mid, (pmap, nmap, ymap, smap), meta) for mid, meta in meta_map.items()]\n",
        "    print(f\"[2] Processing {len(queue)} matches (Total Pool: {len(queue)})...\")\n",
        "\n",
        "    with ProcessPoolExecutor() as exe:\n",
        "        rows = list(tqdm(exe.map(process_record, queue), total=len(queue)))\n",
        "\n",
        "    # Output\n",
        "    df = pd.DataFrame(rows)\n",
        "    if not df.empty:\n",
        "        df.sort_values(by=[\"Authority_FirstAncestor\", \"Name\"], inplace=True)\n",
        "\n",
        "    # Columns: Authority on far right\n",
        "    cols = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\",\n",
        "            \"fa_1 masked\", \"FirstAncestor_pair cojoined\", \"Authority_FirstAncestor\"]\n",
        "    df = df[cols]\n",
        "\n",
        "    df.to_csv(CSV_OUT, index=False, encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\")\n",
        "\n",
        "    # Save Vitals\n",
        "    pd.DataFrame([\n",
        "        {\"line\": f\"Records tagged and filtered by NPFX: {len(queue)}\"},\n",
        "        {\"line\": f\"After manual filter, total records: {len(df)}\"}\n",
        "    ]).to_csv(VITALS_OUT, index=False, encoding=\"iso-8859-15\")\n",
        "\n",
        "    print(f\"[DONE] Database generated: {CSV_OUT} ({len(df)} rows)\")\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L00tCdaZ17C",
        "outputId": "e5586f02-85fa-4d57-cbf0-ffb1200340ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      CELL 1: THE ENGINE\n",
            "============================================================\n",
            "[1] Parsing yates_study_2025.ged...\n",
            "[2] Processing 1700 matches (Total Pool: 1700)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1700/1700 [06:08<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Database generated: engine_database.csv (1700 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 2] Production Publisher (KeyError Fix)\n",
        "# Logic: Generates the full website + Print Edition.\n",
        "#        - FIX: Generates the Print Page BEFORE renaming columns to avoid KeyError.\n",
        "#        - Includes UTF-8 encoding, Narrative Header, and Flexbox Navigation.\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import html\n",
        "import socket\n",
        "import pytz\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      CELL 2: PRODUCTION PUBLISHER\")\n",
        "print(\"      (KeyError Fix / Print Edition)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. SESSION & CONFIGURATION\n",
        "# ==============================================================================\n",
        "HOST = os.environ.get(\"FTP_HOST\", \"\").strip()\n",
        "USER = os.environ.get(\"FTP_USER\", \"\").strip()\n",
        "PASS = os.environ.get(\"FTP_PASS\", \"\").strip()\n",
        "try: PORT = int(os.environ.get(\"FTP_PORT\", 21))\n",
        "except: PORT = 21\n",
        "\n",
        "if not HOST:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HOST = userdata.get(\"FTP_HOST\")\n",
        "        USER = userdata.get(\"FTP_USER\")\n",
        "        PASS = userdata.get(\"FTP_PASS\")\n",
        "    except: pass\n",
        "\n",
        "if not HOST or not USER or not PASS:\n",
        "    print(\"\\n[CRITICAL] Credentials not found! Please Run [CELL 1] again.\")\n",
        "    raise RuntimeError(\"Session context missing\")\n",
        "\n",
        "def connect_session():\n",
        "    socket.setdefaulttimeout(60)\n",
        "    ftps = FTP_TLS(timeout=60)\n",
        "    ftps.connect(HOST, PORT)\n",
        "    ftps.auth()\n",
        "    ftps.login(USER, PASS)\n",
        "    try: ftps.prot_p(); ftps.set_pasv(True)\n",
        "    except: pass\n",
        "    return ftps\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. DATA LOADING\n",
        "# ==============================================================================\n",
        "CSV_INPUT = \"engine_database.csv\"\n",
        "KEY_FILE  = \"match_to_unmasked.csv\"\n",
        "\n",
        "if not os.path.exists(CSV_INPUT):\n",
        "    raise FileNotFoundError(f\"Missing {CSV_INPUT}. Run Cell 1.\")\n",
        "\n",
        "df = pd.read_csv(CSV_INPUT, encoding=\"iso-8859-15\")\n",
        "print(f\"[2] Loaded Data: {len(df)} records.\")\n",
        "\n",
        "unmask_map = {}\n",
        "if os.path.exists(KEY_FILE):\n",
        "    try:\n",
        "        with open(KEY_FILE, mode='r', encoding='utf-8-sig', errors='replace') as f:\n",
        "            try: dialect = csv.Sniffer().sniff(f.read(1024)); f.seek(0)\n",
        "            except: dialect = 'excel'; f.seek(0)\n",
        "            reader = csv.reader(f, dialect)\n",
        "            for row in reader:\n",
        "                if len(row) < 2: continue\n",
        "                code = row[0].strip().lower()\n",
        "                name = row[1].strip()\n",
        "                if code and name and code != \"code\":\n",
        "                    unmask_map[code] = name\n",
        "        print(f\"    - [OK] Loaded {len(unmask_map)} unmasking keys.\")\n",
        "    except: pass\n",
        "\n",
        "def get_real_name(val):\n",
        "    s_val = str(val).strip()\n",
        "    return unmask_map.get(s_val.lower(), s_val)\n",
        "\n",
        "if \"Match to\" in df.columns:\n",
        "    df[\"Match to\"] = df[\"Match to\"].apply(get_real_name)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CONTENT GENERATION\n",
        "# ==============================================================================\n",
        "print(\"[3] Generating Content...\")\n",
        "\n",
        "COL_ANCESTOR = \"Authority_FirstAncestor\"\n",
        "if \"FirstAncestor_pair cojoined\" in df.columns: COL_ANCESTOR = \"FirstAncestor_pair cojoined\"\n",
        "elif \"fa_1 masked\" in df.columns: COL_ANCESTOR = \"fa_1 masked\"\n",
        "\n",
        "COL_LINEAGE = \"Yates DNA Ancestral Line\"\n",
        "if COL_LINEAGE not in df.columns: COL_LINEAGE = COL_ANCESTOR\n",
        "\n",
        "REMOTE_SUBDIR = \"ons-study\"\n",
        "TNG_BASE_URL  = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "TNG_SUFFIX    = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "\n",
        "def clean_id(val):\n",
        "    rid = str(val).strip().replace('@', '')\n",
        "    if rid == 'nan': return \"\"\n",
        "    if rid.isdigit(): return \"I\" + rid\n",
        "    return rid\n",
        "\n",
        "def get_matchee_name(row):\n",
        "    lineage = str(row.get(COL_LINEAGE, ''))\n",
        "    if not lineage: return \"Unknown\"\n",
        "    parts = lineage.split(' -> ')\n",
        "    return parts[-1] if parts else \"Unknown\"\n",
        "\n",
        "def generate_insight(ancestor_name, df_subset):\n",
        "    total = len(df_subset)\n",
        "    unique = df_subset['Match to'].nunique()\n",
        "    if unique > 20: strength = \"Massive\"\n",
        "    elif unique > 10: strength = \"Very Strong\"\n",
        "    elif unique > 4: strength = \"Solid\"\n",
        "    else: strength = \"Emerging\"\n",
        "    narrative = f\"<strong>Analysis:</strong> The genetic case for <em>{ancestor_name}</em> is <strong>{strength}</strong>. \"\n",
        "    narrative += f\"A total of {unique} different descendants have validated this line with {total} DNA matches. \"\n",
        "    return narrative\n",
        "\n",
        "def build_narrative(row):\n",
        "    part_name = str(row.get('Match to', 'Unknown'))\n",
        "    matchee_name = get_matchee_name(row)\n",
        "    cm = str(row.get('cM', '0'))\n",
        "    anc = str(row.get(COL_ANCESTOR, 'Unknown'))\n",
        "    rid = clean_id(row.get('ID#', ''))\n",
        "    linked_name = part_name\n",
        "    if rid: linked_name = f'<a href=\"{TNG_BASE_URL}{rid}{TNG_SUFFIX}\" target=\"_blank\"><b>{part_name}</b></a>'\n",
        "    return f\"{linked_name} matches {matchee_name} as a {cm} cM relative; they share a Yates ancestral line descending from {anc}.\"\n",
        "\n",
        "df['Long_Narrative'] = df.apply(build_narrative, axis=1)\n",
        "\n",
        "def build_linked_lineage(row):\n",
        "    lineage = str(row.get(COL_LINEAGE, ''))\n",
        "    part_name = str(row.get('Match to', ''))\n",
        "    rid = clean_id(row.get('ID#', ''))\n",
        "    participant_link = f\"<b>{part_name}</b>\"\n",
        "    if rid: participant_link = f'<a href=\"{TNG_BASE_URL}{rid}{TNG_SUFFIX}\" target=\"_blank\"><b>{part_name}</b></a>'\n",
        "    if part_name and part_name not in lineage: return f\"{lineage} (matches: {participant_link})\"\n",
        "    elif part_name and part_name in lineage: return lineage.replace(part_name, participant_link)\n",
        "    return lineage\n",
        "\n",
        "df['Linked_Tree_Line'] = df.apply(build_linked_lineage, axis=1)\n",
        "\n",
        "est = pytz.timezone('US/Eastern')\n",
        "timestamp_str = datetime.now(est).strftime(\"%B %d, %Y\")\n",
        "full_timestamp_str = datetime.now(est).strftime(\"%B %d, %Y %-I:%M %p EST\")\n",
        "\n",
        "# --- NARRATIVE HEADER ---\n",
        "SITE_INFO_HTML = \"\"\"\n",
        "<div style=\"background: #e0f2f1; border: 1px solid #b2dfdb; padding: 20px; margin: 20px auto; width: 90%; border-radius: 8px; font-family: sans-serif;\">\n",
        "    <h2 style=\"color: #006064; margin-top: 0; border-bottom: 2px solid #004d40; padding-bottom: 10px;\">Establishing Kinship Through Collateral DNA Saturation</h2>\n",
        "    <p style=\"color: #333; line-height: 1.6; font-size: 1.05em;\">\n",
        "        <strong>Methodology:</strong> This register does not rely on single \"golden matches\" to prove kinship. Instead, it employs\n",
        "        <em>Collateral DNA Saturation</em>—overloading the genealogical scales with volume, pattern, and repetition.\n",
        "        By triangulating matches from multiple independent testers across multiple sibling lines, we establish a preponderance of biological evidence\n",
        "        that supplements traditional paper trails.\n",
        "    </p>\n",
        "    <ul style=\"color: #444; line-height: 1.5; font-size: 0.95em;\">\n",
        "        <li><strong>Evidence Strength:</strong> The matches listed below represent verified genetic connections greater than 7 cM.</li>\n",
        "        <li><strong>Forensic Analysis:</strong> The cumulative pattern of these matches serves as the genealogical equivalent of credible witnesses.</li>\n",
        "        <li><strong>Navigation:</strong> Use the links above to switch between the <em>DNA Network</em> (grouped analysis) and this <em>Register</em> (detailed proof).</li>\n",
        "    </ul>\n",
        "    <div style=\"text-align: right; font-size: 0.9em; margin-top: 10px;\">\n",
        "        <a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\" style=\"color: #00838f; font-weight: bold;\">Read Full Methodology &raquo;</a>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# --- NAV BLOCK ---\n",
        "NAV_HTML = \"\"\"\n",
        "<style>\n",
        "nav.oldnav ul {\n",
        "    display: flex;\n",
        "    flex-wrap: wrap;\n",
        "    justify-content: center;\n",
        "    background-color: #006064 !important;\n",
        "    border-bottom: 2px solid #00acc1 !important;\n",
        "    margin: 0;\n",
        "    padding: 0;\n",
        "    list-style: none;\n",
        "}\n",
        "nav.oldnav li { display: inline-block; }\n",
        "nav.oldnav a {\n",
        "    display: block;\n",
        "    padding: 10px 15px;\n",
        "    text-decoration: none;\n",
        "    color: #e0f7fa !important;\n",
        "    font-size: 14px;\n",
        "}\n",
        "nav.oldnav a:hover { background-color: #00838f !important; }\n",
        "</style>\n",
        "<nav class=\"oldnav\">\n",
        "  <ul>\n",
        "    <li><a href=\"/ons-study/contents.shtml\">Contents</a></li>\n",
        "    <li><a href=\"/ons-study/yates_ancestor_register.shtml\">DNA Register</a></li>\n",
        "    <li><a href=\"/ons-study/just-trees.shtml\">Trees</a></li>\n",
        "    <li><a href=\"/ons-study/dna_network.shtml\">DNA Network</a></li>\n",
        "    <li><a href=\"/ons-study/lineage_proof.html\" style=\"color:#ffeb3b !important; font-weight:bold;\">Lineage Proof</a></li>\n",
        "    <li><a href=\"/ons-study/subscribe_updates.shtml\">Subscribe Updates</a></li>\n",
        "    <li><a href=\"/ons-study/share_matches.shtml\">Share Your Matches</a></li>\n",
        "    <li><a href=\"/ons-study/gedmatchkits.htm\">Gedmatch Kits</a></li>\n",
        "    <li><a href=\"/ons-study/cousin_list_print.htm\">Print Cousin List</a></li>\n",
        "    <li><a href=\"/ons-study/yates_ancestor_register.csv\">Download CSV</a></li>\n",
        "    <li><a href=\"/ons-study/yates_ancestor_register.xlsx\">Download Excel</a></li>\n",
        "    <li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\">Study Details</a></li>\n",
        "    <li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\">Theory in Action</a></li>\n",
        "    <li><a href=\"/gengen/images/cousin-calculator.jpg\" target=\"_blank\">Cousin Connection</a></li>\n",
        "    <li><a href=\"/gengen/images/Shared_cM_Project_v4.jpg\" target=\"_blank\">Cousin by DNA</a></li>\n",
        "  </ul>\n",
        "</nav>\n",
        "\"\"\"\n",
        "\n",
        "# Escaped backslash for Python string safety\n",
        "SORT_JS = r\"\"\"<script type=\"text/javascript\">(function(){function textOf(c){return(c&&(c.textContent||c.innerText)||'').replace(/\\s+/g,' ').trim().toLowerCase();}function sortTable(t,i,d){if(!(t&&t.tBodies&&t.tBodies[0]))return;var tb=t.tBodies[0],r=Array.prototype.slice.call(tb.rows||[]),asc=(d==='asc');r.sort(function(a,b){var A=textOf(a.cells[i]),B=textOf(b.cells[i]),nA=parseFloat(A.replace(/[^0-9.\\-]/g,'')),nB=parseFloat(B.replace(/[^0-9.\\-]/g,''));if(!isNaN(nA)&&!isNaN(nB))return asc?(nA-nB):(nB-nA);return(A<B)?(asc?-1:1):(A>B)?(asc?1:-1):0;});var f=document.createDocumentFragment();for(var k=0;k<r.length;k++)f.appendChild(r[k]);tb.appendChild(f);}function makeSortable(t){if(!(t&&t.tHead&&t.tHead.rows.length))return;var th=t.tHead.rows[0].cells;for(var i=0;i<th.length;i++){(function(idx){var h=th[idx],d='asc';h.style.cursor='pointer';h.onclick=function(){d=(d==='asc')?'desc':'asc';for(var j=0;j<th.length;j++)th[j].innerHTML=th[j].innerHTML.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'');h.innerHTML+=(d==='asc'?' (asc)':' (desc)');sortTable(t,idx,d);};})(i);}}function init(){var t=document.getElementsByTagName('table');for(var i=0;i<t.length;i++)if(/\\bsortable\\b/.test(t[i].className))makeSortable(t[i]);}if(document.readyState==='loading')document.addEventListener('DOMContentLoaded',init,false);else init();})();</script>\"\"\"\n",
        "\n",
        "def make_page(title, content, count, view_type=\"\", extra_script=\"\"):\n",
        "    head = f\"\"\"<div style=\"background:#f4f4f4; border-top:1px solid #ddd; border-bottom:1px solid #ddd; font-family:sans-serif; font-size:12px; color:#555; padding:8px 15px; text-align:center; margin-bottom:0;\"><strong>Last updated:</strong> {full_timestamp_str} &nbsp;|&nbsp; <strong>Autosomal matches:</strong> {count:,} &nbsp;|&nbsp; <strong>Showing:</strong> {count:,}</div>\"\"\"\n",
        "    style_overrides = \"\"\"<style>.table-scroll-wrapper { text-align: center; width: 100%; } #reg-table { margin: 0 auto; width: 96%; } #reg-table th { position: sticky; top: 0; z-index: 6; background-color: #f8f9fa; border-bottom: 2px solid #ccc; height: 20px; } #reg-table td { text-align: left; padding: 8px 15px; } .anc-block { margin-bottom: 25px; } .anc-header { background-color: #e0f2f1; padding: 10px; border: 1px solid #b2dfdb; border-radius: 4px; text-align:left; margin-bottom: 5px; } .anc-title { font-weight: bold; font-size: 1.1em; color: #004d40; } .anc-stats { font-size: 0.9em; color: #555; margin-left: 10px; } .net-row { font-size: 0.95em; border-bottom: 1px solid #eee; padding: 5px; } .net-participant { font-weight: bold; color: #006064; } .excluded-row { opacity: 0.4; text-decoration: line-through; } .insight-box { background-color: #fffde7; border-left: 4px solid #fbc02d; padding: 10px; margin: 5px 0 10px 0; font-family: sans-serif; font-size: 0.95em; color: #333; }</style>\"\"\"\n",
        "\n",
        "    toggle_html = \"\"\n",
        "    narrative_block = \"\"\n",
        "    if view_type in ['ancestor', 'participant']:\n",
        "        narrative_block = SITE_INFO_HTML\n",
        "        if view_type == 'ancestor':\n",
        "            toggle_html = f\"\"\"<div style=\"text-align:center; padding:10px; margin-bottom:10px; font-family:sans-serif; font-size:14px; background:#e0f7fa; border:1px solid #b2ebf2;\"><strong>Sort Register:</strong> &nbsp;<a href=\"ons_yates_dna_register.shtml\" style=\"font-weight:bold; color:#006064;\">By Ancestral Line</a> &nbsp;|&nbsp; <a href=\"ons_yates_dna_register_participants.shtml\" style=\"color:#00acc1; text-decoration:none;\">By Participant Name</a></div>\"\"\"\n",
        "        else:\n",
        "            toggle_html = f\"\"\"<div style=\"text-align:center; padding:10px; margin-bottom:10px; font-family:sans-serif; font-size:14px; background:#e0f7fa; border:1px solid #b2ebf2;\"><strong>Sort Register:</strong> &nbsp;<a href=\"ons_yates_dna_register.shtml\" style=\"color:#00acc1; text-decoration:none;\">By Ancestral Line</a> &nbsp;|&nbsp; <a href=\"ons_yates_dna_register_participants.shtml\" style=\"font-weight:bold; color:#006064;\">By Participant Name</a></div>\"\"\"\n",
        "    elif view_type in ['tree_za', 'tree_az']:\n",
        "        link_za = f'<span style=\"font-weight:bold; color:#000;\">Z-A</span>' if view_type == 'tree_za' else f'<a href=\"just-trees.shtml\" style=\"color:#006064; text-decoration:underline;\">Z-A</a>'\n",
        "        link_az = f'<span style=\"font-weight:bold; color:#000;\">A-Z</span>' if view_type == 'tree_az' else f'<a href=\"just-trees-az.shtml\" style=\"color:#006064; text-decoration:underline;\">A-Z</a>'\n",
        "        toggle_html = f\"\"\"<div style=\"text-align:center; font-family:sans-serif; font-size:16px; margin: 15px 0 10px 0;\">Individual Yates Family trees: &nbsp; {link_za} &nbsp;|&nbsp; {link_az}</div>\"\"\"\n",
        "\n",
        "    return f\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>{title}</title><link rel=\"stylesheet\" href=\"partials_unified.css\"><link rel=\"stylesheet\" href=\"dna_tree_styles.css\">{style_overrides}{extra_script}</head><body id=\"top\"><div class=\"wrap\"><h1 class=\"centerline\">{title}</h1><div id=\"nav-slot\">{head}{NAV_HTML}</div>{narrative_block}{toggle_html}{content}{SORT_JS}</div></body></html>\"\"\"\n",
        "\n",
        "# --- PRINT PAGE GENERATOR ---\n",
        "def make_print_page(df_sorted):\n",
        "    style = \"\"\"<style>\n",
        "        body { font-family: serif; font-size: 11pt; margin: 20px; color: #000; }\n",
        "        h1 { font-size: 18pt; text-align: center; margin-bottom: 10px; }\n",
        "        .meta { text-align: center; font-size: 10pt; color: #555; margin-bottom: 20px; }\n",
        "        table { width: 100%; border-collapse: collapse; }\n",
        "        td { border-bottom: 1px solid #ccc; padding: 5px 2px; vertical-align: top; }\n",
        "        tr:nth-child(even) { background-color: #f9f9f9; }\n",
        "        button { padding: 10px 20px; font-size: 14px; background: #eee; border: 1px solid #999; cursor: pointer; display: block; margin: 0 auto 20px auto; }\n",
        "        @media print {\n",
        "            button { display: none; }\n",
        "            body { margin: 0; }\n",
        "        }\n",
        "    </style>\"\"\"\n",
        "    html = f\"\"\"<!DOCTYPE html><html><head><meta charset=\"UTF-8\"><title>Yates Cousin List</title>{style}</head><body>\n",
        "    <h1>Yates DNA Cousin List</h1>\n",
        "    <div class=\"meta\">Generated: {timestamp_str} | Total Matches: {len(df_sorted)}</div>\n",
        "    <button onclick=\"window.print()\">Print / Save as PDF</button>\n",
        "    <table><tbody>\"\"\"\n",
        "    for _, row in df_sorted.iterrows():\n",
        "        html += f\"<tr><td>{row['Long_Narrative']}</td></tr>\"\n",
        "    html += \"</tbody></table></body></html>\"\n",
        "    return html\n",
        "\n",
        "# 1. PARTICIPANT SORT & PRINT GENERATION\n",
        "# (We assume Participant Sort order for the Print List)\n",
        "df['_sort_key'] = df[\"Match to\"].astype(str).apply(lambda x: x.strip().split()[-1] if x.strip() else \"\")\n",
        "df.sort_values(by=['_sort_key', 'Match to'], ascending=[True, True], inplace=True)\n",
        "\n",
        "# Generate Print Page (Before renaming any columns!)\n",
        "html_print = make_print_page(df)\n",
        "\n",
        "# Generate Participant HTML Page\n",
        "header_name = \"Participants who tested-Who they matched-Oldest known Yates ancestor\"\n",
        "# We make a copy to rename for the HTML table, preserving the original DF for other sorts if needed\n",
        "df_par = df.copy()\n",
        "df_par.rename(columns={'Long_Narrative': header_name}, inplace=True)\n",
        "html_par = make_page(\"ONS Yates Study DNA Register\", f'<div class=\"table-scroll-wrapper\">{df_par.to_html(columns=[header_name], index=False, border=1, classes=\"dataframe sortable\", escape=False, table_id=\"reg-table\")}</div>', len(df), \"participant\")\n",
        "\n",
        "# 2. ANCESTOR SORT\n",
        "df.sort_values(by=[COL_ANCESTOR, COL_LINEAGE], ascending=[False, True], inplace=True)\n",
        "df_anc = df.copy()\n",
        "df_anc.rename(columns={'Long_Narrative': header_name}, inplace=True)\n",
        "html_anc = make_page(\"ONS Yates Study DNA Register\", f'<div class=\"table-scroll-wrapper\">{df_anc.to_html(columns=[header_name], index=False, border=1, classes=\"dataframe sortable\", escape=False, table_id=\"reg-table\")}</div>', len(df), \"ancestor\")\n",
        "\n",
        "# 3. TREES VIEW\n",
        "df_tree = df[['Linked_Tree_Line', COL_ANCESTOR, COL_LINEAGE]].copy()\n",
        "df_tree.rename(columns={'Linked_Tree_Line': 'TEMP_TREE_HEADER'}, inplace=True)\n",
        "df_tree.sort_values(by=[COL_ANCESTOR, COL_LINEAGE], ascending=[False, True], inplace=True)\n",
        "html_tree_za = make_page(\"Ancestor Register (Trees View)\", f'<div class=\"table-scroll-wrapper\">{df_tree[[\"TEMP_TREE_HEADER\"]].to_html(index=False, border=1, classes=\"dataframe sortable\", escape=False, table_id=\"reg-table\").replace(\"<th>TEMP_TREE_HEADER</th>\", \"<th>&nbsp;</th>\")}</div>', len(df), \"tree_za\")\n",
        "df_tree.sort_values(by=[COL_ANCESTOR, COL_LINEAGE], ascending=[True, True], inplace=True)\n",
        "html_tree_az = make_page(\"Ancestor Register (Trees View)\", f'<div class=\"table-scroll-wrapper\">{df_tree[[\"TEMP_TREE_HEADER\"]].to_html(index=False, border=1, classes=\"dataframe sortable\", escape=False, table_id=\"reg-table\").replace(\"<th>TEMP_TREE_HEADER</th>\", \"<th>&nbsp;</th>\")}</div>', len(df), \"tree_az\")\n",
        "\n",
        "# 4. NETWORK VIEW\n",
        "def last_name_sort(name):\n",
        "    parts = str(name).strip().split()\n",
        "    return parts[-1].lower() + \" \" + \" \".join(parts[:-1]).lower() if parts else \"\"\n",
        "participants = sorted(df['Match to'].astype(str).unique().tolist(), key=last_name_sort)\n",
        "checkbox_list = \"\".join([f'<div style=\"text-align:left; padding:2px;\"><label style=\"font-weight:normal; font-size:13px; cursor:pointer;\"><input type=\"checkbox\" name=\"partSelect\" value=\"{html.escape(p)}\" onchange=\"filterNetwork()\">&nbsp;{html.escape(p)}</label></div>' for p in participants])\n",
        "filter_ui = f'<div style=\"background:#f4f4f4; border-bottom:1px solid #ccc; margin-bottom:20px; padding:15px;\"><div style=\"text-align:center; font-weight:bold; margin-bottom:8px; color:#006064;\">Network Control Center</div><div style=\"display:flex; justify-content:center; gap:20px; align-items:start;\"><div style=\"width:300px; height:120px; overflow-y:scroll; background:white; border:1px solid #ccc; padding:5px;\">{checkbox_list}</div><div style=\"display:flex; flex-direction:column; gap:8px;\"><button onclick=\"selectAll(true)\" style=\"padding:5px 10px; font-size:12px;\">Select All</button><button onclick=\"selectAll(false)\" style=\"padding:5px 10px; font-size:12px;\">Clear All</button></div></div><div style=\"text-align:center; font-size:12px; color:#666; margin-top:5px;\"><em>Check multiple boxes to combine results.</em></div></div>'\n",
        "\n",
        "network_content = \"\"\n",
        "anc_counts = df[COL_ANCESTOR].value_counts()\n",
        "for anc in anc_counts.index.tolist():\n",
        "    anc_df = df[df[COL_ANCESTOR] == anc]\n",
        "    if anc_df['Match to'].nunique() < 2: continue\n",
        "    matches_count = len(anc_df)\n",
        "    narrative_text = generate_insight(str(anc), anc_df)\n",
        "    network_content += f'<div class=\"anc-block\" data-ancestor=\"{html.escape(str(anc))}\"><div class=\"anc-header\"><span class=\"anc-title\">{html.escape(str(anc))}</span><span class=\"anc-stats\">({matches_count} Matches from {anc_df[\"Match to\"].nunique()} Participants)</span></div><div class=\"insight-box\">{narrative_text}</div><table class=\"sortable\" border=\"1\" style=\"width:100%; margin-top:0;\"><thead><tr><th>Participant</th><th>Matchee</th><th>cM</th><th style=\"text-align:left;\">Full Lineage</th><th>Action</th></tr></thead><tbody>'\n",
        "    for _, row in anc_df.iterrows():\n",
        "        part_name = str(row.get('Match to', ''))\n",
        "        matchee_name = get_matchee_name(row)\n",
        "        lineage_txt = str(row.get(COL_LINEAGE, ''))\n",
        "        network_content += f'<tr class=\"net-row\" data-participant=\"{html.escape(part_name)}\"><td class=\"net-participant\">{part_name}</td><td>{matchee_name}</td><td>{row.get(\"cM\",\"\")}</td><td style=\"font-size:0.9em; text-align:left;\">{lineage_txt}</td><td style=\"text-align:center;\"><button onclick=\"toggleExclude(this)\" style=\"font-size:0.8em;\">Ignore</button></td></tr>'\n",
        "    network_content += \"</tbody></table></div>\"\n",
        "\n",
        "net_js = \"\"\"<script>function filterNetwork(){var checkboxes=document.querySelectorAll('input[name=\"partSelect\"]:checked');var selected=Array.from(checkboxes).map(cb=>cb.value);var rows=document.querySelectorAll('.net-row');rows.forEach(r=>{if(selected.length===0||selected.includes(r.getAttribute('data-participant'))){r.style.display='';}else{r.style.display='none';}});var blocks=document.querySelectorAll('.anc-block');blocks.forEach(b=>{var visibleRows=b.querySelectorAll('.net-row:not([style*=\"display: none\"])');b.style.display=visibleRows.length>0?'':'none';});}function selectAll(check){var checkboxes=document.querySelectorAll('input[name=\"partSelect\"]');checkboxes.forEach(cb=>cb.checked=check);filterNetwork();}function toggleExclude(btn){var row=btn.closest('tr');if(row.classList.contains('excluded-row')){row.classList.remove('excluded-row');btn.innerText=\"Ignore\";}else{row.classList.add('excluded-row');btn.innerText=\"Restore\";}}</script>\"\"\"\n",
        "html_network = make_page(\"Participating DNA Network\", f'{filter_ui}<div class=\"table-scroll-wrapper\" style=\"width:96%; margin:0 auto;\">{network_content}</div>', len(df), \"\", net_js)\n",
        "\n",
        "lin = df[COL_LINEAGE].value_counts().reset_index(); lin.columns = ['Lineage', 'Count']\n",
        "lin_html = make_page(\"Lineage Count Report\", f'<div class=\"centerline\"><p>Total Unique Lines: {len(lin)}</p></div>{lin.to_html(index=False, border=1, classes=\"dataframe sortable\")}', len(lin), \"\")\n",
        "mat = df['Match to'].value_counts().reset_index(); mat.columns = ['Participant', 'Entries']\n",
        "mat_html = make_page(\"Match Count Report\", f'<div class=\"centerline\"><p>Participants: {len(mat)}</p></div>{mat.to_html(index=False, border=1, classes=\"dataframe sortable\")}', len(mat), \"\")\n",
        "\n",
        "# Upload\n",
        "print(f\"\\n[4] Uploading to {HOST}...\")\n",
        "try:\n",
        "    ftps = connect_session()\n",
        "    try: ftps.cwd(f\"/{REMOTE_SUBDIR}\")\n",
        "    except:\n",
        "        try: ftps.cwd(f\"/public_html/{REMOTE_SUBDIR}\")\n",
        "        except: pass\n",
        "\n",
        "    uploads = {\n",
        "        \"yates_ancestor_register.shtml\": html_anc,\n",
        "        \"ons_yates_dna_register.shtml\": html_anc,\n",
        "        \"ons_yates_dna_register_participants.shtml\": html_par,\n",
        "        \"cousin_list_print.htm\": html_print,\n",
        "        \"just-trees.shtml\": html_tree_za,\n",
        "        \"just-trees-az.shtml\": html_tree_az,\n",
        "        \"dna_network.shtml\": html_network,\n",
        "        \"lineage_count.shtml\": lin_html,\n",
        "        \"match_count.shtml\": mat_html\n",
        "    }\n",
        "    for fn, content in uploads.items():\n",
        "        with open(fn, \"w\", encoding=\"utf-8\") as f: f.write(content)\n",
        "        with open(fn, \"rb\") as fh: ftps.storbinary(f\"STOR {fn}\", fh)\n",
        "        print(f\"    - Uploaded: {fn}\")\n",
        "    ftps.quit()\n",
        "    print(\"\\n[SUCCESS] All Files Published.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n[ERROR] Upload Failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gHT5b0esX0p",
        "outputId": "6f9033b6-2a65-4151-ec61-a55bab17f2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      CELL 2: PRODUCTION PUBLISHER\n",
            "      (KeyError Fix / Print Edition)\n",
            "============================================================\n",
            "[2] Loaded Data: 1700 records.\n",
            "    - [OK] Loaded 94 unmasking keys.\n",
            "[3] Generating Content...\n",
            "\n",
            "[4] Uploading to ftp.one-name.net...\n",
            "    - Uploaded: yates_ancestor_register.shtml\n",
            "    - Uploaded: ons_yates_dna_register.shtml\n",
            "    - Uploaded: ons_yates_dna_register_participants.shtml\n",
            "    - Uploaded: cousin_list_print.htm\n",
            "    - Uploaded: just-trees.shtml\n",
            "    - Uploaded: just-trees-az.shtml\n",
            "    - Uploaded: dna_network.shtml\n",
            "    - Uploaded: lineage_count.shtml\n",
            "    - Uploaded: match_count.shtml\n",
            "\n",
            "[SUCCESS] All Files Published.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 4] The Proof Engine (Final Nav Update)\n",
        "# Logic: Forensic Dashboard.\n",
        "#        - NAV UPDATE: Flexbox-centered navigation that handles multi-line wrapping gracefully.\n",
        "#        - FEATURES: Surname Sorting, Rich Lineage Injection, and Forensic Analysis text.\n",
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      CELL 4: THE PROOF ENGINE\")\n",
        "print(\"      (Final Navigation Update)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CONFIGURATION\n",
        "# ==============================================================================\n",
        "HOST = os.environ.get(\"FTP_HOST\", \"\").strip()\n",
        "USER = os.environ.get(\"FTP_USER\", \"\").strip()\n",
        "PASS = os.environ.get(\"FTP_PASS\", \"\").strip()\n",
        "try: PORT = int(os.environ.get(\"FTP_PORT\", 21))\n",
        "except: PORT = 21\n",
        "\n",
        "if not HOST:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        HOST = userdata.get(\"FTP_HOST\")\n",
        "        USER = userdata.get(\"FTP_USER\")\n",
        "        PASS = userdata.get(\"FTP_PASS\")\n",
        "    except: pass\n",
        "\n",
        "REMOTE_SUBDIR = \"ons-study\"\n",
        "GEDCOM_FILE   = \"yates_study_2025.ged\"\n",
        "CSV_DB_FILE   = \"engine_database.csv\"\n",
        "KEY_FILE      = \"match_to_unmasked.csv\"\n",
        "OUTPUT_HTML   = \"lineage_proof.html\"\n",
        "WEB_BASE_URL  = \"https://yates.one-name.net/ons-study/\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. LOAD UNMASKING KEY\n",
        "# ==============================================================================\n",
        "unmask_map = {}\n",
        "if os.path.exists(KEY_FILE):\n",
        "    try:\n",
        "        with open(KEY_FILE, mode='r', encoding='utf-8-sig', errors='replace') as f:\n",
        "            try: dialect = csv.Sniffer().sniff(f.read(1024)); f.seek(0)\n",
        "            except: dialect = 'excel'; f.seek(0)\n",
        "            reader = csv.reader(f, dialect)\n",
        "            for row in reader:\n",
        "                if len(row) < 2: continue\n",
        "                code = row[0].strip().lower()\n",
        "                name = row[1].strip()\n",
        "                if code and name and code != \"code\":\n",
        "                    unmask_map[code] = name\n",
        "        print(f\"[1] Loaded {len(unmask_map)} privacy keys.\")\n",
        "    except Exception as e:\n",
        "        print(f\"    [WARN] Key file error: {e}\")\n",
        "\n",
        "def resolve_name(raw_text):\n",
        "    clean_text = raw_text.strip()\n",
        "    if clean_text.lower() in unmask_map: return unmask_map[clean_text.lower()]\n",
        "    if \"&\" in clean_text:\n",
        "        parts = clean_text.split('&')\n",
        "        potential_code = parts[-1].strip().lower()\n",
        "        potential_code = re.sub(r'[^a-z0-9_,-]', '', potential_code)\n",
        "        if potential_code in unmask_map: return unmask_map[potential_code]\n",
        "    return clean_text\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. SOURCE A: MACRO DATA (CSV)\n",
        "# ==============================================================================\n",
        "if not os.path.exists(CSV_DB_FILE): raise FileNotFoundError(f\"Missing {CSV_DB_FILE}\")\n",
        "\n",
        "df = pd.read_csv(CSV_DB_FILE, encoding=\"iso-8859-15\").fillna(\"\")\n",
        "col_anc = \"Authority_FirstAncestor\"\n",
        "if \"FirstAncestor_pair cojoined\" in df.columns: col_anc = \"FirstAncestor_pair cojoined\"\n",
        "sort_col = \"fa_1 masked\" if \"fa_1 masked\" in df.columns else col_anc\n",
        "col_lin = \"Yates DNA Ancestral Line\"\n",
        "\n",
        "ancestor_list = df[[col_anc, sort_col]].drop_duplicates().sort_values(by=sort_col)[col_anc].tolist()\n",
        "\n",
        "lineage_lookup = {}\n",
        "csv_payload = []\n",
        "for _, row in df.iterrows():\n",
        "    raw_part = str(row.get(\"Match to\", \"\"))\n",
        "    friendly_part = resolve_name(raw_part)\n",
        "\n",
        "    lineage_str = str(row.get(col_lin, \"\"))\n",
        "    lineage_lookup[friendly_part] = lineage_str\n",
        "\n",
        "    rid = str(row.get(\"ID#\", \"\")).strip().replace(\"@\", \"\")\n",
        "    if rid.isdigit(): rid = \"I\" + rid\n",
        "\n",
        "    csv_payload.append({\n",
        "        \"ancestor\": str(row.get(col_anc, \"\")),\n",
        "        \"participant\": friendly_part,\n",
        "        \"cm\": str(row.get(\"cM\", \"\")),\n",
        "        \"id\": rid,\n",
        "        \"lineage\": lineage_str\n",
        "    })\n",
        "\n",
        "print(f\"[2] Prepared Macro Data ({len(ancestor_list)} groups).\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. SOURCE B: MICRO DATA (GEDCOM Crawl)\n",
        "# ==============================================================================\n",
        "if not os.path.exists(GEDCOM_FILE): raise FileNotFoundError(f\"Missing {GEDCOM_FILE}\")\n",
        "\n",
        "print(f\"[3] Parsing GEDCOM for Deep ID Graph...\")\n",
        "person_map = {}\n",
        "match_nodes = []\n",
        "fam_map = {}\n",
        "\n",
        "def clean_id(raw): return raw.replace('@', '').strip()\n",
        "\n",
        "# A. Parse\n",
        "current_id = None\n",
        "current_fam = None\n",
        "with open(GEDCOM_FILE, 'r', encoding='utf-8', errors='replace') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        parts = line.split(' ', 2)\n",
        "        if len(parts) < 2: continue\n",
        "\n",
        "        if parts[0] == '0' and len(parts) > 2 and 'INDI' in parts[2]:\n",
        "            current_id = clean_id(parts[1])\n",
        "            person_map[current_id] = {'name': 'Unknown', 'famc': None}\n",
        "        elif current_id and parts[0] != '0':\n",
        "            if parts[1] == 'NAME': person_map[current_id]['name'] = parts[2].replace('/', '')\n",
        "            elif parts[1] == 'FAMC': person_map[current_id]['famc'] = clean_id(parts[2])\n",
        "            elif parts[1] == 'NPFX':\n",
        "                payload = parts[2]\n",
        "                cm_val = \"0\"\n",
        "                m = re.search(r'^(\\d+)|(\\d+)\\s*cM', payload, re.IGNORECASE)\n",
        "                if m: cm_val = m.group(1) if m.group(1) else m.group(2)\n",
        "\n",
        "                friendly_name = resolve_name(payload)\n",
        "                final_note = lineage_lookup.get(friendly_name, payload)\n",
        "                match_nodes.append({'id': current_id, 'cm': cm_val, 'note': final_note, 'participant': friendly_name})\n",
        "\n",
        "        if parts[0] == '0' and len(parts) > 2 and 'FAM' in parts[2]:\n",
        "            current_fam = clean_id(parts[1])\n",
        "            fam_map[current_fam] = {'husb': None, 'wife': None}\n",
        "        elif current_fam and parts[0] != '0':\n",
        "            if parts[1] == 'HUSB' and len(parts) > 2: fam_map[current_fam]['husb'] = clean_id(parts[2])\n",
        "            elif parts[1] == 'WIFE' and len(parts) > 2: fam_map[current_fam]['wife'] = clean_id(parts[2])\n",
        "\n",
        "# B. Link\n",
        "for pid, p in person_map.items():\n",
        "    if p['famc'] and p['famc'] in fam_map:\n",
        "        p['father'] = fam_map[p['famc']]['husb']\n",
        "        p['mother'] = fam_map[p['famc']]['wife']\n",
        "\n",
        "# C. Propagate\n",
        "deep_index = {}\n",
        "def bubble_up(start, payload):\n",
        "    q = [start]; vis = set()\n",
        "    while q:\n",
        "        curr = q.pop(0)\n",
        "        if curr in vis: continue\n",
        "        vis.add(curr)\n",
        "        if curr not in deep_index: deep_index[curr] = []\n",
        "        deep_index[curr].append(payload)\n",
        "        if curr in person_map:\n",
        "            if person_map[curr].get('father'): q.append(person_map[curr]['father'])\n",
        "            if person_map[curr].get('mother'): q.append(person_map[curr]['mother'])\n",
        "\n",
        "for m in match_nodes:\n",
        "    payload = {'id': m['id'], 'name': m['participant'], 'cm': m['cm'], 'note': m['note']}\n",
        "    bubble_up(m['id'], payload)\n",
        "\n",
        "final_deep_db = {}\n",
        "for pid, matches in deep_index.items():\n",
        "    final_deep_db[pid] = {'n': person_map[pid]['name'], 'm': matches}\n",
        "\n",
        "print(f\"[4] Built Deep Index for {len(final_deep_db)} ancestors.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. HTML GENERATOR\n",
        "# ==============================================================================\n",
        "html_content = f\"\"\"<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <title>Yates Lineage Proof Engine</title>\n",
        "    <link rel=\"stylesheet\" href=\"partials_unified.css\">\n",
        "    <style>\n",
        "        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; background: #f4f7f6; margin: 0; padding: 20px; }}\n",
        "        .dashboard {{ max-width: 1200px; margin: 0 auto; background: white; padding: 25px; box-shadow: 0 4px 15px rgba(0,0,0,0.1); border-radius: 8px; }}\n",
        "\n",
        "        /* UPDATED NAVIGATION STYLES */\n",
        "        nav.oldnav ul {{\n",
        "            display: flex;\n",
        "            flex-wrap: wrap;\n",
        "            justify-content: center;\n",
        "            background-color: #006064 !important;\n",
        "            border-bottom: 2px solid #00acc1 !important;\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "            list-style: none;\n",
        "        }}\n",
        "        nav.oldnav li {{ display: inline-block; }}\n",
        "        nav.oldnav a {{\n",
        "            display: block;\n",
        "            padding: 10px 15px;\n",
        "            text-decoration: none;\n",
        "            color: #e0f7fa !important;\n",
        "            font-size: 14px;\n",
        "        }}\n",
        "        nav.oldnav a:hover {{ background-color: #00838f !important; }}\n",
        "\n",
        "        /* TABS */\n",
        "        .tabs {{ display: flex; gap: 10px; margin-bottom: 20px; border-bottom: 2px solid #ddd; margin-top: 20px; }}\n",
        "        .tab {{ padding: 12px 25px; cursor: pointer; font-weight: bold; color: #555; border-radius: 8px 8px 0 0; background: #f1f1f1; transition: all 0.2s; }}\n",
        "        .tab.active {{ background: #006064; color: white; }}\n",
        "\n",
        "        /* SECTIONS */\n",
        "        .view-section {{ display: none; padding: 25px; background: #e0f2f1; border-radius: 0 0 8px 8px; border: 1px solid #b2dfdb; }}\n",
        "        .view-section.active {{ display: block; }}\n",
        "\n",
        "        /* CONTROLS */\n",
        "        label {{ font-weight: bold; color: #004d40; display: block; margin-bottom: 5px; }}\n",
        "        select, input {{ width: 100%; padding: 12px; border: 1px solid #aaa; border-radius: 4px; font-size: 16px; margin-bottom: 5px; }}\n",
        "        button {{ padding: 10px 20px; background: #00838f; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 16px; }}\n",
        "        button:hover {{ background: #006064; }}\n",
        "\n",
        "        /* RESULTS */\n",
        "        .proof-box {{ margin-top: 30px; display: none; }}\n",
        "        .ancestor-title {{ font-size: 1.6em; font-weight: bold; color: #004d40; border-bottom: 2px solid #004d40; padding-bottom: 10px; margin-bottom: 15px; }}\n",
        "\n",
        "        .analysis-block {{ background: #fffde7; border-left: 6px solid #fbc02d; padding: 20px; font-size: 1.05em; line-height: 1.6; color: #333; margin-bottom: 20px; }}\n",
        "        .analysis-block b {{ color: #000; }}\n",
        "\n",
        "        table {{ width: 100%; border-collapse: collapse; background: white; border: 1px solid #ccc; }}\n",
        "        th {{ background: #006064; color: white; padding: 12px; text-align: left; font-weight: bold; }}\n",
        "        td {{ padding: 10px 12px; border-bottom: 1px solid #eee; font-size: 0.95em; }}\n",
        "        tr:nth-child(even) {{ background: #f9f9f9; }}\n",
        "        tr:hover {{ background: #e0f7fa; }}\n",
        "        a {{ color: #006064; text-decoration: none; font-weight: bold; }}\n",
        "        a:hover {{ text-decoration: underline; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "\n",
        "<div class=\"dashboard\">\n",
        "    <div style=\"text-align:center; margin-bottom:10px;\">\n",
        "        <h1 style=\"color:#006064; margin:0;\">🧬 Lineage Proof Engine</h1>\n",
        "        <p style=\"color:#666;\">Forensic confirmation of ancestral lines via Collateral DNA Saturation.</p>\n",
        "    </div>\n",
        "\n",
        "    <nav class=\"oldnav\">\n",
        "      <ul>\n",
        "        <li><a href=\"/ons-study/contents.shtml\">Contents</a></li>\n",
        "        <li><a href=\"/ons-study/yates_ancestor_register.shtml\">DNA Register</a></li>\n",
        "        <li><a href=\"/ons-study/just-trees.shtml\">Trees</a></li>\n",
        "        <li><a href=\"/ons-study/dna_network.shtml\">DNA Network</a></li>\n",
        "        <li><a href=\"/ons-study/lineage_proof.html\" style=\"color:#ffeb3b !important; font-weight:bold;\">Lineage Proof</a></li>\n",
        "        <li><a href=\"/ons-study/subscribe_updates.shtml\">Subscribe Updates</a></li>\n",
        "        <li><a href=\"/ons-study/share_matches.shtml\">Share Your Matches</a></li>\n",
        "        <li><a href=\"/ons-study/gedmatchkits.htm\">Gedmatch Kits</a></li>\n",
        "        <li><a href=\"/ons-study/cousin_list_print.htm\">Print Cousin List</a></li>\n",
        "        <li><a href=\"/ons-study/yates_ancestor_register.csv\">Download CSV</a></li>\n",
        "        <li><a href=\"/ons-study/yates_ancestor_register.xlsx\">Download Excel</a></li>\n",
        "        <li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\">Study Details</a></li>\n",
        "        <li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\">Theory in Action</a></li>\n",
        "        <li><a href=\"/gengen/images/cousin-calculator.jpg\" target=\"_blank\">Cousin Connection</a></li>\n",
        "        <li><a href=\"/gengen/images/Shared_cM_Project_v4.jpg\" target=\"_blank\">Cousin by DNA</a></li>\n",
        "      </ul>\n",
        "    </nav>\n",
        "\n",
        "    <div class=\"tabs\">\n",
        "        <div class=\"tab active\" onclick=\"switchTab('macro')\">Option A: Ancestor Pairs (Macro)</div>\n",
        "        <div class=\"tab\" onclick=\"switchTab('micro')\">Option B: ID Search (Sub-Lines)</div>\n",
        "    </div>\n",
        "\n",
        "    <div id=\"view-macro\" class=\"view-section active\">\n",
        "        <label>Select Ancestral Couple:</label>\n",
        "        <select id=\"ancestorSelect\" onchange=\"modeMacro()\">\n",
        "            <option value=\"\">-- Select --</option>\n",
        "        </select>\n",
        "        <div style=\"font-size:0.85em; color:#555; margin-top:5px;\">Displays all matches descending from a major First Ancestor grouping.</div>\n",
        "    </div>\n",
        "\n",
        "    <div id=\"view-micro\" class=\"view-section\">\n",
        "        <label>Enter TNG Person ID (e.g. I47848):</label>\n",
        "        <div style=\"display:flex; gap:10px;\">\n",
        "            <input type=\"text\" id=\"searchInput\" placeholder=\"I...\" onkeyup=\"if(event.key === 'Enter') modeMicro()\">\n",
        "            <button onclick=\"modeMicro()\">Prove Lineage</button>\n",
        "        </div>\n",
        "        <div style=\"font-size:0.85em; color:#555; margin-top:5px;\">Crawls the tree to find every DNA match descending from a specific intermediate ancestor.</div>\n",
        "    </div>\n",
        "\n",
        "    <div id=\"proofContainer\" class=\"proof-box\">\n",
        "        <div id=\"proofHeader\" class=\"ancestor-title\"></div>\n",
        "        <div id=\"analysisBlock\" class=\"analysis-block\"></div>\n",
        "        <table id=\"dataTable\">\n",
        "            <thead>\n",
        "                <tr>\n",
        "                    <th width=\"25%\">Participant</th>\n",
        "                    <th width=\"10%\">ID</th>\n",
        "                    <th width=\"10%\">cM</th>\n",
        "                    <th>Lineage / Note</th>\n",
        "                </tr>\n",
        "            </thead>\n",
        "            <tbody id=\"tableBody\"></tbody>\n",
        "        </table>\n",
        "    </div>\n",
        "</div>\n",
        "\n",
        "<script>\n",
        "    const ANCESTORS = {json.dumps(ancestor_list)};\n",
        "    const DB_CSV = {json.dumps(csv_payload)};\n",
        "    const DB_DEEP = {json.dumps(final_deep_db)};\n",
        "\n",
        "    const sel = document.getElementById('ancestorSelect');\n",
        "    ANCESTORS.forEach(a => {{\n",
        "        const opt = document.createElement('option');\n",
        "        opt.value = a; opt.innerText = a;\n",
        "        sel.appendChild(opt);\n",
        "    }});\n",
        "\n",
        "    function switchTab(mode) {{\n",
        "        document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));\n",
        "        document.querySelectorAll('.view-section').forEach(v => v.classList.remove('active'));\n",
        "        if(mode === 'macro') {{\n",
        "            document.querySelector('.tab:nth-child(1)').classList.add('active');\n",
        "            document.getElementById('view-macro').classList.add('active');\n",
        "        }} else {{\n",
        "            document.querySelector('.tab:nth-child(2)').classList.add('active');\n",
        "            document.getElementById('view-micro').classList.add('active');\n",
        "        }}\n",
        "        document.getElementById('proofContainer').style.display = 'none';\n",
        "    }}\n",
        "\n",
        "    function getSurname(fullName) {{\n",
        "        if(!fullName) return \"\";\n",
        "        let clean = fullName.trim();\n",
        "        if(clean.indexOf(',') > -1) {{\n",
        "            return clean.split(',')[0].trim().toUpperCase();\n",
        "        }} else {{\n",
        "            let parts = clean.split(' ');\n",
        "            return parts[parts.length-1].trim().toUpperCase();\n",
        "        }}\n",
        "    }}\n",
        "\n",
        "    function sortMatches(matches, key) {{\n",
        "        return matches.sort((a, b) => {{\n",
        "            const surA = getSurname(a[key]);\n",
        "            const surB = getSurname(b[key]);\n",
        "            const nameA = (a[key] || \"\").toUpperCase();\n",
        "            const nameB = (b[key] || \"\").toUpperCase();\n",
        "            if(surA < surB) return -1;\n",
        "            if(surA > surB) return 1;\n",
        "            if(nameA < nameB) return -1;\n",
        "            if(nameA > nameB) return 1;\n",
        "            return 0;\n",
        "        }});\n",
        "    }}\n",
        "\n",
        "    function modeMacro() {{\n",
        "        const target = document.getElementById('ancestorSelect').value;\n",
        "        if(!target) return;\n",
        "        let matches = DB_CSV.filter(r => r.ancestor === target);\n",
        "        matches = sortMatches(matches, \"participant\");\n",
        "        renderTable(target, matches, 'macro');\n",
        "    }}\n",
        "\n",
        "    function modeMicro() {{\n",
        "        let raw = document.getElementById('searchInput').value.trim();\n",
        "        let id = raw.replace(/[^a-zA-Z0-9]/g, \"\");\n",
        "        if (id.match(/^\\\\d+$/)) id = \"I\" + id;\n",
        "        id = id.toUpperCase();\n",
        "        const record = DB_DEEP[id];\n",
        "        if(!record) {{ alert(\"ID \" + id + \" has no downstream DNA matches.\"); return; }}\n",
        "        let matches = record.m;\n",
        "        matches = sortMatches(matches, \"name\");\n",
        "        renderTable(record.n + \" (\" + id + \")\", matches, 'micro');\n",
        "    }}\n",
        "\n",
        "    function generateStats(title, matches, type) {{\n",
        "        const key = type === 'macro' ? 'participant' : 'name';\n",
        "        const unique = new Set(matches.map(m => m[key])).size;\n",
        "        const total = matches.length;\n",
        "\n",
        "        let totalCM = 0; let maxCM = 0;\n",
        "        matches.forEach(m => {{\n",
        "            const val = parseFloat(m.cm) || 0;\n",
        "            totalCM += val;\n",
        "            if(val > maxCM) maxCM = val;\n",
        "        }});\n",
        "        const avgCM = (total > 0) ? (totalCM / total).toFixed(1) : 0;\n",
        "\n",
        "        let html = `<p><strong>Collateral Saturation Analysis:</strong></p>`;\n",
        "        html += `<p>The genealogical relationship to <b>${{title}}</b> is supported by a cumulative overload of genetic evidence. `;\n",
        "        html += `This line is validated by <b>${{unique}} independent testers</b> contributing a total of <b>${{total}} DNA matches</b>.</p>`;\n",
        "        html += `<p>The group shares a total of <b>${{totalCM.toFixed(0)}} cM</b> (Average: ${{avgCM}} cM; Max: ${{maxCM}} cM). `;\n",
        "\n",
        "        if (unique >= 5) {{\n",
        "            html += `The high number of unique participants creates a <strong>Triangulation Cluster</strong>, providing strong evidence that this couple is the genetic common ancestor. `;\n",
        "            html += `This non-random distribution affirms the study's central claim: kinship is supported by DNA saturation, not conjecture.</p>`;\n",
        "        }} else if (unique >= 2) {{\n",
        "             html += `The presence of multiple independent descendants confirms this line, though additional collateral testing would further strengthen the specific path proof.</p>`;\n",
        "        }} else {{\n",
        "             html += `Evidence is currently limited to a single participant line. While valid, this represents an emerging cluster that requires future triangulation.</p>`;\n",
        "        }}\n",
        "        html += `<p style=\"font-size:0.9em; color:#555; margin-top:10px;\"><em>Methodology Note: This analysis employs the Genealogical Proof Standard (GPS), prioritizing match volume and pattern consistency over isolated data points.</em></p>`;\n",
        "        return html;\n",
        "    }}\n",
        "\n",
        "    function renderTable(title, matches, type) {{\n",
        "        document.getElementById('proofContainer').style.display = 'block';\n",
        "        document.getElementById('proofHeader').innerText = title;\n",
        "        document.getElementById('analysisBlock').innerHTML = generateStats(title, matches, type);\n",
        "\n",
        "        const tbody = document.getElementById('tableBody');\n",
        "        tbody.innerHTML = \"\";\n",
        "\n",
        "        matches.forEach(m => {{\n",
        "            const tr = document.createElement('tr');\n",
        "            let name = type==='macro' ? m.participant : m.name;\n",
        "            let id = m.id; let cm = m.cm; let note = type==='macro' ? m.lineage : m.note;\n",
        "\n",
        "            if(name && name.indexOf(' ') === -1 && name.length > 3) name = name.toUpperCase();\n",
        "            let nameLink = `<b>${{name}}</b>`;\n",
        "            if(id) nameLink = `<a href=\"https://yates.one-name.net/tng/verticalchart.php?personID=${{id}}&tree=tree1&parentset=0&display=vertical&generations=15\" target=\"_blank\">${{name}}</a>`;\n",
        "\n",
        "            tr.innerHTML = `<td>${{nameLink}}</td><td>${{id}}</td><td>${{cm}}</td><td style=\"font-size:0.95em; color:#555;\">${{note}}</td>`;\n",
        "            tbody.appendChild(tr);\n",
        "        }});\n",
        "    }}\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. UPLOAD\n",
        "# ==============================================================================\n",
        "with open(OUTPUT_HTML, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"[5] Uploading {OUTPUT_HTML}...\")\n",
        "try:\n",
        "    ftps = FTP_TLS(timeout=60)\n",
        "    ftps.connect(HOST, PORT)\n",
        "    ftps.auth()\n",
        "    ftps.login(USER, PASS)\n",
        "    try: ftps.prot_p(); ftps.set_pasv(True)\n",
        "    except: pass\n",
        "\n",
        "    try: ftps.cwd(f\"/{REMOTE_SUBDIR}\")\n",
        "    except:\n",
        "        try: ftps.cwd(f\"/public_html/{REMOTE_SUBDIR}\")\n",
        "        except: pass\n",
        "\n",
        "    with open(OUTPUT_HTML, \"rb\") as f:\n",
        "        ftps.storbinary(f\"STOR {OUTPUT_HTML}\", f)\n",
        "\n",
        "    print(f\"    - [SUCCESS] Tool Live: {WEB_BASE_URL}{OUTPUT_HTML}\")\n",
        "    ftps.quit()\n",
        "except Exception as e:\n",
        "    print(f\"    - [ERROR] Upload failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhyxp9OAk9xe",
        "outputId": "0f21c04a-86ea-4f8a-d00e-a7b1a07d4142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      CELL 4: THE PROOF ENGINE\n",
            "      (Final Navigation Update)\n",
            "============================================================\n",
            "[1] Loaded 94 privacy keys.\n",
            "[2] Prepared Macro Data (1211 groups).\n",
            "[3] Parsing GEDCOM for Deep ID Graph...\n",
            "[4] Built Deep Index for 17622 ancestors.\n",
            "[5] Uploading lineage_proof.html...\n",
            "    - [SUCCESS] Tool Live: https://yates.one-name.net/ons-study/lineage_proof.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mdymryD4o2j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST Cell"
      ],
      "metadata": {
        "id": "ydh7RNi7elx4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sx2uidReheCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPACE"
      ],
      "metadata": {
        "id": "a-cTNFwwEHrO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 0"
      ],
      "metadata": {
        "id": "pV0T5zIN0qvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.01.31-UNIFIED)\n",
        "# - Complete & runnable Colab cell: one contiguous block, no fragments.\n",
        "# - Source ASCII-only; any file writes must use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Authority:\n",
        "#     * Colab work dir: /content\n",
        "#     * Scripts pulled from: /partials/colab_cells/*.py  (server)\n",
        "#     * Latest GEDCOM pulled from: /tng/gedcom/*.ged     (server) if no local GEDCOM exists\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2026.01.31-UNIFIED | Encoding=ISO-8859-15\n",
        "# - Execution order (after pulls): cell1.py, cell2.py, cell2b.py, cell2c.py, cell2k.py, cell3.py\n",
        "# - IMPORTANT: Scripts are executed via \"python3 -u script.py\" (NOT exec/compile) to avoid\n",
        "#   multiprocessing pickling failures under ProcessPoolExecutor.\n",
        "# ====================================================================\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2026.01.31-UNIFIED | Encoding=ISO-8859-15\")\n",
        "\n",
        "DECLARED_LINES = 999  # audit-only; not enforced\n",
        "print(\"[AUDIT] DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "import os\n",
        "import socket\n",
        "import traceback\n",
        "import hashlib\n",
        "import glob\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "\n",
        "# ---------- 0) Env / secrets ----------\n",
        "\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\", \"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\", \"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\", \"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\", \"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\", \"\") or \"\").strip().strip(\"/\")\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\n",
        "    \"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\"\n",
        "    % (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\")\n",
        ")\n",
        "\n",
        "if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "    raise SystemExit(\"[FATAL] Missing FTP_HOST/FTP_USER/FTP_PASS; cannot run orchestrator.\")\n",
        "\n",
        "# ---------- 1) FTPS helpers ----------\n",
        "\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _ftps_connect():\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()  # Explicit FTPS\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for seg in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.cwd(seg)\n",
        "            except all_errors:\n",
        "                try:\n",
        "                    ftps.mkd(seg)\n",
        "                except all_errors:\n",
        "                    pass\n",
        "                ftps.cwd(seg)\n",
        "    return ftps\n",
        "\n",
        "def _sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def _safe_nlst(ftps):\n",
        "    try:\n",
        "        return ftps.nlst()\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# ---------- 2) Pull authority scripts from /partials/colab_cells ----------\n",
        "\n",
        "SCRIPT_REMOTE_DIR = \"/partials/colab_cells\"\n",
        "SCRIPT_NAMES = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell2k.py\", \"cell3.py\"]\n",
        "\n",
        "def pull_authority_scripts():\n",
        "    print(\"[STEP] Pulling authority scripts from server ...\")\n",
        "    pulled = 0\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        try:\n",
        "            pwd0 = ftps.pwd()\n",
        "        except Exception:\n",
        "            pwd0 = \"(unknown)\"\n",
        "        print(\"[OK] Connected via FTPS (explicit AUTH TLS).\")\n",
        "        print(\"[INFO] Initial PWD on server: %s\" % pwd0)\n",
        "\n",
        "        # Navigate to script dir\n",
        "        try:\n",
        "            try:\n",
        "                ftps.cwd(\"/\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            for seg in [p for p in SCRIPT_REMOTE_DIR.split(\"/\") if p]:\n",
        "                ftps.cwd(seg)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Unable to cwd to %s (%s)\" % (SCRIPT_REMOTE_DIR, e))\n",
        "\n",
        "        print(\"[INFO] Using remote dir for scripts: %s\" % SCRIPT_REMOTE_DIR)\n",
        "\n",
        "        listing = _safe_nlst(ftps)\n",
        "        if listing:\n",
        "            print(\"[INFO] Remote listing sample (first 25): %s\" % \", \".join(listing[:25]))\n",
        "        else:\n",
        "            print(\"[WARN] Remote listing is empty/unavailable for %s\" % SCRIPT_REMOTE_DIR)\n",
        "\n",
        "        os.chdir(\"/content\")\n",
        "        for name in SCRIPT_NAMES:\n",
        "            try:\n",
        "                local_path = os.path.join(\"/content\", name)\n",
        "                with open(local_path, \"wb\") as f:\n",
        "                    ftps.retrbinary(\"RETR \" + name, f.write)\n",
        "                sz = os.path.getsize(local_path)\n",
        "                sh = _sha256_of_file(local_path)\n",
        "                print(\"[PULL] %s -> %s  size=%d  sha256=%s\" % (name, local_path, sz, sh))\n",
        "                pulled += 1\n",
        "            except Exception as e:\n",
        "                print(\"[MISS] Could not pull %s: %s\" % (name, e))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Pulled %d script(s) from authority shelf.\" % pulled)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Script pull failed:\", e)\n",
        "        traceback.print_exc()\n",
        "    return pulled\n",
        "\n",
        "# ---------- 3) GEDCOM selection: prefer local, else pull latest ----------\n",
        "\n",
        "GEDCOM_REMOTE_DIR = \"/tng/gedcom\"\n",
        "LOCAL_GED_DIR = \"/content\"\n",
        "\n",
        "def _pick_local_gedcom_if_present():\n",
        "    \"\"\"\n",
        "    Prefer any *.ged already in /content.\n",
        "    Deterministic: newest mtime, tie-breaker lexicographic.\n",
        "    \"\"\"\n",
        "    os.chdir(LOCAL_GED_DIR)\n",
        "    geds = glob.glob(\"*.ged\")\n",
        "    if not geds:\n",
        "        return None\n",
        "    def _key(p):\n",
        "        try:\n",
        "            return (float(os.path.getmtime(p)), str(p))\n",
        "        except Exception:\n",
        "            return (0.0, str(p))\n",
        "    geds.sort(key=_key, reverse=True)\n",
        "    chosen = geds[0]\n",
        "    try:\n",
        "        ts = datetime.fromtimestamp(os.path.getmtime(chosen)).isoformat(sep=\" \", timespec=\"seconds\")\n",
        "    except Exception:\n",
        "        ts = \"unknown\"\n",
        "    print(\"[INFO] Local GEDCOM present in /content. Using: %s (mtime=%s)\" % (chosen, ts))\n",
        "    return os.path.join(LOCAL_GED_DIR, chosen)\n",
        "\n",
        "def _choose_latest_gedcom(ftps, names):\n",
        "    \"\"\"\n",
        "    Pick the newest *.ged using MDTM if available; fallback = last alphabetically.\n",
        "    \"\"\"\n",
        "    ged_files = [n for n in names if n.lower().endswith(\".ged\")]\n",
        "    if not ged_files:\n",
        "        return None\n",
        "\n",
        "    latest_name = None\n",
        "    latest_ts = None\n",
        "\n",
        "    for nm in ged_files:\n",
        "        ts = None\n",
        "        try:\n",
        "            resp = ftps.sendcmd(\"MDTM \" + nm)  # '213 YYYYMMDDhhmmss'\n",
        "            parts = resp.strip().split()\n",
        "            if len(parts) == 2 and parts[0] == \"213\":\n",
        "                ts = parts[1]\n",
        "        except Exception:\n",
        "            ts = None\n",
        "        if ts is None:\n",
        "            ts = \"00000000000000\" + nm\n",
        "        if latest_ts is None or ts > latest_ts:\n",
        "            latest_ts = ts\n",
        "            latest_name = nm\n",
        "    return latest_name\n",
        "\n",
        "def pull_latest_gedcom_if_needed():\n",
        "    \"\"\"\n",
        "    If a GEDCOM already exists locally, do NOT pull from server.\n",
        "    Otherwise pull latest from /tng/gedcom and ensure exactly one *.ged in /content.\n",
        "    \"\"\"\n",
        "    local = _pick_local_gedcom_if_present()\n",
        "    if local:\n",
        "        print(\"[STEP] Skipping server GEDCOM pull (local GEDCOM already present).\")\n",
        "        print(\"[INFO] Cell 1 will see local GEDCOM: %s\" % local)\n",
        "        return\n",
        "\n",
        "    print(\"\\n[STEP] Pulling latest GEDCOM from %s ...\" % GEDCOM_REMOTE_DIR)\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        try:\n",
        "            ftps.cwd(\"/\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        for seg in [p for p in GEDCOM_REMOTE_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "        names = _safe_nlst(ftps)\n",
        "        if not names:\n",
        "            print(\"[WARN] No files listed in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        latest = _choose_latest_gedcom(ftps, names)\n",
        "        if not latest:\n",
        "            print(\"[WARN] No .ged files found in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        print(\"[INFO] Latest GEDCOM on server: %s\" % latest)\n",
        "\n",
        "        # Clean any old local GEDCOMs so Cell 1 cannot accidentally pick the wrong one\n",
        "        try:\n",
        "            for fname in os.listdir(LOCAL_GED_DIR):\n",
        "                if fname.lower().endswith(\".ged\"):\n",
        "                    try:\n",
        "                        os.remove(os.path.join(LOCAL_GED_DIR, fname))\n",
        "                        print(\"[CLEAN] Removed old local GEDCOM:\", fname)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] Could not remove %s: %s\" % (fname, e))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Could not scan local GED dir:\", e)\n",
        "\n",
        "        local_path = os.path.join(LOCAL_GED_DIR, latest)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR \" + latest, f.write)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        sz = os.path.getsize(local_path)\n",
        "        print(\"[OK] Pulled GEDCOM -> %s  size=%d bytes\" % (local_path, sz))\n",
        "        print(\"[INFO] Cell 1 will now see exactly one *.ged in /content.\")\n",
        "    except Exception:\n",
        "        print(\"[WARN] GEDCOM pull failed; Cell 1 will use any existing local *.ged instead.\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 4) Run scripts in order (subprocess; fixes ProcessPool pickling) ----------\n",
        "\n",
        "def run_script(path):\n",
        "    print(\"\\n[RUN] %s\" % path)\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[SKIP] %s not found in /content.\" % path)\n",
        "        return\n",
        "\n",
        "    # Use the same Python interpreter, unbuffered output.\n",
        "    cmd = [sys.executable, \"-u\", path]\n",
        "\n",
        "    try:\n",
        "        # Stream stdout+stderr together so logs appear in-order in Colab.\n",
        "        p = subprocess.Popen(\n",
        "            cmd,\n",
        "            cwd=\"/content\",\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            encoding=\"utf-8\",\n",
        "            errors=\"replace\",\n",
        "            env=os.environ.copy(),\n",
        "        )\n",
        "\n",
        "        assert p.stdout is not None\n",
        "        for line in p.stdout:\n",
        "            # Print already has newline; avoid double spacing.\n",
        "            print(line.rstrip(\"\\n\"))\n",
        "\n",
        "        rc = p.wait()\n",
        "        if rc == 0:\n",
        "            print(\"[DONE] %s\" % path)\n",
        "        else:\n",
        "            print(\"[ERROR] Script failed (exit=%d): %s\" % (rc, path))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Exception while running %s: %s\" % (path, e))\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    os.chdir(\"/content\")\n",
        "\n",
        "    pulled = pull_authority_scripts()\n",
        "    if pulled == 0:\n",
        "        print(\"[FATAL] No authority scripts pulled; aborting.\")\n",
        "        return\n",
        "\n",
        "    pull_latest_gedcom_if_needed()\n",
        "\n",
        "    script_order = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell2k.py\", \"cell3.py\"]\n",
        "    print(\"\\n[STEP] Running scripts in order: %s\" % \", \".join(script_order))\n",
        "    for s in script_order:\n",
        "        run_script(os.path.join(\"/content\", s))\n",
        "\n",
        "main()\n",
        "\n",
        "print(\"\\n--- Cell 0 That's all folks, Orchestrator complete (authority scripts pulled, GEDCOM local-preferred, then Cell1/2/2b/2c/2k/3 executed) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkMmob1Y7uWv",
        "outputId": "ae7cbd16-cef5-4600-9566-4e4731d3f43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2026.01.31-UNIFIED | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=999\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[STEP] Pulling authority scripts from server ...\n",
            "[OK] Connected via FTPS (explicit AUTH TLS).\n",
            "[INFO] Initial PWD on server: /\n",
            "[INFO] Using remote dir for scripts: /partials/colab_cells\n",
            "[INFO] Remote listing sample (first 25): cell2.py, cell2b.py, cell2b_NetworkAuthority.py, yates_authority_first_ancestor_map.py, yates_authority_lockin.py, cell2k.py, cell2d.py, ., cell2c.py, .., cell0_netupdate.py, cell3.py, cell1.py, cell0.py, colab_cell_summary.py\n",
            "[PULL] cell1.py -> /content/cell1.py  size=25644  sha256=cf68391fda61f4bd\n",
            "[PULL] cell2.py -> /content/cell2.py  size=32739  sha256=de7cf166da009ec2\n",
            "[PULL] cell2b.py -> /content/cell2b.py  size=51109  sha256=990c463e7a0fe0c0\n",
            "[PULL] cell2c.py -> /content/cell2c.py  size=33026  sha256=22fd337bd39e7abf\n",
            "[PULL] cell2k.py -> /content/cell2k.py  size=45269  sha256=77a8738001871c6b\n",
            "[PULL] cell3.py -> /content/cell3.py  size=40678  sha256=8e80a0aa139618f2\n",
            "[OK] Pulled 6 script(s) from authority shelf.\n",
            "[INFO] Local GEDCOM present in /content. Using: yates_study_2025.ged (mtime=2026-02-06 01:04:11)\n",
            "[STEP] Skipping server GEDCOM pull (local GEDCOM already present).\n",
            "[INFO] Cell 1 will see local GEDCOM: /content/yates_study_2025.ged\n",
            "\n",
            "[STEP] Running scripts in order: cell1.py, cell2.py, cell2b.py, cell2c.py, cell2k.py, cell3.py\n",
            "\n",
            "[RUN] /content/cell1.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2026.02.03-UNIFIED-BASELINE+AUTHORITY-PERSIST | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[INFO] Local GEDCOM present in content. Selected newest: yates_study_2025.ged (mtime=2026-02-06 01:04:11)\n",
            "[INFO] Using GEDCOM: yates_study_2025.ged\n",
            "GEDCOM contained 63510 total records\n",
            "Records tagged and filtered by NPFX: 1700\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1700\n",
            "After manual filter, total records: 93\n",
            "[OK] Wrote autosomal_count.txt = 1700\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: February 5, 2026 8:14 PM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 93 individuals with chunk-based parallel...\n",
            "\n",
            "Building Yates Lines (Stage 1):   0%|          | 0/93 [00:00<?, ?it/s]\n",
            "Building Yates Lines (Stage 1):  54%|█████▍    | 50/93 [00:13<00:11,  3.71it/s]\n",
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:25<00:00,  3.63it/s]\n",
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:25<00:00,  3.64it/s]\n",
            "2026-02-06 01:15:04,661 - INFO - Exported FIRST ANCESTOR PAIRS CSV -> first_ancestor_pairs.csv\n",
            "[OK] Wrote first_ancestor_pairs.csv (93 rows)\n",
            "[REPORT] Authority_FirstAncestor populated for 93 / 93 rows\n",
            "2026-02-06 01:15:04,665 - INFO - Exported CSV -> final_combined_df_with_value_labels.csv\n",
            "2026-02-06 01:15:04,671 - INFO - Exported HTML -> cell1_work_table.htm\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploaded: first_ancestor_pairs.csv -> /partials/first_ancestor_pairs.csv\n",
            "[OK] Uploads complete to /partials/\n",
            "[OK] First ancestor pairs CSV at: /partials/first_ancestor_pairs.csv\n",
            "\n",
            "--- Cell 1 Complete: lineage tokens include ID + display name + birth-death years. ---\n",
            "--- first_ancestor_pairs.csv: FirstPair_LastFirst is a slug (no spaces/commas): yatesjohn&gaterjoane. ---\n",
            "[DONE] /content/cell1.py\n",
            "\n",
            "[RUN] /content/cell2.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2_3Col_DisplayOnlyFirstAncestor | Version=2026.02.03-DISPLAYONLY-AUTHCOL-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=1880\n",
            "[LAYOUT] Column widths (px): 1=220 2=1240 3=420\n",
            "[OK] Loaded CSV: 93 rows, 7 cols\n",
            "[OK] Authority column detected: Authority_FirstAncestor\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Saved render: /content/yates_ancestor_register.shtml\n",
            "[DEBUG] SSI nav include present: True\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 52323\n",
            "partials/ons_yates_dna_register.shtml : 52323\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "CSS:       https://yates.one-name.net/partials/partials_unified.css\n",
            "\n",
            "--- Cell 2 complete (DISPLAY-ONLY First Ancestor from CSV Authority_FirstAncestor; DISPLAY SWAP 2<->3; Match Summary sorts by First Ancestor) ---\n",
            "[DONE] /content/cell2.py\n",
            "\n",
            "[RUN] /content/cell2b.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts_UnifiedHeaders | Version=2026.02.01-CELL2HDR-CELL2B1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 5, 2026 8:14 PM\n",
            "[OK] Loaded CSV for counts: 93 rows, 7 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 74956\n",
            "partials/lineage_count.shtml : 94428\n",
            "partials/cousin_list_print.htm : 34197\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "[DONE] /content/cell2b.py\n",
            "\n",
            "[RUN] /content/cell2c.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 5, 2026 8:14 PM\n",
            "[OK] Loaded CSV for DNA Network: 93 rows, 7 cols\n",
            "[PULL] dna/network_first_ancestors.txt -> /content/dna_network_first_ancestors.txt\n",
            "[INFO] Authority file synced from server.\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[INFO] Loaded 56 authority First Ancestors from dna_network_first_ancestors.txt\n",
            "[INFO] First 10 authority entries:\n",
            "   1. I47639 John Yates 1430-&I55584 Still Searching\n",
            "   2. I24496 John Yates 1581-1648&I24497 Joane Gater 1587-1666\n",
            "   3. I12324 William Yates 1720-&I12323 Anne Betty Thornborough 1725-\n",
            "   4. I19162 William Yates 1795-1866&I19163 Sally Parker 1806-\n",
            "   5. I12442 Thomas Yates 1670-&I5717 Monica Drury 1670-\n",
            "   6. I12192 Samuel Bedford Yates 1757-1844&I12193 Jane Millsaps 1753-1845\n",
            "   7. I37203 William Yates 1786-1862&I37260 Elizabeth Edwards 1793-1866\n",
            "   8. I25516 Thomas Yates 1700-1754&I55675 Still Searching\n",
            "   9. I53956 Phillip Yates &I55707 Still Searching\n",
            "  10. I10576 Joseph Yates 1754-&I10168 Mary Leigh 1753-1811\n",
            "[INFO] Filtered DNA network rows by authority list: 93 -> 93\n",
            "[OK] Wrote DNA Network partial: /content/partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 114782\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network: https://yates.one-name.net/partials/dna_network.shtml\n",
            "[DONE] /content/cell2c.py\n",
            "\n",
            "[RUN] /content/cell2k.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 5, 2026 8:14 PM\n",
            "[OK] Loaded CSV for unified DNA Network: 93 rows, 7 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote unified DNA Network partial: /content/partials/dna_network.shtml\n",
            "[OK] Wrote register CSV: /content/dna_network_register.csv\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[PUT] dna_network_register.csv -> dna/dna_network_register.csv\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 138301\n",
            "dna/dna_network_register.csv : 79487\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\n",
            "Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\n",
            "[DONE] /content/cell2k.py\n",
            "\n",
            "[RUN] /content/cell3.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=93, cols=7\n",
            "Resolver not found locally; attempting server pull ...\n",
            "[OK] Pulled resolver from server -> match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 93 / 93  unmatched: 0\n",
            "Using authority: /content/first_ancestor_pairs.csv\n",
            "[OK] Authority map ready: 25 pairs\n",
            "[OK] Column 1 replaced with First Ancestor (authority).\n",
            "[OK] Lineage formatting applied to columns: Yates DNA Ancestral Line, Authority_FirstAncestor\n",
            "[OK] Alpha sort applied by first couple: First Ancestor | keys= __sort_surname__,__sort_given__,__sort_momsurname__\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/first_ancestor_pairs.csv; enriched prefix exclusion applied) ---\n",
            "[DONE] /content/cell3.py\n",
            "\n",
            "--- Cell 0 That's all folks, Orchestrator complete (authority scripts pulled, GEDCOM local-preferred, then Cell1/2/2b/2c/2k/3 executed) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "JvOlmbj91AGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload + FIRST ANCESTOR PAIRS CSV (Sortable) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.01.31-UNIFIED-BASELINE + GOLDEN_RULE_AUTHORITY_PERSIST)\n",
        "# - Complete and runnable Colab cell, one contiguous block (no fragments).\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout/colors via /partials/partials_unified.css (single baseline).\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2026.02.03-UNIFIED-BASELINE+AUTHORITY-PERSIST | Encoding=ISO-8859-15\n",
        "# - Golden Rule (operational): No inference downstream; authority persisted here:\n",
        "#     Adds column Authority_FirstAncestor to final_combined_df_with_value_labels.csv\n",
        "#     Authority_FirstAncestor := FirstPair_LastFirst from first_ancestor_pairs.csv for same MatchID\n",
        "# =========================================================================================================\n",
        "\n",
        "import os, re, glob, logging, socket, traceback\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "from string import Template\n",
        "\n",
        "CELL_NAME = \"Cell1_FTPS_Explicit\"\n",
        "VERSION   = \"2026.02.03-UNIFIED-BASELINE+AUTHORITY-PERSIST\"\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=%s | Version=%s | Encoding=ISO-8859-15\" % (CELL_NAME, VERSION))\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(CELL_NAME)\n",
        "\n",
        "def _now_est_string():\n",
        "    try:\n",
        "        from zoneinfo import ZoneInfo\n",
        "        tz = ZoneInfo(\"America/New_York\")\n",
        "        now = datetime.now(tz)\n",
        "    except Exception:\n",
        "        now = datetime.now()\n",
        "    month_name = now.strftime(\"%B\")\n",
        "    day = now.day\n",
        "    year = now.year\n",
        "    hour_24 = now.hour\n",
        "    minute = now.minute\n",
        "    ampm = \"AM\" if hour_24 < 12 else \"PM\"\n",
        "    hour_12 = hour_24 % 12\n",
        "    if hour_12 == 0:\n",
        "        hour_12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, day, year, hour_12, minute, ampm)\n",
        "\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\",\"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\",\"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\",\"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\",\"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\",\"\") or \"\").strip().strip(\"/\")\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\" %\n",
        "      (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\"))\n",
        "\n",
        "def _ftps_connect():\n",
        "    if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "        raise RuntimeError(\"Missing FTP_HOST/FTP_USER/FTP_PASS.\")\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for p in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(p)\n",
        "        except all_errors:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except all_errors:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "\n",
        "def _ftps_upload(ftps, local_path, remote_name):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR \" + remote_name, fh)\n",
        "    print(\"[OK] Uploaded: %s -> %s/%s\" % (local_path, ftps.pwd().rstrip(\"/\"), remote_name))\n",
        "\n",
        "REMOTE_DIR        = \"partials\"\n",
        "CSV_OUT_LOCAL     = \"final_combined_df_with_value_labels.csv\"\n",
        "HTML_OUT_LOCAL    = \"cell1_work_table.htm\"\n",
        "ABS_CSV_URL       = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(CSV_OUT_LOCAL))\n",
        "ABS_HOME_URL      = \"/index.htm\"\n",
        "VITALS_CSV_PATH        = \"dna_vitals.csv\"\n",
        "AUTOSOMAL_COUNT_TXT    = \"autosomal_count.txt\"\n",
        "\n",
        "# First ancestor pairs CSV (sortable)\n",
        "FIRST_ANCESTOR_PAIRS_LOCAL = \"first_ancestor_pairs.csv\"\n",
        "ABS_FIRST_ANCESTOR_PAIRS_URL = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(FIRST_ANCESTOR_PAIRS_LOCAL))\n",
        "\n",
        "def _pick_local_gedcom_if_present():\n",
        "    geds = glob.glob(\"*.ged\")\n",
        "    if not geds:\n",
        "        return \"\"\n",
        "    def _key(p):\n",
        "        try:\n",
        "            return (float(os.path.getmtime(p)), str(p))\n",
        "        except Exception:\n",
        "            return (0.0, str(p))\n",
        "    geds.sort(key=_key, reverse=True)\n",
        "    chosen = geds[0]\n",
        "    try:\n",
        "        ts = datetime.fromtimestamp(os.path.getmtime(chosen)).isoformat(sep=\" \", timespec=\"seconds\")\n",
        "    except Exception:\n",
        "        ts = \"unknown\"\n",
        "    print(\"[INFO] Local GEDCOM present in content. Selected newest: %s (mtime=%s)\" % (chosen, ts))\n",
        "    return chosen\n",
        "\n",
        "def _ensure_gedcom_available():\n",
        "    chosen = _pick_local_gedcom_if_present()\n",
        "    if chosen:\n",
        "        return chosen\n",
        "    if \"download_latest_gedcom_from_tng\" in globals() and callable(globals()[\"download_latest_gedcom_from_tng\"]):\n",
        "        print(\"[INFO] No local GEDCOM found. Calling existing download_latest_gedcom_from_tng() ...\")\n",
        "        try:\n",
        "            globals()[\"download_latest_gedcom_from_tng\"]()\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] download_latest_gedcom_from_tng() failed:\", e)\n",
        "    return _pick_local_gedcom_if_present()\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        return self.gen_person.strip(\"@\")\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            cm = v.split(\"&\")[0].strip()\n",
        "        elif \"**\" in v:\n",
        "            cm = v.split(\"**\")[0].strip()\n",
        "        else:\n",
        "            cm = v.strip()\n",
        "        try:\n",
        "            int(cm)\n",
        "            return cm\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            s = v.split(\"&\")[1]\n",
        "            return (s.split(\"**\")[0] if \"**\" in s else s).strip()\n",
        "        return \"\"\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        return v.split(\"**\")[1].strip() if \"**\" in v else \"\"\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "        self.total_records = 0\n",
        "        self.npfx_count = 0\n",
        "        self.ydna_count = 0\n",
        "        self.autosomal_count = 0\n",
        "        self.after_manual_filter_total = 0\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(\" \", 2)\n",
        "            if not parts or not parts[0].isdigit():\n",
        "                continue\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith(\"@\") and tag.endswith(\"@\") and value == \"INDI\":\n",
        "                total += 1\n",
        "                current = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current)\n",
        "            elif current is not None:\n",
        "                if level == 2 and tag == \"NPFX\":\n",
        "                    npfx_count += 1\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                    if value and \"**\" in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal = npfx_count - ydna_count\n",
        "\n",
        "        self.total_records = total\n",
        "        self.npfx_count = npfx_count\n",
        "        self.ydna_count = ydna_count\n",
        "        self.autosomal_count = autosomal\n",
        "\n",
        "        print(\"GEDCOM contained %d total records\" % total)\n",
        "        print(\"Records tagged and filtered by NPFX: %d\" % npfx_count)\n",
        "        print(\"Records with YDNA information: %d\" % ydna_count)\n",
        "        print(\"Autosomal matches (NPFX minus YDNA): %d\" % autosomal)\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        try:\n",
        "            df_filter = pd.read_excel(\"filtered_ids.xlsx\")\n",
        "            manual_ids = set(str(x) for x in df_filter[\"ID\"])\n",
        "            self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_ids]\n",
        "            print(\"After manual filter, total records: %d\" % len(self.filter_pool))\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "\n",
        "        self.after_manual_filter_total = len(self.filter_pool)\n",
        "        return autosomal\n",
        "\n",
        "def _chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "def _extract_display_name_from_indi_block(txt):\n",
        "    if not txt:\n",
        "        return \"Unknown Name\"\n",
        "    m = re.search(r\"(^|\\n)1 NAME ([^\\n\\r]+)\", \"\\n\" + txt)\n",
        "    if not m:\n",
        "        return \"Unknown Name\"\n",
        "    name_line = (m.group(2) or \"\").strip()\n",
        "    if \"/\" not in name_line:\n",
        "        return name_line.strip() or \"Unknown Name\"\n",
        "    parts = name_line.split(\"/\", 2)\n",
        "    given = (parts[0] or \"\").strip()\n",
        "    sur = (parts[1] or \"\").strip()\n",
        "    out = (given + \" \" + sur).strip()\n",
        "    out = re.sub(r\"\\s+\", \" \", out)\n",
        "    return out if out else \"Unknown Name\"\n",
        "\n",
        "def _extract_year_from_date_line(date_line):\n",
        "    years = re.findall(r\"(\\d{4})\", str(date_line or \"\"))\n",
        "    return years[-1] if years else \"\"\n",
        "\n",
        "def _extract_birth_death_years_from_indi_block(txt):\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    b_year = \"\"\n",
        "    d_year = \"\"\n",
        "    m = re.search(r\"(^|\\n)1 BIRT\\b.*?(?:\\n2 DATE ([^\\n\\r]+))?\", \"\\n\" + txt, flags=re.S)\n",
        "    if m:\n",
        "        b_year = _extract_year_from_date_line(m.group(2) or \"\")\n",
        "    m2 = re.search(r\"(^|\\n)1 DEAT\\b.*?(?:\\n2 DATE ([^\\n\\r]+))?\", \"\\n\" + txt, flags=re.S)\n",
        "    if m2:\n",
        "        d_year = _extract_year_from_date_line(m2.group(2) or \"\")\n",
        "    if not b_year and not d_year:\n",
        "        return \"\"\n",
        "    return \"%s-%s\" % (b_year, d_year)\n",
        "\n",
        "def _find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        _find_parents(father_id, generation + 1, parents_map)\n",
        "    if mother_id:\n",
        "        _find_parents(mother_id, generation + 1, parents_map)\n",
        "\n",
        "def _find_distant(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(_find_distant(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(_find_distant(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "def _filter_lineage(winning_ids, gen_table, display_name_map, years_map):\n",
        "    matching = []\n",
        "    for generation, pair in gen_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_ids or id2 in winning_ids:\n",
        "            matching.append((generation, pair))\n",
        "    matching.sort(key=lambda x: x[0])\n",
        "\n",
        "    lines = []\n",
        "    for _, pair in matching:\n",
        "        pid1, pid2 = pair\n",
        "        n1 = (display_name_map.get(pid1, \"Unknown Name\") or \"Unknown Name\").replace(\"~\", \" \").strip()\n",
        "        n2 = (display_name_map.get(pid2, \"Unknown Name\") or \"Unknown Name\").replace(\"~\", \" \").strip()\n",
        "        y1 = (years_map.get(pid1, \"\") or \"\").replace(\"~\", \" \").strip()\n",
        "        y2 = (years_map.get(pid2, \"\") or \"\").replace(\"~\", \" \").strip()\n",
        "        t1 = \"%s~%s~%s\" % (pid1, n1, y1)\n",
        "        t2 = \"%s~%s~%s\" % (pid2, n2, y2)\n",
        "        lines.append(\"%s&%s\" % (t1, t2))\n",
        "\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "def _process_record(individual_id, ged, parents_map, display_name_map, years_map):\n",
        "    global generation_table, visited_pairs\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    _find_parents(individual_id, 1, parents_map)\n",
        "    paths = _find_distant(individual_id, parents_map)\n",
        "    best_score, best_path = None, None\n",
        "    for path in paths:\n",
        "        score = 0\n",
        "        for idx, pid in enumerate(path):\n",
        "            nm = (display_name_map.get(pid, \"\") or \"\")\n",
        "            if \"Yates\" in nm:\n",
        "                score += (idx + 1)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score, best_path = score, path\n",
        "    best_path = best_path or []\n",
        "    best_ids  = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str  = _filter_lineage(set(best_ids), generation_table, display_name_map, years_map)\n",
        "\n",
        "    cm_value = \"\"\n",
        "    sort_value = \"\"\n",
        "    ydna_value = \"\"\n",
        "    for ds in ged.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value   = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "\n",
        "    short_name = display_name_map.get(individual_id, \"Unknown Name\")\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "# ---------- First ancestor pair CSV ----------\n",
        "\n",
        "def _slug_no_space_no_comma(s):\n",
        "    s = (s or \"\").lower()\n",
        "    s = s.replace(\",\", \"\").replace(\" \", \"\")\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def _name_to_lastfirst_pretty_and_slug(display_name):\n",
        "    s = (display_name or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    if not s or s.lower() == \"unknown name\":\n",
        "        pretty = \"Unknown\"\n",
        "        return pretty, _slug_no_space_no_comma(pretty)\n",
        "    parts = s.split(\" \")\n",
        "    if len(parts) == 1:\n",
        "        pretty = parts[0]\n",
        "        return pretty, _slug_no_space_no_comma(pretty)\n",
        "    last = parts[-1]\n",
        "    given = \" \".join(parts[:-1]).strip()\n",
        "    pretty = (\"%s, %s\" % (last, given)) if given else last\n",
        "    return pretty, _slug_no_space_no_comma(pretty)\n",
        "\n",
        "def _parse_first_pair_tokens(lineage_str):\n",
        "    s = (lineage_str or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
        "    first_seg = s.split(\"~~~\", 1)[0].strip()\n",
        "    if \"&\" not in first_seg:\n",
        "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
        "    left, right = first_seg.split(\"&\", 1)\n",
        "\n",
        "    def _split_token(tok):\n",
        "        tok = (tok or \"\").strip()\n",
        "        parts = tok.split(\"~\")\n",
        "        pid = (parts[0] if len(parts) > 0 else \"\").strip()\n",
        "        nm  = (parts[1] if len(parts) > 1 else \"\").strip()\n",
        "        yrs = (parts[2] if len(parts) > 2 else \"\").strip()\n",
        "        return pid, nm, yrs\n",
        "\n",
        "    pid1, nm1, yrs1 = _split_token(left)\n",
        "    pid2, nm2, yrs2 = _split_token(right)\n",
        "\n",
        "    lf1_pretty, lf1_slug = _name_to_lastfirst_pretty_and_slug(nm1)\n",
        "    lf2_pretty, lf2_slug = _name_to_lastfirst_pretty_and_slug(nm2)\n",
        "\n",
        "    pair_lastfirst = \"%s&%s\" % (lf1_slug, lf2_slug)\n",
        "    return (pid1, lf1_pretty, yrs1, pid2, lf2_pretty, yrs2, pair_lastfirst)\n",
        "\n",
        "def main():\n",
        "    gedcom_path = _ensure_gedcom_available()\n",
        "    if not gedcom_path:\n",
        "        print(\"No GEDCOM files found in content, and no download produced one.\")\n",
        "        return False\n",
        "\n",
        "    print(\"[INFO] Using GEDCOM: %s\" % gedcom_path)\n",
        "\n",
        "    ged = Gedcom(gedcom_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "\n",
        "    with open(AUTOSOMAL_COUNT_TXT, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "    print(\"[OK] Wrote %s = %d\" % (AUTOSOMAL_COUNT_TXT, autosomal_count))\n",
        "\n",
        "    last_updated_text = _now_est_string()\n",
        "    print(\"[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: %s\" % last_updated_text)\n",
        "\n",
        "    npfx_count = int(ged.npfx_count)\n",
        "    after_manual_filter_total = int(ged.after_manual_filter_total or len(ged.filter_pool))\n",
        "\n",
        "    vitals_lines = [\n",
        "        \"Records tagged and filtered by NPFX: %d\" % npfx_count,\n",
        "        \"After manual filter, total records: %d\" % after_manual_filter_total,\n",
        "        \"LAST_UPDATED_TEXT: %s\" % last_updated_text,\n",
        "    ]\n",
        "    pd.DataFrame({\"line\": vitals_lines}).to_csv(\n",
        "        VITALS_CSV_PATH,\n",
        "        index=False,\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    )\n",
        "    print(\"[OK] Wrote dna_vitals.csv -> %s\" % os.path.abspath(VITALS_CSV_PATH))\n",
        "\n",
        "    with open(gedcom_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find(\"\\n\")\n",
        "        flend = len(blk) if flend == -1 else flend\n",
        "        first_line = blk[:flend]\n",
        "        if \"@\" in first_line:\n",
        "            s = first_line.find(\"@\") + 1\n",
        "            e = first_line.find(\"@\", s)\n",
        "            rec_id = first_line[s:e].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    families = {}\n",
        "    display_name_map = {}\n",
        "    years_map = {}\n",
        "\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if \"FAM\" in txt[:50]:\n",
        "            father_idx = txt.find(\"1 HUSB @\")\n",
        "            husb_id = txt[father_idx + len(\"1 HUSB @\"):txt.find(\"@\", father_idx + len(\"1 HUSB @\"))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find(\"1 WIFE @\")\n",
        "            wife_id = txt[wife_idx + len(\"1 WIFE @\"):txt.find(\"@\", wife_idx + len(\"1 WIFE @\"))] if wife_idx != -1 else None\n",
        "            kids = [ln.split(\"@\")[1] for ln in txt.split(\"\\n\") if ln.strip().startswith(\"1 CHIL @\")]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "        if \"INDI\" in txt[:50]:\n",
        "            display_name_map[rec_id] = _extract_display_name_from_indi_block(txt)\n",
        "            years_map[rec_id] = _extract_birth_death_years_from_indi_block(txt)\n",
        "\n",
        "    for _, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(\"Processing %d individuals with chunk-based parallel...\" % len(individual_ids))\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "\n",
        "    from functools import partial as _partial\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as ex, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in _chunks(individual_ids, chunk_size):\n",
        "            func = _partial(_process_record, ged=ged, parents_map=parents_map, display_name_map=display_name_map, years_map=years_map)\n",
        "            results = list(ex.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "\n",
        "    # ----- First ancestor pair CSV -----\n",
        "    pairs_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        match_id = str(row.get(\"ID#\", \"\") or \"\").strip()\n",
        "        lineage  = str(row.get(\"Yates DNA Ancestral Line\", \"\") or \"\")\n",
        "        pid1, lf1, yrs1, pid2, lf2, yrs2, pair_lastfirst = _parse_first_pair_tokens(lineage)\n",
        "        pairs_rows.append({\n",
        "            \"MatchID\": match_id,\n",
        "            \"FirstPair_Ancestor1_ID\": pid1,\n",
        "            \"FirstPair_Ancestor1_LastFirst\": lf1,\n",
        "            \"FirstPair_Ancestor1_Years\": yrs1,\n",
        "            \"FirstPair_Ancestor2_ID\": pid2,\n",
        "            \"FirstPair_Ancestor2_LastFirst\": lf2,\n",
        "            \"FirstPair_Ancestor2_Years\": yrs2,\n",
        "            \"FirstPair_LastFirst\": pair_lastfirst,\n",
        "        })\n",
        "\n",
        "    df_pairs = pd.DataFrame(pairs_rows)\n",
        "    if not df_pairs.empty:\n",
        "        df_pairs.sort_values(by=[\"FirstPair_LastFirst\", \"MatchID\"], inplace=True)\n",
        "\n",
        "    with open(FIRST_ANCESTOR_PAIRS_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df_pairs.to_csv(index=False))\n",
        "    logger.info(\"Exported FIRST ANCESTOR PAIRS CSV -> %s\", FIRST_ANCESTOR_PAIRS_LOCAL)\n",
        "    print(\"[OK] Wrote %s (%d rows)\" % (FIRST_ANCESTOR_PAIRS_LOCAL, len(df_pairs)))\n",
        "\n",
        "    # ============================================================\n",
        "    # GOLDEN RULE: Persist authority into the MAIN CSV (NO DOWNSTREAM INFERENCE)\n",
        "    # Authority_FirstAncestor := df_pairs.FirstPair_LastFirst by MatchID\n",
        "    # ============================================================\n",
        "    required_cols = {\"MatchID\", \"FirstPair_LastFirst\"}\n",
        "    miss = required_cols - set(df_pairs.columns)\n",
        "    if miss:\n",
        "        raise RuntimeError(\"Authority pairs missing required columns: %s\" % sorted(miss))\n",
        "\n",
        "    authority_lookup = dict(\n",
        "        zip(\n",
        "            df_pairs[\"MatchID\"].astype(str),\n",
        "            df_pairs[\"FirstPair_LastFirst\"].astype(str),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    df[\"Authority_FirstAncestor\"] = (\n",
        "        df[\"ID#\"].astype(str).map(authority_lookup).fillna(\"\")\n",
        "    )\n",
        "\n",
        "    non_blank = (df[\"Authority_FirstAncestor\"].astype(str).str.strip() != \"\").sum()\n",
        "    print(\"[REPORT] Authority_FirstAncestor populated for %d / %d rows\" % (non_blank, len(df)))\n",
        "\n",
        "    # ----- Existing main CSV export (now includes Authority_FirstAncestor) -----\n",
        "    with open(CSV_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df.to_csv(index=False))\n",
        "    logger.info(\"Exported CSV -> %s\", CSV_OUT_LOCAL)\n",
        "\n",
        "    # Include Authority_FirstAncestor in the working HTML table (optional but useful for audit)\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Authority_FirstAncestor\", \"Yates DNA Ancestral Line\"]\n",
        "\n",
        "    # Ensure table is styled/recognized as sortable by your site conventions\n",
        "    table_html = df.to_html(index=False, columns=final_cols, escape=False, border=1, classes=[\"sortable\"])\n",
        "\n",
        "    page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Cell 1 Working Table</title>\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/partials_unified.css\" />\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"wrap\">\n",
        "  <h1>Cell 1 Working Table</h1>\n",
        "\n",
        "  <div class=\"updated\">\n",
        "    <a href=\"$HOME\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "    &nbsp;|&nbsp; Last updated: $LAST_UPDATED_TEXT\n",
        "    &nbsp;|&nbsp; Download: <a href=\"$CSV\">$CSV</a>\n",
        "    &nbsp;|&nbsp; First ancestor pairs: <a href=\"$PAIRS\">$PAIRS</a>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"downloads\">\n",
        "    <a href=\"$CSV\">/partials/$CSV_NAME</a>\n",
        "    &nbsp;|&nbsp;\n",
        "    <a href=\"$PAIRS\">/partials/$PAIRS_NAME</a>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"table-scroll-wrapper\">\n",
        "    <div class=\"table-scroll\">\n",
        "      $TABLE\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\"\"\")\n",
        "\n",
        "    page = page_tpl.safe_substitute(\n",
        "        HOME=ABS_HOME_URL,\n",
        "        CSV=ABS_CSV_URL,\n",
        "        CSV_NAME=os.path.basename(ABS_CSV_URL),\n",
        "        PAIRS=ABS_FIRST_ANCESTOR_PAIRS_URL,\n",
        "        PAIRS_NAME=os.path.basename(ABS_FIRST_ANCESTOR_PAIRS_URL),\n",
        "        TABLE=table_html,\n",
        "        LAST_UPDATED_TEXT=last_updated_text,\n",
        "    )\n",
        "\n",
        "    with open(HTML_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(page)\n",
        "    logger.info(\"Exported HTML -> %s\", HTML_OUT_LOCAL)\n",
        "\n",
        "    return True\n",
        "\n",
        "ok = main()\n",
        "\n",
        "if ok and FTP_HOST and FTP_USER and FTP_PASS:\n",
        "    print(\"[INFO] Uploading artifacts to /partials/ ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        _ftps_ensure_dir(ftps, \"partials\")\n",
        "        for p in [CSV_OUT_LOCAL, HTML_OUT_LOCAL, VITALS_CSV_PATH, AUTOSOMAL_COUNT_TXT, FIRST_ANCESTOR_PAIRS_LOCAL]:\n",
        "            try:\n",
        "                _ftps_upload(ftps, p, os.path.basename(p))\n",
        "            except Exception as e:\n",
        "                print(\"[ERROR] Upload failed for %s: %s\" % (p, e))\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Uploads complete to /partials/\")\n",
        "        print(\"[OK] First ancestor pairs CSV at: %s\" % ABS_FIRST_ANCESTOR_PAIRS_URL)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing creds or build failed).\")\n",
        "\n",
        "print(\"\\n--- Cell 1 Complete: lineage tokens include ID + display name + birth-death years. ---\")\n",
        "print(\"--- first_ancestor_pairs.csv: FirstPair_LastFirst is a slug (no spaces/commas): yatesjohn&gaterjoane. ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload + FIRST ANCESTOR PAIRS CSV (Sortable) ======\n"
      ],
      "metadata": {
        "id": "jp7nii2aiz0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8480de14-45dd-40e1-ac9b-a751d6565614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2026.02.03-UNIFIED-BASELINE+AUTHORITY-PERSIST | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[INFO] Local GEDCOM present in content. Selected newest: yates_study_2025.ged (mtime=2026-02-06 01:04:11)\n",
            "[INFO] Using GEDCOM: yates_study_2025.ged\n",
            "GEDCOM contained 63510 total records\n",
            "Records tagged and filtered by NPFX: 1700\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1700\n",
            "After manual filter, total records: 93\n",
            "[OK] Wrote autosomal_count.txt = 1700\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: February 5, 2026 8:10 PM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 93 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:27<00:00,  3.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Wrote first_ancestor_pairs.csv (93 rows)\n",
            "[REPORT] Authority_FirstAncestor populated for 93 / 93 rows\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploaded: first_ancestor_pairs.csv -> /partials/first_ancestor_pairs.csv\n",
            "[OK] Uploads complete to /partials/\n",
            "[OK] First ancestor pairs CSV at: /partials/first_ancestor_pairs.csv\n",
            "\n",
            "--- Cell 1 Complete: lineage tokens include ID + display name + birth-death years. ---\n",
            "--- first_ancestor_pairs.csv: FirstPair_LastFirst is a slug (no spaces/commas): yatesjohn&gaterjoane. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "TKAmqiIIDaxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: iso-8859-15 -*-\n",
        "# Cell2_3Col_DisplayOnlyFirstAncestor - SWAP COL 2 & 3 (DISPLAY) + GOLDEN SORT RULE\n",
        "# Version=2026.02.03-DISPLAYONLY-AUTHCOL-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR\n",
        "#\n",
        "# GOLDEN RULE (operational):\n",
        "# - Cell 1 creates the single authority once and persists it into the main CSV.\n",
        "# - Cell 2 is DISPLAY-ONLY for \"First Ancestor\" (no map builds, no inference).\n",
        "#\n",
        "# DISPLAY RULE:\n",
        "# - \"First Ancestor\" is read ONLY from the CSV column: Authority_FirstAncestor\n",
        "#   (generated by Cell 1). Whatever is present is displayed (including unknowns).\n",
        "# - If the column is missing, we fail fast with a clear error.\n",
        "#\n",
        "# GOLDEN SORT RULE (locked):\n",
        "# - Display columns 1, 2, 3 with columns 2 and 3 swapped:\n",
        "#     1) Match to\n",
        "#     2) Match Summary\n",
        "#     3) First Ancestor\n",
        "# - Clicking \"Match Summary\" sorts using the data in \"First Ancestor\".\n",
        "#\n",
        "# Notes:\n",
        "# - Paste into a Colab cell OR run as a script in the same working directory.\n",
        "# - Output written as iso-8859-15 with xmlcharrefreplace.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_3Col_DisplayOnlyFirstAncestor | Version=2026.02.03-DISPLAYONLY-AUTHCOL-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# ---------- A) LAYOUT CONTROL BLOCK ----------\n",
        "COL_1_PX = 220\n",
        "COL_2_PX = 1240\n",
        "COL_3_PX = 420\n",
        "COL_WIDTHS = [COL_1_PX, COL_2_PX, COL_3_PX]\n",
        "TABLE_TOTAL_WIDTH_PX = sum(COL_WIDTHS)\n",
        "\n",
        "print(\"[LAYOUT] TABLE_TOTAL_WIDTH_PX=%d\" % TABLE_TOTAL_WIDTH_PX)\n",
        "print(\"[LAYOUT] Column widths (px): 1=%d 2=%d 3=%d\" % (COL_1_PX, COL_2_PX, COL_3_PX))\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Display-only authority column (generated by Cell 1)\n",
        "AUTH_COL_PREFER = [\"Authority_FirstAncestor\", \"authority_firstancestor\", \"authority_first_ancestor\", \"FirstAncestor_Authority\"]\n",
        "\n",
        "LOCAL_HTML        = \"yates_ancestor_register.shtml\"\n",
        "REMOTE_HTML_CANON = posixpath.join(\"partials\", \"yates_ancestor_register.shtml\")\n",
        "REMOTE_HTML_LEG   = posixpath.join(\"partials\", \"ons_yates_dna_register.shtml\")\n",
        "\n",
        "FTP_DIR  = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR        = \"partials\"\n",
        "SERVER_MAPPING_BASENAME    = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE      = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# ---------- Enriched exclusion prefix (formatted lineage) ----------\n",
        "# (Display only: affects the \"whose ... are <first couple>\" text, not the authority value)\n",
        "LINEAGE_SPOUSE_SEP = \" & \"\n",
        "\n",
        "ENRICHED_EXCLUDE_PREFIX = (\n",
        "    \"John Yates (1430-) & Still Searching ~ \"\n",
        "    \"William Yates (1389-1440) & Still Searching ~ \"\n",
        "    \"William Yates (1420-) & Still Searching ~ \"\n",
        "    \"Edmund Yates (1445-1472) & Margaret Cornell ~ \"\n",
        "    \"Richard Yates (1440-1498) & Joan Ashendon (1445-1499) ~ \"\n",
        "    \"John Yates (1471-1544) & Alice Hyde (1498-1523) ~ \"\n",
        "    \"Thomas Yates (1509-1565) & Elizabeth Fauconer (-1562) ~\"\n",
        ")\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) CSV helpers ----------\n",
        "def _read_csv_anyenc(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    dfx = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            dfx = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            dfx = None\n",
        "    if dfx is None:\n",
        "        raise RuntimeError(\"Unable to read CSV %s: %s\" % (path, last))\n",
        "    return dfx\n",
        "\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    dfm = _read_csv_anyenc(path)\n",
        "    if dfm.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    dfm = dfm.iloc[:, :2].copy()\n",
        "    dfm.columns = [\"code\", \"unmasked\"]\n",
        "    dfm[\"code\"]     = dfm[\"code\"].astype(str).str.strip().str.lower()\n",
        "    dfm[\"unmasked\"] = dfm[\"unmasked\"].astype(str).str.strip()\n",
        "    dfm = dfm[dfm[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if dfm.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return dfm\n",
        "\n",
        "# ---------- 3.1) Resolver ----------\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # pull mapping\n",
        "        local_name = SERVER_MAPPING_LOCAL_CACHE\n",
        "        try:\n",
        "            with open(local_name, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % SERVER_MAPPING_BASENAME, f.write)\n",
        "            print(\"[PULL] %s -> %s\" % (SERVER_MAPPING_BASENAME, os.path.abspath(local_name)))\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                \"Resolver not found on server: /%s (%s). Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "                % (_remote_path(SERVER_MAPPING_REMOTE), e)\n",
        "            )\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Text utils ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"  + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, couple_text_html):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        couple_text_html,\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 4.1) Couple display parsing (for Match Summary only) ----------\n",
        "def _scrub_side_keep_name_years(side_raw: str):\n",
        "    side_raw = _clean_piece(side_raw or \"\")\n",
        "    if not side_raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "\n",
        "    if \"~\" in side_raw:\n",
        "        bits = [b.strip() for b in side_raw.split(\"~\")]\n",
        "        bits = [b for b in bits if b != \"\"]\n",
        "        if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "            pid = bits[0].upper()\n",
        "            nm  = normalize_person_name(bits[1]) if len(bits) >= 2 else \"\"\n",
        "            yrs = _clean_piece(bits[2]) if len(bits) >= 3 else \"\"\n",
        "            return (pid, nm, yrs)\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", side_raw, flags=re.I)\n",
        "    if m:\n",
        "        pid  = m.group(1).upper()\n",
        "        rest = _clean_piece(m.group(2))\n",
        "        yrs  = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs  = _clean_piece(m2.group(1).replace(\" \", \"\"))\n",
        "            rest = _clean_piece(rest[:m2.start()])\n",
        "        nm = normalize_person_name(rest) if rest else \"\"\n",
        "        return (pid, nm, yrs)\n",
        "\n",
        "    nm2 = smart_titlecase(side_raw) if \" \" in side_raw else smart_titlecase(surname_given_from_token(side_raw)[0])\n",
        "    return (\"\", nm2, \"\")\n",
        "\n",
        "def _couple_display_from_token(raw_token: str):\n",
        "    raw = _clean_piece(raw_token or \"\")\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", raw, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        _pid, nm, yrs = _scrub_side_keep_name_years(raw)\n",
        "        disp = nm or raw\n",
        "        if yrs:\n",
        "            disp = disp + \" (%s)\" % yrs\n",
        "        return disp\n",
        "\n",
        "    _f_id, f_nm, f_yrs = _scrub_side_keep_name_years(parts[0])\n",
        "    _m_id, m_nm, m_yrs = _scrub_side_keep_name_years(parts[1])\n",
        "\n",
        "    f_disp = f_nm or normalize_person_name(parts[0])\n",
        "    m_disp = m_nm or normalize_person_name(parts[1])\n",
        "\n",
        "    if f_yrs:\n",
        "        f_disp = f_disp + \" (%s)\" % f_yrs\n",
        "    if m_yrs:\n",
        "        m_disp = m_disp + \" (%s)\" % m_yrs\n",
        "\n",
        "    return \"%s%s%s\" % (f_disp, LINEAGE_SPOUSE_SEP, m_disp)\n",
        "\n",
        "# ---------- Enriched exclusion (display-only) ----------\n",
        "def _norm_couple_for_match(s: str) -> str:\n",
        "    return re.sub(r\"\\s{2,}\", \" \", str(s or \"\")).strip().lower()\n",
        "\n",
        "_EXCLUDE_COUPLES = [\n",
        "    c.strip()\n",
        "    for c in re.split(r\"\\s*~\\s*\", (ENRICHED_EXCLUDE_PREFIX or \"\").strip().strip(\"~\"))\n",
        "    if c and c.strip()\n",
        "]\n",
        "\n",
        "def _strip_paren_years_anywhere(s: str) -> str:\n",
        "    return re.sub(r\"\\([^)]*\\)\", \"\", str(s or \"\")).strip()\n",
        "\n",
        "def _is_anchor_couple(couple_text: str) -> bool:\n",
        "    t = _strip_paren_years_anywhere(couple_text).lower()\n",
        "    return (\"francis yates\" in t) and (\"jane tichborne\" in t)\n",
        "\n",
        "def _apply_enriched_exclusion_to_couples(couples):\n",
        "    couples = [c for c in (couples or []) if c and str(c).strip()]\n",
        "    if not couples:\n",
        "        return couples\n",
        "\n",
        "    if _EXCLUDE_COUPLES and len(couples) >= len(_EXCLUDE_COUPLES):\n",
        "        ok = True\n",
        "        for i in range(len(_EXCLUDE_COUPLES)):\n",
        "            if _norm_couple_for_match(couples[i]) != _norm_couple_for_match(_EXCLUDE_COUPLES[i]):\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            couples = couples[len(_EXCLUDE_COUPLES):]\n",
        "\n",
        "    if couples:\n",
        "        for i, c in enumerate(couples):\n",
        "            if _is_anchor_couple(c):\n",
        "                couples = couples[i:]\n",
        "                break\n",
        "\n",
        "    return couples\n",
        "\n",
        "# ---------- 5) Read main CSV ----------\n",
        "def find_col(df0, patterns, prefer_exact=None):\n",
        "    cols = list(df0.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df0.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "df = _read_csv_anyenc(CSV_IN)\n",
        "print(\"[OK] Loaded CSV: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "id_col    = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col  = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "cm_col    = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "path_col  = find_col(df, [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "                     [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"])\n",
        "\n",
        "auth_col  = find_col(df, [r\"authority[_\\s-]*first[_\\s-]*ancestor\"], AUTH_COL_PREFER)\n",
        "\n",
        "for req, nm in [(id_col,\"ID#/PersonID\"), (match_col,\"Match to\"), (name_col,\"Name\"), (cm_col,\"cM\"), (path_col,\"Lineage\")]:\n",
        "    if not req:\n",
        "        raise ValueError(\"CSV missing required column: %s\" % nm)\n",
        "\n",
        "if not auth_col:\n",
        "    raise RuntimeError(\n",
        "        \"CSV is missing Authority_FirstAncestor (expected created by Cell 1). \"\n",
        "        \"Found columns: %s\" % list(df.columns)\n",
        "    )\n",
        "\n",
        "print(\"[OK] Authority column detected:\", auth_col)\n",
        "\n",
        "# ---------- 5.1) Vitals (optional) ----------\n",
        "AUTOSOMAL_MATCHES = \"\"\n",
        "LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    raw = str(raw or \"\").replace(\"UTC\",\"\").strip()\n",
        "    m = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})[ T](\\d{2}):(\\d{2})(?::(\\d{2}))?$\", raw)\n",
        "    if not m:\n",
        "        return raw\n",
        "    Y, Mo, D, h, mi, s = [int(x or \"0\") for x in m.groups()]\n",
        "    import datetime as _dt\n",
        "    dt = _dt.datetime(Y, Mo, D, h, mi, s) - _dt.timedelta(hours=5)\n",
        "    months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "    hh = dt.hour\n",
        "    ampm = \"AM\" if hh < 12 else \"PM\"\n",
        "    h12 = hh % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (months[dt.month-1], dt.day, dt.year, h12, dt.minute, ampm)\n",
        "\n",
        "def _format_int_with_commas(s):\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"[^0-9\\-]\", \"\", str(s))\n",
        "    if not t:\n",
        "        return str(s)\n",
        "    try:\n",
        "        return \"{:,}\".format(int(t))\n",
        "    except Exception:\n",
        "        return str(s)\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global AUTOSOMAL_MATCHES, LAST_UPDATED_TEXT\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will omit counts and last-updated text.\")\n",
        "        return\n",
        "    vdf = _read_csv_anyenc(path)\n",
        "    flat = [str(cell) for row in vdf.astype(str).values.tolist() for cell in row]\n",
        "    autosomal = last_text = None\n",
        "    for cell in flat:\n",
        "        if autosomal is None and \"Records tagged and filtered by NPFX\" in cell:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", cell)\n",
        "            if m:\n",
        "                autosomal = m.group(1)\n",
        "        if last_text is None and \"LAST_UPDATED_TEXT\" in cell:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", cell)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "    if last_text is not None:\n",
        "        last_text = _friendly_ts_from_utc(last_text)\n",
        "    AUTOSOMAL_MATCHES = _format_int_with_commas(autosomal) if autosomal else \"\"\n",
        "    LAST_UPDATED_TEXT = last_text or \"\"\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "# ---------- 6) Transform ----------\n",
        "_setup_resolver()\n",
        "\n",
        "out_match_to = []\n",
        "out_first_ancestor = []\n",
        "out_summary  = []\n",
        "out_sort_key = []\n",
        "\n",
        "def _clean_authority_cell(v):\n",
        "    # The authority value should already be a slug like: yatesjohn&gaterjoane\n",
        "    # Keep it deterministic; only strip whitespace.\n",
        "    s = str(v or \"\")\n",
        "    s = s.replace(\"\\u00a0\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw  = row.get(match_col, \"\")\n",
        "    subject_name = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_html = \"<strong>%s</strong>\" % _html.escape(subject_name or \"\", quote=False)\n",
        "\n",
        "    pid          = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_raw  = row.get(name_col, \"\")\n",
        "    matchee_name = norm_matchee_name(matchee_raw) or subject_name\n",
        "\n",
        "    if pid:\n",
        "        matchee_url = (\n",
        "            \"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\"\n",
        "            % (TNG_BASE, pid, TNG_TREE)\n",
        "        )\n",
        "        matchee_name_html = '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (\n",
        "            _html.escape(matchee_url, quote=True),\n",
        "            _html.escape(matchee_name or \"\", quote=False),\n",
        "        )\n",
        "    else:\n",
        "        matchee_name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val      = row.get(cm_col, \"0\")\n",
        "    raw_lineage = row.get(path_col, \"\")\n",
        "\n",
        "    # Display couple for summary (enriched exclusion applies here only)\n",
        "    raw_tokens  = split_tokens(raw_lineage)\n",
        "    couple_disps = [_couple_display_from_token(t) for t in raw_tokens]\n",
        "    couple_disps = _apply_enriched_exclusion_to_couples(couple_disps)\n",
        "\n",
        "    gens_total  = len(couple_disps)\n",
        "    couple_disp = couple_disps[0] if couple_disps else \"\"\n",
        "    couple_html = _html.escape(couple_disp or \"\", quote=False) if couple_disp else \"\"\n",
        "\n",
        "    summary_html = build_header(subject_name_html, cm_val, matchee_name_html, gens_total, couple_html)\n",
        "\n",
        "    # Authority-first-ancestor: DISPLAY-ONLY from the CSV\n",
        "    auth_key_clean = _clean_authority_cell(row.get(auth_col, \"\"))\n",
        "\n",
        "    # Whatever is present is displayed (including unknowns).\n",
        "    display_first = auth_key_clean.replace(\"&\", \"&#38;\") if auth_key_clean else \"\"\n",
        "    sort_key = auth_key_clean if auth_key_clean else \"zzzzzzzzzzzzzzzzzzzzzzzz\"\n",
        "\n",
        "    out_match_to.append(_html.escape(subject_name or \"\", quote=False))\n",
        "    out_first_ancestor.append(display_first)\n",
        "    out_summary.append(summary_html)\n",
        "    out_sort_key.append(sort_key)\n",
        "\n",
        "df_out = pd.DataFrame({\n",
        "    \"Match to\": out_match_to,\n",
        "    \"First Ancestor\": out_first_ancestor,\n",
        "    \"Match Summary\": out_summary,\n",
        "    \"__sort__\": out_sort_key,\n",
        "})\n",
        "df_out = df_out.sort_values(by=\"__sort__\", kind=\"mergesort\").drop(columns=[\"__sort__\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- 7) HTML ----------\n",
        "ROOT_VAR_STYLE = '<style type=\"text/css\">:root{--table-width-px:%dpx;}</style>' % int(TABLE_TOTAL_WIDTH_PX)\n",
        "\n",
        "updated_label = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT or \"\")\n",
        "_updated_parts = [updated_label]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "UPDATED_BLOCK = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join([p for p in _updated_parts if p.strip()]) + '</div>'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls controls-spaced centerline\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "LATE_OVERRIDE_BLOCK = \"\"\n",
        "\n",
        "# Display order: 1) Match to, 2) Match Summary, 3) First Ancestor\n",
        "col_headers = [\n",
        "    (\"Match to\", \"center\"),\n",
        "    (\"Match Summary\", \"left\"),\n",
        "    (\"First Ancestor\", \"center\"),\n",
        "]\n",
        "col_data = [\n",
        "    df_out[\"Match to\"].tolist(),\n",
        "    df_out[\"Match Summary\"].tolist(),\n",
        "    df_out[\"First Ancestor\"].tolist(),\n",
        "]\n",
        "\n",
        "thead_cells = []\n",
        "for idx, (hdr, align) in enumerate(col_headers):\n",
        "    wpx = COL_WIDTHS[idx]\n",
        "    style_attr = \"width:%dpx; display:table-cell !important;\" % wpx\n",
        "    if align == \"center\":\n",
        "        thead_cells.append('<th class=\"center-header\" style=\"%s\">%s</th>' % (style_attr, hdr))\n",
        "    else:\n",
        "        thead_cells.append('<th class=\"left-header\" style=\"%s\">%s</th>' % (style_attr, hdr))\n",
        "\n",
        "thead_html = (\n",
        "    '<thead style=\"display:table-header-group !important;\">\\n'\n",
        "    '  <tr style=\"display:table-row !important;\">'\n",
        "    + \"\".join(thead_cells)\n",
        "    + \"</tr>\\n</thead>\"\n",
        ")\n",
        "\n",
        "tbody_lines = [\"<tbody>\"]\n",
        "for r in range(len(df_out)):\n",
        "    cells = []\n",
        "    for c in range(len(col_headers)):\n",
        "        wpx = COL_WIDTHS[c]\n",
        "        val = col_data[c][r]\n",
        "        val_str = \"\" if val is None else str(val)\n",
        "        cells.append('<td style=\"width:%dpx;\">%s</td>' % (wpx, val_str))\n",
        "    tbody_lines.append(\"  <tr>\" + \"\".join(cells) + \"</tr>\")\n",
        "tbody_lines.append(\"</tbody>\")\n",
        "tbody_html = \"\\n\".join(tbody_lines)\n",
        "\n",
        "html_table = (\n",
        "    '<table border=\"1\" class=\"dataframe sortable dna-register-table\" id=\"refactor-table\">'\n",
        "    + thead_html + \"\\n\" + tbody_html + \"</table>\"\n",
        ")\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div class=\"table-scroll\" id=\"bottom-scroll\">%s</div>'\n",
        "    \"</div>\"\n",
        ") % (html_table,)\n",
        "\n",
        "JS_NAV_REPAIR = \"\"\n",
        "\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "$ROOT_VAR_STYLE\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $LATE_OVERRIDE_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "\n",
        "$JS_NAV_REPAIR\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '')\n",
        "      .replace(/\\\\s+/g,' ')\n",
        "      .trim()\n",
        "      .toLowerCase();\n",
        "  }\n",
        "\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''), 10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){\n",
        "      return String(n||'');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "\n",
        "  function sortTableByKey(tbl, keyColIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[keyColIndex]);\n",
        "      var B = textOf(b.cells[keyColIndex]);\n",
        "\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,''));\n",
        "      var nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "\n",
        "      if(!isNaN(nA) && !isNaN(nB)){\n",
        "        return asc ? (nA - nB) : (nB - nA);\n",
        "      }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      frag.appendChild(rows[i]);\n",
        "    }\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "\n",
        "    // Display indexes after swap:\n",
        "    //   0 Match to\n",
        "    //   1 Match Summary   <-- user clicks here\n",
        "    //   2 First Ancestor  <-- key column\n",
        "    function keyIndexForDisplayIndex(displayIdx){\n",
        "      return (displayIdx === 1) ? 2 : displayIdx;\n",
        "    }\n",
        "\n",
        "    for(var i=0;i<ths.length;i++){\n",
        "      (function(displayIdx){\n",
        "        var th  = ths[displayIdx];\n",
        "        var dir = 'asc';\n",
        "\n",
        "        th.addEventListener('click', function(){\n",
        "          dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "\n",
        "          for (var j = 0; j < ths.length; j++){\n",
        "            ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "          }\n",
        "\n",
        "          th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "\n",
        "          var keyIdx = keyIndexForDisplayIndex(displayIdx);\n",
        "          sortTableByKey(tbl, keyIdx, dir);\n",
        "        }, false);\n",
        "      })(i);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "\n",
        "    var tb   = tbl.tBodies[0];\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt  = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "\n",
        "    box.addEventListener('input',  onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindSearch();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK           = HEAD_LINK,\n",
        "    ROOT_VAR_STYLE      = ROOT_VAR_STYLE,\n",
        "    UPDATED_BLOCK       = UPDATED_BLOCK,\n",
        "    NAV_BLOCK           = NAV_BLOCK,\n",
        "    LATE_OVERRIDE_BLOCK = LATE_OVERRIDE_BLOCK,\n",
        "    CONTROLS_BLOCK      = CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER      = SCROLL_WRAPPER,\n",
        "    JS_NAV_REPAIR       = JS_NAV_REPAIR,\n",
        ")\n",
        "\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"[OK] Saved render: %s\" % os.path.abspath(LOCAL_HTML))\n",
        "print(\"[DEBUG] SSI nav include present:\", (\"<!--#include\" in final_html))\n",
        "\n",
        "# ---------- 8) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload main HTML failed: %s\" % e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [_remote_path(REMOTE_HTML_CANON), _remote_path(REMOTE_HTML_LEG)]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\")\n",
        "        print(\"Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\")\n",
        "        print(\"CSS:       https://yates.one-name.net/partials/partials_unified.css\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session: %s\" % e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "\n",
        "print(\"\\n--- Cell 2 complete (DISPLAY-ONLY First Ancestor from CSV Authority_FirstAncestor; DISPLAY SWAP 2<->3; Match Summary sorts by First Ancestor) ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hXyd44dpMQD",
        "outputId": "fd437fb7-de74-4d12-92f6-0046e2f99a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_3Col_DisplayOnlyFirstAncestor | Version=2026.02.03-DISPLAYONLY-AUTHCOL-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=1880\n",
            "[LAYOUT] Column widths (px): 1=220 2=1240 3=420\n",
            "[OK] Loaded CSV: 93 rows, 7 cols\n",
            "[OK] Authority column detected: Authority_FirstAncestor\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Saved render: /content/yates_ancestor_register.shtml\n",
            "[DEBUG] SSI nav include present: True\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 52323\n",
            "partials/ons_yates_dna_register.shtml : 52323\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "CSS:       https://yates.one-name.net/partials/partials_unified.css\n",
            "\n",
            "--- Cell 2 complete (DISPLAY-ONLY First Ancestor from CSV Authority_FirstAncestor; DISPLAY SWAP 2<->3; Match Summary sorts by First Ancestor) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2b"
      ],
      "metadata": {
        "id": "jg9EmTNcTOi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 2b (Unified headers from Cell 2 + Cell 2b functionality) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.01-CELL2HDR-CELL2B1)\n",
        "# * Complete & runnable Colab cell - one contiguous block.\n",
        "# * Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# * XHTML 1.0 Transitional; typography/layout/colors via /partials/partials_unified.css (Cell 2 baseline).\n",
        "# * No nav fallback markup. SSI include only (Cell 2 behavior).\n",
        "# * Fix: single updated/header line with a single \"Showing: <span id='showing-count'></span>\" (JS-populated).\n",
        "# * Fix: updateShowing() counts visible rows in the register table when filtering via selection menu.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2b_Counts_UnifiedHeaders | Version=2026.02.01-CELL2HDR-CELL2B1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# NOTE: In Colab, the notebook cell text is not reliably introspectable for an exact line-count audit.\n",
        "DECLARED_LINES = -1\n",
        "print(\"[AUDIT] DECLARED_LINES={}\".format(DECLARED_LINES))\n",
        "\n",
        "import os, re, posixpath, socket, traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "# Cell 2 baseline stylesheet (single canonical CSS)\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "# Shared nav include (SSI) - no fallback\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "# vitals authority (built by Cell 1)\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "# TNG settings for cousin links\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "# Local partial paths\n",
        "MATCH_COUNT_LOCAL   = os.path.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_LOCAL = os.path.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_LOCAL  = os.path.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# Remote partial paths\n",
        "MATCH_COUNT_REMOTE   = posixpath.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_REMOTE = posixpath.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_REMOTE  = posixpath.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# ---------- 1a) Load vitals authority from dna_vitals.csv ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    last_updated_raw = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display, last_updated_raw\n",
        "\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display, last_updated_raw\n",
        "\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_raw = ts\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display, last_updated_raw\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY, LAST_UPDATED_RAW = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver (match_to_unmasked.csv on server) ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) CSV + name helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ASCII-only separators.\n",
        "SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(')([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def _split_first_last(display: str):\n",
        "    d = _clean_piece(display or \"\")\n",
        "    if not d:\n",
        "        return (\"\", \"\")\n",
        "    parts = d.split()\n",
        "    if len(parts) == 1:\n",
        "        return (\"\", parts[0])\n",
        "    return (\" \".join(parts[:-1]), parts[-1])\n",
        "\n",
        "def _name_bold_last_html(display: str) -> str:\n",
        "    first, last = _split_first_last(display)\n",
        "    if not last and not first:\n",
        "        return \"\"\n",
        "    if not first:\n",
        "        return '<span class=\"mc-last\">%s</span>' % _html.escape(last, quote=False)\n",
        "    return '%s <span class=\"mc-last\">%s</span>' % (_html.escape(first, quote=False), _html.escape(last, quote=False))\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "\n",
        "    def _side_to_name(s):\n",
        "        s = _clean_piece(s)\n",
        "        if \"~\" in s:\n",
        "            bits = [b.strip() for b in s.split(\"~\") if b.strip() != \"\"]\n",
        "            if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "                if len(bits) >= 2:\n",
        "                    return normalize_person_name(bits[1])\n",
        "                return \"\"\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "\n",
        "    left = _side_to_name(parts[0])\n",
        "    right = _side_to_name(parts[1])\n",
        "    if left and right:\n",
        "        return (left, right)\n",
        "    return (\"\", \"\")\n",
        "\n",
        "# ---------- 4b) Lineage label normalization ----------\n",
        "_AMP_SPLIT = re.compile(r\"\\s*(?:&|and|AND|\\+)\\s*\", re.I)\n",
        "\n",
        "def normalize_parents_label(raw: str) -> str:\n",
        "    raw = _clean_piece(raw or \"\")\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    parts = _AMP_SPLIT.split(raw, maxsplit=1)\n",
        "    if len(parts) == 2:\n",
        "        left = smart_titlecase(parts[0])\n",
        "        right = smart_titlecase(parts[1])\n",
        "        left = _clean_piece(left)\n",
        "        right = _clean_piece(right)\n",
        "        if left and right:\n",
        "            return \"%s & %s\" % (left, right)\n",
        "    return smart_titlecase(raw)\n",
        "\n",
        "def _parse_enriched_parent_side(side_raw: str):\n",
        "    side_raw = _clean_piece(side_raw or \"\")\n",
        "    if not side_raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "    if \"~\" in side_raw:\n",
        "        bits = [b.strip() for b in side_raw.split(\"~\")]\n",
        "        bits = [b for b in bits if b != \"\"]\n",
        "        if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "            pid = bits[0].upper()\n",
        "            nm = normalize_person_name(bits[1]) if len(bits) >= 2 else \"\"\n",
        "            yrs = _clean_piece(bits[2]) if len(bits) >= 3 else \"\"\n",
        "            return (pid, nm, yrs)\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", side_raw, flags=re.I)\n",
        "    if m:\n",
        "        pid = m.group(1).upper()\n",
        "        rest = _clean_piece(m.group(2))\n",
        "        yrs = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs = _clean_piece(m2.group(1).replace(\" \", \"\"))\n",
        "            rest = _clean_piece(rest[:m2.start()])\n",
        "        nm = normalize_person_name(rest) if rest else \"\"\n",
        "        return (pid, nm, yrs)\n",
        "\n",
        "    nm2 = smart_titlecase(side_raw) if \" \" in side_raw else smart_titlecase(surname_given_from_token(side_raw)[0])\n",
        "    return (\"\", nm2, \"\")\n",
        "\n",
        "def _lineage_label_with_links(first_raw: str) -> str:\n",
        "    raw = _clean_piece(first_raw or \"\")\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", raw, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return '<span class=\"lc-new\">%s</span>' % _html.escape(normalize_parents_label(raw) or raw, quote=False)\n",
        "\n",
        "    f_id, f_nm, f_yrs = _parse_enriched_parent_side(parts[0])\n",
        "    m_id, m_nm, m_yrs = _parse_enriched_parent_side(parts[1])\n",
        "\n",
        "    def _side_html(pid, name, yrs, legacy_side):\n",
        "        name = _clean_piece(name)\n",
        "        yrs = _clean_piece(yrs)\n",
        "        label = _html.escape(normalize_person_name(legacy_side), quote=False)\n",
        "        if name:\n",
        "            label = _html.escape(name, quote=False)\n",
        "        if yrs:\n",
        "            label = label + ' <span class=\"mc-code\">(%s)</span>' % _html.escape(yrs, quote=False)\n",
        "        if pid:\n",
        "            href = \"%s/familychart.php?personID=%s&tree=%s\" % (TNG_BASE.rstrip(\"/\"), pid.upper(), TNG_TREE)\n",
        "            return '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (_html.escape(href, quote=True), label)\n",
        "        return label\n",
        "\n",
        "    left = _side_html(f_id, f_nm, f_yrs, parts[0])\n",
        "    right = _side_html(m_id, m_nm, m_yrs, parts[1])\n",
        "    return '<span class=\"lc-new\">%s &amp; %s</span>' % (left, right)\n",
        "\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
        "    return t\n",
        "\n",
        "# ---------- CSS helpers (page scoped) ----------\n",
        "_MATCH_COUNT_ONECOL_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \"/* Match Count: ONE centered column, alpha order, scroll shows max 10 rows. */\\n\"\n",
        "    \"#ref-table{border-collapse:separate !important; border-spacing:0 !important;}\\n\"\n",
        "    \"#ref-table thead{position:absolute !important; left:-9999px !important; top:auto !important; width:1px !important; height:1px !important; overflow:hidden !important;}\\n\"\n",
        "    \"#ref-table, #ref-table tbody{display:block !important;}\\n\"\n",
        "    \"#ref-tb{padding:6px 4px 2px 4px !important;display:block !important;width: var(--mc-col-w) !important;max-width: 92vw !important;margin: 0 auto !important;box-sizing:border-box !important;--mc-row-h: 40px;--mc-col-w: 360px;max-height: calc(var(--mc-row-h) * 10) !important;overflow-y: auto !important;overflow-x: hidden !important;}\\n\"\n",
        "    \".tile-head{width: var(--mc-col-w) !important;max-width: 92vw !important;margin: 0 auto 6px auto !important;padding: 6px 10px !important;box-sizing:border-box !important;border: 1px solid #ddd !important;border-radius: 10px !important;background: #f7f7f7 !important;display:flex !important;align-items:center !important;justify-content:space-between !important;font-weight:bold !important;}\\n\"\n",
        "    \".tile-head .th-left{flex:1 1 auto !important; min-width:0 !important;}\\n\"\n",
        "    \".tile-head .th-right{flex:0 0 auto !important; margin-left:10px !important; white-space:nowrap !important;}\\n\"\n",
        "    \"#ref-tb tr{display:flex !important; align-items:center !important;box-sizing:border-box !important;height: var(--mc-row-h) !important;margin:0 0 6px 0 !important;padding:0 10px !important;border:1px solid #ddd !important;border-radius:10px !important;background:#fff !important;overflow:hidden !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row{border-color:#d1a500 !important; box-shadow:0 0 0 2px rgba(209,165,0,0.15) inset !important;}\\n\"\n",
        "    \"#ref-tb td{border:none !important; padding:0 !important;}\\n\"\n",
        "    \"#ref-tb td.mc-name{flex:1 1 auto !important; min-width:0 !important; white-space:nowrap !important; overflow:hidden !important; text-overflow:ellipsis !important;}\\n\"\n",
        "    \"#ref-tb td.mc-count{flex:0 0 auto !important; margin-left:10px !important; font-weight:bold !important;}\\n\"\n",
        "    \"#ref-tb a.count-pick{text-decoration:none !important; padding:2px 6px !important; border:1px solid #ccc !important; border-radius:8px !important; display:inline-block !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row a.count-pick{border-color:#d1a500 !important;}\\n\"\n",
        "    \".mc-last{font-weight:bold !important;}\\n\"\n",
        "    \".mc-code{font-size:90% !important; color:#666 !important;}\\n\"\n",
        "    \"/* Ensure register headers stay visible */\\n\"\n",
        "    \"#reg-list thead{display:table-header-group !important;}\\n\"\n",
        "    \"#reg-list thead tr{display:table-row !important;}\\n\"\n",
        "    \"#reg-list thead th{display:table-cell !important;}\\n\"\n",
        "    \"#reg-list th{position:sticky !important; top:0 !important; background:#ffffff !important; z-index:10 !important;}\\n\"\n",
        "    \"@media print{ #ref-tb{max-height:none !important; overflow:visible !important; width:auto !important; max-width:none !important; margin:0 !important;} #ref-table thead{position:static !important; left:auto !important; width:auto !important; height:auto !important; overflow:visible !important;}}\\n\"\n",
        "    \"</style>\\n\"\n",
        ")\n",
        "\n",
        "_LINEAGE_COUNT_ONECOL_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \":root{--lc-col-w:min(760px,calc(100vw - 28px));}\\n\"\n",
        "    \".table-scroll.lineage-scroll{max-height:520px;overflow-y:auto;overflow-x:hidden;}\\n\"\n",
        "    \".table-scroll.reg-scroll{max-height:none;overflow:visible;}\\n\"\n",
        "    \"#ref-table{border-collapse:separate !important;border-spacing:0 !important;}\\n\"\n",
        "    \"#ref-table thead{position:absolute !important; left:-9999px !important; top:auto !important; width:1px !important; height:1px !important; overflow:hidden !important;}\\n\"\n",
        "    \"#ref-table,#ref-table tbody{display:block !important;}\\n\"\n",
        "    \".tile-head{width:var(--lc-col-w) !important;margin:0 auto 6px auto !important;padding:6px 10px !important;box-sizing:border-box !important;border:1px solid #ddd !important;border-radius:12px !important;background:#f7f7f7 !important;display:flex !important;align-items:center !important;justify-content:space-between !important;font-weight:bold !important;}\\n\"\n",
        "    \".tile-head .th-left{flex:1 1 auto !important; min-width:0 !important;}\\n\"\n",
        "    \".tile-head .th-right{flex:0 0 auto !important; margin-left:10px !important; white-space:nowrap !important;}\\n\"\n",
        "    \"#ref-tb{padding:4px 4px 2px 4px !important;display:block !important;width:var(--lc-col-w) !important;margin:0 auto !important;}\\n\"\n",
        "    \"#ref-tb tr{display:flex !important;align-items:center !important;gap:6px !important;width:100% !important;margin:5px auto !important;padding:7px 9px !important;border:1px solid #ddd !important;border-radius:12px !important;background:#fff !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row{border-color:#d1a500 !important;box-shadow:0 0 0 2px rgba(209,165,0,0.15) inset !important;}\\n\"\n",
        "    \"#ref-tb td{border:none !important;padding:0 !important;vertical-align:top !important;}\\n\"\n",
        "    \"#ref-tb td.lc-name{flex:1 1 auto !important;white-space:normal !important;overflow:visible !important;text-overflow:clip !important;}\\n\"\n",
        "    \"#ref-tb td.lc-count{flex:0 0 auto !important;margin-left:0 !important;font-weight:bold !important;white-space:nowrap !important;}\\n\"\n",
        "    \"#ref-tb a.count-pick{text-decoration:none !important;padding:2px 7px !important;border:1px solid #ccc !important;border-radius:10px !important;display:inline-block !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row a.count-pick{border-color:#d1a500 !important;}\\n\"\n",
        "    \".lc-new{display:block;margin-top:0;}\\n\"\n",
        "    \".lc-new a{text-decoration:none;}\\n\"\n",
        "    \".lc-new a:hover{text-decoration:underline;}\\n\"\n",
        "    \"#reg-list thead{display:table-header-group !important;}\\n\"\n",
        "    \"#reg-list thead tr{display:table-row !important;}\\n\"\n",
        "    \"#reg-list thead th{display:table-cell !important;}\\n\"\n",
        "    \"#reg-list th{position:sticky !important; top:0 !important; background:#ffffff !important; z-index:10 !important;}\\n\"\n",
        "    \"@media print{ #ref-table thead{position:static !important; left:auto !important; width:auto !important; height:auto !important; overflow:visible !important;}}\\n\"\n",
        "    \"</style>\\n\"\n",
        ")\n",
        "\n",
        "# ---------- Cell 2 header builder (single updated line; single Showing span) ----------\n",
        "def _build_updated_block() -> str:\n",
        "    # Match Cell 2 structure: Last updated: <span id=last-updated>...</span> | Autosomal matches: N | Showing: <span id=showing-count></span>\n",
        "    parts = []\n",
        "    parts.append('Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_DISPLAY or \"(unknown)\", quote=False))\n",
        "    parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES_TEXT or \"(unknown)\", quote=False))\n",
        "    parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "    return '<div class=\"updated centerline\">%s</div>' % (' &nbsp;|&nbsp; '.join(parts))\n",
        "\n",
        "def _partial_head(title, helper_css=\"\"):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n\"\n",
        "        \"<head>\\n\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        \"<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\" />\\n\"\n",
        "        \"<title>%s</title>\\n\" % _html.escape(title, quote=False)\n",
        "        + HEAD_LINK + \"\\n\"\n",
        "        + (helper_css or \"\")\n",
        "        + \"</head>\\n<body id=\\\"top\\\">\\n\"\n",
        "        \"<div class=\\\"wrap\\\">\\n\"\n",
        "        \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title, quote=False)\n",
        "        + _build_updated_block() + \"\\n\"\n",
        "        + NAV_BLOCK + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "          \"Showing: \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "          \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    # updateShowing() mirrors Cell 2 idea: compute visible row count.\n",
        "    # Here we count visible rows in reg-list (the register) when it exists.\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\\n\"\n",
        "        \"function formatWithCommas(n){\\n\"\n",
        "        \" try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }\\n\"\n",
        "        \" catch(e){ return String(n||''); }\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function countVisibleRowsInTable(tableId){\\n\"\n",
        "        \" var t=document.getElementById(tableId);\\n\"\n",
        "        \" if(!(t && t.tBodies && t.tBodies.length)) return 0;\\n\"\n",
        "        \" var rows=t.tBodies[0].rows, n=0;\\n\"\n",
        "        \" for(var i=0;i<rows.length;i++){ if(rows[i].style.display !== 'none') n++; }\\n\"\n",
        "        \" return n;\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function updateShowing(){\\n\"\n",
        "        \" var el=document.getElementById('showing-count');\\n\"\n",
        "        \" if(!el) return;\\n\"\n",
        "        \" var n = 0;\\n\"\n",
        "        \" if(document.getElementById('reg-list')){ n = countVisibleRowsInTable('reg-list'); }\\n\"\n",
        "        \" else if(document.getElementById('ref-table')){ n = countVisibleRowsInTable('ref-table'); }\\n\"\n",
        "        \" el.textContent = formatWithCommas(n);\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function ySelEachRow(tb, cb){ if(!tb) return; var rows=tb.getElementsByTagName('tr'); for(var i=0;i<rows.length;i++){cb(rows[i]);} }\\n\"\n",
        "        \"function ySelClear(tr){ if(!tr) return; tr.removeAttribute('data-selected'); var cls=tr.className||''; cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,''); tr.className=cls; tr.style.backgroundColor=''; }\\n\"\n",
        "        \"function ySelToggle(a){ var tr=a; while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;} if(!tr) return false; var sel=tr.getAttribute('data-selected')==='1'; if(sel){ ySelClear(tr);}else{ tr.setAttribute('data-selected','1'); var cls=tr.className||''; if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';} tr.style.backgroundColor='#fff2cc'; } updateShowing(); return false; }\\n\"\n",
        "        \"function ySelGetTBody(tbodyId){ var tb=document.getElementById(tbodyId); if(tb) return tb; var t=document.getElementById('ref-table'); if(!t) return null; if(t.tBodies&&t.tBodies.length){return t.tBodies[0];} return t; }\\n\"\n",
        "        \"function ySelShowSelected(tbodyId){\\n\"\n",
        "        \" var tb=ySelGetTBody(tbodyId); if(!tb) return false;\\n\"\n",
        "        \" ySelEachRow(tb,function(tr){ var sel=tr.getAttribute('data-selected')==='1'; tr.style.display=sel?'':'none'; });\\n\"\n",
        "        \" var rl=document.getElementById('reg-list');\\n\"\n",
        "        \" if(rl){\\n\"\n",
        "        \"  var selVals=[];\\n\"\n",
        "        \"  ySelEachRow(tb,function(tr){ if(tr.getAttribute('data-selected')==='1'){ var v=tr.getAttribute('data-filter')||tr.getAttribute('data-lineage')||tr.getAttribute('data-code')||tr.getAttribute('data-q')||''; if(v){selVals.push(v);} } });\\n\"\n",
        "        \"  if(selVals.length===0){ updateShowing(); return false; }\\n\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\\n\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"    var r=rows[i]; var lv=r.getAttribute('data-filter')||r.getAttribute('data-lineage')||r.getAttribute('data-code')||'';\\n\"\n",
        "        \"    var show=false; for(var j=0;j<selVals.length;j++){ if(lv===selVals[j]){show=true; break;} }\\n\"\n",
        "        \"    r.style.display=show?'':'none';\\n\"\n",
        "        \"  }\\n\"\n",
        "        \" }\\n\"\n",
        "        \" updateShowing();\\n\"\n",
        "        \" return false;\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function ySelShowAll(tbodyId){ var tb=ySelGetTBody(tbodyId); if(!tb) return false; ySelEachRow(tb,function(tr){tr.style.display='';}); var rl=document.getElementById('reg-list'); if(rl){ var rows=rl.getElementsByTagName('tr'); for(var i=0;i<rows.length;i++){rows[i].style.display='';} } updateShowing(); return false; }\\n\"\n",
        "        \"function ySelReset(tbodyId){ var tb=ySelGetTBody(tbodyId); if(!tb) return false; ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);}); var rl=document.getElementById('reg-list'); if(rl){ var rows=rl.getElementsByTagName('tr'); for(var i=0;i<rows.length;i++){rows[i].style.display='';} } updateShowing(); return false; }\\n\"\n",
        "        \"window.ySelToggle=ySelToggle; window.ySelShowSelected=ySelShowSelected; window.ySelShowAll=ySelShowAll; window.ySelReset=ySelReset;\\n\"\n",
        "        \"document.addEventListener('DOMContentLoaded', function(){ updateShowing(); }, false);\\n\"\n",
        "        \"})();\\n\"\n",
        "        \"//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(row, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        name_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        name_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "    return subject_name_html, name_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Match Count partial (ONE COL, ALPHA) ----------\n",
        "def build_match_count_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    codes_raw = main_df[match_col].astype(str).map(lambda x: x.strip())\n",
        "    keys_norm = codes_raw.map(_norm_code_for_count)\n",
        "\n",
        "    counts_series = keys_norm.value_counts(dropna=False)\n",
        "    counts = counts_series.reset_index()\n",
        "    if counts.shape[1] >= 2:\n",
        "        counts.columns = [\"norm_key\", \"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"] = counts.index.astype(str)\n",
        "        counts[\"Count\"] = counts_series.values\n",
        "        counts = counts[[\"norm_key\", \"Count\"]]\n",
        "\n",
        "    first_display = {}\n",
        "    raw_list = codes_raw.tolist()\n",
        "    norm_list = keys_norm.tolist()\n",
        "    for code_disp, k in zip(raw_list, norm_list):\n",
        "        if k not in first_display and str(k) != \"\":\n",
        "            first_display[k] = code_disp\n",
        "\n",
        "    counts[\"Code\"] = counts[\"norm_key\"].map(lambda k: first_display.get(k, k))\n",
        "    counts[\"Unmasked\"] = counts[\"norm_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "\n",
        "    disp_names = []\n",
        "    sort_alpha = []\n",
        "    for _, r in counts.iterrows():\n",
        "        code = str(r.get(\"Code\", \"\") or \"\").strip()\n",
        "        unm = str(r.get(\"Unmasked\", \"\") or \"\").strip()\n",
        "        label = (unm or code).strip()\n",
        "        disp = normalize_person_name(label)\n",
        "        disp_names.append(disp)\n",
        "        sort_alpha.append((disp or label).lower())\n",
        "\n",
        "    counts[\"Disp\"] = disp_names\n",
        "    counts[\"SortAlpha\"] = sort_alpha\n",
        "    counts = counts.sort_values(by=[\"SortAlpha\", \"Count\"], ascending=[True, False], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    total_participants = int(len(counts))\n",
        "    page_title = \"Network participants (matches): %d\" % total_participants\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(page_title, helper_css=_MATCH_COUNT_ONECOL_CSS))\n",
        "\n",
        "    html.append('<div class=\"tile-head\"><span class=\"th-left\">Match to</span><span class=\"th-right\">Count</span></div>')\n",
        "\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th>Match to</th><th>Count</th>')\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in counts.iterrows():\n",
        "        code = str(r.get(\"Code\", \"\") or \"\").strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        norm_key = _norm_code_for_count(code)\n",
        "\n",
        "        unm = (str(r.get(\"Unmasked\", \"\") or \"\")).strip()\n",
        "        label = (unm or code).strip()\n",
        "\n",
        "        disp = str(r.get(\"Disp\", \"\") or \"\").strip()\n",
        "        name_part = _name_bold_last_html(disp) if disp else _html.escape(label, quote=False)\n",
        "\n",
        "        code_part = \"\"\n",
        "        if code:\n",
        "            code_part = ' <span class=\"mc-code\">(%s)</span>' % _html.escape(code, quote=False)\n",
        "        disp_html = name_part + code_part\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td class=\\\"mc-name\\\">%s</td>\"\n",
        "            \"<td class=\\\"mc-count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td></tr>\"\n",
        "            % (\n",
        "                _html.escape(label, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                disp_html,\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected participant(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead style=\"display:table-header-group !important;\"><tr style=\"display:table-row !important;\">'\n",
        "        '<th style=\"display:table-cell !important;\">Match to</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Name</th>'\n",
        "        '<th style=\"display:table-cell !important;\">cM</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        code_raw = str(row.get(match_col, \"\")).strip()\n",
        "        if not code_raw:\n",
        "            continue\n",
        "        norm_key = _norm_code_for_count(code_raw)\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(row, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Lineage Count partial ----------\n",
        "def build_lineage_count_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    first_series = (\n",
        "        main_df.get(\"First Ancestor\", pd.Series(dtype=str))\n",
        "        .astype(str)\n",
        "        .map(lambda x: x.strip())\n",
        "    )\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    vis_labels = []\n",
        "    sort_alpha = []\n",
        "    for _, r in lin_df.iterrows():\n",
        "        raw = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        vis = normalize_parents_label(raw) or raw\n",
        "        vis_labels.append(vis)\n",
        "        sort_alpha.append(vis.lower())\n",
        "    lin_df[\"VisLabel\"] = vis_labels\n",
        "    lin_df[\"SortAlpha\"] = sort_alpha\n",
        "\n",
        "    lin_df = lin_df.sort_values([\"Count\", \"First Ancestor\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Lineage Count\", helper_css=_LINEAGE_COUNT_ONECOL_CSS))\n",
        "    html[-1] = html[-1].replace('<div class=\"table-scroll\">', '<div class=\"table-scroll lineage-scroll\">', 1)\n",
        "\n",
        "    html.append('<div class=\"tile-head\"><span class=\"th-left\">First Ancestor</span><span class=\"th-right\">Count</span></div>')\n",
        "\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th>First Ancestor</th><th>Count</th>')\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first_raw = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        lineage_key = first_raw\n",
        "\n",
        "        parents_html = _lineage_label_with_links(first_raw)\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td class=\\\"lc-name\\\">%s</td>\"\n",
        "            \"<td class=\\\"lc-count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first_raw, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(lineage_key, quote=True),\n",
        "                _html.escape(lineage_key, quote=True),\n",
        "                parents_html,\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append(\"</div>\")\n",
        "    html.append('<div class=\"table-scroll reg-scroll\">\\n')\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected lineage(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead style=\"display:table-header-group !important;\"><tr style=\"display:table-row !important;\">'\n",
        "        '<th style=\"display:table-cell !important;\">Match to</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Name</th>'\n",
        "        '<th style=\"display:table-cell !important;\">cM</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        first = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        if not first:\n",
        "            continue\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(row, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 9) Cousin printable partial ----------\n",
        "def build_cousin_print_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    rows = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        subject_raw = row.get(match_col, \"\")\n",
        "        subject_name = normalize_person_name(MATCH_TO_UNMASKED.get(str(subject_raw).strip().lower(), subject_raw))\n",
        "        subject_name_html = \"<strong>%s</strong>\" % subject_name if subject_name else \"\"\n",
        "\n",
        "        pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "        matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "        if pid:\n",
        "            matchee_name_html = (\n",
        "                '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "                'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "                % (TNG_BASE, pid, TNG_TREE, matchee_name)\n",
        "            )\n",
        "        else:\n",
        "            matchee_name_html = matchee_name\n",
        "\n",
        "        cm_val = row.get(cm_col, \"0\")\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        gens_total = len(tokens)\n",
        "\n",
        "        if \"common_husband\" in main_df.columns and \"common_wife\" in main_df.columns:\n",
        "            husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "            wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "            if not husband_raw and not wife_raw:\n",
        "                husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "        else:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "        header_html = build_header(subject_name_html or subject_name, cm_val, matchee_name_html, gens_total, husband_raw, wife_raw)\n",
        "        rows.append(header_html)\n",
        "\n",
        "    rows_sorted = sorted(rows)\n",
        "\n",
        "    html_rows = [\n",
        "        '<table border=\"1\" id=\"refactor-table\" class=\"sortable\"><thead><tr><th>Match Summary</th></tr></thead><tbody>'\n",
        "    ]\n",
        "    for v in rows_sorted:\n",
        "        html_rows.append(\"<tr><td>%s</td></tr>\" % v)\n",
        "    html_rows.append(\"</tbody></table>\")\n",
        "\n",
        "    cousin_html = (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\" \"\n",
        "        \"\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\"><head>\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\"\n",
        "        \"<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\" />\"\n",
        "        \"<title>Cousin List (Printable)</title>\"\n",
        "        + HEAD_LINK +\n",
        "        \"</head><body onload=\\\"window.print();\\\">\"\n",
        "        \"<div class=\\\"wrap\\\">\"\n",
        "        \"<h1 class=\\\"centerline\\\">Cousin List (Printable)</h1>\"\n",
        "        \"<div class=\\\"table-scroll\\\">%s</div>\"\n",
        "        \"</div></body></html>\"\n",
        "        % \"\".join(html_rows)\n",
        "    )\n",
        "    return cousin_html\n",
        "\n",
        "# ---------- 10) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for counts: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(df, [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"], [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"])\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column for First Ancestor.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    first_ancestors = []\n",
        "    for _, row in df.iterrows():\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    mc_html = build_match_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(MATCH_COUNT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(mc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(MATCH_COUNT_LOCAL))\n",
        "\n",
        "    lc_html = build_lineage_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(LINEAGE_COUNT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(lc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(LINEAGE_COUNT_LOCAL))\n",
        "\n",
        "    cousin_html = build_cousin_print_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(COUSIN_PRINT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(COUSIN_PRINT_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, MATCH_COUNT_LOCAL, _remote_path(MATCH_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, LINEAGE_COUNT_LOCAL, _remote_path(LINEAGE_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, COUSIN_PRINT_LOCAL, _remote_path(COUSIN_PRINT_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload partials failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [_remote_path(MATCH_COUNT_REMOTE), _remote_path(LINEAGE_COUNT_REMOTE), _remote_path(COUSIN_PRINT_REMOTE)]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.shtml\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2b (Unified headers from Cell 2 + Cell 2b functionality) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAZRkyHlz_tR",
        "outputId": "862268bc-39dd-41a4-bcc2-0eb1770dc903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts_UnifiedHeaders | Version=2026.02.01-CELL2HDR-CELL2B1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 5, 2026 8:10 PM\n",
            "[OK] Loaded CSV for counts: 93 rows, 7 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 74956\n",
            "partials/lineage_count.shtml : 94428\n",
            "partials/cousin_list_print.htm : 34197\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2b_NetworkAuthority"
      ],
      "metadata": {
        "id": "MUDyjlbJbQoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2b_NetworkAuthority - Write dna_network_first_ancestors.txt ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Reads the same master CSV used by Cell 2b and derives a de-duplicated\n",
        "#   list of \"first ancestors\" from the lineage/ancestral-line column.\n",
        "# - Writes that list as /content/dna_network_first_ancestors.txt, one per line.\n",
        "# - This file is then consumed by Cell 2d (SaveNetworkAuthority) and Cell 2c\n",
        "#   (Match Specific Produced DNA Network).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2b_NetworkAuthority | Version=2025.12.06 | Encoding=ISO-8859-15 | DECLARED_LINES=160\n",
        "\n",
        "DECLARED_LINES = 160\n",
        "print(\n",
        "    \"[CONFIRM] Golden Rules active | \"\n",
        "    \"Cell=Cell2b_NetworkAuthority | \"\n",
        "    \"Version=2025.12.06 | \"\n",
        "    \"Encoding=ISO-8859-15 | \"\n",
        "    \"DECLARED_LINES=%d\" % DECLARED_LINES\n",
        ")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "\n",
        "# This should match the master CSV name used by Cell 2b.\n",
        "MASTER_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Authority file that Cell 2d expects and that Cell 2c already uses.\n",
        "AUTHORITY_PATH = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# ---------- 2) Helpers ----------\n",
        "\n",
        "def _clean_piece(text):\n",
        "    \\\"\\\"\\\"Normalize whitespace and tildes inside a lineage token.\\\"\\\"\\\"\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "# Same separator logic we used in Cell 2c: split a lineage path into tokens.\n",
        "SEP_RE = re.compile(\n",
        "    r\"\\s*(?:\"\n",
        "    r\"\\u2192\"          # unicode right arrow\n",
        "    r\"|&rarr;\"         # HTML right arrow\n",
        "    r\"|;|>|,\"\n",
        "    r\"|~{2,}\"          # multiple tildes\n",
        "    r\"|/{2,}\"          # double slash\n",
        "    r\"|\\|{2,}\"         # double pipe\n",
        "    r\")\\s*\"\n",
        ")\n",
        "\n",
        "def split_tokens(s):\n",
        "    \\\"\\\"\\\"Split a lineage path string into tokens using SEP_RE.\\\"\\\"\\\"\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    \\\"\\\"\\\"\n",
        "    Find a column in df using regex patterns and optional preferred names.\n",
        "    Returns the column name or None.\n",
        "    \\\"\\\"\\\"\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    # Preferred exact names first\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    # Otherwise use regex patterns\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ---------- 3) Load master CSV ----------\n",
        "\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(MASTER_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "\n",
        "if df is None:\n",
        "    raise SystemExit(\n",
        "        \"[ERROR] Unable to read CSV: %s (%r)\" % (MASTER_CSV, _last_err)\n",
        "    )\n",
        "\n",
        "print(\n",
        "    \"[OK] Loaded master CSV for network authority: %d rows, %d cols\"\n",
        "    % (len(df), len(df.columns))\n",
        ")\n",
        "\n",
        "# ---------- 4) Identify lineage / ancestral-line column ----------\n",
        "\n",
        "line_col = find_col(\n",
        "    df,\n",
        "    patterns=[\n",
        "        r\"(yates\\s*dna\\s*ancestral\\s*line)\",\n",
        "        r\"(ancestral\\s*line)\",\n",
        "        r\"(lineage)\"\n",
        "    ],\n",
        "    prefer_exact=[\n",
        "        \"Yates DNA Ancestral Line\",\n",
        "        \"Ancestral Line\",\n",
        "        \"Lineage\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "if not line_col:\n",
        "    raise SystemExit(\n",
        "        \"[ERROR] Cannot find lineage/ancestral-line column in master CSV.\"\n",
        "    )\n",
        "\n",
        "print(\"[OK] Using lineage column:\", repr(line_col))\n",
        "\n",
        "# ---------- 5) Derive first ancestors and de-duplicate ----------\n",
        "\n",
        "first_ancestors = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    tokens = split_tokens(row.get(line_col, \"\"))\n",
        "    first = _clean_piece(tokens[0]) if tokens else \"\"\n",
        "    if first:\n",
        "        first_ancestors.append(first)\n",
        "\n",
        "total_raw = len(first_ancestors)\n",
        "unique_list = []\n",
        "seen = set()\n",
        "\n",
        "# Preserve original encounter order, but discard duplicates and empties.\n",
        "for anc in first_ancestors:\n",
        "    key = anc.strip()\n",
        "    if not key:\n",
        "        continue\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    unique_list.append(key)\n",
        "\n",
        "print(\"[INFO] Total first-ancestor tokens collected: %d\" % total_raw)\n",
        "print(\"[INFO] Unique non-empty first ancestors:    %d\" % len(unique_list))\n",
        "\n",
        "# Optional: show a short preview in the notebook\n",
        "for idx, anc in enumerate(unique_list[:25], start=1):\n",
        "    print(\"  %2d. %s\" % (idx, anc))\n",
        "\n",
        "if not unique_list:\n",
        "    print(\"[WARN] No non-empty first ancestors found. Authority file will NOT be written.\")\n",
        "else:\n",
        "    # ---------- 6) Write dna_network_first_ancestors.txt ----------\n",
        "    try:\n",
        "        # Ensure directory exists (AUTHORITY_PATH is just a filename in /content).\n",
        "        os.makedirs(os.path.dirname(AUTHORITY_PATH) or \".\", exist_ok=True)\n",
        "        with open(\n",
        "            AUTHORITY_PATH,\n",
        "            \"w\",\n",
        "            encoding=\"iso-8859-15\",\n",
        "            errors=\"xmlcharrefreplace\",\n",
        "        ) as f:\n",
        "            for anc in unique_list:\n",
        "                f.write(anc.strip() + \"\\n\")\n",
        "        print(\"[OK] Wrote authority file:\", os.path.abspath(AUTHORITY_PATH))\n",
        "        print(\"[OK] Lines written:\", len(unique_list))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Failed to write authority file:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Cell2b_NetworkAuthority complete (authority file ready for Cell 2d) ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2b_NetworkAuthority - Write dna_network_first_ancestors.txt ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "8kpgsxWD3WTx",
        "outputId": "4f13123c-97c0-4ebe-8f61-32f05e1d8eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unexpected character after line continuation character (ipython-input-169464351.py, line 39)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-169464351.py\"\u001b[0;36m, line \u001b[0;32m39\u001b[0m\n\u001b[0;31m    \\\"\\\"\\\"Normalize whitespace and tildes inside a lineage token.\\\"\\\"\\\"\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2c"
      ],
      "metadata": {
        "id": "ah9XcKRcqLU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2c - Match Specific Produced DNA Network ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G4)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout via /partials/dna_tree_styles.css (linked only).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "DNA_NETWORK_LOCAL = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "DNA_NETWORK_REMOTE = posixpath.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "# Authority list is maintained on the server at /dna/network_first_ancestors.txt\n",
        "DNA_NETWORK_AUTH_REMOTE = \"dna/network_first_ancestors.txt\"\n",
        "DNA_NETWORK_AUTH_LOCAL = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# ---------- 1a) Load vitals ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def sync_authority_from_server():\n",
        "    \"\"\"\n",
        "    Pull /dna/network_first_ancestors.txt from the server (if it exists)\n",
        "    into DNA_NETWORK_AUTH_LOCAL.\n",
        "    \"\"\"\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[INFO] FTP creds missing; authority sync skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        remote = _remote_path(DNA_NETWORK_AUTH_REMOTE)\n",
        "        ok = ftp_download_if_exists(ftps, remote, DNA_NETWORK_AUTH_LOCAL)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        if ok:\n",
        "            print(\"[INFO] Authority file synced from server.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Authority sync failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = False\n",
        "        try:\n",
        "            local_name = SERVER_MAPPING_LOCAL_CACHE\n",
        "            with open(local_name, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % SERVER_MAPPING_BASENAME, f.write)\n",
        "            print(\"[PULL] %s -> %s\" % (SERVER_MAPPING_BASENAME, os.path.abspath(local_name)))\n",
        "            ok = True\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                if os.path.exists(SERVER_MAPPING_LOCAL_CACHE):\n",
        "                    os.remove(SERVER_MAPPING_LOCAL_CACHE)\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"[MISS] %s (%s)\" % (SERVER_MAPPING_BASENAME, e))\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) Helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ASCII-only separators.\n",
        "SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\",\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    if \"-\" in token:\n",
        "        token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) HTML shell ----------\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        + \"%s\\n\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        + \"<title>%s</title>\\n\" % _html.escape(title)\n",
        "        + \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        + \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title)\n",
        "        + \"<div class=\\\"updated centerline\\\">\"\n",
        "        \"Last updated: %s &nbsp;|&nbsp; \"\n",
        "        \"Showing: %s\"\n",
        "        \"</div>\\n\" % (\n",
        "            _html.escape(LAST_UPDATED_DISPLAY),\n",
        "            _html.escape(AUTOSOMAL_MATCHES_TEXT),\n",
        "        )\n",
        "        + NAV_BLOCK\n",
        "        + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "        \"Showing: \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "        \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function ySelEachRow(tb, cb){\"\n",
        "        \" if(!tb) return;\"\n",
        "        \" var rows=tb.getElementsByTagName('tr');\"\n",
        "        \" for(var i=0;i<rows.length;i++){cb(rows[i]);}\"\n",
        "        \"}\"\n",
        "        \"function ySelClear(tr){\"\n",
        "        \" if(!tr) return;\"\n",
        "        \" tr.removeAttribute('data-selected');\"\n",
        "        \" var cls=tr.className||'';\"\n",
        "        \" cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,'');\"\n",
        "        \" tr.className=cls;\"\n",
        "        \" tr.style.backgroundColor='';\"\n",
        "        \"}\"\n",
        "        \"function ySelToggle(a){\"\n",
        "        \" var tr=a;\"\n",
        "        \" while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;}\"\n",
        "        \" if(!tr) return false;\"\n",
        "        \" var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \" if(sel){\"\n",
        "        \"  ySelClear(tr);\"\n",
        "        \" }else{\"\n",
        "        \"  tr.setAttribute('data-selected','1');\"\n",
        "        \"  var cls=tr.className||'';\"\n",
        "        \"  if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';}\"\n",
        "        \"  tr.style.backgroundColor='#fff2cc';\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelGetTBody(tbodyId){\"\n",
        "        \" var tb=document.getElementById(tbodyId);\"\n",
        "        \" return tb || null;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowSelected(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){\"\n",
        "        \"  var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \"  tr.style.display=sel?'':'none';\"\n",
        "        \" });\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowAll(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display='';});\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelReset(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);});\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"window.ySelToggle=ySelToggle;\"\n",
        "        \"window.ySelShowSelected=ySelShowSelected;\"\n",
        "        \"window.ySelShowAll=ySelShowAll;\"\n",
        "        \"window.ySelReset=ySelReset;\"\n",
        "        \"})();\"\n",
        "        \"(function(){\"\n",
        "        \" function collectFirstAncestors(){\"\n",
        "        \"  var rows=document.querySelectorAll('#ref-tb tr');\"\n",
        "        \"  var seen=Object.create(null);\"\n",
        "        \"  var out=[];\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    var cells=rows[i].getElementsByTagName('td');\"\n",
        "        \"    if(!cells.length) continue;\"\n",
        "        \"    var txt=(cells[0].textContent||cells[0].innerText||'').replace(/\\\\s+/g,' ').trim();\"\n",
        "        \"    if(!txt) continue;\"\n",
        "        \"    if(!seen[txt]){seen[txt]=true; out.push(txt);}\"\n",
        "        \"  }\"\n",
        "        \"  return out;\"\n",
        "        \" }\"\n",
        "        \" function setStatus(msg,isError){\"\n",
        "        \"  var span=document.getElementById('save-network-status');\"\n",
        "        \"  if(!span) return;\"\n",
        "        \"  span.textContent=msg;\"\n",
        "        \"  span.style.color=isError?'#990000':'#006600';\"\n",
        "        \" }\"\n",
        "        \" function onClickSaveNetwork(){\"\n",
        "        \"  var ancestors=collectFirstAncestors();\"\n",
        "        \"  if(!ancestors.length){\"\n",
        "        \"    setStatus('No ancestors found to save.',true);\"\n",
        "        \"    return;\"\n",
        "        \"  }\"\n",
        "        \"  setStatus('Saving '+ancestors.length+' ancestors...',false);\"\n",
        "        \"  fetch('/dna/save_network.php',{\"\n",
        "        \"    method:'POST',\"\n",
        "        \"    headers:{'Content-Type':'application/json'},\"\n",
        "        \"    body:JSON.stringify({ancestors:ancestors})\"\n",
        "        \"  }).then(function(resp){\"\n",
        "        \"    if(!resp.ok){throw new Error('HTTP '+resp.status);}\"\n",
        "        \"    return resp.json();\"\n",
        "        \"  }).then(function(data){\"\n",
        "        \"    if(data&&data.status==='ok'){\"\n",
        "        \"      var n=(typeof data.saved==='number')?data.saved:ancestors.length;\"\n",
        "        \"      setStatus('Saved '+n+' ancestors to network authority.',false);\"\n",
        "        \"    }else{\"\n",
        "        \"      setStatus('Unexpected response while saving network.',true);\"\n",
        "        \"    }\"\n",
        "        \"  }).catch(function(err){\"\n",
        "        \"    setStatus('Save failed: '+err,true);\"\n",
        "        \"  });\"\n",
        "        \" }\"\n",
        "        \" function init(){\"\n",
        "        \"  var btn=document.getElementById('save-network-btn');\"\n",
        "        \"  if(!btn) return;\"\n",
        "        \"  btn.addEventListener('click',onClickSaveNetwork,false);\"\n",
        "        \" }\"\n",
        "        \" if(document.readyState==='loading'){\"\n",
        "        \"  document.addEventListener('DOMContentLoaded',init,false);\"\n",
        "        \" }else{\"\n",
        "        \"  init();\"\n",
        "        \" }\"\n",
        "        \"})();\"\n",
        "        \"\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(\n",
        "    row,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return subject_name_html, matchee_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Network authority + page builder ----------\n",
        "def _load_network_authority(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_network_first_ancestors.txt not found; using all First Ancestors.\")\n",
        "        return []\n",
        "    vals = []\n",
        "    with open(path, \"r\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as fh:\n",
        "        for line in fh:\n",
        "            t = line.strip()\n",
        "            if t:\n",
        "                vals.append(t)\n",
        "    if not vals:\n",
        "        print(\"[INFO] dna_network_first_ancestors.txt is empty; using all First Ancestors.\")\n",
        "        return []\n",
        "    print(\"[INFO] Loaded %d authority First Ancestors from %s\" % (len(vals), path))\n",
        "    if vals:\n",
        "        preview = vals[:10]\n",
        "        print(\"[INFO] First 10 authority entries:\")\n",
        "        for i, fa in enumerate(preview, 1):\n",
        "            print(\"  %2d. %s\" % (i, fa))\n",
        "    return vals\n",
        "\n",
        "def build_network_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    # Build First Ancestor (raw, not normalized) and full lineage\n",
        "    first_ancestors = []\n",
        "    full_lineages = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        path_raw = str(row.get(path_col, \"\") or \"\")\n",
        "        tokens = split_tokens(path_raw)\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "        full_lineages.append(_clean_piece(path_raw))\n",
        "    df = main_df.copy()\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "    df[\"Full Lineage\"] = full_lineages\n",
        "\n",
        "    # Apply authority filter if present\n",
        "    auth_vals = _load_network_authority(DNA_NETWORK_AUTH_LOCAL)\n",
        "    if auth_vals:\n",
        "        auth_set = set(auth_vals)\n",
        "        before = len(df)\n",
        "        df = df[df[\"First Ancestor\"].isin(auth_set)].copy()\n",
        "        after = len(df)\n",
        "        print(\"[INFO] Filtered DNA network rows by authority list: %d -> %d\" % (before, after))\n",
        "        if after == 0:\n",
        "            print(\"[WARN] Authority filter eliminated all rows; falling back to full dataset.\")\n",
        "            df = main_df.copy()\n",
        "            df[\"First Ancestor\"] = first_ancestors\n",
        "            df[\"Full Lineage\"] = full_lineages\n",
        "    else:\n",
        "        print(\"[INFO] No authority filter applied; DNA network uses all rows.\")\n",
        "\n",
        "    # Summary counts for top table (deduplicated First Ancestors)\n",
        "    first_series = df[\"First Ancestor\"].astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values(\n",
        "        [\"Count\", \"First Ancestor\"],\n",
        "        ascending=[False, True],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    title = \"Match Specific Produced DNA Network\"\n",
        "    html.append(_partial_head(title))\n",
        "\n",
        "    # 7a) First Ancestor summary table (top, deduplicated)\n",
        "    html.append('<h2 class=\"centerline\">Match Specific Produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"ref-table\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th style=\"width:80%\">First Ancestor</th>'\n",
        "        '<th style=\"width:20%\">Showing</th>'\n",
        "        '</tr></thead><tbody id=\"ref-tb\">'\n",
        "    )\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td class=\\\"count\\\">%d</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    # 7a.1) Button + status line to push current First Ancestors to /dna/save_network.php\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin:10px 0 15px 0;\">'\n",
        "        '<button id=\"save-network-btn\" type=\"button\">Update DNA Network Authority</button>'\n",
        "        '<span id=\"save-network-status\" style=\"margin-left:8px; font-size:90%;\"></span>'\n",
        "        '</div>'\n",
        "    )\n",
        "\n",
        "    # 7b) DNA Register rows table (below)\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for produced DNA network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>First Ancestor</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Full Lineage</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "    for _, row in df.iterrows():\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "        first = _html.escape(str(row.get(\"First Ancestor\", \"\")).strip())\n",
        "        lineage = _html.escape(str(row.get(\"Full Lineage\", \"\")).strip())\n",
        "\n",
        "        tr = (\n",
        "            \"<tr>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (match_to_html, first, cm_html, lineage, header_html)\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for DNA Network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    # Sync the authority list from the server (dna/network_first_ancestors.txt)\n",
        "    sync_authority_from_server()\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    network_html = build_network_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        DNA_NETWORK_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(network_html)\n",
        "    print(\"[OK] Wrote DNA Network partial:\", os.path.abspath(DNA_NETWORK_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; upload of dna_network.shtml skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, DNA_NETWORK_LOCAL, _remote_path(DNA_NETWORK_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network.shtml failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p = _remote_path(DNA_NETWORK_REMOTE)\n",
        "        sz = ftp_size(ftps, p)\n",
        "        print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URL ---\")\n",
        "        print(\"DNA Network: https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session for dna_network.shtml:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2c - Match Specific Produced DNA Network ======\n"
      ],
      "metadata": {
        "id": "L0AlOeKGBwo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7ffb29-9c18-4c00-bac0-26f1a2c84be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 5, 2026 3:42 PM\n",
            "[OK] Loaded CSV for DNA Network: 93 rows, 7 cols\n",
            "[PULL] dna/network_first_ancestors.txt -> /content/dna_network_first_ancestors.txt\n",
            "[INFO] Authority file synced from server.\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[INFO] Loaded 56 authority First Ancestors from dna_network_first_ancestors.txt\n",
            "[INFO] First 10 authority entries:\n",
            "   1. I47639 John Yates 1430-&I55584 Still Searching\n",
            "   2. I24496 John Yates 1581-1648&I24497 Joane Gater 1587-1666\n",
            "   3. I12324 William Yates 1720-&I12323 Anne Betty Thornborough 1725-\n",
            "   4. I19162 William Yates 1795-1866&I19163 Sally Parker 1806-\n",
            "   5. I12442 Thomas Yates 1670-&I5717 Monica Drury 1670-\n",
            "   6. I12192 Samuel Bedford Yates 1757-1844&I12193 Jane Millsaps 1753-1845\n",
            "   7. I37203 William Yates 1786-1862&I37260 Elizabeth Edwards 1793-1866\n",
            "   8. I25516 Thomas Yates 1700-1754&I55675 Still Searching\n",
            "   9. I53956 Phillip Yates &I55707 Still Searching\n",
            "  10. I10576 Joseph Yates 1754-&I10168 Mary Leigh 1753-1811\n",
            "[INFO] Filtered DNA network rows by authority list: 93 -> 93\n",
            "[OK] Wrote DNA Network partial: /content/partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 114782\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network: https://yates.one-name.net/partials/dna_network.shtml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2d"
      ],
      "metadata": {
        "id": "XM9k3wCyx4Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2d - Check Network Authority (Server Reader) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G2)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - ASCII-only source; any writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Deterministic audit banner + DECLARED_LINES.\n",
        "# - Purpose:\n",
        "#     * Read the current authority list from:\n",
        "#         https://yates.one-name.net/dna/network_first_ancestors.txt\n",
        "#     * De-duplicate it and print a clean, ordered list.\n",
        "# - This cell does NOT write or POST anything; the browser button does that.\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "import requests\n",
        "\n",
        "DECLARED_LINES = 80\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2d_CheckNetworkAuthority \"\n",
        "      \"| Version=2025.12.06-G2 | Encoding=ISO-8859-15 | DECLARED_LINES=%d\"\n",
        "      % DECLARED_LINES)\n",
        "\n",
        "AUTH_URL = \"https://yates.one-name.net/dna/network_first_ancestors.txt\"\n",
        "\n",
        "print(\"[INFO] Fetching authority from:\", AUTH_URL)\n",
        "\n",
        "try:\n",
        "    resp = requests.get(AUTH_URL, timeout=20)\n",
        "    print(\"[INFO] HTTP status:\", resp.status_code)\n",
        "    if resp.status_code != 200:\n",
        "        raise SystemExit(\"[ERROR] Could not fetch authority file.\")\n",
        "    raw_text = resp.text\n",
        "except Exception as e:\n",
        "    raise SystemExit(\"[ERROR] Request failed: %s\" % e)\n",
        "\n",
        "lines = []\n",
        "for ln in raw_text.splitlines():\n",
        "    s = ln.strip()\n",
        "    if not s:\n",
        "        continue\n",
        "    if s.startswith(\"#\"):\n",
        "        continue\n",
        "    lines.append(s)\n",
        "\n",
        "seen = {}\n",
        "unique = []\n",
        "for s in lines:\n",
        "    if s not in seen:\n",
        "        seen[s] = True\n",
        "        unique.append(s)\n",
        "\n",
        "print(\"[INFO] Total lines (including comments/blank):\", len(raw_text.splitlines()))\n",
        "print(\"[INFO] Unique First Ancestors:\", len(unique))\n",
        "\n",
        "for idx, val in enumerate(unique, 1):\n",
        "    print(\" %2d. %s\" % (idx, val))\n",
        "\n",
        "print(\"\\n--- Cell2d_CheckNetworkAuthority complete ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2d - Check Network Authority ======================\n"
      ],
      "metadata": {
        "id": "Y572NoTjDHBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eec347c-869c-4a27-c03e-d6be67e7ab30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2d_CheckNetworkAuthority | Version=2025.12.06-G2 | Encoding=ISO-8859-15 | DECLARED_LINES=80\n",
            "[INFO] Fetching authority from: https://yates.one-name.net/dna/network_first_ancestors.txt\n",
            "[INFO] HTTP status: 200\n",
            "[INFO] Total lines (including comments/blank): 56\n",
            "[INFO] Unique First Ancestors: 56\n",
            "  1. I47639 John Yates 1430-&I55584 Still Searching\n",
            "  2. I24496 John Yates 1581-1648&I24497 Joane Gater 1587-1666\n",
            "  3. I12324 William Yates 1720-&I12323 Anne Betty Thornborough 1725-\n",
            "  4. I19162 William Yates 1795-1866&I19163 Sally Parker 1806-\n",
            "  5. I12442 Thomas Yates 1670-&I5717 Monica Drury 1670-\n",
            "  6. I12192 Samuel Bedford Yates 1757-1844&I12193 Jane Millsaps 1753-1845\n",
            "  7. I37203 William Yates 1786-1862&I37260 Elizabeth Edwards 1793-1866\n",
            "  8. I25516 Thomas Yates 1700-1754&I55675 Still Searching\n",
            "  9. I53956 Phillip Yates &I55707 Still Searching\n",
            " 10. I10576 Joseph Yates 1754-&I10168 Mary Leigh 1753-1811\n",
            " 11. I18752 Joshua Hardy Yates 1775-1849&I18753 Martha Ann Stewart 1775-1840\n",
            " 12. I51112 Richard Yates &None Unknown Name\n",
            " 13. I20135 William Yates 1770-1841&I20134 Mary Needham 1784-1816\n",
            " 14. I22930 Samuel Porter Yates 1822-1865&I22931 Martha Bridges 1831-1882\n",
            " 15. I44892 William Yates 1779-1846&I44891 Phoebe Salt 1782-1816\n",
            " 16. I45964 William Yates 1820-1906&I45965 Esther Pike 1818-1899\n",
            " 17. I23364 William Price Yates 1812-1883&I23439 Elizabeth Ann McKinney 1812-1896\n",
            " 18. I35041 Rice D Yates 1818-1895&I35042 Polly Daniels 1817-1878\n",
            " 19. I39595 Loyd Yates 1760-&I36710 Mary Polly Brasfield 1774-1816\n",
            " 20. I48642 James Yates 1675-1740&None Unknown Name\n",
            " 21. I51266 Nathan James Yates &I54986 Still Searching\n",
            " 22. I51544 Soloman Yates 1812-1860&I51543 Elizabeth McCammack 1805-1870\n",
            " 23. I52488 William Basil Yates 1825-1864&I52489 Martha Bullock 1824-1870\n",
            " 24. I53060 Joseph Yates &I53061 Helen Maghull\n",
            " 25. I53848 Samuel Yates &None Unknown Name\n",
            " 26. I11773 John Yates 1737-1818&I11774 Elizabeth Barfield 1740-\n",
            " 27. I12944 Thomas Yates 1805-1865&I19270 Lydia Lamasters 1797-\n",
            " 28. I13102 Thomas Yates 1766-1853&I18820 Phebe Combs 1781-1853\n",
            " 29. I19423 William Charles Yates 1802-1849&I19422 Emily Milly McManus 1802-1876\n",
            " 30. I28953 Samuel Francis Yates 1804-&I28954 Delilah Brown 1807-\n",
            " 31. I29128 Thomas Yates 1799-1874&I29130 Mary Elizabeth Craun 1809-1881\n",
            " 32. I35923 Matthew Yates 1852-1933&I35924 Emily Jane Hood 1851-1924\n",
            " 33. I36004 William Nicholas Yates 1790-1873&I36005 Elizabeth Hays 1791-\n",
            " 34. I39942 William Thomas Yates 1794-1851&I39943 Nancy Wilkins 1822-1916\n",
            " 35. I40297 Thomas Yates 1747-&I40298 Betty Haslam 1751-\n",
            " 36. I45433 Thomas Yates 1789-1814&I45434 Mary Johnson 1788-1864\n",
            " 37. I45530 Wallace Albert Robinson 1888-1948&I45529 Sallie McLean Jones 1886-1939\n",
            " 38. I46704 Robert Allen Wessner 1943-2021&I46703 Edith D Smith 1943-\n",
            " 39. I50746 Malcolm Yates 1789-&I55571 Still Searching\n",
            " 40. I50968 Uriah Yates 1756-1838&I50969 Sarah Oakes 1752-\n",
            " 41. I51367 Thomas Yates 1616-1687&I51368 Elizabeth Pratt 1618-1722\n",
            " 42. I52263 William Joseph Yates 1826-1900&I52264 Sarah Jane Holstead 1829-1903\n",
            " 43. I52357 Samuel Yates 1798-1878&I52358 Margaret Swisher 1803-1876\n",
            " 44. I53975 Lafayette Yates 1854-1904&I53976 Mary Olive Roe 1862-1948\n",
            " 45. I53988 William M Yates 1837-1906&I53989 Nancy Ann Osborne 1849-1914\n",
            " 46. I54772 William Robert Yates 1860-1937&I54773 Malissa Davis\n",
            " 47. I55112 William Yates 1789-1870&I55113 Hannah House 1795-1870\n",
            " 48. I55192 William Yates 1753-1811&I55193 Anamaria Elizabeth Booth 1755-1843\n",
            " 49. I55303 Thomas Yates 1783-1858&I55302 Abigail Paul 1793-1875\n",
            " 50. I56569 Thomas Yates 1740-1799&I56570 Mary Smith 1737-1787\n",
            " 51. I56667 William Yates 1732-1804&I56666 Ann Ridyard\n",
            " 52. I56758 Thomas Yates &I56759 Betty Hill 1774-1862\n",
            " 53. I57112 William Lewis Yates 1868-1959&I57113 Ellen Isabel Lamping 1867-1937\n",
            " 54. I57512 Stephen Yates 1805-&I57511 Nancy Tomlinson\n",
            " 55. I57731 Allen Caldwell Yates 1783-1862&None Unknown Name\n",
            " 56. I57938 Manford Charles Yates 1853-1908&I57939 Elizabeth Harrington 1855-1918\n",
            "\n",
            "--- Cell2d_CheckNetworkAuthority complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 2k"
      ],
      "metadata": {
        "id": "H2k15Th8cWfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 2k - Unified DNA Network View (FLOW tiles; header-safe; nav-safe; dynamic Showing) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.01-CELL2K-HDRSAFE1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional.\n",
        "# - Uses canonical /partials/partials_unified.css for baseline; adds tiny scoped helper CSS + header-safety CSS.\n",
        "# - SSI nav include inserted once (no duplication). Optional JS nav repair is NOT used here.\n",
        "# - Header placement matches Cell 3 pattern: H1 -> UPDATED block -> NAV -> controls -> table.\n",
        "# - Header \"Showing:\" is dynamic (selection/filter aware) and reflects visible register rows excluding \"No\"/excluded.\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# NOTE: In Colab, the notebook cell text is not reliably introspectable for an exact line-count audit.\n",
        "DECLARED_LINES = -1\n",
        "print(\"[AUDIT] DECLARED_LINES={}\".format(DECLARED_LINES))\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip().strip(\"/\")\n",
        "\n",
        "# Baseline stylesheet (same as Cell 3 / the fixed Cell 2c)\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "# SSI navigation include (inserted once)\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "DNA_NETWORK_LOCAL  = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "DNA_NETWORK_REMOTE = posixpath.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "REGISTER_CSV_LOCAL  = \"dna_network_register.csv\"\n",
        "REGISTER_CSV_REMOTE = \"dna/dna_network_register.csv\"\n",
        "\n",
        "# ---------- 1a) Vitals ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    # Site convention: EST = UTC-5 (no DST)\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) Name + token helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = token.lower()\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token,\n",
        "    )\n",
        "    if \"-\" in token:\n",
        "        token = \"-\".join([w[:1].upper() + w[1:] for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token[0].upper() + token[1:]\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def format_name_bold_last(display_name: str) -> str:\n",
        "    s = _clean_piece(display_name or \"\")\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    parts = [p for p in s.split(\" \") if p]\n",
        "    if len(parts) == 1:\n",
        "        return '<span class=\"dn-last\">%s</span>' % _html.escape(parts[0])\n",
        "    first = \" \".join(parts[:-1])\n",
        "    last = parts[-1]\n",
        "    return (\n",
        "        '<span class=\"dn-first\">%s</span> <span class=\"dn-last\">%s</span>'\n",
        "        % (_html.escape(first), _html.escape(last))\n",
        "    )\n",
        "\n",
        "def _last_first_keys(display_name: str):\n",
        "    s = _clean_piece(display_name or \"\")\n",
        "    if not s:\n",
        "        return (\"\", \"\")\n",
        "    parts = [p for p in s.split(\" \") if p]\n",
        "    if len(parts) == 1:\n",
        "        return (parts[0].lower(), \"\")\n",
        "    last = parts[-1].lower()\n",
        "    first = parts[0].lower()\n",
        "    return (last, first)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(x):\n",
        "        return smart_titlecase(x) if \" \" in x else smart_titlecase(surname_given_from_token(x)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_display_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_display_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) HTML shell (header placement + header safety) ----------\n",
        "def _partial_head(title: str) -> str:\n",
        "    # Header safety learned from Cell 3: force THEAD/TH visible even if upstream CSS hides them.\n",
        "    late_style = (\n",
        "        '<style type=\"text/css\">\\n'\n",
        "        '/* Header/Nav safety. */\\n'\n",
        "        '#nav-slot, #nav-slot nav, #nav-slot .oldnav, #nav-slot .navbar{display:block !important; visibility:visible !important; opacity:1 !important;}\\n'\n",
        "        'table.sortable thead{display:table-header-group !important; visibility:visible !important;}\\n'\n",
        "        'table.sortable thead th{display:table-cell !important; visibility:visible !important;}\\n'\n",
        "        '/* Participants: flow tiles; show Name (bold last) + row-count only. */\\n'\n",
        "        'table.dn-participants thead{display:none !important;}\\n'\n",
        "        'table.dn-participants tbody tr{display:inline-block;vertical-align:top;margin:4px 10px 4px 0;padding:4px 8px;border:1px solid #ddd;border-radius:4px;background:#fff;white-space:nowrap;width:auto;max-width:360px;}\\n'\n",
        "        'table.dn-participants tbody td{display:inline-block;padding:0 6px 0 0;vertical-align:baseline;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(1){display:none !important;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(2){min-width:0;max-width:280px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(3){display:none !important;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(4){min-width:18px;text-align:right;padding-right:0;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(2) .dn-last{font-weight:700;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(2) .dn-first{font-weight:400;}\\n'\n",
        "        '/* Sticky left column for register table. */\\n'\n",
        "        '#reg-list{border-collapse:collapse;}\\n'\n",
        "        '#reg-list th, #reg-list td{white-space:nowrap;}\\n'\n",
        "        '#reg-list thead th{position:sticky; top:0; z-index:3;}\\n'\n",
        "        '#reg-list th:first-child, #reg-list td:first-child{position:sticky; left:0; z-index:2; background:#ffffff;}\\n'\n",
        "        '</style>\\n'\n",
        "    )\n",
        "\n",
        "    # Updated block (match Cell 3 / Cell 2c conventions)\n",
        "    parts = []\n",
        "    parts.append('Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_DISPLAY))\n",
        "    if AUTOSOMAL_MATCHES_TEXT and AUTOSOMAL_MATCHES_TEXT != \"(unknown)\":\n",
        "        parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES_TEXT))\n",
        "    parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "    updated_block = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join(parts) + '</div>'\n",
        "\n",
        "    # SSI include inside a slot (matches Cell 3 pattern; no JS repair here)\n",
        "    nav_slot = '<div id=\"nav-slot\">%s</div>' % NAV_BLOCK\n",
        "\n",
        "    menu = (\n",
        "        '<div class=\"selection-menu centerline\">'\n",
        "        'View: '\n",
        "        '<a href=\"#\" onclick=\"return dnShowSelected();\">Selected</a> &nbsp;|&nbsp; '\n",
        "        '<a href=\"#\" onclick=\"return dnShowAll();\">All</a> &nbsp;|&nbsp; '\n",
        "        '<a href=\"#\" onclick=\"return dnReset();\">Reset</a>'\n",
        "        '</div>'\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        '<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n'\n",
        "        ' \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n'\n",
        "        '<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\\n'\n",
        "        '<head>\\n'\n",
        "        '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\\n'\n",
        "        '<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n'\n",
        "        '<title>%s</title>\\n'\n",
        "        '%s\\n'\n",
        "        '%s'\n",
        "        '</head>\\n'\n",
        "        '<body>\\n'\n",
        "        '<div class=\"wrap\">\\n'\n",
        "        '<h1 class=\"centerline\">%s</h1>\\n'\n",
        "        '%s\\n'\n",
        "        '%s\\n'\n",
        "        '%s\\n'\n",
        "        '<div class=\"table-scroll\">\\n'\n",
        "        % (_html.escape(title), HEAD_LINK, late_style, _html.escape(title), updated_block, nav_slot, menu)\n",
        "    )\n",
        "\n",
        "def _partial_tail() -> str:\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\\n\"\n",
        "        \"  var selectedMatches={};\\n\"\n",
        "        \"  var viewMode='all';\\n\"\n",
        "        \"  function hasSelection(){for(var k in selectedMatches){if(selectedMatches.hasOwnProperty(k)){return true;}}return false;}\\n\"\n",
        "        \"  function setRowSelected(tr, sel){if(!tr) return; tr.setAttribute('data-selected', sel ? '1' : '0'); tr.style.backgroundColor = sel ? '#fff2cc' : '';}\\n\"\n",
        "        \"  function setParticipantsVisible(show){var wrap=document.getElementById('participants-wrapper'); if(!wrap) return; wrap.style.display = show ? '' : 'none';}\\n\"\n",
        "        \"  function setFirstAncVisible(show){var wrap=document.getElementById('first-anc-wrapper'); if(!wrap) return; wrap.style.display = show ? '' : 'none';}\\n\"\n",
        "        \"  function formatWithCommas(n){try{var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US');}catch(e){return String(n||'');}}\\n\"\n",
        "        \"  function updateShowingCount(n){var el=document.getElementById('showing-count'); if(!el) return; el.textContent=formatWithCommas(n);}\\n\"\n",
        "        \"  function updateSummaryFromVisible(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    var participants={}; var firstAncestors={}; var visibleLines=0;\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      var r=rows[i];\\n\"\n",
        "        \"      if(r.style.display==='none') continue;\\n\"\n",
        "        \"      if(r.getAttribute('data-excluded')==='1') continue;\\n\"\n",
        "        \"      var incCell=r.querySelector('.dn-include-cell');\\n\"\n",
        "        \"      if(incCell){var inc=(incCell.textContent||''); var incNorm=inc.toLowerCase().replace(/\\\\s+/g,''); if(incNorm!=='yes') continue;}\\n\"\n",
        "        \"      visibleLines++;\\n\"\n",
        "        \"      var mk=r.getAttribute('data-match')||'';\\n\"\n",
        "        \"      var fa=r.getAttribute('data-first')||'';\\n\"\n",
        "        \"      if(mk){participants[mk]=true;} if(fa){firstAncestors[fa]=true;}\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    var pCount=0, faCount=0, k;\\n\"\n",
        "        \"    for(k in participants){if(participants.hasOwnProperty(k)){pCount++;}}\\n\"\n",
        "        \"    for(k in firstAncestors){if(firstAncestors.hasOwnProperty(k)){faCount++;}}\\n\"\n",
        "        \"    var selCount=0; for(k in selectedMatches){if(selectedMatches.hasOwnProperty(k)){selCount++;}}\\n\"\n",
        "        \"    var elSel=document.getElementById('dn-sum-selected');\\n\"\n",
        "        \"    var elP=document.getElementById('dn-sum-part');\\n\"\n",
        "        \"    var elL=document.getElementById('dn-sum-lines');\\n\"\n",
        "        \"    var elFA=document.getElementById('dn-sum-fa');\\n\"\n",
        "        \"    if(elSel){elSel.textContent='Selected Participant: '+selCount;}\\n\"\n",
        "        \"    if(elP){elP.textContent='Network Participants: '+pCount;}\\n\"\n",
        "        \"    if(elL){elL.textContent='Network Ancestral lines: '+visibleLines;}\\n\"\n",
        "        \"    if(elFA){elFA.textContent='Network First Ancestors: '+faCount;}\\n\"\n",
        "        \"    updateShowingCount(visibleLines);\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function updateRegister(){\\n\"\n",
        "        \"    var regRows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    if(viewMode!=='selected' || !hasSelection()){\\n\"\n",
        "        \"      for(var i=0;i<regRows.length;i++){regRows[i].style.display='';}\\n\"\n",
        "        \"    }else{\\n\"\n",
        "        \"      var focusFirst={};\\n\"\n",
        "        \"      for(var i2=0;i2<regRows.length;i2++){\\n\"\n",
        "        \"        var r2=regRows[i2];\\n\"\n",
        "        \"        var mk=r2.getAttribute('data-match')||'';\\n\"\n",
        "        \"        var fa=r2.getAttribute('data-first')||'';\\n\"\n",
        "        \"        if(mk && fa && selectedMatches[mk]){focusFirst[fa]=true;}\\n\"\n",
        "        \"      }\\n\"\n",
        "        \"      for(var i3=0;i3<regRows.length;i3++){\\n\"\n",
        "        \"        var r3=regRows[i3];\\n\"\n",
        "        \"        var fa2=r3.getAttribute('data-first')||'';\\n\"\n",
        "        \"        var show=!!focusFirst[fa2];\\n\"\n",
        "        \"        r3.style.display = show ? '' : 'none';\\n\"\n",
        "        \"      }\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    updateSummaryFromVisible();\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function dnToggleMatchRow(tr){\\n\"\n",
        "        \"    if(!tr) return false;\\n\"\n",
        "        \"    var key=tr.getAttribute('data-match')||'';\\n\"\n",
        "        \"    if(!key) return false;\\n\"\n",
        "        \"    if(selectedMatches[key]){delete selectedMatches[key]; setRowSelected(tr,false);}else{selectedMatches[key]=true; setRowSelected(tr,true);}\\n\"\n",
        "        \"    updateRegister(); return false;\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function wireMatchRows(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#match-tb tr');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      (function(r){r.onclick=function(e){dnToggleMatchRow(r); if(e && e.preventDefault){e.preventDefault();}};})(rows[i]);\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function dnToggleFirstAnc(){var wrap=document.getElementById('first-anc-wrapper'); if(!wrap) return false; var hidden=(wrap.style.display==='none'); setFirstAncVisible(hidden); return false;}\\n\"\n",
        "        \"  function dnShowSelected(){viewMode='selected'; updateRegister(); setParticipantsVisible(false); setFirstAncVisible(false); return false;}\\n\"\n",
        "        \"  function dnShowAll(){viewMode='all'; updateRegister(); setParticipantsVisible(true); setFirstAncVisible(true); return false;}\\n\"\n",
        "        \"  function dnReset(){\\n\"\n",
        "        \"    selectedMatches={}; viewMode='all';\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#match-tb tr');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){setRowSelected(rows[i],false);}\\n\"\n",
        "        \"    var rrows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    for(var j=0;j<rrows.length;j++){\\n\"\n",
        "        \"      rrows[j].style.display=''; rrows[j].setAttribute('data-excluded','0'); rrows[j].style.opacity='';\\n\"\n",
        "        \"      var c=rrows[j].querySelector('.dn-include-cell'); if(c){c.textContent='Yes';}\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    updateSummaryFromVisible(); setParticipantsVisible(true); setFirstAncVisible(true); return false;\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function toggleExcludeRow(tr){\\n\"\n",
        "        \"    if(!tr) return;\\n\"\n",
        "        \"    var cur=(tr.getAttribute('data-excluded')==='1');\\n\"\n",
        "        \"    var next=!cur;\\n\"\n",
        "        \"    tr.setAttribute('data-excluded', next ? '1' : '0');\\n\"\n",
        "        \"    tr.style.opacity = next ? '0.45' : '';\\n\"\n",
        "        \"    var cell=tr.querySelector('.dn-include-cell'); if(cell){cell.textContent = next ? 'No' : 'Yes';}\\n\"\n",
        "        \"    updateSummaryFromVisible();\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function wireRegisterRows(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      (function(r){\\n\"\n",
        "        \"        var cell=r.querySelector('.dn-include-cell'); if(!cell) return;\\n\"\n",
        "        \"        cell.onclick=function(e){toggleExcludeRow(r); if(e && e.preventDefault){e.preventDefault();} return false;};\\n\"\n",
        "        \"      })(rows[i]);\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function dnDownloadCurrentCSV(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    var out=[];\\n\"\n",
        "        \"    function esc(v){\\n\"\n",
        "        \"      if(v==null){v='';}\\n\"\n",
        "        \"      v=String(v);\\n\"\n",
        "        \"      if(v.indexOf('\\\"')>=0||v.indexOf(',')>=0||v.indexOf('\\\\n')>=0||v.indexOf('\\\\r')>=0){v='\\\"'+v.replace(/\\\"/g,'\\\"\\\"')+'\\\"';}\\n\"\n",
        "        \"      return v;\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    out.push('Match to,First Ancestor,Include in proof,Match Summary,Full Lineage');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      var r=rows[i];\\n\"\n",
        "        \"      if(r.style.display==='none') continue;\\n\"\n",
        "        \"      if(r.getAttribute('data-excluded')==='1') continue;\\n\"\n",
        "        \"      var tds=r.getElementsByTagName('td');\\n\"\n",
        "        \"      if(tds.length<5) continue;\\n\"\n",
        "        \"      var inc=(tds[2].textContent||'');\\n\"\n",
        "        \"      var incNorm=inc.toLowerCase().replace(/\\\\s+/g,'');\\n\"\n",
        "        \"      if(incNorm!=='yes') continue;\\n\"\n",
        "        \"      var match_to=(tds[0].textContent||'').trim();\\n\"\n",
        "        \"      var first=(tds[1].textContent||'').trim();\\n\"\n",
        "        \"      var summary=(tds[3].textContent||'').trim();\\n\"\n",
        "        \"      var lineage=(tds[4].textContent||'').trim();\\n\"\n",
        "        \"      out.push([esc(match_to),esc(first),esc(inc),esc(summary),esc(lineage)].join(','));\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    var csv=out.join('\\\\r\\\\n');\\n\"\n",
        "        \"    var blob=new Blob([csv],{type:'text/csv'});\\n\"\n",
        "        \"    var url=URL.createObjectURL(blob);\\n\"\n",
        "        \"    var a=document.createElement('a'); a.href=url; a.download='dna_network_register_live.csv';\\n\"\n",
        "        \"    document.body.appendChild(a); a.click(); document.body.removeChild(a); URL.revokeObjectURL(url);\\n\"\n",
        "        \"    return false;\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  window.dnShowSelected=dnShowSelected;\\n\"\n",
        "        \"  window.dnShowAll=dnShowAll;\\n\"\n",
        "        \"  window.dnReset=dnReset;\\n\"\n",
        "        \"  window.dnToggleFirstAnc=dnToggleFirstAnc;\\n\"\n",
        "        \"  window.dnDownloadCurrentCSV=dnDownloadCurrentCSV;\\n\"\n",
        "        \"  function init(){wireMatchRows(); wireRegisterRows(); setParticipantsVisible(true); setFirstAncVisible(true); updateRegister();}\\n\"\n",
        "        \"  if(document.readyState==='loading'){document.addEventListener('DOMContentLoaded',init,false);}else{init();}\\n\"\n",
        "        \"})();\\n\"\n",
        "        \"//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) Row builder ----------\n",
        "def build_register_row(row, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str):\n",
        "    subject_raw = str(row.get(match_col, \"\") or \"\")\n",
        "    subject_code = subject_raw.strip()\n",
        "    key = subject_code.lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_code)\n",
        "\n",
        "    subject_display = normalize_person_name(subject_unmasked or subject_code)\n",
        "    subject_display_html = _html.escape(subject_display or \"\")\n",
        "\n",
        "    match_to_html = subject_display_html\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    base_matchee = norm_matchee_name(row.get(name_col, \"\")) or subject_display\n",
        "    matchee_name = normalize_person_name(base_matchee)\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_display_html or subject_display,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return match_to_html, header_html, key\n",
        "\n",
        "# ---------- 7) Unified page + CSV builder ----------\n",
        "def build_network_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str):\n",
        "    first_ancestors = []\n",
        "    full_lineages = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        path_raw = str(row.get(path_col, \"\") or \"\")\n",
        "        tokens = split_tokens(path_raw)\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "        full_lineages.append(_clean_piece(path_raw))\n",
        "    df = main_df.copy()\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "    df[\"Full Lineage\"] = full_lineages\n",
        "\n",
        "    first_series = df[\"First Ancestor\"].astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values([\"Count\", \"First Ancestor\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    # Participants: derive + normalize + sort by last name; keep code in data (hidden in UI)\n",
        "    part_df = df[[match_col]].copy()\n",
        "    part_df[\"match_key\"] = part_df[match_col].astype(str).str.strip().str.lower()\n",
        "    part_df[\"match_code_raw\"] = part_df[match_col].astype(str).str.strip()\n",
        "\n",
        "    unmasked_series = part_df[\"match_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "    part_df[\"match_label\"] = unmasked_series\n",
        "    mask_empty = part_df[\"match_label\"] == \"\"\n",
        "    part_df.loc[mask_empty, \"match_label\"] = part_df.loc[mask_empty, \"match_code_raw\"]\n",
        "    part_df[\"match_label\"] = part_df[\"match_label\"].map(lambda x: normalize_person_name(x) if str(x).strip() else \"\")\n",
        "    part_df = part_df[part_df[\"match_key\"] != \"\"]\n",
        "\n",
        "    if part_df.empty:\n",
        "        p_counts = pd.DataFrame(columns=[\"match_key\", \"match_code_raw\", \"match_label\", \"rows\"])\n",
        "    else:\n",
        "        grp = part_df.groupby(\"match_key\")\n",
        "        rows_series = grp.size().rename(\"rows\")\n",
        "        code_series = grp[\"match_code_raw\"].first()\n",
        "        label_series = grp[\"match_label\"].first()\n",
        "        p_counts = pd.concat([code_series, label_series, rows_series], axis=1).reset_index()\n",
        "        p_counts.columns = [\"match_key\", \"match_code_raw\", \"match_label\", \"rows\"]\n",
        "\n",
        "        p_counts[\"sort_last\"] = p_counts[\"match_label\"].map(lambda x: _last_first_keys(x)[0])\n",
        "        p_counts[\"sort_first\"] = p_counts[\"match_label\"].map(lambda x: _last_first_keys(x)[1])\n",
        "\n",
        "        p_counts = p_counts.sort_values(\n",
        "            [\"sort_last\", \"sort_first\", \"match_label\", \"match_key\"],\n",
        "            ascending=[True, True, True, True],\n",
        "            kind=\"mergesort\",\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "    full_participants = int(p_counts.shape[0])\n",
        "    full_lines = int(first_series[first_series != \"\"].shape[0])\n",
        "    full_first_anc = int(len(vc.index))\n",
        "\n",
        "    html = []\n",
        "    title = \"Match Specific Produced DNA Network\"\n",
        "    html.append(_partial_head(title))\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">Match Specific Produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<p class=\"centerline\">This unified view shows participants (matches), First Ancestors '\n",
        "        'produced by the current DNA Network, and the detailed DNA Register rows below. '\n",
        "        'Click tiles in the participants section to focus on a subset; use the Selected / All / Reset '\n",
        "        'links above to change the view. In the register, you can mark individual lines as '\n",
        "        'excluded from the proof.</p>'\n",
        "    )\n",
        "\n",
        "    # Participants: header includes total count; tiles show Name (bold last) + row-count number only\n",
        "    html.append('<div id=\"participants-wrapper\">')\n",
        "    html.append('<h3>Network participants (matches): %d</h3>' % full_participants)\n",
        "    if p_counts.empty:\n",
        "        html.append('<p><em>No participants could be derived from the produced DNA network.</em></p>')\n",
        "    else:\n",
        "        html.append('<table id=\"participants-table\" class=\"dn-participants sortable\" border=\"1\">')\n",
        "        html.append('<tbody id=\"match-tb\">')\n",
        "        for _, r in p_counts.iterrows():\n",
        "            mkey = str(r.get(\"match_key\", \"\")).strip().lower()\n",
        "            code_raw = str(r.get(\"match_code_raw\", \"\")).strip()\n",
        "            label = str(r.get(\"match_label\", \"\")).strip()\n",
        "            rows_count = int(str(r.get(\"rows\", \"0\")).strip() or \"0\")\n",
        "\n",
        "            label_html = format_name_bold_last(label)\n",
        "\n",
        "            tr = (\n",
        "                '<tr data-match=\"%s\">'\n",
        "                '<td></td>'\n",
        "                '<td>%s</td>'\n",
        "                '<td>%s</td>'\n",
        "                '<td class=\"count\">%d</td>'\n",
        "                '</tr>'\n",
        "                % (\n",
        "                    _html.escape(mkey, quote=True),\n",
        "                    label_html,\n",
        "                    _html.escape(code_raw),\n",
        "                    rows_count,\n",
        "                )\n",
        "            )\n",
        "            html.append(tr)\n",
        "        html.append(\"</tbody></table>\")\n",
        "    html.append(\"</div>\")  # participants-wrapper\n",
        "\n",
        "    # First Ancestor table + toggle\n",
        "    html.append('<h3>First Ancestors produced by this DNA Network</h3>')\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin-bottom:4px;\">'\n",
        "        '<a href=\"#\" onclick=\"return dnToggleFirstAnc();\">Hide / show First Ancestors table</a>'\n",
        "        '</div>'\n",
        "    )\n",
        "    html.append('<div id=\"first-anc-wrapper\">')\n",
        "    if lin_df.empty:\n",
        "        html.append('<p><em>No First Ancestors could be derived from the produced DNA network.</em></p>')\n",
        "    else:\n",
        "        html.append(\n",
        "            '<table id=\"first-anc-table\" class=\"sortable\" border=\"1\">'\n",
        "            '<thead><tr>'\n",
        "            '<th style=\"width:80%\">First Ancestor</th>'\n",
        "            '<th style=\"width:20%\">Showing</th>'\n",
        "            '</tr></thead><tbody id=\"anc-tb\">'\n",
        "        )\n",
        "        for _, r in lin_df.iterrows():\n",
        "            first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "            cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "            tr = (\n",
        "                '<tr data-first=\"%s\">'\n",
        "                '<td>%s</td>'\n",
        "                '<td class=\"count\">%d</td>'\n",
        "                '</tr>'\n",
        "                % (\n",
        "                    _html.escape(first, quote=True),\n",
        "                    _html.escape(first),\n",
        "                    cnt,\n",
        "                )\n",
        "            )\n",
        "            html.append(tr)\n",
        "        html.append(\"</tbody></table>\")\n",
        "    html.append(\"</div>\")  # first-anc-wrapper\n",
        "\n",
        "    # Summary block + CSV link\n",
        "    html.append(\n",
        "        '<table id=\"dn-summary\" class=\"summary-block\" border=\"0\" style=\"margin:10px auto 6px auto;\">'\n",
        "        '<tr>'\n",
        "        '<td><strong>Current proof scope:</strong></td>'\n",
        "        '<td>'\n",
        "        '<span id=\"dn-sum-selected\">Selected Participant: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-part\">Network Participants: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-lines\">Network Ancestral lines: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-fa\">Network First Ancestors: 0</span> &nbsp;|&nbsp; '\n",
        "        '<a href=\"#\" onclick=\"return dnDownloadCurrentCSV();\">Download register CSV</a>'\n",
        "        '</td>'\n",
        "        '</tr>'\n",
        "        '</table>'\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: '%%' needed because this string is used with % formatting below\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin:2px 0 10px 0; font-size:90%%;\">'\n",
        "        'Full study scope (all network lines): '\n",
        "        'Participants: %d  |  Ancestral lines: %d  |  First Ancestors: %d'\n",
        "        '</div>' % (full_participants, full_lines, full_first_anc)\n",
        "    )\n",
        "\n",
        "    # Register rows + CSV rows\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>First Ancestor</th>'\n",
        "        '<th>Include in proof</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '<th>Full Lineage</th>'\n",
        "        '</tr></thead><tbody id=\"reg-tb\">'\n",
        "    )\n",
        "\n",
        "    csv_rows = []\n",
        "    tag_re = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        subject_html, header_html, mkey = build_register_row(row, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "        first_raw = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        first_esc = _html.escape(first_raw)\n",
        "        lineage_raw = str(row.get(\"Full Lineage\", \"\")).strip()\n",
        "        lineage_esc = _html.escape(lineage_raw)\n",
        "\n",
        "        tr = (\n",
        "            '<tr data-match=\"%s\" data-first=\"%s\" data-excluded=\"0\">'\n",
        "            '<td>%s</td>'\n",
        "            '<td>%s</td>'\n",
        "            '<td class=\"dn-include-cell\">Yes</td>'\n",
        "            '<td>%s</td>'\n",
        "            '<td>%s</td>'\n",
        "            '</tr>'\n",
        "            % (\n",
        "                _html.escape(mkey, quote=True),\n",
        "                _html.escape(first_raw, quote=True),\n",
        "                subject_html,\n",
        "                first_esc,\n",
        "                header_html,\n",
        "                lineage_esc,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "        summary_plain = tag_re.sub(\"\", header_html)\n",
        "        csv_rows.append({\n",
        "            \"Match code\": mkey,\n",
        "            \"First Ancestor\": first_raw,\n",
        "            \"Include in proof\": \"Yes\",\n",
        "            \"Match Summary\": summary_plain,\n",
        "            \"Full Lineage\": lineage_raw,\n",
        "        })\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html), csv_rows\n",
        "\n",
        "# ---------- 8) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for unified DNA Network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    network_html, csv_rows = build_network_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "    with open(DNA_NETWORK_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(network_html)\n",
        "    print(\"[OK] Wrote unified DNA Network partial:\", os.path.abspath(DNA_NETWORK_LOCAL))\n",
        "\n",
        "    if csv_rows:\n",
        "        reg_df = pd.DataFrame(csv_rows, columns=[\n",
        "            \"Match code\",\n",
        "            \"First Ancestor\",\n",
        "            \"Include in proof\",\n",
        "            \"Match Summary\",\n",
        "            \"Full Lineage\",\n",
        "        ])\n",
        "        reg_df.to_csv(\n",
        "            REGISTER_CSV_LOCAL,\n",
        "            index=False,\n",
        "            encoding=\"iso-8859-15\",\n",
        "            errors=\"xmlcharrefreplace\",\n",
        "        )\n",
        "        print(\"[OK] Wrote register CSV:\", os.path.abspath(REGISTER_CSV_LOCAL))\n",
        "    else:\n",
        "        print(\"[WARN] No register rows; CSV not written.\")\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; upload of dna_network.shtml and CSV skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, DNA_NETWORK_LOCAL, _remote_path(DNA_NETWORK_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network.shtml failed:\", e)\n",
        "\n",
        "        if csv_rows:\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, REGISTER_CSV_LOCAL, _remote_path(REGISTER_CSV_REMOTE))\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] Upload register CSV failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p_html = _remote_path(DNA_NETWORK_REMOTE)\n",
        "        sz_html = ftp_size(ftps, p_html)\n",
        "        print(\"%s : %s\" % (p_html, sz_html if sz_html is not None else \"(SIZE unsupported)\"))\n",
        "        if csv_rows:\n",
        "            p_csv = _remote_path(REGISTER_CSV_REMOTE)\n",
        "            sz_csv = ftp_size(ftps, p_csv)\n",
        "            print(\"%s : %s\" % (p_csv, sz_csv if sz_csv is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URL ---\")\n",
        "        print(\"DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "        if csv_rows:\n",
        "            print(\"Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session for dna_network.shtml / register CSV:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2k - Unified DNA Network View (FLOW tiles; header-safe; nav-safe; dynamic Showing) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmrAfgftE8Q4",
        "outputId": "9e2b5baf-6ff1-4af4-f871-ad8ac2ac6773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 5, 2026 3:42 PM\n",
            "[OK] Loaded CSV for unified DNA Network: 93 rows, 7 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote unified DNA Network partial: /content/partials/dna_network.shtml\n",
            "[OK] Wrote register CSV: /content/dna_network_register.csv\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[PUT] dna_network_register.csv -> dna/dna_network_register.csv\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 138301\n",
            "dna/dna_network_register.csv : 79487\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\n",
            "Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "INiJljOS1kRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 3 - Ancestor Register (Trees View; .shtml + SSI nav repair) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.02-CELL3-COL1=FIRST-ANCESTOR + ENRICHED-EXCLUDE1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# - Authority source for \"First Ancestor\" (same as Cell 2):\n",
        "#     /partials/first_ancestor_pairs.csv (downloaded via FTP) provides mapping:\n",
        "#       (FirstPair_Ancestor1_ID, FirstPair_Ancestor2_ID) -> FirstPair_LastFirst\n",
        "# - Change in this build:\n",
        "#     (1) Column 1 is REPLACED with \"First Ancestor\" values (authority key).\n",
        "#     (2) The extra trailing \"First Ancestor\" column is removed to avoid duplication.\n",
        "#     (3) UPDATED EXCLUSION: drop the enriched early-ancestor prefix up to Frances Yates & Jane Tichborne.\n",
        "#\n",
        "# Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "from string import Template as _T\n",
        "\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "# ---------- Display Policy ----------\n",
        "SUPPRESS_ID_COLUMN = True\n",
        "SUPPRESS_EMBEDDED_IDS_IN_TEXT = True\n",
        "\n",
        "LINEAGE_SPOUSE_SEP = \" & \"\n",
        "LINEAGE_COUPLE_SEP = \" ~ \"\n",
        "\n",
        "ALPHA_BY_FIRST_ANCESTOR_FATHER = True\n",
        "ALPHA_TIEBREAK_MOTHER_SURNAME  = True\n",
        "\n",
        "# ---------- Enriched exclusion prefix (formatted lineage) ----------\n",
        "# This is the exact prefix to strip from the formatted lineage display.\n",
        "# It intentionally ends with a trailing \"~\" to remove the separator too.\n",
        "ENRICHED_EXCLUDE_PREFIX = (\n",
        "    \"John Yates (1430-) & Still Searching ~ \"\n",
        "    \"William Yates (1389-1440) & Still Searching ~ \"\n",
        "    \"William Yates (1420-) & Still Searching ~ \"\n",
        "    \"Edmund Yates (1445-1472) & Margaret Cornell ~ \"\n",
        "    \"Richard Yates (1440-1498) & Joan Ashendon (1445-1499) ~ \"\n",
        "    \"John Yates (1471-1544) & Alice Hyde (1498-1523) ~ \"\n",
        "    \"Thomas Yates (1509-1565) & Elizabeth Fauconer (-1562) ~\"\n",
        ")\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip().strip(\"/\")\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "OUTPUT_NAME = \"just-trees.shtml\"\n",
        "REMOTE_HTML = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "DNA_CSS_HREF     = \"/partials/dna_tree_styles.css\"\n",
        "DNA_CSS_VERSION  = \"v2025-11-23-g3\"\n",
        "UNIFIED_CSS_HREF = \"/partials/partials_unified.css\"\n",
        "UNIFIED_CSS_VER  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "\n",
        "HEAD_LINK = (\n",
        "    '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />\\n'\n",
        "    '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />'\n",
        ") % (DNA_CSS_HREF, DNA_CSS_VERSION, UNIFIED_CSS_HREF, UNIFIED_CSS_VER)\n",
        "\n",
        "TABLE_WIDTH_PX = 5550\n",
        "\n",
        "# ---------- Authority file (same as Cell 2) ----------\n",
        "AUTH_REMOTE_DIR   = \"partials\"\n",
        "AUTH_BASENAME     = \"first_ancestor_pairs.csv\"\n",
        "AUTH_LOCAL_CACHE  = \"first_ancestor_pairs.server.csv\"\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV: %s rows=%d, cols=%d\" % (INPUT_CSV, len(df), len(df.columns)))\n",
        "\n",
        "if \"haplogroup\" not in df.columns:\n",
        "    df[\"haplogroup\"] = \"\"\n",
        "else:\n",
        "    df[\"haplogroup\"] = df[\"haplogroup\"].fillna(\"\")\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX = 0\n",
        "B_IDX = 1\n",
        "C_IDX = 2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\", \" \")\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
        "    return t.lower()\n",
        "\n",
        "LOCAL_RESOLVER = \"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER = \"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_file_from_server(remote_dir, basename, local_out):\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", \"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try:\n",
        "                        ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            ftps.mkd(p)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        ftps.cwd(p)\n",
        "            try:\n",
        "                ftps.cwd(remote_dir)\n",
        "            except Exception:\n",
        "                pass\n",
        "            with open(local_out, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % basename, f.write)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Server pull failed for %s/%s: %s\" % (remote_dir, basename, e))\n",
        "        return False\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path))\n",
        "        return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    ok = _pull_file_from_server(\"partials\", \"match_to_unmasked.csv\", \"match_to_unmasked.csv\")\n",
        "    if ok:\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    return local_path\n",
        "\n",
        "LOCAL_RESOLVER = _pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _read_csv_anyenc(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    dfx = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            dfx = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            dfx = None\n",
        "    if dfx is None:\n",
        "        raise RuntimeError(\"Unable to read CSV %s: %s\" % (path, last))\n",
        "    return dfx\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "    last = None\n",
        "    m = None\n",
        "    for enc in (\"utf-8-sig\", \"iso-8859-15\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            m = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            m = None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last)\n",
        "        return {}\n",
        "    cols = {c.lower(): c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\")\n",
        "        return {}\n",
        "    m = m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"] = m[cols[\"code\"]].map(_norm_code)\n",
        "    m[\"__val__\"] = m[cols[\"unmasked\"]].astype(str)\n",
        "    m = m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map = _load_resolver_to_map(LOCAL_RESOLVER)\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw = df.iloc[:, B_IDX].astype(str)\n",
        "masked_key = masked_raw.map(_norm_code)\n",
        "resolved   = masked_key.map(resolver_map)\n",
        "df.iloc[:, C_IDX] = resolved.fillna(\"\")\n",
        "\n",
        "print(\n",
        "    \"[OK] Column B -> C mapping: %d / %d  unmatched: %d\"\n",
        "    % (int(resolved.notna().sum()), len(df), len(df) - int(resolved.notna().sum()))\n",
        ")\n",
        "\n",
        "# ---------- Lineage formatting helpers ----------\n",
        "ID_TOKEN_RE = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "COUPLE_SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|/{2,}|\\|{2,}|~{2,})\\s*\", re.I)\n",
        "SPOUSE_SPLIT_RE = re.compile(r\"\\s*(?:&| and | AND |\\+)\\s*\", re.I)\n",
        "\n",
        "def _scrub_side_keep_name_years(side_text):\n",
        "    s = str(side_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)~([^~]+?)~(\\d{4}\\s*-\\s*(?:\\d{4})?)$\", s, flags=re.I)\n",
        "    if m:\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", (m.group(2) or \"\").strip())\n",
        "        yrs = re.sub(r\"\\s+\", \"\", (m.group(3) or \"\").strip())\n",
        "        return (\"%s (%s)\" % (name, yrs)) if name and yrs else (name or \"\")\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)~([^~]+?)(?:~([^~]+?))?$\", s, flags=re.I)\n",
        "    if m:\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", (m.group(2) or \"\").strip())\n",
        "        tail = re.sub(r\"\\s{2,}\", \" \", (m.group(3) or \"\").strip())\n",
        "        if tail and re.search(r\"\\d{4}\", tail):\n",
        "            tail = re.sub(r\"\\s+\", \"\", tail)\n",
        "            return (\"%s (%s)\" % (name, tail)) if name else \"\"\n",
        "        if tail and name:\n",
        "            return (\"%s %s\" % (name, tail)).strip()\n",
        "        return (name or tail or \"\").strip()\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", s, flags=re.I)\n",
        "    if m:\n",
        "        rest = (m.group(2) or \"\").strip()\n",
        "        yrs = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs = re.sub(r\"\\s+\", \"\", m2.group(1))\n",
        "            rest = rest[:m2.start()].strip()\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", rest).strip()\n",
        "        return (\"%s (%s)\" % (name, yrs)) if name and yrs else (name or \"\")\n",
        "\n",
        "    s = ID_TOKEN_RE.sub(\"\", s).replace(\"~\", \" \")\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------- Enriched exclusion (drop very old lead-in couples) ----------\n",
        "# Goal: keep the displayed lineage focused starting at:\n",
        "#   Francis Yates (1541-1588) & Jane Tichborne (1548-1580)\n",
        "#\n",
        "# We support two compatible mechanisms:\n",
        "#   (A) Exact prefix removal using the explicit ENRICHED_EXCLUDE_PREFIX couples.\n",
        "#   (B) Anchor-based trimming: if the anchor couple is present, drop everything before it.\n",
        "\n",
        "def _norm_couple_for_match(s: str) -> str:\n",
        "    # lower + collapse whitespace; keep punctuation so we can match the authority text precisely\n",
        "    return re.sub(r\"\\s{2,}\", \" \", str(s or \"\")).strip().lower()\n",
        "\n",
        "# Build couple list from the explicit prefix string (safe if user updates prefix later)\n",
        "_EXCLUDE_COUPLES = [\n",
        "    c.strip()\n",
        "    for c in re.split(r\"\\s*~\\s*\", (ENRICHED_EXCLUDE_PREFIX or \"\").strip().strip(\"~\"))\n",
        "    if c and c.strip()\n",
        "]\n",
        "\n",
        "def _strip_paren_years_anywhere(s: str) -> str:\n",
        "    return re.sub(r\"\\([^)]*\\)\", \"\", str(s or \"\")).strip()\n",
        "\n",
        "def _is_anchor_couple(couple_text: str) -> bool:\n",
        "    t = _strip_paren_years_anywhere(couple_text).lower()\n",
        "    return (\"francis yates\" in t) and (\"jane tichborne\" in t)\n",
        "\n",
        "def _apply_enriched_exclusion(joined: str) -> str:\n",
        "    s = str(joined or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "\n",
        "    couples = [c.strip() for c in s.split(LINEAGE_COUPLE_SEP) if c and c.strip()]\n",
        "    if not couples:\n",
        "        return s\n",
        "\n",
        "    # (A) Exact prefix removal by couple list match (robust to spacing)\n",
        "    if _EXCLUDE_COUPLES and len(couples) >= len(_EXCLUDE_COUPLES):\n",
        "        ok = True\n",
        "        for i in range(len(_EXCLUDE_COUPLES)):\n",
        "            if _norm_couple_for_match(couples[i]) != _norm_couple_for_match(_EXCLUDE_COUPLES[i]):\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            couples = couples[len(_EXCLUDE_COUPLES):]\n",
        "\n",
        "    # (B) Anchor trim if prefix did not match (or if data varies)\n",
        "    if couples:\n",
        "        for i, c in enumerate(couples):\n",
        "            if _is_anchor_couple(c):\n",
        "                couples = couples[i:]\n",
        "                break\n",
        "\n",
        "    return LINEAGE_COUPLE_SEP.join([c for c in couples if c]).strip()\n",
        "\n",
        "def _format_lineage_cell(text):\n",
        "    s = str(text or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "    couples = [t.strip() for t in COUPLE_SEP_RE.split(s) if t and t.strip()]\n",
        "    if not couples:\n",
        "        couples = [s]\n",
        "    out_couples = []\n",
        "    for c in couples:\n",
        "        parts = [p.strip() for p in SPOUSE_SPLIT_RE.split(c, maxsplit=1) if p and p.strip()]\n",
        "        if len(parts) == 2:\n",
        "            father = _scrub_side_keep_name_years(parts[0]).strip()\n",
        "            mother = _scrub_side_keep_name_years(parts[1]).strip()\n",
        "            couple = (father + LINEAGE_SPOUSE_SEP + mother).strip()\n",
        "        else:\n",
        "            couple = _scrub_side_keep_name_years(c).strip()\n",
        "        couple = re.sub(r\"\\s{2,}\", \" \", couple).strip()\n",
        "        out_couples.append(couple)\n",
        "    joined = (LINEAGE_COUPLE_SEP.join([c for c in out_couples if c])).strip()\n",
        "    joined = re.sub(r\"\\s{2,}\", \" \", joined).strip()\n",
        "\n",
        "    # Apply enriched early-ancestor truncation (Frances Yates & Jane Tichborne focus)\n",
        "    joined = _apply_enriched_exclusion(joined)\n",
        "\n",
        "    return joined\n",
        "\n",
        "\n",
        "def _maybe_format_lineage_columns(df_in):\n",
        "    if not SUPPRESS_EMBEDDED_IDS_IN_TEXT:\n",
        "        return df_in\n",
        "    df_out = df_in.copy()\n",
        "    pat = re.compile(r\"(ancestral|lineage|tree|path|ancestor|line)\", re.I)\n",
        "    cols = [c for c in df_out.columns if pat.search(str(c or \"\")) and str(c).strip().lower() != 'first ancestor']\n",
        "    if not cols:\n",
        "        for c in df_out.columns:\n",
        "            try:\n",
        "                ser = df_out[c].astype(str)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if ser.str.contains(r\"\\bI\\d+~\", regex=True, na=False).any() or ser.str.contains(r\"\\bI\\d+\\b\", regex=True, na=False).any():\n",
        "                if str(c).strip().lower() != 'first ancestor':\n",
        "                    cols.append(c)\n",
        "    cols = list(dict.fromkeys(cols))\n",
        "    if not cols:\n",
        "        return df_out\n",
        "    for c in cols:\n",
        "        try:\n",
        "            df_out[c] = df_out[c].astype(str).map(_format_lineage_cell)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"[OK] Lineage formatting applied to columns:\", \", \".join([str(c) for c in cols]))\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def _strip_years(name_text):\n",
        "    s = str(name_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s*\\([^)]*\\)\\s*$\", \"\", s).strip()\n",
        "\n",
        "def _first_last_tokens(person_text):\n",
        "    s = _strip_years(person_text)\n",
        "    parts = s.split()\n",
        "    if not parts:\n",
        "        return (\"\", \"\")\n",
        "    return (parts[0], parts[-1])\n",
        "\n",
        "def _first_ancestor_sort_key(lineage_text):\n",
        "    s = str(lineage_text or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\")\n",
        "    first_couple = s.split(LINEAGE_COUPLE_SEP, 1)[0].strip()\n",
        "    father = \"\"\n",
        "    mother = \"\"\n",
        "    if LINEAGE_SPOUSE_SEP in first_couple:\n",
        "        father, mother = [p.strip() for p in first_couple.split(LINEAGE_SPOUSE_SEP, 1)]\n",
        "    else:\n",
        "        father = first_couple.strip()\n",
        "    f_given, f_surname = _first_last_tokens(father)\n",
        "    m_surname = \"\"\n",
        "    if ALPHA_TIEBREAK_MOTHER_SURNAME:\n",
        "        _m_given, m_surname = _first_last_tokens(mother)\n",
        "    return (f_surname.lower(), f_given.lower(), m_surname.lower())\n",
        "\n",
        "# ---------- Authority load + first-ancestor value ----------\n",
        "def _find_col(df0, patterns, prefer_exact=None):\n",
        "    cols = list(df0.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df0.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def load_authority_map() -> dict:\n",
        "    local_path = AUTH_LOCAL_CACHE if os.path.exists(AUTH_LOCAL_CACHE) else AUTH_BASENAME\n",
        "    if not os.path.exists(local_path):\n",
        "        print(\"Authority not found locally; attempting server pull ...\")\n",
        "        ok = _pull_file_from_server(\"partials\", AUTH_BASENAME, AUTH_LOCAL_CACHE)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"Authority file missing: expected /partials/%s\" % AUTH_BASENAME)\n",
        "        local_path = AUTH_LOCAL_CACHE\n",
        "        print(\"[OK] Pulled authority from server -> %s\" % local_path)\n",
        "    else:\n",
        "        print(\"Using authority:\", os.path.abspath(local_path))\n",
        "\n",
        "    adf = _read_csv_anyenc(local_path)\n",
        "    a1_col  = _find_col(adf, [r\"ancestor1.*_id$\", r\"firstpair_ancestor1_id$\"], [\"FirstPair_Ancestor1_ID\"])\n",
        "    a2_col  = _find_col(adf, [r\"ancestor2.*_id$\", r\"firstpair_ancestor2_id$\"], [\"FirstPair_Ancestor2_ID\"])\n",
        "    key_col = _find_col(adf, [r\"firstpair_lastfirst$\"], [\"FirstPair_LastFirst\"])\n",
        "    if not (a1_col and a2_col and key_col):\n",
        "        raise RuntimeError(\"Authority CSV missing required columns. Need Ancestor1_ID, Ancestor2_ID, FirstPair_LastFirst.\")\n",
        "    m = {}\n",
        "    for _, r in adf.iterrows():\n",
        "        a1 = str(r.get(a1_col, \"\")).strip()\n",
        "        a2 = str(r.get(a2_col, \"\")).strip()\n",
        "        k  = re.sub(r\"\\s+\", \"\", str(r.get(key_col, \"\")))\n",
        "        if not (a1 and a2 and k):\n",
        "            continue\n",
        "        m[(a1.upper(), a2.upper())] = k\n",
        "        m[(a2.upper(), a1.upper())] = k\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Authority map built empty from %s\" % local_path)\n",
        "    print(\"[OK] Authority map ready: %d pairs\" % (len(m)//2))\n",
        "    return m\n",
        "\n",
        "AUTH_COUPLE_KEY_MAP = load_authority_map()\n",
        "\n",
        "def _canon_side(txt):\n",
        "    t = re.sub(r\"\\([^)]*\\)\", \"\", str(txt or \"\"))\n",
        "    t = re.sub(r\"[^A-Za-z0-9]+\", \"\", t).lower()\n",
        "    return t\n",
        "\n",
        "\n",
        "def _is_unknown_like(name_text):\n",
        "    s = str(name_text or \"\").strip().lower()\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", s).strip()\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    if not s:\n",
        "        return True\n",
        "    if s in (\"unknown\", \"none\", \"noneunknownname\"):\n",
        "        return True\n",
        "    if \"unknown name\" in s:\n",
        "        return True\n",
        "    if s.replace(\" \", \"\") in (\"noneunknownname\", \"unknownname\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _canon_lastfirst(name_text):\n",
        "    # Canonicalize a person name into a LastFirst key (no punctuation),\n",
        "    # matching the authority file convention (FirstPair_LastFirst).\n",
        "    if _is_unknown_like(name_text):\n",
        "        return \"unknown\"\n",
        "    s = str(name_text or \"\").strip()\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", s).strip()\n",
        "    toks = re.findall(r\"[A-Za-z0-9]+\", s.lower())\n",
        "    if not toks:\n",
        "        return \"\"\n",
        "    if len(toks) == 1:\n",
        "        return re.sub(r\"[^a-z0-9]+\", \"\", toks[0])\n",
        "    last = toks[-1]\n",
        "    first = \"\".join(toks[:-1])\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"\", last + first)\n",
        "\n",
        "def _extract_first_couple_ids(raw_lineage_text: str):\n",
        "    s = str(raw_lineage_text or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\", \"\")\n",
        "    couples = [t.strip() for t in COUPLE_SEP_RE.split(s) if t and t.strip()]\n",
        "    first = couples[0] if couples else s\n",
        "    parts = [p.strip() for p in SPOUSE_SPLIT_RE.split(first, maxsplit=1) if p and p.strip()]\n",
        "    father_raw = parts[0] if len(parts) >= 1 else \"\"\n",
        "    mother_raw = parts[1] if len(parts) >= 2 else \"\"\n",
        "    f_id = \"\"\n",
        "    m_id = \"\"\n",
        "    mf = re.search(r\"\\b(I\\d+)\\b\", father_raw, flags=re.I)\n",
        "    if mf:\n",
        "        f_id = mf.group(1).upper()\n",
        "    mm = re.search(r\"\\b(I\\d+)\\b\", mother_raw, flags=re.I)\n",
        "    if mm:\n",
        "        m_id = mm.group(1).upper()\n",
        "    f_disp = _scrub_side_keep_name_years(father_raw).strip()\n",
        "    m_disp = _scrub_side_keep_name_years(mother_raw).strip()\n",
        "    return (f_id, m_id, f_disp, m_disp)\n",
        "\n",
        "def _first_ancestor_authority_value(raw_lineage_text: str) -> str:\n",
        "    f_id, m_id, f_disp, m_disp = _extract_first_couple_ids(raw_lineage_text)\n",
        "\n",
        "    # Primary: authority lookup by IDs\n",
        "    if f_id and m_id:\n",
        "        k = AUTH_COUPLE_KEY_MAP.get((f_id, m_id), \"\")\n",
        "        if k:\n",
        "            return re.sub(r\"\\s+\", \"\", k)\n",
        "\n",
        "    # Fallback: LastFirst canonical key, synchronized with authority convention\n",
        "    if f_disp and m_disp:\n",
        "        return _canon_lastfirst(f_disp) + \"&\" + _canon_lastfirst(m_disp)\n",
        "    if f_disp:\n",
        "        return _canon_lastfirst(f_disp)\n",
        "    return \"\"\n",
        "\n",
        "# ---------- Vitals ----------\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "LAST_UPDATED_TEXT  = \"\"\n",
        "AUTOSOMAL_MATCHES  = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    s = str(raw or \"\").strip()\n",
        "    if not s:\n",
        "        return \"(unknown)\"\n",
        "    s = s.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M\", \"%Y-%m-%dT%H:%M:%S\"]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(s, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24  = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12  = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, dt_est.day, dt_est.year, h12, dt_est.minute, ampm)\n",
        "\n",
        "def _format_num_with_commas(raw_val):\n",
        "    s_digits = re.sub(r\"[^0-9\\-]\", \"\", str(raw_val or \"\"))\n",
        "    if not s_digits:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return \"{:,}\".format(int(s_digits))\n",
        "    except Exception:\n",
        "        return s_digits\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global LAST_UPDATED_TEXT, AUTOSOMAL_MATCHES\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will be blank for vitals.\")\n",
        "        return\n",
        "    vdf = None\n",
        "    for enc in (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "            break\n",
        "        except Exception:\n",
        "            vdf = None\n",
        "    if vdf is None:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv\")\n",
        "        return\n",
        "    flat = [str(cell) for row in vdf.astype(str).values.tolist() for cell in row]\n",
        "    autosomal_raw = None\n",
        "    last_text = None\n",
        "    for cell in flat:\n",
        "        if autosomal_raw is None and \"Records tagged and filtered by NPFX\" in cell:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", cell)\n",
        "            if m:\n",
        "                autosomal_raw = m.group(1)\n",
        "        if last_text is None and \"LAST_UPDATED_TEXT\" in cell:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", cell)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "    if last_text is not None:\n",
        "        LAST_UPDATED_TEXT = _friendly_ts_from_utc(last_text)\n",
        "    AUTOSOMAL_MATCHES = _format_num_with_commas(autosomal_raw)\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "updated_str = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT or \"\")\n",
        "_updated_parts = [updated_str]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "UPDATED_BLOCK = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join(_updated_parts) + '</div>'\n",
        "\n",
        "NAV_BLOCK = '<div id=\"nav-slot\"><!--#include virtual=\"/partials/nav_block.shtml\" --></div>'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ---------- Display DF ----------\n",
        "display_df = df.copy()\n",
        "\n",
        "# drop ID#\n",
        "if SUPPRESS_ID_COLUMN and display_df.shape[1] >= 1:\n",
        "    display_df = display_df.drop(columns=[display_df.columns[A_IDX]], errors=\"ignore\")\n",
        "\n",
        "# Identify lineage/path column BEFORE formatting (so IDs still exist for authority lookup)\n",
        "lineage_cols_raw = [c for c in display_df.columns if re.search(r\"(ancestral|lineage|tree|path|ancestor|line)\", str(c or \"\"), re.I)]\n",
        "AUTH_LINEAGE_COL = lineage_cols_raw[0] if lineage_cols_raw else None\n",
        "\n",
        "# Compute authority first-ancestor values from the raw lineage column\n",
        "if AUTH_LINEAGE_COL:\n",
        "    fa_values = display_df[AUTH_LINEAGE_COL].astype(str).map(_first_ancestor_authority_value)\n",
        "else:\n",
        "    fa_values = pd.Series([\"\"] * len(display_df))\n",
        "\n",
        "# REPLACE COLUMN 1 with First Ancestor values and rename header\n",
        "if display_df.shape[1] >= 1:\n",
        "    first_col_name = display_df.columns[0]\n",
        "    display_df[first_col_name] = fa_values.astype(str).map(lambda x: re.sub(r\"\\s+\", \"\", x).replace('&', '&#38;'))\n",
        "    display_df = display_df.rename(columns={first_col_name: \"First Ancestor\"})\n",
        "    print(\"[OK] Column 1 replaced with First Ancestor (authority).\")\n",
        "else:\n",
        "    print(\"[WARN] display_df has no columns to replace.\")\n",
        "\n",
        "# Remove any other \"First Ancestor\" columns to avoid duplication (keep the first one)\n",
        "fa_cols = [c for c in display_df.columns if str(c) == \"First Ancestor\"]\n",
        "if len(fa_cols) > 1:\n",
        "    keep_first = fa_cols[0]\n",
        "    drop_rest = fa_cols[1:]\n",
        "    display_df = display_df.drop(columns=drop_rest, errors=\"ignore\")\n",
        "    print(\"[OK] Dropped duplicate First Ancestor columns:\", \", \".join(drop_rest))\n",
        "\n",
        "# Now apply lineage formatting (removes embedded IDs AND applies enriched exclusion)\n",
        "display_df = _maybe_format_lineage_columns(display_df)\n",
        "\n",
        "# Alpha sort by first couple (kept)\n",
        "if ALPHA_BY_FIRST_ANCESTOR_FATHER:\n",
        "    lineage_cols = [c for c in display_df.columns if re.search(r\"(ancestral|lineage|tree|path|ancestor|line)\", str(c or \"\"), re.I)]\n",
        "    sort_col = lineage_cols[0] if lineage_cols else None\n",
        "    if sort_col:\n",
        "        sort_keys = display_df[sort_col].astype(str).map(_first_ancestor_sort_key)\n",
        "        display_df[\"__sort_surname__\"]    = [k[0] for k in sort_keys]\n",
        "        display_df[\"__sort_given__\"]      = [k[1] for k in sort_keys]\n",
        "        display_df[\"__sort_momsurname__\"] = [k[2] for k in sort_keys]\n",
        "\n",
        "        by_cols = [\"__sort_surname__\", \"__sort_given__\"]\n",
        "        if ALPHA_TIEBREAK_MOTHER_SURNAME:\n",
        "            by_cols.append(\"__sort_momsurname__\")\n",
        "\n",
        "        display_df = display_df.sort_values(by=by_cols, ascending=[True]*len(by_cols), kind=\"mergesort\").reset_index(drop=True)\n",
        "        display_df = display_df.drop(columns=[\"__sort_surname__\", \"__sort_given__\", \"__sort_momsurname__\"], errors=\"ignore\")\n",
        "        print(\"[OK] Alpha sort applied by first couple:\", sort_col, \"| keys=\", \",\".join(by_cols))\n",
        "\n",
        "# ---------- HTML table ----------\n",
        "visible_cols = [c for c in display_df.columns if c]\n",
        "\n",
        "table_html = display_df.to_html(\n",
        "    index=False,\n",
        "    columns=visible_cols,\n",
        "    escape=False,\n",
        "    border=1,\n",
        "    classes=\"dataframe sortable\"\n",
        ")\n",
        "\n",
        "if 'id=\"refactor-table\"' not in table_html:\n",
        "    table_html = re.sub(r\"<table([^>]*)>\", r'<table\\1 id=\"refactor-table\">', table_html, count=1)\n",
        "\n",
        "if 'class=\"dataframe sortable\"' not in table_html and \"sortable\" not in table_html:\n",
        "    table_html = table_html.replace('class=\"dataframe\"', 'class=\"dataframe sortable\"', 1)\n",
        "\n",
        "table_html = table_html.replace(\"<tbody>\\n<tr>\", \"<tbody>\\n<tr id=\\\"first-row\\\">\", 1)\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div id=\"top-scroll\" class=\"scroll-sync-top\">'\n",
        "    '<div class=\"scroll-sync-top-inner\" style=\"width:%dpx;\"></div>'\n",
        "    '</div>'\n",
        "    '<div id=\"bottom-scroll\" class=\"table-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (TABLE_WIDTH_PX, table_html)\n",
        "\n",
        "LATE_STYLE = r\"\"\"\n",
        "<style type=\"text/css\">\n",
        "#nav-slot, #nav-slot nav, #nav-slot .oldnav, #nav-slot .navbar{\n",
        "  display:block !important;\n",
        "  visibility:visible !important;\n",
        "  opacity:1 !important;\n",
        "}\n",
        "table.sortable thead{ display:table-header-group !important; visibility:visible !important; }\n",
        "table.sortable thead th{ display:table-cell !important; visibility:visible !important; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "JS_NAV_REPAIR = r\"\"\"\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function hasNavContainer(el){\n",
        "    if(!el) return false;\n",
        "    var n = el.querySelector('nav.oldnav, nav.navbar, .oldnav, .navbar');\n",
        "    return !!n;\n",
        "  }\n",
        "  function wrapFirstUL(el){\n",
        "    if(!el) return false;\n",
        "    var ul = el.querySelector('ul');\n",
        "    if(!ul) return false;\n",
        "    var nav = document.createElement('nav');\n",
        "    nav.className = 'oldnav';\n",
        "    nav.appendChild(ul);\n",
        "    while(el.firstChild){ el.removeChild(el.firstChild); }\n",
        "    el.appendChild(nav);\n",
        "    return true;\n",
        "  }\n",
        "  function looksLikeSSICommentOnly(el){\n",
        "    if(!el) return true;\n",
        "    var txt = (el.textContent || '').replace(/\\s+/g,'').toLowerCase();\n",
        "    if(!txt) return true;\n",
        "    if(txt.indexOf('<!--#include') >= 0) return true;\n",
        "    return false;\n",
        "  }\n",
        "  function injectRemoteNav(el){\n",
        "    try{\n",
        "      var xhr = new XMLHttpRequest();\n",
        "      xhr.open('GET', '/partials/nav_block.shtml', true);\n",
        "      xhr.onreadystatechange = function(){\n",
        "        if(xhr.readyState === 4){\n",
        "          if(xhr.status >= 200 && xhr.status < 300){\n",
        "            el.innerHTML = xhr.responseText;\n",
        "            if(!hasNavContainer(el)){\n",
        "              wrapFirstUL(el);\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      };\n",
        "      xhr.send(null);\n",
        "    }catch(e){}\n",
        "  }\n",
        "\n",
        "  function repairNav(){\n",
        "    var slot = document.getElementById('nav-slot');\n",
        "    if(!slot) return;\n",
        "\n",
        "    if(looksLikeSSICommentOnly(slot)){\n",
        "      injectRemoteNav(slot);\n",
        "      return;\n",
        "    }\n",
        "    if(!hasNavContainer(slot)){\n",
        "      wrapFirstUL(slot);\n",
        "      return;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if(document.readyState === 'loading'){\n",
        "    document.addEventListener('DOMContentLoaded', repairNav, false);\n",
        "  } else {\n",
        "    repairNav();\n",
        "  }\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "page_tpl = _T(r\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "$LATE_STYLE\n",
        "<style type=\"text/css\">\n",
        "/* Sticky second column (index 2) for Trees table */\n",
        "#refactor-table th:nth-child(2),\n",
        "#refactor-table td:nth-child(2){\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "#refactor-table th:nth-child(2){\n",
        "  z-index:7;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '').replace(/\\s+/g,' ').trim().toLowerCase();\n",
        "  }\n",
        "  function sortTable(tbl, colIndex, dir, keyColIndex){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "    var kIdx = (typeof keyColIndex === 'number') ? keyColIndex : colIndex;\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[kIdx]), B = textOf(b.cells[kIdx]);\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\-]/g,''));\n",
        "      if(!isNaN(nA) && !isNaN(nB)){ return asc ? (nA-nB) : (nB-nA); }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dir = 'asc';\n",
        "      th.addEventListener('click', function(){\n",
        "        dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "        var hdr = (th.textContent || th.innerText || '');\n",
        "        hdr = hdr.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'').replace(/\\s+/g,' ').trim().toLowerCase();\n",
        "        // Golden rule sync: sorting the lineage column uses First Ancestor (authority) as the key.\n",
        "        // Column 1 is 'First Ancestor' (index 0) in this build.\n",
        "        var keyColIndex = null;\n",
        "        if(hdr === 'yates dna ancestral line'){\n",
        "          keyColIndex = 0;\n",
        "        }\n",
        "\n",
        "        for (var j = 0; j < ths.length; j++){\n",
        "          ths[j].innerHTML = ths[j].innerHTML.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'');\n",
        "        }\n",
        "        th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl, idx, dir, (keyColIndex === null ? undefined : keyColIndex));\n",
        "      }, false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\-]/g,''),10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){ return String(n||''); }\n",
        "  }\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\+/g,' ')) : '';\n",
        "  }\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb = tbl.tBodies[0];\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200 ? 'block' : 'none'); }\n",
        "    toggle();\n",
        "    window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "  function bindSyncedScrollbars(){\n",
        "    var topScroll    = document.getElementById('top-scroll');\n",
        "    var bottomScroll = document.getElementById('bottom-scroll');\n",
        "    if(!(topScroll && bottomScroll)) return;\n",
        "    var syncing = false;\n",
        "    topScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      bottomScroll.scrollLeft = topScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "    bottomScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      topScroll.scrollLeft = bottomScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindBackToTop();\n",
        "    bindSearch();\n",
        "    bindSyncedScrollbars();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "$JS_NAV_REPAIR\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    LATE_STYLE=LATE_STYLE,\n",
        "    JS_NAV_REPAIR=JS_NAV_REPAIR,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER=SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df = display_df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w:\n",
        "        export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for seg in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host = os.environ.get(\"FTP_HOST\")\n",
        "ftp_user = os.environ.get(\"FTP_USER\")\n",
        "ftp_pass = os.environ.get(\"FTP_PASS\")\n",
        "ftp_port = int(os.environ.get(\"FTP_PORT\", \"21\") or \"21\")\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            with open(OUTPUT_NAME, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            with open(LOCAL_CSV, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/first_ancestor_pairs.csv; enriched prefix exclusion applied) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8A8Jk5T6vXq_",
        "outputId": "47d4649b-620b-4106-b3ba-e10fe3823531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=93, cols=7\n",
            "Resolver not found locally; attempting server pull ...\n",
            "[OK] Pulled resolver from server -> match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 93 / 93  unmatched: 0\n",
            "Using authority: /content/first_ancestor_pairs.csv\n",
            "[OK] Authority map ready: 25 pairs\n",
            "[OK] Column 1 replaced with First Ancestor (authority).\n",
            "[OK] Lineage formatting applied to columns: Yates DNA Ancestral Line, Authority_FirstAncestor\n",
            "[OK] Alpha sort applied by first couple: First Ancestor | keys= __sort_surname__,__sort_given__,__sort_momsurname__\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/first_ancestor_pairs.csv; enriched prefix exclusion applied) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OLD Above"
      ],
      "metadata": {
        "id": "Y6yI0d6qESg-"
      }
    }
  ]
}