{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX4EJ39MeQrQ1P06nZbIQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/Ztest__01_30_1711_2025_HTML%2BValue_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "a00ffc34-3f42-4f58-ea45-bfaa91474c99",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: python-gedcom in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.11/dist-packages (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ztest_01_28_2033(works!)_2025_stable\n",
        "\n",
        "# Standard Libraries\n",
        "import csv\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# GEDCOM Parsing\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Alignment\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts first and last name from a GEDCOM record.\n",
        "    Handles missing or malformed names gracefully.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "\n",
        "    if name_start == 5 or name_end == -1:  # Meaning '1 NAME ' was not found\n",
        "        return \"UnknownName\"\n",
        "\n",
        "    name = record[name_start:name_end].strip()\n",
        "\n",
        "    # Handle cases where no '/' is present in the name\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")  # Take first 10 characters as default name\n",
        "\n",
        "    # Extract first and last name\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]  # first 10 chars\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}  # Global dictionary to hold name->ID mapping\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # weâ€™ll modify name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0  # Count YDNA occurrences\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1  # YDNA found\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "        print(f'Records with YDNA information: {ydna_count}')\n",
        "        print(f'Autosomal matches: {autosomal_count}')\n",
        "\n",
        "        # First-level filter: only those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Optional second-level filter from an Excel file\n",
        "        manual_filter_activated = True  # or False\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [\n",
        "                    dataset for dataset in self.filter_pool\n",
        "                    if dataset.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    # Just automatically return the first GEDCOM file found\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    # If you want a manual selection, uncomment the below while loop:\n",
        "    #\n",
        "    # while True:\n",
        "    #     for i, file in enumerate(gedcom_files, start=1):\n",
        "    #         print(f\"{i}. {file}\")\n",
        "    #     try:\n",
        "    #         selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "    #         if 1 <= selected_num <= len(gedcom_files):\n",
        "    #             return gedcom_files[selected_num - 1]\n",
        "    #         else:\n",
        "    #             print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "    #     except ValueError:\n",
        "    #         print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "################################################################################\n",
        "#          Execute GEDCOM Parsing & Build Our Filter Pool                      #\n",
        "################################################################################\n",
        "gedcom_file_path = select_gedcom_file()\n",
        "if gedcom_file_path:\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    # Gather individuals (last_name, individual_id) from the filter pool\n",
        "    individuals = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    ################################################################################\n",
        "    # Function: Extract ID from GEDCOM Record\n",
        "    ################################################################################\n",
        "    def extract_id(record):\n",
        "        \"\"\"\n",
        "        Extracts the ID from a GEDCOM record.\n",
        "        A valid ID is enclosed within '@' symbols.\n",
        "        \"\"\"\n",
        "        id_start = record.find('@') + 1\n",
        "        id_end = record.find('@', id_start)\n",
        "\n",
        "        if id_start == 0 or id_end == -1:  # If '@' is missing\n",
        "            return \"UnknownID\"\n",
        "\n",
        "        return record[id_start:id_end].strip()\n",
        "\n",
        "    # Read the GEDCOM file as raw text, parse out records\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "\n",
        "    data = data.split('\\n0 ')  # Split records based on GEDCOM structure\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "else:\n",
        "    print(\"No GEDCOM file selected; exiting.\")\n",
        "    raise SystemExit\n",
        "\n",
        "################################################################################\n",
        "#        Functions to Traverse & Score Ancestors, Build Data for DataFrame     #\n",
        "################################################################################\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # We'll update anchor_gen1 if found\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    # Build generation_table, visited_pairs\n",
        "    find_parents(individual_id, 1, records)\n",
        "\n",
        "    # All possible ancestor paths for that ID\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "\n",
        "    filtered_ancestral_line_names = []\n",
        "\n",
        "    # Gather more info from the dataset\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "\n",
        "    # Build ancestral line (exclude anchor_gen1 itself)\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    # Reverse order\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "\n",
        "    # Check we did not accidentally include anchor_gen1\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(\n",
        "            f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\"\n",
        "        )\n",
        "\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "#         Build Rows for DataFrame from the Filter Pool                        #\n",
        "################################################################################\n",
        "combined_df_rows = []\n",
        "for dataset in gedcom_instance.filter_pool:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(\n",
        "        individual_id, gedcom_instance, records\n",
        "    )\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    individual_name = extract_name(records.get(individual_id, \"\"))\n",
        "\n",
        "    combined_df_rows.append(\n",
        "        [individual_id, sort, individual_name, cm, filtered_ancestral_line_str]\n",
        "    )\n",
        "\n",
        "################################################################################\n",
        "#       NO NEED TO MODIFY ABOVE THIS SECTION 28-1-2025                         #\n",
        "################################################################################\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "################################################################################\n",
        "#       Create and Populate Main DataFrame (combined_df)\n",
        "################################################################################\n",
        "columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "# Initialize the value_store dictionary\n",
        "value_store = {}\n",
        "\n",
        "# Populate value_store with data from the DataFrame, including a placeholder for 'Value'\n",
        "for _, row in combined_df.iterrows():\n",
        "    value_store[row[\"ID#\"]] = {\n",
        "        \"Match to\": row[\"Match to\"],\n",
        "        \"Name\": row[\"Name\"],\n",
        "        \"cM\": row[\"cM\"],\n",
        "        \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "        \"Value\": None  # Placeholder for 'Value'\n",
        "    }\n",
        "\n",
        "################################################################################\n",
        "#       Remove miscellaneous Distant ancestors (combined_df)\n",
        "################################################################################\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "    prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "\n",
        "# Order and clean up columns\n",
        "ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = combined_df[ordered_columns]\n",
        "combined_df.index += 1\n",
        "combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "################################################################################\n",
        "#       Segmentation and Frequency Analysis\n",
        "################################################################################\n",
        "def parse_line_to_pairs(line, delimiter=\"~~~\"):\n",
        "    \"\"\"Splits a given ancestral line into named pairs based on the delimiter.\"\"\"\n",
        "    return line.strip().split(delimiter)\n",
        "\n",
        "def identify_all_shared_segments(df, ancestral_col, min_shared=2, min_size=2):\n",
        "    \"\"\"\n",
        "    Identify all possible shared segments (groups of min_size ancestral lines) across the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame containing the dataset.\n",
        "    - ancestral_col (str): The column name containing ancestral lines.\n",
        "    - min_shared (int): Minimum number of records a segment must appear in to be considered shared.\n",
        "    - min_size (int): Minimum number of ancestral lines in a segment.\n",
        "\n",
        "    Returns:\n",
        "    - shared_segments_sorted (dict): Dictionary with shared segment tuples as keys and their frequencies as values.\n",
        "    \"\"\"\n",
        "    segment_counts = defaultdict(int)\n",
        "\n",
        "    # Iterate over each record\n",
        "    for idx, row in df.iterrows():\n",
        "        # Ensure the ancestral_col exists and is not NaN\n",
        "        if ancestral_col not in row or pd.isna(row[ancestral_col]):\n",
        "            continue  # Skip if the column is missing or NaN\n",
        "\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "\n",
        "        # Generate all possible combinations of lines with size >= min_size\n",
        "        for size in range(min_size, len(lines)+1):\n",
        "            for subset in combinations(sorted(lines), size):\n",
        "                segment_counts[subset] += 1\n",
        "\n",
        "    # Filter segments that meet the minimum sharing threshold\n",
        "    shared_segments = {segment: count for segment, count in segment_counts.items() if count >= min_shared}\n",
        "\n",
        "    # Sort segments by size descending to prioritize larger segments\n",
        "    shared_segments_sorted = dict(sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "    return shared_segments_sorted\n",
        "\n",
        "def calculate_value_multiple_segments_refined(df, ancestral_col, shared_segments):\n",
        "    \"\"\"\n",
        "    Calculate the 'Revised_Value' metric for each record based on multiple shared segments.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame containing the dataset.\n",
        "    - ancestral_col (str): The column name containing ancestral lines.\n",
        "    - shared_segments (dict): Dictionary of shared segments and their frequencies.\n",
        "\n",
        "    Returns:\n",
        "    - df (pd.DataFrame): DataFrame with an updated 'Revised_Value' column.\n",
        "    \"\"\"\n",
        "    # Sort shared segments by size descending to prioritize larger segments\n",
        "    sorted_segments = sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "    # Initialize/Reinitialize Revised_Value column\n",
        "    df['Revised_Value'] = 0\n",
        "\n",
        "    # Iterate over each record\n",
        "    for idx, row in df.iterrows():\n",
        "        # Ensure the ancestral_col exists and is not NaN\n",
        "        if ancestral_col not in row or pd.isna(row[ancestral_col]):\n",
        "            df.at[idx, 'Revised_Value'] = 0\n",
        "            continue  # Skip if the column is missing or NaN\n",
        "\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "        value = 0\n",
        "        lines_copy = lines.copy()\n",
        "\n",
        "        for segment, freq in sorted_segments:\n",
        "            segment_list = list(segment)\n",
        "            # Check if all lines in the segment are present in the record\n",
        "            if all(line in lines_copy for line in segment_list):\n",
        "                # Add to value: frequency\n",
        "                value += freq\n",
        "                # Remove these lines to prevent double-counting\n",
        "                for line in segment_list:\n",
        "                    lines_copy.remove(line)\n",
        "\n",
        "        # Add 1 for each remaining unique ancestral line\n",
        "        value += len(lines_copy)\n",
        "\n",
        "        # Assign the calculated value\n",
        "        df.at[idx, 'Revised_Value'] = value\n",
        "\n",
        "    return df\n",
        "\n",
        "################################################################################\n",
        "#       Main Processing\n",
        "################################################################################\n",
        "# Example: Replace this with your actual DataFrame loading method\n",
        "# For instance, if you're reading from a CSV:\n",
        "# combined_df = pd.read_csv(\"your_data.csv\")\n",
        "\n",
        "# For demonstration, let's create a sample DataFrame similar to your structure\n",
        "data = {\n",
        "    'ID#': ['I52816', 'I47130', 'I52836', 'I52807', 'I52810', 'I52559', 'I52802', 'I52798'],\n",
        "    'Surname': ['leedon', 'leedon', 'leedon', 'leedon', 'leedon', 'yates,timothyj', 'leedon', 'leedon'],\n",
        "    'Person': ['DucksonPam', 'CrabtreeChadEdmun', 'RileySandra', 'RileyRonldPaul', 'LewallenDonaldChr', 'LeeDonnaStel', 'HoukW', 'HoukJeffrey'],\n",
        "    'Number': [116, 25, 63, 173, 154, 12, 369, 448],\n",
        "    'Value': [3, 3, 3, 3, 3, 1, 2, 2],\n",
        "    'Yates DNA Ancestral Line': [\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~LeeAlbertEls&LauderbackGoldieIre~~~WheelerRobertJef&YatesRosaAlafa~~~RileyAlveEdwin&WheelerWinifred',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~WheelerRobertJef&YatesRosaAlafa~~~RileyAlveEdwin&WheelerWinifred~~~RileyRonaldRay&ShafferShirleyR',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~LeeAlbertEls&LauderbackGoldieIre~~~WheelerRobertJef&YatesRosaAlafa~~~RileyAlveEdwin&WheelerWinifred',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~LeeAlbertEls&LauderbackGoldieIre~~~WheelerRobertJef&YatesRosaAlafa~~~RileyAlveEdwin&WheelerWinifred~~~RileyAlvinPaul&KellyKathleeA',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~LeeAlbertEls&LauderbackGoldieIre~~~WheelerRobertJef&YatesRosaAlafa~~~McMullenRalphDona&WheelerDorothyKa',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~LeeAlbertEls&LauderbackGoldieIre~~~AdamsJohnHenry&LeeVioletEli',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~AdamsJohnHenry&LeeVioletEli~~~HoukGeorgeEdw&AdamsShirley',\n",
        "        'YatesWilliam&ParkerSally~~~YatesLevi&CooleyRebecca~~~YatesAmbroseDu&CooleyElizabeth~~~LeeGeorgeTra&YatesLarcena~~~AdamsJohnHenry&LeeVioletEli~~~HoukGeorgeEdw&AdamsShirley'\n",
        "    ]\n",
        "}\n",
        "\n",
        "combined_df = pd.DataFrame(data)\n",
        "\n",
        "# Define the ancestral lines column name\n",
        "ancestral_col = \"Yates DNA Ancestral Line\"\n",
        "\n",
        "# Reorder columns so \"Value\" appears before \"Yates DNA Ancestral Line\"\n",
        "ordered_columns = [\"ID#\", \"Surname\", \"Person\", \"Number\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "# Check if all ordered_columns exist in combined_df\n",
        "missing_columns = set(ordered_columns) - set(combined_df.columns)\n",
        "if missing_columns:\n",
        "    print(f\"Error: The following columns are missing in the DataFrame: {missing_columns}\")\n",
        "else:\n",
        "    combined_df = combined_df[ordered_columns]\n",
        "    print(\"Columns reordered successfully.\")\n",
        "\n",
        "# Perform segmentation to identify shared segments\n",
        "shared_segments_found = identify_all_shared_segments(combined_df, ancestral_col, min_shared=2, min_size=2)\n",
        "\n",
        "# Create a DataFrame for segment frequency analysis\n",
        "seg_df = pd.DataFrame(\n",
        "    [(\"~~~\".join(seg), freq) for seg, freq in shared_segments_found.items()],\n",
        "    columns=[\"Segment\", \"Frequency\"]\n",
        ").sort_values(by=\"Frequency\", ascending=False)\n",
        "\n",
        "# Display shared segments and their frequencies\n",
        "print(\"\\nAll Shared Segments and Frequencies:\")\n",
        "print(seg_df)\n",
        "\n",
        "# Calculate 'Revised_Value' for each record based on shared segments\n",
        "df_final = calculate_value_multiple_segments_refined(combined_df, ancestral_col, shared_segments_found)\n",
        "\n",
        "# Display the final DataFrame with 'Revised_Value'\n",
        "print(\"\\nFinal DataFrame with Revised_Value:\")\n",
        "print(df_final[['ID#', 'Surname', 'Person', 'Value', 'Revised_Value']])\n",
        "\n",
        "# Create mapping of ID# to Revised_Value\n",
        "id_to_value_map = df_final.set_index(\"ID#\")[\"Revised_Value\"].to_dict()\n",
        "\n",
        "# Optionally, display the mapping\n",
        "print(\"\\nMapping of ID# to Revised_Value:\")\n",
        "print(id_to_value_map)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Update value_store Without Printing the Debugging Report\n",
        "################################################################################\n",
        "def update_value_store(record_ids, ancestral_lines, segments, value_store):\n",
        "    \"\"\"\n",
        "    Updates value_store with correct 'TOTAL' values for each ID.\n",
        "    \"\"\"\n",
        "    for record_id, line in zip(record_ids, ancestral_lines):\n",
        "        total_contribution = 0  # Reset total per record\n",
        "        for segment, freq in segments.items():\n",
        "            # Check if the segment is present in the ancestral lines\n",
        "            if all(s in line for s in segment):\n",
        "                total_contribution += freq\n",
        "\n",
        "        # Update value_store with the correct 'Value'\n",
        "        if record_id in value_store:\n",
        "            value_store[record_id][\"Value\"] = total_contribution\n",
        "        else:\n",
        "            print(f\"WARNING: ID {record_id} not found in value_store\")\n",
        "\n",
        "################################################################################\n",
        "# Initialize value_store\n",
        "################################################################################\n",
        "# Create value_store as a dictionary with ID# as keys and relevant information as values\n",
        "value_store = combined_df.set_index(\"ID#\").T.to_dict()\n",
        "\n",
        "################################################################################\n",
        "# Execute Value Calculation\n",
        "################################################################################\n",
        "# Extract ancestral lines\n",
        "all_lines = combined_df[\"Yates DNA Ancestral Line\"].tolist()\n",
        "# Extract segments_found as shared_segments_found\n",
        "segments_found = shared_segments_found\n",
        "\n",
        "update_value_store(\n",
        "    record_ids=combined_df[\"ID#\"].tolist(),\n",
        "    ancestral_lines=all_lines,\n",
        "    segments=segments_found,\n",
        "    value_store=value_store\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Ensure combined_df is updated before HTML output\n",
        "################################################################################\n",
        "combined_df[\"Value\"] = combined_df[\"ID#\"].map(lambda id_: value_store[id_][\"Value\"] if id_ in value_store else None)\n",
        "\n",
        "# Create hotlink function\n",
        "def create_hotlink(row):\n",
        "    \"\"\"\n",
        "    Generates an HTML hyperlink for the ID# column.\n",
        "    \"\"\"\n",
        "    url_base = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "    additional_params = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "\n",
        "    if pd.notnull(row[\"ID#\"]):\n",
        "        return f'<a href=\"{url_base}{row[\"ID#\"]}{additional_params}\">{row[\"ID#\"]}</a>'\n",
        "    return \"\"  # Return empty if no valid ID#\n",
        "\n",
        "# Apply the hotlink function and replace \"ID#\" with clickable links\n",
        "if \"ID#\" in combined_df.columns:\n",
        "    combined_df[\"ID#\"] = combined_df.apply(create_hotlink, axis=1)\n",
        "else:\n",
        "    print(\"Error: 'ID#' column not found in DataFrame\")\n",
        "\n",
        "# Reorder columns so \"Value\" appears before \"Yates DNA Ancestral Line\"\n",
        "#ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "#combined_df = combined_df[ordered_columns]\n",
        "\n",
        "# Sort dataset by \"Yates DNA Ancestral Line\" in descending order\n",
        "combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "\n",
        "###############################################################################\n",
        "#               Configure these Booleans to Enable/Disable Outputs            #\n",
        "###############################################################################\n",
        "GENERATE_MAIN_HTML = True\n",
        "GENERATE_MINIMAL_HTML = False  # âœ… Ensure this variable is declared!\n",
        "\n",
        "################################################################################\n",
        "#      Conditional Output: MAIN HTML (Use Boolean Switch)\n",
        "################################################################################\n",
        "if GENERATE_MAIN_HTML:\n",
        "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    output_html_path = f\"htmloutput_{current_datetime}.html\"\n",
        "\n",
        "    # CSS for styling\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table, th, td {\n",
        "      border: 1px solid black;\n",
        "      border-collapse: collapse;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "      text-align: center;\n",
        "    }\n",
        "    td {\n",
        "      text-align: center;\n",
        "    }\n",
        "    td:nth-child(6) {  /* Yates DNA Ancestral Line column */\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the HTML table\n",
        "    html_main = css_style + combined_df.to_html(\n",
        "        index=False,\n",
        "        classes=\"sortable\",\n",
        "        escape=False,\n",
        "        border=0  # Let CSS handle borders\n",
        "    )\n",
        "\n",
        "    # Ensure \"Yates DNA Ancestral Line\" remains left-aligned\n",
        "    html_main = html_main.replace(\n",
        "        \"<th>Yates DNA Ancestral Line</th>\",\n",
        "        '<th style=\"text-align:left;\">Yates DNA Ancestral Line</th>'\n",
        "    )\n",
        "\n",
        "    # Save HTML to file\n",
        "    with open(output_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_main)\n",
        "\n",
        "################################################################################\n",
        "#      Minimal 2-Column HTML Output (Optional)\n",
        "################################################################################\n",
        "def generate_html_output_with_headers(dict_data):\n",
        "    \"\"\"\n",
        "    Creates an HTML table containing only:\n",
        "      - 'Match to'\n",
        "      - 'Yates DNA Ancestral Line'\n",
        "    sorted DESC by 'Yates DNA Ancestral Line'.\n",
        "    \"\"\"\n",
        "    sorted_data = sorted(dict_data, key=lambda x: x.get(\"Yates DNA Ancestral Line\", \"\"), reverse=True)\n",
        "\n",
        "    html_head = \"\"\"\n",
        "<style>\n",
        "table, th, td {\n",
        "  border: 1px solid black;\n",
        "  border-collapse: collapse;\n",
        "}\n",
        "th {\n",
        "  background-color: #f2f2f2;\n",
        "  text-align: left;\n",
        "}\n",
        "</style>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th style=\"text-align:left;\">Match to</th>\n",
        "    <th style=\"text-align:left;\">Yates DNA Ancestral Line</th>\n",
        "  </tr>\n",
        "\"\"\"\n",
        "    html_body = []\n",
        "    for row in sorted_data:\n",
        "        match_to = row.get(\"Match to\", \"\")\n",
        "        yates_line = row.get(\"Yates DNA Ancestral Line\", \"\")\n",
        "        html_body.append(f\"  <tr><td>{match_to}</td><td>{yates_line}</td></tr>\")\n",
        "\n",
        "    return \"\\n\".join([html_head] + html_body + [\"</table>\"])\n",
        "\n",
        "if GENERATE_MINIMAL_HTML:\n",
        "    records_for_html = combined_df[[\"Match to\", \"Yates DNA Ancestral Line\"]].to_dict(orient=\"records\")\n",
        "    minimal_html_code = generate_html_output_with_headers(records_for_html)\n",
        "\n",
        "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    minimal_html_path = f\"minimal_htmloutput_{current_datetime}.html\"\n",
        "\n",
        "    with open(minimal_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(minimal_html_code)\n",
        "\n",
        "################################################################################\n",
        "# Export Final Data\n",
        "################################################################################\n",
        "#combined_df.to_csv(\"final_combined_df.csv\", index=False)\n",
        "#seg_df.to_csv(\"segments_discovered.csv\", index=False)\n",
        "#presence_df.to_csv(\"segments_presence_absence.csv\", index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e205fdd1-26d7-4784-9a9f-e476b342dac4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 7\n",
            "After manual filter, total records: 8\n",
            "Records tagged and filtered by NPFX: 8\n",
            "Columns reordered successfully.\n",
            "\n",
            "All Shared Segments and Frequencies:\n",
            "                                               Segment  Frequency\n",
            "156  YatesLevi&CooleyRebecca~~~YatesWilliam&ParkerS...          8\n",
            "154  YatesAmbroseDu&CooleyElizabeth~~~YatesLevi&Coo...          8\n",
            "155  YatesAmbroseDu&CooleyElizabeth~~~YatesWilliam&...          8\n",
            "119  YatesAmbroseDu&CooleyElizabeth~~~YatesLevi&Coo...          8\n",
            "109  LeeGeorgeTra&YatesLarcena~~~YatesLevi&CooleyRe...          7\n",
            "..                                                 ...        ...\n",
            "130  HoukGeorgeEdw&AdamsShirley~~~LeeGeorgeTra&Yate...          2\n",
            "131  HoukGeorgeEdw&AdamsShirley~~~LeeGeorgeTra&Yate...          2\n",
            "132  HoukGeorgeEdw&AdamsShirley~~~LeeGeorgeTra&Yate...          2\n",
            "133  HoukGeorgeEdw&AdamsShirley~~~YatesAmbroseDu&Co...          2\n",
            "165  HoukGeorgeEdw&AdamsShirley~~~YatesWilliam&Park...          2\n",
            "\n",
            "[166 rows x 2 columns]\n",
            "\n",
            "Final DataFrame with Revised_Value:\n",
            "      ID#         Surname             Person  Value  Revised_Value\n",
            "0  I52816          leedon         DucksonPam      3              3\n",
            "1  I47130          leedon  CrabtreeChadEdmun      3              5\n",
            "2  I52836          leedon        RileySandra      3              3\n",
            "3  I52807          leedon     RileyRonldPaul      3              4\n",
            "4  I52810          leedon  LewallenDonaldChr      3              5\n",
            "5  I52559  yates,timothyj       LeeDonnaStel      1              6\n",
            "6  I52802          leedon              HoukW      2              2\n",
            "7  I52798          leedon        HoukJeffrey      2              2\n",
            "\n",
            "Mapping of ID# to Revised_Value:\n",
            "{'I52816': 3, 'I47130': 5, 'I52836': 3, 'I52807': 4, 'I52810': 5, 'I52559': 6, 'I52802': 2, 'I52798': 2}\n"
          ]
        }
      ]
    }
  ]
}