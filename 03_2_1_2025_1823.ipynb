{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQGFYAv55CSPlfXCN1LmKy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/03_2_1_2025_1823.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "85f44078-aad8-4a4b-b614-9a4b47370f1d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: python-gedcom in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.11/dist-packages (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_28_2033(works!)_2025_stable\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script with Enhanced Optimizations, Parallel Processing,\n",
        "and Progress Reporting\n",
        "---------------------------------------------------------------------------\n",
        "This script parses a GEDCOM file, calculates a \"Value\" from shared ancestral segments,\n",
        "computes standardized metrics (Z-Score, Robust Z-Score, Percentile Rank), and creates a\n",
        "Composite Score and descriptor (High, Moderate, Medium, Low, or Very Low) for each record.\n",
        "It uses enhanced parallel processing with chunking and increased caching, and it displays a\n",
        "progress bar so you can monitor progress. The final DataFrame (with \"Name\" suppressed) is\n",
        "exported to CSV and HTML (with the Yates DNA Ancestral Line left aligned).\n",
        "\"\"\"\n",
        "\n",
        "# Standard Libraries\n",
        "import csv, glob, logging, functools, os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# For progress reporting:\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Global variables used in ancestral processing\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    \"\"\"Stores and extracts details for an individual from a GEDCOM file.\"\"\"\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        \"\"\"Store a detail value (e.g., NAME, NPFX) under the given key.\"\"\"\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        \"\"\"Return the individual's ID (without '@') and set the anchor name.\"\"\"\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        \"\"\"Return the anchor name.\"\"\"\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        \"\"\"Return the NPFX field value.\"\"\"\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        \"\"\"Extract and return the cM value from the NPFX field.\"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        \"\"\"Extract and return the sort value (used as 'Match to') from the NPFX field.\"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        \"\"\"Extract and return the YDNA value from the NPFX field.\"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        \"\"\"Return the FAMC value without '@' symbols.\"\"\"\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts the first and last name from a GEDCOM record.\n",
        "    Returns a concatenation of last and first names, or \"UnknownName\" if not found.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    if name_start == 5 or name_end == -1:\n",
        "        return \"UnknownName\"\n",
        "    name = record[name_start:name_end].strip()\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    return last_name[:10].rstrip('/').replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    \"\"\"\n",
        "    Parses a GEDCOM file and builds a filter pool of individuals with NPFX data.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        return file_name.replace(' ', '_').lower()\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        \"\"\"Parses the GEDCOM file and filters individuals with NPFX data.\"\"\"\n",
        "        global name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "                logger.info(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "################################################################################\n",
        "# Caching: Increase lru_cache maxsize for compute_value_for_line\n",
        "################################################################################\n",
        "@functools.lru_cache(maxsize=5000)\n",
        "def compute_value_for_line(ancestral_line, sorted_segments_hashable):\n",
        "    \"\"\"\n",
        "    Compute the Value for an ancestral line using precomputed shared segments.\n",
        "    The second parameter is a frozenset (hashable) version of the shared segments dictionary.\n",
        "    Returns an integer score.\n",
        "    \"\"\"\n",
        "    sorted_segments = dict(sorted_segments_hashable)\n",
        "    if pd.isna(ancestral_line) or ancestral_line.strip() == \"\":\n",
        "        return 0\n",
        "    lines = [line.strip() for line in ancestral_line.split(\"~~~\") if line.strip()]\n",
        "    value = 0\n",
        "    lines_copy = lines.copy()\n",
        "    for segment, freq in sorted_segments.items():\n",
        "        segment_list = list(segment)\n",
        "        counter_seg = Counter(segment_list)\n",
        "        counter_lines = Counter(lines_copy)\n",
        "        if all(counter_lines[k] >= counter_seg[k] for k in counter_seg):\n",
        "            value += freq\n",
        "            for k, cnt in counter_seg.items():\n",
        "                for _ in range(cnt):\n",
        "                    lines_copy.remove(k)\n",
        "    value += len(lines_copy)\n",
        "    return value\n",
        "\n",
        "################################################################################\n",
        "# Other Utility Functions: extract_id, find_parents, etc.\n",
        "################################################################################\n",
        "def extract_id(record):\n",
        "    \"\"\"Extracts the ID from a GEDCOM record.\"\"\"\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    if id_start == 0 or id_end == -1:\n",
        "        return \"UnknownID\"\n",
        "    return record[id_start:id_end].strip()\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    \"\"\"Calculates a score based on ancestral paths containing 'Yates'.\"\"\"\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    \"\"\"Processes an individual to build its ancestral line and related details.\"\"\"\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\")\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "# Parallel Processing: Wrapper for processing a single record\n",
        "################################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, records):\n",
        "    \"\"\"\n",
        "    Wrapper to process an individual record.\n",
        "    Returns a list: [individual_id, sort, name, cM, filtered ancestral line string].\n",
        "    \"\"\"\n",
        "    individual_data, filtered_line = process_individual(individual_id, gedcom_instance, records)\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    name = extract_name(records.get(individual_id, \"\"))\n",
        "    return [individual_id, sort, name, cm, filtered_line]\n",
        "\n",
        "################################################################################\n",
        "# Utility: Function to split list into chunks\n",
        "################################################################################\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "################################################################################\n",
        "# Main Execution\n",
        "################################################################################\n",
        "def main():\n",
        "    # Define select_gedcom_file within main\n",
        "    def select_gedcom_file():\n",
        "        gedcom_files = glob.glob('*.ged')\n",
        "        if not gedcom_files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return gedcom_files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom_file()\n",
        "    if gedcom_file_path:\n",
        "        gedcom_instance = Gedcom(gedcom_file_path)\n",
        "        gedcom_instance.parse_gedcom()\n",
        "        individual_ids = [dataset.get_gen_person() for dataset in gedcom_instance.filter_pool]\n",
        "        print(f\"Records tagged and filtered by NPFX: {len(individual_ids)}\")\n",
        "        def extract_id(record):\n",
        "            id_start = record.find('@') + 1\n",
        "            id_end = record.find('@', id_start)\n",
        "            if id_start == 0 or id_end == -1:\n",
        "                return \"UnknownID\"\n",
        "            return record[id_start:id_end].strip()\n",
        "        with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "            data = file.read()\n",
        "        data = data.split('\\n0 ')\n",
        "        records = {extract_id(record): record for record in data}\n",
        "    else:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    ################################################################################\n",
        "    # Parallel processing: Process records in parallel in chunks with a progress bar.\n",
        "    ################################################################################\n",
        "    combined_df_rows = []\n",
        "    chunk_size = 50  # Adjust chunk size as needed\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Using %s workers for parallel processing.\", max_workers)\n",
        "    # Wrap the chunk loop with tqdm to show progress\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        for chunk in tqdm(list(chunks(individual_ids, chunk_size)), desc=\"Processing chunks\"):\n",
        "            process_func = functools.partial(process_record_wrapper, gedcom_instance=gedcom_instance, records=records)\n",
        "            results = list(executor.map(process_func, chunk))\n",
        "            combined_df_rows.extend(results)\n",
        "\n",
        "    ################################################################################\n",
        "    # Create and Populate Main DataFrame (combined_df)\n",
        "    ################################################################################\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "    # Build a value_store dictionary (if needed)\n",
        "    value_store = {}\n",
        "    for _, row in combined_df.iterrows():\n",
        "        value_store[row[\"ID#\"]] = {\n",
        "            \"Match to\": row[\"Match to\"],\n",
        "            \"Name\": row[\"Name\"],\n",
        "            \"cM\": row[\"cM\"],\n",
        "            \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "            \"Value\": None\n",
        "        }\n",
        "\n",
        "    def remove_prefix(row):\n",
        "        \"\"\"Remove unwanted prefix from the ancestral line.\"\"\"\n",
        "        ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "        prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "        if ancestral_line.startswith(prefix_to_remove):\n",
        "            row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "        return row\n",
        "\n",
        "    combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "    ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    combined_df = combined_df[ordered_columns]\n",
        "    combined_df.index += 1\n",
        "    combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    ################################################################################\n",
        "    # Optimize Segmentation Calculation: Vectorize \"Value\" Calculation with Caching\n",
        "    ################################################################################\n",
        "    def identify_all_shared_segments(df, ancestral_col, min_shared=2, min_size=2):\n",
        "        \"\"\"\n",
        "        Identify all shared segments in the ancestral line.\n",
        "        Returns a dictionary with segment tuples as keys and frequencies as values.\n",
        "        \"\"\"\n",
        "        segment_counts = defaultdict(int)\n",
        "        for _, row in df.iterrows():\n",
        "            if pd.isna(row[ancestral_col]):\n",
        "                continue\n",
        "            lines = [line.strip() for line in row[ancestral_col].split(\"~~~\") if line.strip()]\n",
        "            for size in range(min_size, len(lines) + 1):\n",
        "                for subset in combinations(sorted(lines), size):\n",
        "                    segment_counts[subset] += 1\n",
        "        shared_segments = {segment: count for segment, count in segment_counts.items() if count >= min_shared}\n",
        "        return dict(sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "    df_process = combined_df[[\"ID#\", \"Yates DNA Ancestral Line\"]].copy()\n",
        "    df_process[\"Value\"] = 0\n",
        "    ancestral_col = \"Yates DNA Ancestral Line\"\n",
        "    shared_segments_found = identify_all_shared_segments(df_process, ancestral_col, min_shared=2, min_size=2)\n",
        "    seg_df = pd.DataFrame(\n",
        "        [(\"~~~\".join(seg), freq) for seg, freq in shared_segments_found.items()],\n",
        "        columns=[\"Segment\", \"Frequency\"]\n",
        "    ).sort_values(by=\"Frequency\", ascending=False)\n",
        "    # Convert shared_segments_found to a hashable type\n",
        "    shared_segments_hashable = frozenset(shared_segments_found.items())\n",
        "    df_process[\"Value\"] = df_process[ancestral_col].apply(lambda line: compute_value_for_line(str(line), shared_segments_hashable))\n",
        "    id_to_value_map = df_process.set_index(\"ID#\")[\"Value\"].to_dict()\n",
        "    merged_df = combined_df.merge(df_process[[\"ID#\", \"Value\"]], on=\"ID#\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
        "    if \"Value_new\" in merged_df.columns:\n",
        "        merged_df[\"Value\"] = merged_df[\"Value_new\"]\n",
        "        merged_df.drop(columns=[\"Value_new\"], inplace=True)\n",
        "    combined_df = merged_df\n",
        "    combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "\n",
        "    ################################################################################\n",
        "    # Calculate Additional Scores: Z-Score, Robust Z-Score, and Percentile Rank\n",
        "    ################################################################################\n",
        "    combined_df[\"Value\"] = pd.to_numeric(combined_df[\"Value\"], errors=\"coerce\")\n",
        "    mean_value = combined_df[\"Value\"].mean()\n",
        "    std_value = combined_df[\"Value\"].std()\n",
        "    combined_df[\"Z-Score\"] = (combined_df[\"Value\"] - mean_value) / std_value\n",
        "    median_value = combined_df[\"Value\"].median()\n",
        "    mad_value = np.median(np.abs(combined_df[\"Value\"] - median_value))\n",
        "    if mad_value == 0:\n",
        "        combined_df[\"Robust Z-Score\"] = 0\n",
        "    else:\n",
        "        combined_df[\"Robust Z-Score\"] = (combined_df[\"Value\"] - median_value) / (mad_value * 1.4826)\n",
        "    combined_df[\"Percentile Rank\"] = combined_df[\"Value\"].rank(pct=True) * 100\n",
        "\n",
        "    ################################################################################\n",
        "    # Composite Metric Integration:\n",
        "    # Compute \"Composite Score\" as the average of the absolute Z-Score,\n",
        "    # absolute Robust Z-Score, and normalized Percentile Rank,\n",
        "    # then assign a descriptor.\n",
        "    ################################################################################\n",
        "    combined_df[\"Composite Score\"] = (\n",
        "        combined_df[\"Z-Score\"].abs() +\n",
        "        combined_df[\"Robust Z-Score\"].abs() +\n",
        "        (combined_df[\"Percentile Rank\"] / 100)\n",
        "    ) / 3\n",
        "    combined_df[\"Composite Score\"] = combined_df[\"Composite Score\"].round(2)\n",
        "\n",
        "    def assign_descriptor(score):\n",
        "        \"\"\"Assigns a descriptor based on the composite score.\"\"\"\n",
        "        if score >= 2.0:\n",
        "            return \"High\"\n",
        "        elif score >= 1.5:\n",
        "            return \"Moderate\"\n",
        "        elif score >= 1.0:\n",
        "            return \"Medium\"\n",
        "        elif score >= 0.5:\n",
        "            return \"Low\"\n",
        "        else:\n",
        "            return \"Very Low\"\n",
        "\n",
        "    combined_df[\"Composite Significance\"] = combined_df[\"Composite Score\"].apply(assign_descriptor)\n",
        "\n",
        "    ################################################################################\n",
        "    # Repair Final DataFrame Columns:\n",
        "    # Final order: \"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"\n",
        "    # (Note: \"Name\" is suppressed from the output.)\n",
        "    ################################################################################\n",
        "    final_columns = [\"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"]\n",
        "    combined_df = combined_df[final_columns]\n",
        "    logger.info(\"Final DataFrame columns: %s\", combined_df.columns.tolist())\n",
        "    print(combined_df.head(10))\n",
        "\n",
        "    ################################################################################\n",
        "    # Export the final DataFrame with the additional scores to CSV\n",
        "    ################################################################################\n",
        "    output_filename = \"final_combined_df_with_composite_scores.csv\"\n",
        "    combined_df.to_csv(output_filename, index=False)\n",
        "    logger.info(\"Final DataFrame with composite scores exported to '%s'.\", output_filename)\n",
        "\n",
        "    ################################################################################\n",
        "    # Export the Final DataFrame to HTML with left-aligned Yates DNA Ancestral Line\n",
        "    ################################################################################\n",
        "    html_filename = \"HTML_combined_df_with_composite_scores.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table {\n",
        "      width: 100%;\n",
        "      border-collapse: collapse;\n",
        "      margin: 20px 0;\n",
        "    }\n",
        "    table, th, td {\n",
        "      border: 1px solid #333;\n",
        "    }\n",
        "    th, td {\n",
        "      padding: 8px 12px;\n",
        "      text-align: center;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "    }\n",
        "    /* Left-align the 6th column (\"Yates DNA Ancestral Line\") */\n",
        "    td:nth-child(6) {\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    html_content = css_style + combined_df.to_html(\n",
        "        index=False,\n",
        "        columns=final_columns,\n",
        "        escape=False\n",
        "    )\n",
        "    with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Final DataFrame exported to HTML file '%s'.\", html_filename)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26937e23-2370-42d6-f3bf-be1934715bc9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 249\n",
            "After manual filter, total records: 250\n",
            "Records tagged and filtered by NPFX: 250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunks: 100%|██████████| 5/5 [02:42<00:00, 32.56s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        ID#        Match to  Value  Composite Score Composite Significance  \\\n",
            "87   I56768           sudie      5             0.48               Very Low   \n",
            "22   I54771  yates,timothyb      4             0.55                    Low   \n",
            "62   I46369     yates,johnh      5             0.48               Very Low   \n",
            "35   I46791    yates,ronald      5             0.48               Very Low   \n",
            "34   I50712    yates,ronald      6             0.41               Very Low   \n",
            "114  I53986          marmar      4             0.55                    Low   \n",
            "8    I57116      yatesarlew      3             0.63                    Low   \n",
            "140  I52269         klingal      4             0.55                    Low   \n",
            "218  I51318        beardali      6             0.41               Very Low   \n",
            "217  I55561     birdwelljac      7             0.34               Very Low   \n",
            "\n",
            "                              Yates DNA Ancestral Line  \n",
            "87   YatesWilliamTh&WilkinsNancy~~~YatesMartinLut&H...  \n",
            "22   YatesWilliamRo&DavisMalissa~~~YatesWilliamEd&K...  \n",
            "62   YatesWilliamPr&McKinneyElizabeth~~~YatesJamesM...  \n",
            "35   YatesWilliamPr&McKinneyElizabeth~~~YatesJamesM...  \n",
            "34   YatesWilliamNi&HaysElizabeth~~~YatesJamesWesl&...  \n",
            "114  YatesWilliamM&OsborneNancyAnn~~~YatesWilliamV&...  \n",
            "8    YatesWilliamLe&LampingEllenIsab~~~YatesJosephM...  \n",
            "140  YatesWilliamJo&HolsteadSarahJane~~~YatesWillia...  \n",
            "218  YatesWilliamHe&ForsterAnnMary~~~YatesJohnHenry...  \n",
            "217  YatesWilliamCh&McManusEmilyMill~~~YatesJohnFle...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "compute_value_for_line(\"YatesWilliam~~~YatesLevi~~~YatesAmbrose\", frozenset({(('YatesWilliam', 'YatesLevi', 'YatesAmbrose'), 5)}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5z8ZEL_BQZe",
        "outputId": "e0d28e5b-99bb-40d6-d0c0-234b4c3902ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "306 ns ± 4.65 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
          ]
        }
      ]
    }
  ]
}