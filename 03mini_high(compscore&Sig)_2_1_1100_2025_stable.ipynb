{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2cbsx7aCBBv2o+TnrqlTv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/03mini_high(compscore%26Sig)_2_1_1100_2025_stable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "ec19e331-227e-40bd-afb6-d1e8b45c435f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 03mini-high_01_31_2029_2025_stable\n",
        "\n",
        "# Standard Libraries\n",
        "import csv\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# GEDCOM Parsing\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Alignment\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts first and last name from a GEDCOM record.\n",
        "    Handles missing or malformed names gracefully.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    if name_start == 5 or name_end == -1:  # Meaning '1 NAME ' was not found\n",
        "        return \"UnknownName\"\n",
        "    name = record[name_start:name_end].strip()\n",
        "    # Handle cases where no '/' is present in the name\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")  # Take first 10 characters as default name\n",
        "    # Extract first and last name\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]  # first 10 chars\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}  # Global dictionary to hold name->ID mapping\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # we’ll modify name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0  # Count YDNA occurrences\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1  # YDNA found\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "        print(f'Records with YDNA information: {ydna_count}')\n",
        "        print(f'Autosomal matches: {autosomal_count}')\n",
        "\n",
        "        # First-level filter: only those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Optional second-level filter from an Excel file\n",
        "        manual_filter_activated = True  # or False\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "                self.filter_pool = [\n",
        "                    dataset for dataset in self.filter_pool\n",
        "                    if dataset.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "    # Automatically select the first GEDCOM file found.\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "################################################################################\n",
        "#          Execute GEDCOM Parsing & Build Our Filter Pool                      #\n",
        "################################################################################\n",
        "gedcom_file_path = select_gedcom_file()\n",
        "if gedcom_file_path:\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "    # Gather individuals (last_name, individual_id) from the filter pool\n",
        "    individuals = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    ################################################################################\n",
        "    # Function: Extract ID from GEDCOM Record\n",
        "    ################################################################################\n",
        "    def extract_id(record):\n",
        "        \"\"\"\n",
        "        Extracts the ID from a GEDCOM record.\n",
        "        A valid ID is enclosed within '@' symbols.\n",
        "        \"\"\"\n",
        "        id_start = record.find('@') + 1\n",
        "        id_end = record.find('@', id_start)\n",
        "        if id_start == 0 or id_end == -1:\n",
        "            return \"UnknownID\"\n",
        "        return record[id_start:id_end].strip()\n",
        "\n",
        "    # Read the GEDCOM file as raw text and split into records\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "else:\n",
        "    print(\"No GEDCOM file selected; exiting.\")\n",
        "    raise SystemExit\n",
        "\n",
        "################################################################################\n",
        "#        Functions to Traverse & Score Ancestors, Build Data for DataFrame     #\n",
        "################################################################################\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # We'll update anchor_gen1 if found\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    # Build generation_table and visited_pairs\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "\n",
        "    # Gather more info from the dataset\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "\n",
        "    # Build ancestral line (exclude anchor_gen1 itself)\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\")\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "#         Build Rows for DataFrame from the Filter Pool                        #\n",
        "################################################################################\n",
        "combined_df_rows = []\n",
        "for dataset in gedcom_instance.filter_pool:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(\n",
        "        individual_id, gedcom_instance, records\n",
        "    )\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    individual_name = extract_name(records.get(individual_id, \"\"))\n",
        "    combined_df_rows.append(\n",
        "        [individual_id, sort, individual_name, cm, filtered_ancestral_line_str]\n",
        "    )\n",
        "\n",
        "################################################################################\n",
        "#       NO NEED TO MODIFY ABOVE THIS SECTION 28-1-2025                         #\n",
        "################################################################################\n",
        "\n",
        "# Create and Populate Main DataFrame (combined_df) using all desired columns\n",
        "# (Note: We include \"Name\" for internal processing, but will suppress it later.)\n",
        "columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "# Initialize the value_store dictionary (if needed)\n",
        "value_store = {}\n",
        "for _, row in combined_df.iterrows():\n",
        "    value_store[row[\"ID#\"]] = {\n",
        "        \"Match to\": row[\"Match to\"],\n",
        "        \"Name\": row[\"Name\"],\n",
        "        \"cM\": row[\"cM\"],\n",
        "        \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "        \"Value\": None  # Placeholder for 'Value'\n",
        "    }\n",
        "\n",
        "################################################################################\n",
        "#       Remove miscellaneous Distant ancestors (combined_df)\n",
        "################################################################################\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "    prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "\n",
        "# Order and clean up columns (do not drop 'Match to' and 'Name')\n",
        "ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = combined_df[ordered_columns]\n",
        "combined_df.index += 1\n",
        "combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "\n",
        "################################################################################\n",
        "# Ensure Required Columns Exist in Memory (for processing)\n",
        "################################################################################\n",
        "expected_columns = [\"ID#\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "for col in expected_columns:\n",
        "    if col not in combined_df.columns:\n",
        "        if col == \"Value\":\n",
        "            combined_df[col] = 0\n",
        "        else:\n",
        "            combined_df[col] = \"Unknown\"\n",
        "\n",
        "################################################################################\n",
        "#       Segmentation and Frequency Analysis\n",
        "################################################################################\n",
        "def parse_line_to_pairs(line, delimiter=\"~~~\"):\n",
        "    return line.strip().split(delimiter) if pd.notna(line) else []\n",
        "\n",
        "def identify_all_shared_segments(df, ancestral_col, min_shared=2, min_size=2):\n",
        "    segment_counts = defaultdict(int)\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.isna(row[ancestral_col]):\n",
        "            continue\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "        for size in range(min_size, len(lines) + 1):\n",
        "            for subset in combinations(sorted(lines), size):\n",
        "                segment_counts[subset] += 1\n",
        "    shared_segments = {segment: count for segment, count in segment_counts.items() if count >= min_shared}\n",
        "    return dict(sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "def calculate_value(df, ancestral_col, shared_segments):\n",
        "    sorted_segments = sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "    df[\"Value\"] = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.isna(row[ancestral_col]):\n",
        "            df.at[idx, \"Value\"] = 0\n",
        "            continue\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "        value = 0\n",
        "        lines_copy = lines.copy()\n",
        "        # Use Counter to correctly remove duplicates.\n",
        "        for segment, freq in sorted_segments:\n",
        "            segment_list = list(segment)\n",
        "            counter_seg = Counter(segment_list)\n",
        "            counter_lines = Counter(lines_copy)\n",
        "            if all(counter_lines[k] >= counter_seg[k] for k in counter_seg):\n",
        "                value += freq\n",
        "                for k, cnt in counter_seg.items():\n",
        "                    for _ in range(cnt):\n",
        "                        lines_copy.remove(k)\n",
        "        value += len(lines_copy)\n",
        "        df.at[idx, \"Value\"] = value\n",
        "    return df\n",
        "\n",
        "################################################################################\n",
        "#       Main Processing for Segmentation & Value Calculation\n",
        "################################################################################\n",
        "# Create a separate DataFrame for processing so that we do not lose \"Match to\" and \"Name\"\n",
        "df_process = combined_df[[\"ID#\", \"Yates DNA Ancestral Line\"]].copy()\n",
        "df_process[\"Value\"] = 0\n",
        "\n",
        "ancestral_col = \"Yates DNA Ancestral Line\"\n",
        "shared_segments_found = identify_all_shared_segments(df_process, ancestral_col, min_shared=2, min_size=2)\n",
        "seg_df = pd.DataFrame(\n",
        "    [(\"~~~\".join(seg), freq) for seg, freq in shared_segments_found.items()],\n",
        "    columns=[\"Segment\", \"Frequency\"]\n",
        ").sort_values(by=\"Frequency\", ascending=False)\n",
        "\n",
        "df_process = calculate_value(df_process, ancestral_col, shared_segments_found)\n",
        "print(\"\\n✅ Final Processed DataFrame with Value (df_process):\")\n",
        "print(df_process[[\"ID#\", \"Value\"]])\n",
        "id_to_value_map = df_process.set_index(\"ID#\")[\"Value\"].to_dict()\n",
        "\n",
        "# Merge the calculated \"Value\" column back into combined_df.\n",
        "merged_df = combined_df.merge(df_process[[\"ID#\", \"Value\"]], on=\"ID#\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
        "if \"Value\" not in merged_df.columns and \"Value_new\" in merged_df.columns:\n",
        "    merged_df.rename(columns={\"Value_new\": \"Value\"}, inplace=True)\n",
        "else:\n",
        "    if \"Value_new\" in merged_df.columns:\n",
        "        merged_df[\"Value\"] = merged_df[\"Value_new\"]\n",
        "        merged_df.drop(columns=[\"Value_new\"], inplace=True)\n",
        "combined_df = merged_df\n",
        "\n",
        "combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "\n",
        "################################################################################\n",
        "# Calculate Additional Scores: Standard Z-Score, Robust Z-Score, and Percentile Rank\n",
        "################################################################################\n",
        "combined_df[\"Value\"] = pd.to_numeric(combined_df[\"Value\"], errors='coerce')\n",
        "mean_value = combined_df[\"Value\"].mean()\n",
        "std_value = combined_df[\"Value\"].std()\n",
        "combined_df[\"Z-Score\"] = (combined_df[\"Value\"] - mean_value) / std_value\n",
        "median_value = combined_df[\"Value\"].median()\n",
        "mad_value = np.median(np.abs(combined_df[\"Value\"] - median_value))\n",
        "if mad_value == 0:\n",
        "    combined_df[\"Robust Z-Score\"] = 0\n",
        "else:\n",
        "    combined_df[\"Robust Z-Score\"] = (combined_df[\"Value\"] - median_value) / (mad_value * 1.4826)\n",
        "combined_df[\"Percentile Rank\"] = combined_df[\"Value\"].rank(pct=True) * 100\n",
        "\n",
        "################################################################################\n",
        "# Calculate Composite Significance Score and Assign Descriptors\n",
        "################################################################################\n",
        "combined_df[\"Composite Score\"] = (\n",
        "    combined_df[\"Z-Score\"].abs() +\n",
        "    combined_df[\"Robust Z-Score\"].abs() +\n",
        "    (combined_df[\"Percentile Rank\"] / 100)\n",
        ") / 3\n",
        "\n",
        "def assign_descriptor(score):\n",
        "    if score >= 2.0:\n",
        "        return \"High\"\n",
        "    elif score >= 1.5:\n",
        "        return \"Moderate\"\n",
        "    elif score >= 1.0:\n",
        "        return \"Medium\"\n",
        "    elif score >= 0.5:\n",
        "        return \"Low\"\n",
        "    else:\n",
        "        return \"Very Low\"\n",
        "\n",
        "combined_df[\"Composite Significance\"] = combined_df[\"Composite Score\"].apply(assign_descriptor)\n",
        "\n",
        "################################################################################\n",
        "# Repair Final DataFrame Columns\n",
        "# Final columns order:\n",
        "# \"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"\n",
        "# Note: \"Name\" is suppressed from the output.\n",
        "################################################################################\n",
        "final_columns = [\"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"]\n",
        "for col in final_columns:\n",
        "    if col not in combined_df.columns:\n",
        "        combined_df[col] = None\n",
        "combined_df = combined_df[final_columns]\n",
        "print(\"\\nDEBUG: Final DataFrame after repairing columns\")\n",
        "print(\"Columns:\", combined_df.columns.tolist())\n",
        "print(combined_df.head(10))\n",
        "\n",
        "################################################################################\n",
        "# Export the final DataFrame with the additional scores to CSV\n",
        "################################################################################\n",
        "output_filename = \"final_combined_df_with_composite_scores.csv\"\n",
        "combined_df.to_csv(output_filename, index=False)\n",
        "print(f\"Final DataFrame with composite scores exported to '{output_filename}'.\")\n",
        "\n",
        "################################################################################\n",
        "# Export the Final DataFrame to HTML\n",
        "################################################################################\n",
        "html_filename = \"HTML_combined_df_with_composite_scores.html\"\n",
        "\n",
        "# Define a simple CSS style for the HTML table\n",
        "css_style = \"\"\"\n",
        "<style>\n",
        "table {\n",
        "  width: 100%;\n",
        "  border-collapse: collapse;\n",
        "  margin: 20px 0;\n",
        "}\n",
        "table, th, td {\n",
        "  border: 1px solid #333;\n",
        "}\n",
        "th, td {\n",
        "  padding: 8px 12px;\n",
        "  text-align: center;\n",
        "}\n",
        "th {\n",
        "  background-color: #f2f2f2;\n",
        "}\n",
        "/* Left-align the 6th column (\"Yates DNA Ancestral Line\") */\n",
        "td:nth-child(6) {\n",
        "  text-align: left;\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Use to_html with the specified final columns.\n",
        "html_content = css_style + combined_df.to_html(\n",
        "    index=False,\n",
        "    columns=[\"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"],\n",
        "    escape=False\n",
        ")\n",
        "\n",
        "# Write the HTML content to a file.\n",
        "with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(html_content)\n",
        "\n",
        "print(f\"Final DataFrame exported to HTML file '{html_filename}'.\")\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0023e762-9f4c-4459-8b30-7a0380a0cb23"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 118\n",
            "After manual filter, total records: 119\n",
            "Records tagged and filtered by NPFX: 119\n",
            "\n",
            "✅ Final Processed DataFrame with Value (df_process):\n",
            "        ID#  Value\n",
            "2    I18235      2\n",
            "103  I51017      3\n",
            "104  I51033      7\n",
            "105  I51044      7\n",
            "112  I54946      6\n",
            "..      ...    ...\n",
            "117  I54982      4\n",
            "16   I45200     14\n",
            "61   I48626      3\n",
            "3    I44882      2\n",
            "4    I44893      2\n",
            "\n",
            "[119 rows x 2 columns]\n",
            "\n",
            "DEBUG: Final DataFrame after repairing columns\n",
            "Columns: ['ID#', 'Match to', 'Value', 'Composite Score', 'Composite Significance', 'Yates DNA Ancestral Line']\n",
            "        ID#         Match to  Value  Composite Score Composite Significance  \\\n",
            "36   I46369      yates,johnh      5         0.549676                    Low   \n",
            "52   I48842  girtain,theresa      7         0.325882               Very Low   \n",
            "73   I48720  girtain,kathryn      8         0.234293               Very Low   \n",
            "118  I44893     girtain,alma      2         0.928085                    Low   \n",
            "117  I44882     girtain,alma      2         0.928085                    Low   \n",
            "7    I51637       yatesjohnh      8         0.234293               Very Low   \n",
            "35   I46213      yates,johnh      7         0.325882               Very Low   \n",
            "94   I48663     girtain,andy      8         0.234293               Very Low   \n",
            "51   I48829  girtain,theresa      7         0.325882               Very Low   \n",
            "93   I45533     girtain,andy     10         0.479698               Very Low   \n",
            "\n",
            "                              Yates DNA Ancestral Line  \n",
            "36   YatesWilliamPr&McKinneyElizabeth~~~YatesJamesM...  \n",
            "52   YatesWilliam&ThornburyAnne~~~YatesWilliam&Henr...  \n",
            "73   YatesWilliam&ThornburyAnne~~~YatesWilliam&Henr...  \n",
            "118  YatesWilliam&SaltPhoebe~~~YatesThomas&OwenViol...  \n",
            "117  YatesWilliam&SaltPhoebe~~~YatesThomas&OwenViol...  \n",
            "7    YatesWilliam&SaltPhoebe~~~FarnworthWilliamMa&Y...  \n",
            "35   YatesWilliam&NeedhamMary~~~YatesPatrick&Chamne...  \n",
            "94   YatesWilliam&NeedhamMary~~~YatesPatrick&Chamne...  \n",
            "51   YatesWilliam&NeedhamMary~~~YatesPatrick&Chamne...  \n",
            "93   YatesWilliam&BondMargaretC~~~YatesBartholome&S...  \n",
            "Final DataFrame with composite scores exported to 'final_combined_df_with_composite_scores.csv'.\n",
            "Final DataFrame exported to HTML file 'HTML_combined_df_with_composite_scores.html'.\n"
          ]
        }
      ]
    }
  ]
}