{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHjrh0XsbTLZrVqqDH5bFw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/Gold__1_%26_2_%26_3_20250521.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "88e5f3cc-ab30-4f6d-ba9e-0538c5449362",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.3\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#credentials\n",
        "\n",
        "import os\n",
        "\n",
        "# Gmail SMTP creds\n",
        "os.environ['GMAIL_USER']         = 'yatesvilleron@gmail.com'\n",
        "os.environ['GMAIL_APP_PASSWORD'] = 'qtziwiblytgrlzvx'\n",
        "\n",
        "# FTPS upload creds — make sure FTP_PASS is exactly your password, no < or >\n",
        "os.environ['FTP_HOST']       = 'ftp.one-name.net'\n",
        "os.environ['FTP_PORT']       = '21'\n",
        "os.environ['FTP_USER']       = 'admin@yates.one-name.net'\n",
        "os.environ['FTP_PASS']       = 'v(i83lfQB@dB'\n"
      ],
      "metadata": {
        "id": "971jlPTnBVfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 20250513\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script using:\n",
        " - Chunk-based Parallel Processing for Speed (Stage 1: genealogical line creation)\n",
        " - A Trie-based approach, then final \"Value\" = 5 * (number of couples with node.count >=2) + (total couples)\n",
        "\n",
        "For ancestral lines where none of the couples are repeated (a one-off line), the Value is still computed.\n",
        "Now, instead of composite scoring, two new columns are added:\n",
        "  - Value Range (the numeric bracket)\n",
        "  - Value Label (a descriptive label)\n",
        "\n",
        "Exports final CSV/HTML sorted by \"Yates DNA Ancestral Line\", including a 'haplogroup' column.\n",
        "\"\"\"\n",
        "import csv\n",
        "import glob\n",
        "import logging\n",
        "import functools\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# Global Variables\n",
        "###############################################################################\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "###############################################################################\n",
        "# Trie Data Structure\n",
        "###############################################################################\n",
        "class TrieNode:\n",
        "    \"\"\"A simple Trie node for storing a couple and counting how many lines pass here.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.children = {}\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert_line(self, couples_list):\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple not in current.children:\n",
        "                current.children[couple] = TrieNode()\n",
        "            current = current.children[couple]\n",
        "            current.count += 1\n",
        "\n",
        "    def get_couple_count(self, couples_list):\n",
        "        counts = []\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple in current.children:\n",
        "                current = current.children[couple]\n",
        "                counts.append(current.count)\n",
        "            else:\n",
        "                counts.append(0)\n",
        "                break\n",
        "        return counts\n",
        "\n",
        "###############################################################################\n",
        "# Utility: chunk generator\n",
        "###############################################################################\n",
        "def chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "###############################################################################\n",
        "# GedcomDataset\n",
        "###############################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "###############################################################################\n",
        "# Gedcom Class\n",
        "###############################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "        return autosomal_count\n",
        "\n",
        "###############################################################################\n",
        "# quick_extract_name\n",
        "###############################################################################\n",
        "def quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start)\n",
        "    if end == -1:\n",
        "        end = len(full_text)\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "###############################################################################\n",
        "# Parents & Ancestors\n",
        "###############################################################################\n",
        "def find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def find_distant_ancestors(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(find_distant_ancestors(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(find_distant_ancestors(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "###############################################################################\n",
        "# filter_ancestral_line\n",
        "###############################################################################\n",
        "def filter_ancestral_line(winning_path_ids, generation_table_local, names_map):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table_local:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    matching_table.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for gen, pair in matching_table:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(f\"{name_pair[0]}&{name_pair[1]}\")\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "###############################################################################\n",
        "# process_record_wrapper (parallel) - STAGE 1\n",
        "###############################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, parents_map)\n",
        "    distant_anc_paths = find_distant_ancestors(individual_id, parents_map)\n",
        "\n",
        "    best_score = None\n",
        "    best_path = None\n",
        "    for path in distant_anc_paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx+1) for idx, nm in enumerate(name_path) if 'Yates' in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_path = path\n",
        "\n",
        "    if not best_path:\n",
        "        best_path = []\n",
        "\n",
        "    best_path_cleaned = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str = filter_ancestral_line(set(best_path_cleaned), generation_table, names_map)\n",
        "\n",
        "    cm_value = ''\n",
        "    sort_value = ''\n",
        "    ydna_value = ''\n",
        "    for ds in gedcom_instance.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    # Return columns: ID#, Match to, Name, cM, Yates DNA Ancestral Line, haplogroup\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "###############################################################################\n",
        "# main()\n",
        "###############################################################################\n",
        "def main():\n",
        "    def select_gedcom():\n",
        "        files = glob.glob(\"*.ged\")\n",
        "        if not files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom()\n",
        "    if not gedcom_file_path:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    ged = Gedcom(gedcom_file_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "    filter_count = len(ged.filter_pool)\n",
        "\n",
        "    with open(\"autosomal_count.txt\", \"w\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "\n",
        "    print(\"Records tagged and filtered by NPFX:\", filter_count)\n",
        "\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    blocks = raw_data.split('\\n0 ')\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find('\\n')\n",
        "        if flend == -1:\n",
        "            flend = len(blk)\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            start = first_line.find('@') + 1\n",
        "            end = first_line.find('@', start)\n",
        "            rec_id = first_line[start:end].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    names_map = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        nm = quick_extract_name(\"\\n\" + txt)\n",
        "        names_map[rec_id] = nm\n",
        "\n",
        "    families = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            husb_id = txt[father_idx+len('1 HUSB @'):txt.find('@', father_idx+len('1 HUSB @'))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            wife_id = txt[wife_idx+len('1 WIFE @'):txt.find('@', wife_idx+len('1 WIFE @'))] if wife_idx != -1 else None\n",
        "            kids = [ln.split('@')[1] for ln in txt.split('\\n') if ln.strip().startswith('1 CHIL @')]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(f\"Processing {len(individual_ids)} individuals with chunk-based parallel...\")\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Starting chunk-based parallel processing with %d workers.\", max_workers)\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            func = functools.partial(process_record_wrapper, gedcom_instance=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(executor.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def remove_specific_prefix(row):\n",
        "        prefix = \"YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "        if row[\"Yates DNA Ancestral Line\"].startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = row[\"Yates DNA Ancestral Line\"][len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(remove_specific_prefix, axis=1)\n",
        "\n",
        "    logger.info(\"Building Trie from reversed lines...\")\n",
        "    trie = Trie()\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.notna(line_str) and line_str.strip():\n",
        "            trie.insert_line([x.strip() for x in line_str.split(\"~~~\") if x.strip()])\n",
        "\n",
        "    values, prefix_counts = [], []\n",
        "    logger.info(\"Computing 'Value' = 5*(#couples with node.count >=2) + (total couples) ...\")\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            values.append(0)\n",
        "            prefix_counts.append(0)\n",
        "        else:\n",
        "            couples_list = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "            node_counts = trie.get_couple_count(couples_list)\n",
        "            prefix_count = sum(1 for c in node_counts if c >= 2)\n",
        "            values.append(5 * prefix_count + len(couples_list))\n",
        "            prefix_counts.append(prefix_count)\n",
        "\n",
        "    df[\"Value\"], df[\"PrefixCount\"] = values, prefix_counts\n",
        "\n",
        "    def assign_value_range_label(val):\n",
        "        try:\n",
        "            v = float(val)\n",
        "        except:\n",
        "            return \"\", \"\"\n",
        "        if v >= 60: return \">=60\", \"1-likely correct\"\n",
        "        if 47 <= v <= 59: return \"59~47\", \"2-lines forming\"\n",
        "        if 34 <= v <= 46: return \"46~34\", \"3-patterns emerging\"\n",
        "        if 21 <= v <= 33: return \"33~21\", \"4-notable patterns\"\n",
        "        if 8 <= v <= 20: return \"20~8\", \"5-patterns stable\"\n",
        "        if 1 <= v <= 7:  return f\"{v:.0f}\", \"6-need research\"\n",
        "        return f\"{v:.0f}\", \"0-uncategorized\"\n",
        "\n",
        "    ranges, labels = zip(*(assign_value_range_label(v) for v in df[\"Value\"]))\n",
        "    df[\"Value Range\"], df[\"Value Label\"] = ranges, labels\n",
        "\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "    df.drop(\"PrefixCount\", axis=1, inplace=True)\n",
        "\n",
        "    csv_name = \"final_combined_df_with_value_labels.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    logger.info(\"Exported final DataFrame to '%s'.\", csv_name)\n",
        "\n",
        "    html_name = \"HTML_combined_df_with_value_labels.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n",
        "    table, th, td { border: 1px solid #333; }\n",
        "    th, td { padding: 8px 12px; text-align: center; }\n",
        "    th { background-color: #f2f2f2; }\n",
        "    /* Left-align the last column */\n",
        "    td:nth-child(7) { text-align: left; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Value Range\", \"Value Label\", \"Yates DNA Ancestral Line\"]\n",
        "    html_content = css_style + df.to_html(index=False, columns=final_cols, escape=False)\n",
        "    with open(html_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Exported HTML to '%s'.\", html_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    try:\n",
        "        display(Javascript('alert(\"✅ GEDCOM processing (and HTML export) is complete!\");'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import smtplib, ssl\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "def send_email(subject, body, to_addr):\n",
        "    smtp_server = 'smtp.gmail.com'\n",
        "    port = 465\n",
        "    sender = os.environ['GMAIL_USER']\n",
        "    password = os.environ['GMAIL_APP_PASSWORD']\n",
        "    msg = MIMEText(body)\n",
        "    msg['Subject'] = subject\n",
        "    msg['From'] = sender\n",
        "    msg['To'] = to_addr\n",
        "    context = ssl.create_default_context()\n",
        "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
        "        server.login(sender, password)\n",
        "        server.send_message(msg)\n",
        "\n",
        "# Email summary\n",
        "df_summary = pd.read_csv(\"final_combined_df_with_value_labels.csv\")\n",
        "total = len(df_summary)\n",
        "top5 = df_summary.sort_values('Value', ascending=False).head(5)['Yates DNA Ancestral Line'].tolist()\n",
        "summary = f\"GEDCOM processing complete!\\n\\nTotal lines: {total}\\nTop 5 lines:\\n\" + \"\\n\".join(f\"- {line}\" for line in top5)\n",
        "send_email(subject=\"✅ Cell #1 Report Ready\", body=summary, to_addr=os.environ['GMAIL_USER'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Qh13Q-WUmVu3",
        "outputId": "0f5040de-7293-4ca6-8abf-4be68e80c461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:filtered_ids.xlsx not found. Skipping second-level manual filter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEDCOM contained 60283 total records\n",
            "Records tagged and filtered by NPFX: 1467\n",
            "Records with YDNA information: 90\n",
            "Autosomal matches: 1377\n",
            "Records tagged and filtered by NPFX: 1467\n",
            "Processing 1467 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 1467/1467 [13:57<00:00,  1.75it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "alert(\"✅ GEDCOM processing (and HTML export) is complete!\");"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: XHTML Template + Export + Root FTP Upload 20250513\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "import os\n",
        "\n",
        "# ————— Load Data —————\n",
        "df = pd.read_csv(\"final_combined_df_with_value_labels.csv\")\n",
        "\n",
        "# ————— Blank out any NaN haplogroups —————\n",
        "df['haplogroup'] = df['haplogroup'].fillna('')\n",
        "\n",
        "\n",
        "# ————— Hyperlink haplogroup values, send all Y- designations to a single overview page —————\n",
        "hap_base = \"gengen/haplogroup/\"\n",
        "ydna_overview = \"gengen/Y-designation-overview.htm\"\n",
        "\n",
        "df['haplogroup'] = (\n",
        "    df['haplogroup']\n",
        "      .fillna(\"\")\n",
        "      .apply(lambda x: (\n",
        "          f'<a href=\"{ydna_overview}\" target=\"_blank\">{x}</a>'\n",
        "            if x.startswith(\"Y-\") else\n",
        "          f'<a href=\"{hap_base}{x}.htm\" target=\"_blank\">{x}</a>'\n",
        "            if x else\n",
        "          \"\"\n",
        "      ))\n",
        ")\n",
        "\n",
        "\n",
        "# ————— Load counts —————\n",
        "try:\n",
        "    with open(\"autosomal_count.txt\", \"r\") as f:\n",
        "        autosomal_count = int(f.read().strip())\n",
        "except Exception:\n",
        "    autosomal_count = None\n",
        "\n",
        "prev_count = None\n",
        "additional_str = \"\"\n",
        "if os.path.exists(\"autosomal_count_prev.txt\"):\n",
        "    try:\n",
        "        with open(\"autosomal_count_prev.txt\", \"r\") as f:\n",
        "            prev_count = int(f.read().strip())\n",
        "        if autosomal_count is not None and prev_count is not None:\n",
        "            diff_count = autosomal_count - prev_count\n",
        "            additional_str = f\" (+{diff_count} since last run)\"\n",
        "    except Exception:\n",
        "        additional_str = \"\"\n",
        "\n",
        "# ————— Get current Eastern time & formatted timestamp —————\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "updated_str = now.strftime(\"%d %B %Y at %H%M hours EDT\")\n",
        "\n",
        "# ————— XHTML Template —————\n",
        "full_html_template = \"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        "  \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" xml:lang=\"en\" lang=\"en\">\n",
        "<head>\n",
        "  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/>\n",
        "  <meta name=\"GENERATOR\" content=\"Yatesville\"/>\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/>\n",
        "  <title>DNA Report Card</title>\n",
        "  <script src=\"../sorttable.js\" type=\"text/javascript\"></script>\n",
        "  <style type=\"text/css\">\n",
        "    body { font-family: Arial, Helvetica, sans-serif; font-size: 14px; background-color: #faf9d3; }\n",
        "    .output-table table {\n",
        "      width:100%; border-collapse:collapse; margin:15px 0; background-color:#faf9d3;\n",
        "    }\n",
        "    .output-table table, .output-table th, .output-table td {\n",
        "      border:1px solid #333; text-align:center; padding:5px 8px; background-color:#faf9d3;\n",
        "    }\n",
        "    .output-table th { background-color:#ffffcc; white-space:nowrap; }\n",
        "    .output-table th:hover { background-color:#ffeb99; }\n",
        "    .output-table td:nth-child(6) { min-width:180px; }\n",
        "    .output-table td:last-child, .output-table th:last-child {\n",
        "      text-align:left; white-space:nowrap;\n",
        "    }\n",
        "    /* make the haplogroup (3rd) column wider by ~6 characters */\n",
        "    .output-table th:nth-child(3),\n",
        "    .output-table td:nth-child(3) {\n",
        "  min-width: 140px;\n",
        "}\n",
        "\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "<div align=\"center\">\n",
        "  <table class=\"fullpage-definedsection\" cellpadding=\"0\"><tr valign=\"top\"><td>\n",
        "    <table class=\"headersection\" cellpadding=\"0\"><tr valign=\"top\"><td></td></tr></table>\n",
        "    <table class=\"mainsection\" cellpadding=\"7\">\n",
        "      <tr valign=\"top\"><td>\n",
        "        <h2>A report card for your DNA family tree</h2>\n",
        "        <font size=\"-2\">\n",
        "          Return to <a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\">Study Home</a>\n",
        "          &nbsp;|&nbsp;\n",
        "          Autosomal matches: {autosomal_count}{additional_str}\n",
        "          &nbsp;|&nbsp;\n",
        "          Updated: {updated_timestamp}\n",
        "        </font>\n",
        "        <p>Imagine you have a report card for your family tree that tells you how your family tree compares to other collateral family tree lines.<br><br>Here is how we break it down:</p>\n",
        "        <p>Think of value like the total number of points you get from finding all the important family connections in your tree<br>\n",
        "        and comparing them to all the other trees included in the Yates study.</p>\n",
        "        <p>We then group them as a way to signal which ones seem to have potential for study:\n",
        "          <b>>60:</b> likely correct, <b>59–47:</b> lines forming, <b>46–34:</b> patterns emerging,\n",
        "          <b>33–21:</b> notable patterns, <b>20–8:</b> patterns stable, <b>7–1:</b> and 6-need research.</p>\n",
        "        <p><b><i><font size=\"-1\">Click on the header to sort any column</font></i></b>\n",
        "          (And, remember <a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\">what this is telling</a> us....)</p>\n",
        "\n",
        "      </td></tr>\n",
        "    </table>\n",
        "    <div class=\"output-table\" style=\"margin-top:10px;\">\n",
        "      <!-- TABLE_PLACEHOLDER -->\n",
        "    </div>\n",
        "  </td></tr></table>\n",
        "</div>\n",
        "<button onclick=\"topFunction()\" id=\"myBtn\" title=\"Go to top\"\n",
        "  style=\"position:fixed;bottom:40px;right:40px;z-index:99;background-color:red;color:white;\n",
        "         padding:12px 20px;border:none;border-radius:10px;cursor:pointer;font-size:16px;\">\n",
        "  Top\n",
        "</button>\n",
        "<script>\n",
        "let mybutton = document.getElementById(\"myBtn\");\n",
        "window.onscroll = function() {\n",
        "  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {\n",
        "    mybutton.style.display = \"block\";\n",
        "  } else {\n",
        "    mybutton.style.display = \"none\";\n",
        "  }\n",
        "};\n",
        "function topFunction() {\n",
        "  document.body.scrollTop = 0;\n",
        "  document.documentElement.scrollTop = 0;\n",
        "}\n",
        "</script>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "# ————— Build and inject table, counts, and timestamp —————\n",
        "final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Value Range\", \"Value Label\", \"Yates DNA Ancestral Line\"]\n",
        "df.sort_values(by=final_cols[-1], inplace=True)\n",
        "\n",
        "html_table = df.to_html(\n",
        "    index=False,\n",
        "    columns=final_cols,\n",
        "    escape=False,\n",
        "    classes=\"dataframe sortable\"\n",
        ")\n",
        "\n",
        "final_html = (\n",
        "    full_html_template\n",
        "    .replace(\"{autosomal_count}\", str(autosomal_count or \"Unknown\"))\n",
        "    .replace(\"{additional_str}\", additional_str)\n",
        "    .replace(\"{updated_timestamp}\", updated_str)\n",
        ")\n",
        "final_html = final_html.replace(\"<!-- TABLE_PLACEHOLDER -->\", html_table)\n",
        "\n",
        "# ————— Save to local files —————\n",
        "with open(\"HTML_combined_df_with_value_labels.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_html)\n",
        "\n",
        "with open(\"dna_cousin_surname_app.htm\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_html)\n",
        "\n",
        "# ————— FTP Upload to ROOT —————\n",
        "ftp_host = os.environ['FTP_HOST']\n",
        "ftp_port = int(os.environ.get('FTP_PORT', 21))\n",
        "ftp_user = os.environ['FTP_USER']\n",
        "ftp_pass = os.environ['FTP_PASS']\n",
        "\n",
        "def upload_to_root(filenames):\n",
        "    ftps = FTP_TLS()\n",
        "    ftps.connect(ftp_host, ftp_port)\n",
        "    ftps.login(ftp_user, ftp_pass)\n",
        "    ftps.prot_p()\n",
        "    for fname in filenames:\n",
        "        try:\n",
        "            ftps.delete(fname)\n",
        "        except Exception:\n",
        "            pass\n",
        "        with open(fname, 'rb') as f:\n",
        "            print(f\"→ uploading {fname} …\", end=' ')\n",
        "            ftps.storbinary(f\"STOR {fname}\", f)\n",
        "            print(\"done\")\n",
        "        try:\n",
        "            ftps.sendcmd(f\"SITE CHMOD 644 {fname}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    ftps.quit()\n",
        "    print(\"✅ All files uploaded to One Name Study.\")\n",
        "\n",
        "# Run upload\n",
        "upload_to_root([\"dna_cousin_surname_app.htm\"])\n",
        "\n",
        "# ————— Update previous autosomal count for next run —————\n",
        "if autosomal_count is not None:\n",
        "    with open(\"autosomal_count_prev.txt\", \"w\") as f:\n",
        "        f.write(str(autosomal_count))\n"
      ],
      "metadata": {
        "id": "3_8LbmegEYhU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "919a7122-4fd9-4c6e-e5dd-c4649d55ae33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ uploading dna_cousin_surname_app.htm … done\n",
            "✅ All files uploaded to One Name Study.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── CELL #3: Horizontal Y-DNA Grid Generation ────────────────────────────\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "# ── CONFIG ───────────────────────────────────────────────────────────────\n",
        "info_csv    = \"/content/haplogroup_info.csv\"\n",
        "user_csv    = \"/content/y_dna_user_detail.csv\"\n",
        "output_csv  = \"/content/y_dna_grid.csv\"\n",
        "output_html = \"/content/y_dna_grid.htm\"\n",
        "\n",
        "# ── 1) Load & clean haplogroup info ──────────────────────────────────────\n",
        "df_info = pd.read_csv(info_csv)\n",
        "# rename Date→Era if present\n",
        "if \"Date\" in df_info.columns:\n",
        "    df_info.rename(columns={\"Date\": \"Era\"}, inplace=True)\n",
        "# drop duplicate haplogroups\n",
        "df_info = df_info.drop_duplicates(subset=\"Haplogroup\", keep=\"first\")\n",
        "# build (hap, era) pairs in order\n",
        "pairs = list(zip(df_info[\"Haplogroup\"], df_info.get(\"Era\", [\"\"] * len(df_info))))\n",
        "\n",
        "# ── 2) Load user detail table ────────────────────────────────────────────\n",
        "df_users = pd.read_csv(user_csv)\n",
        "# ensure a User_ID column\n",
        "if \"User_ID\" not in df_users.columns:\n",
        "    df_users.rename(columns={df_users.columns[0]: \"User_ID\"}, inplace=True)\n",
        "\n",
        "# ── 3) Detect any extra haps in the user file ────────────────────────────\n",
        "user_haps = [c for c in df_users.columns if c != \"User_ID\"]\n",
        "extras    = [h for h in user_haps if h not in df_info[\"Haplogroup\"].tolist()]\n",
        "\n",
        "# ── 4) Append extras with blank eras, then remove any duplicated haps ────\n",
        "pairs.extend((h, \"\") for h in extras)\n",
        "seen = set()\n",
        "unique_pairs = []\n",
        "for h, e in pairs:\n",
        "    if h not in seen:\n",
        "        seen.add(h)\n",
        "        unique_pairs.append((h, e))\n",
        "hap_order, eras = zip(*unique_pairs)\n",
        "\n",
        "# ── 5) Ensure every hap in hap_order exists as column in df_users ───────\n",
        "for h in hap_order:\n",
        "    if h not in df_users.columns:\n",
        "        df_users[h] = \"\"\n",
        "\n",
        "# ── 6) Build the grid DataFrame ─────────────────────────────────────────\n",
        "df_grid = df_users[[\"User_ID\"] + list(hap_order)]\n",
        "\n",
        "# ── 7) Save CSV ─────────────────────────────────────────────────────────\n",
        "df_grid.to_csv(output_csv, index=False)\n",
        "print(f\"✅ Grid CSV saved to {output_csv}\")\n",
        "\n",
        "# ── 8) Generate HTML ────────────────────────────────────────────────────\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "ts  = now.strftime(\"%-m/%-d/%y, %-I:%M %p EDT\")\n",
        "\n",
        "html = [\n",
        "    \"<!DOCTYPE html>\",\n",
        "    \"<html><head><meta charset='UTF-8'><title>Yates Y-DNA Grid</title>\",\n",
        "    \"<style>\",\n",
        "    \" body { font-family: Arial, sans-serif; margin: 20px; }\",\n",
        "    \" table { border-collapse: collapse; width: 100%; table-layout: auto; }\",\n",
        "    \" th, td { border:1px solid #999; padding:6px; text-align:center; white-space:nowrap; }\",\n",
        "    \" th { background:#333; color:#fff; }\",\n",
        "    \" .era { background:#555; color:#ddd; font-size:0.9em; }\",\n",
        "    \" .blank { background:#ccc; color:#ccc; }\",\n",
        "    \"</style></head><body>\",\n",
        "    \"<h1 style='text-align:center;'>Yates Y-DNA Grid</h1>\",\n",
        "    \"<table>\",\n",
        "    # header row: User_ID + hap names\n",
        "    \"<tr><th>User_ID</th>\"\n",
        "    + \"\".join(f\"<th>{h}</th>\" for h in hap_order)\n",
        "    + \"</tr>\",\n",
        "    # era row\n",
        "    \"<tr><th>Era</th>\"\n",
        "    + \"\".join(f\"<th class='era'>{e}</th>\" for e in eras)\n",
        "    + \"</tr>\",\n",
        "]\n",
        "\n",
        "for _, row in df_grid.iterrows():\n",
        "    line = \"<tr><td style='font-weight:bold;'>\" + str(row[\"User_ID\"]) + \"</td>\"\n",
        "    for h in hap_order:\n",
        "        val = row[h]\n",
        "        # treat NaN or empty string as blank\n",
        "        blank = pd.isna(val) or str(val).strip() == \"\"\n",
        "        cls   = \"blank\" if blank else \"\"\n",
        "        cell  = \"–\" if blank else str(val)\n",
        "        line += f\"<td class='{cls}'>{cell}</td>\"\n",
        "    html.append(line + \"</tr>\")\n",
        "\n",
        "html.extend([\n",
        "    \"</table>\",\n",
        "    f\"<p style='text-align:right; font-size:0.8em;'>{ts}</p>\",\n",
        "    \"</body></html>\",\n",
        "])\n",
        "\n",
        "with open(output_html, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(html))\n",
        "print(f\"✅ Grid HTML saved to {output_html}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xecfLjgt4h-y",
        "outputId": "092949b3-43fa-43e6-d5e2-1673718db483"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Grid CSV saved to /content/y_dna_grid.csv\n",
            "✅ Grid HTML saved to /content/y_dna_grid.htm\n"
          ]
        }
      ]
    }
  ]
}