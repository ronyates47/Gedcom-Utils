{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNydPYHbt1Guz7HcUQYUTyY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/Gold__1_%26_2_%26_3_20251112_1700.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIP"
      ],
      "metadata": {
        "id": "XtvXRl-lcavJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "807f1029-58f4-4285-d895-e11450e594d3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "ERROR: unknown command \"caas_jupyter_tools\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n",
        "!pip caas_jupyter_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ron Rules-QUICK CODE CARD (v2025.10.27-Refined)\n",
        "# - Complete & runnable in Colab; ISO-8859-15 (ASCII-only in source).\n",
        "# - Punctuation in strings use HTML entities (&rsquo; &ldquo; &rdquo; &mdash; &rarr;).\n",
        "# - Deliver Python code (inline, executable, CUT-ready section)\n",
        "# - XHTML 1.0 Transitional; old-school friendly; Times New Roman body.\n",
        "# - Use CUT markers; five # spacer lines follow the STOP marker."
      ],
      "metadata": {
        "id": "g3hSp6RQHgPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CENTERLINE FIX — Update shared CSS + Upload =============================\n",
        "# RON RULES — QUICK CODE CARD (v2025.11.06-CSS-Centerline-Fix)\n",
        "# - Complete & runnable in Colab; ISO-8859-15 safe in source.\n",
        "# - Purpose: ensure \"Last updated …\" (meta) is centered site-wide via shared stylesheet.\n",
        "# - Actions: edit/insert CENTERLINE FIX block in dna_tree_styles.css and FTPS-upload to /partials/.\n",
        "\n",
        "import os, re, socket, posixpath, traceback\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try: os.environ['FTP_DIR']  = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR', '')\n",
        "    try: os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception: os.environ.setdefault('FTP_PORT', '21')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_DIR',  '')\n",
        "    os.environ.setdefault('FTP_PORT', '21')\n",
        "\n",
        "FTP_DIR   = (os.environ.get(\"FTP_DIR\",\"\") or \"\").strip()\n",
        "LOCAL_CSS = \"dna_tree_styles.css\"\n",
        "REMOTE_CSS = posixpath.join(\"partials\", \"dna_tree_styles.css\")\n",
        "\n",
        "# ---------- 1) Ensure local css exists (create if missing) ----------\n",
        "if not os.path.exists(LOCAL_CSS):\n",
        "    with open(LOCAL_CSS, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"/* yates.one-name.net shared stylesheet */\\n\")\n",
        "    print(\"[INFO] Created new local stylesheet:\", os.path.abspath(LOCAL_CSS))\n",
        "else:\n",
        "    print(\"[OK] Using existing local stylesheet:\", os.path.abspath(LOCAL_CSS))\n",
        "\n",
        "# ---------- 2) Insert/replace CENTERLINE FIX block ----------\n",
        "with open(LOCAL_CSS, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    css = f.read()\n",
        "\n",
        "start_tag = \"/* === CENTERLINE FIX (do not remove) START === */\"\n",
        "end_tag   = \"/* === CENTERLINE FIX (do not remove) END === */\"\n",
        "block_re  = re.compile(re.escape(start_tag) + r\".*?\" + re.escape(end_tag), flags=re.S)\n",
        "\n",
        "center_block = f\"\"\"{start_tag}\n",
        ":root {{\n",
        "  /* nothing here yet; reserved for future tokens */\n",
        "}}\n",
        "/* Utility: hard center text, wins against generic .meta rules */\n",
        ".centerline {{ text-align: center !important; }}\n",
        "\n",
        "/* Meta display: keep centered across pages, even if other CSS sets .meta {{text-align:left}} */\n",
        ".wrap .meta, .intro .meta, .updated, .updated.centerline {{\n",
        "  text-align: center !important;\n",
        "}}\n",
        "\n",
        "/* Table meta stamping alignment on partials and main pages */\n",
        "#last-updated, #auto-count, #showing-count {{ /* inline metrics */ }}\n",
        "{end_tag}\n",
        "\"\"\"\n",
        "\n",
        "if block_re.search(css):\n",
        "    css = block_re.sub(center_block, css)\n",
        "    action = \"[OK] Replaced existing CENTERLINE FIX block.\"\n",
        "else:\n",
        "    # Append with a divider so it overrides earlier rules\n",
        "    css += \"\\n\\n/* ------------------------------------------------------------------ */\\n\" + center_block + \"\\n\"\n",
        "    action = \"[OK] Appended CENTERLINE FIX block.\"\n",
        "\n",
        "with open(LOCAL_CSS, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(css)\n",
        "\n",
        "print(action)\n",
        "\n",
        "# ---------- 3) Upload via FTPS ----------\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps.connect(os.environ.get('FTP_HOST',''), int(os.environ.get('FTP_PORT','21')))\n",
        "    ftps.login(os.environ.get('FTP_USER',''), os.environ.get('FTP_PASS',''))\n",
        "    try: ftps.prot_p()\n",
        "    except Exception: pass\n",
        "    try: ftps.set_pasv(True)\n",
        "    except Exception: pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try: ftps.mkd(p)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def ensure_remote_dirs(ftps: FTP_TLS, path: str):\n",
        "    parts = [p for p in path.split(\"/\")[:-1] if p]\n",
        "    for seg in parts:\n",
        "        try: ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try: ftps.mkd(seg)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "if all(os.environ.get(k) for k in (\"FTP_HOST\",\"FTP_USER\",\"FTP_PASS\")):\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        ensure_remote_dirs(ftps, REMOTE_CSS)\n",
        "        with open(LOCAL_CSS, \"rb\") as fh:\n",
        "            ftps.storbinary(f\"STOR {REMOTE_CSS}\", fh)\n",
        "        try: sz = ftps.size(REMOTE_CSS)\n",
        "        except Exception: sz = None\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "        print(f\"[PUT] {LOCAL_CSS} -> /{REMOTE_CSS}  (size: {sz if sz is not None else 'unknown'})\")\n",
        "        print(\"Cache-bust tip: add ?v=\" + datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") + \" once if needed.\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTPS upload:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[SKIP] Missing FTP creds; upload not attempted.\")\n",
        "\n",
        "print(\"\\nNext step:\")\n",
        "print(\"1) In Cell 3 template, ensure the meta block uses class 'centerline':\")\n",
        "print(\"   <div class=\\\"meta centerline\\\">Last updated: <span id=\\\"last-updated\\\"></span> ...</div>\")\n",
        "print(\"   (Cell 2 already uses 'updated centerline'.)\")\n",
        "print(\"2) Re-run Cell 3 to regenerate HTML, or refresh the page with ?v=1.\")\n",
        "# ====== CUT STOP  [1/1] CENTERLINE FIX — Update shared CSS + Upload =============================\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zPfncKCDLVk",
        "outputId": "ea38d46c-3ab9-4264-c4f8-60f06ccdf8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Created new local stylesheet: /content/dna_tree_styles.css\n",
            "[OK] Appended CENTERLINE FIX block.\n",
            "[FAIL] FTPS upload: 553 Can't open that file: No such file or directory\n",
            "\n",
            "Next step:\n",
            "1) In Cell 3 template, ensure the meta block uses class 'centerline':\n",
            "   <div class=\"meta centerline\">Last updated: <span id=\"last-updated\"></span> ...</div>\n",
            "   (Cell 2 already uses 'updated centerline'.)\n",
            "2) Re-run Cell 3 to regenerate HTML, or refresh the page with ?v=1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-992470288.py\", line 109, in <cell line: 0>\n",
            "    ftps.storbinary(f\"STOR {REMOTE_CSS}\", fh)\n",
            "  File \"/usr/lib/python3.12/ftplib.py\", line 495, in storbinary\n",
            "    with self.transfercmd(cmd, rest) as conn:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ftplib.py\", line 393, in transfercmd\n",
            "    return self.ntransfercmd(cmd, rest)[0]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ftplib.py\", line 771, in ntransfercmd\n",
            "    conn, size = super().ntransfercmd(cmd, rest)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ftplib.py\", line 359, in ntransfercmd\n",
            "    resp = self.sendcmd(cmd)\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ftplib.py\", line 281, in sendcmd\n",
            "    return self.getresp()\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/ftplib.py\", line 254, in getresp\n",
            "    raise error_perm(resp)\n",
            "ftplib.error_perm: 553 Can't open that file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL — iOS Font/Spacing CSS Fix + Cache-Busted Reupload =================\n",
        "# RON RULES — QUICK CODE CARD (v2025.11.07-iOS-CSS-Fix)\n",
        "# - Complete & runnable in Colab; ISO-8859-15 (ASCII-only in source).\n",
        "# - No snippets; no fabrication; explicit prints.\n",
        "# - Actions:\n",
        "#   (1) Download /partials/dna_tree_styles.css, append a minimal iOS fix block (safe, idempotent).\n",
        "#   (2) Upload patched css back to /partials/dna_tree_styles.css.\n",
        "#   (3) For selected HTML pages in /partials/, download, inject/refresh '?v=TIMESTAMP' on\n",
        "#       '/partials/dna_tree_styles.css' link, and upload back (cache-bust so iPhone fetches new CSS).\n",
        "# - This cell does NOT change any other content; only the CSS and the cache-buster query on the CSS link.\n",
        "\n",
        "import os, re, io, posixpath, socket, traceback, datetime\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try: os.environ['FTP_DIR']  = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR', '')\n",
        "    try: os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception: os.environ.setdefault('FTP_PORT', '21')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_DIR',  '')\n",
        "    os.environ.setdefault('FTP_PORT', '21')\n",
        "\n",
        "FTP_DIR     = (os.environ.get(\"FTP_DIR\",\"\") or \"\").strip()\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get('FTP_HOST',''), int(os.environ.get('FTP_PORT', 21)))\n",
        "    ftps.login(os.environ.get('FTP_USER',''), os.environ.get('FTP_PASS',''))\n",
        "    try: ftps.prot_p()\n",
        "    except Exception: pass\n",
        "    try: ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception: pass\n",
        "    if FTP_DIR:\n",
        "        parts = [p for p in FTP_DIR.split(\"/\") if p]\n",
        "        for p in parts:\n",
        "            try: ftps.mkd(p)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps: FTP_TLS, remote_path: str):\n",
        "    if \"/\" not in remote_path: return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try: ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try: ftps.mkd(seg)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download(ftps: FTP_TLS, remote_name: str) -> bytes:\n",
        "    buf = io.BytesIO()\n",
        "    ftps.retrbinary(f\"RETR {remote_name}\", buf.write)\n",
        "    return buf.getvalue()\n",
        "\n",
        "def ftp_download_if_exists(ftps: FTP_TLS, remote_name: str) -> bytes:\n",
        "    try:\n",
        "        return ftp_download(ftps, remote_name)\n",
        "    except Exception as e:\n",
        "        raise FileNotFoundError(f\"Missing remote file: {remote_name} ({e})\")\n",
        "\n",
        "def ftp_upload_overwrite(ftps: FTP_TLS, content_bytes: bytes, remote_name: str):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    bio = io.BytesIO(content_bytes)\n",
        "    ftps.storbinary(f\"STOR {remote_name}\", bio)\n",
        "\n",
        "def ftp_size(ftps: FTP_TLS, remote_name: str):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 1) Targets ----------\n",
        "CSS_REMOTE = \"partials/dna_tree_styles.css\"\n",
        "\n",
        "# HTML pages where we refresh the cache-buster on the CSS link.\n",
        "HTML_TARGETS = [\n",
        "    \"partials/yates_ancestor_register.htm\",\n",
        "    \"partials/ons_yates_dna_register.htm\",\n",
        "    \"partials/justdna.htm\",\n",
        "    \"partials/just-trees.htm\",\n",
        "    \"partials/yates_ancestor_register_plus.htm\",\n",
        "    \"partials/work_plus.htm\",\n",
        "    \"partials/match_count.htm\",\n",
        "    \"partials/lineage_count.htm\",\n",
        "    \"partials/cousin_list_print.htm\",\n",
        "]\n",
        "\n",
        "# ---------- 2) Patch builder (idempotent) ----------\n",
        "def patch_css_for_ios(css_text: str) -> str:\n",
        "    # Minimal iOS-friendly adjustments. We wrap in a clearly-marked block; re-running does not duplicate.\n",
        "    # - table.sortable th/td line-height and word-break normalization\n",
        "    # - -webkit-text-size-adjust safeguard\n",
        "    block_header = \"/* --- iOS font/spacing fix (2025-11-07) --- */\"\n",
        "    block_rules  = (\n",
        "        \"html, body { -webkit-text-size-adjust:100%; }\\n\"\n",
        "        \"table.sortable th, table.sortable td { line-height:1.5; word-break:normal; }\\n\"\n",
        "    )\n",
        "    if block_header in css_text:\n",
        "        # Already present; optionally refresh the block content in place to ensure exact rules.\n",
        "        rx = re.compile(r\"/\\*\\s*--- iOS font/spacing fix .*?--- \\*/\\s*[^/]*?(?=$|/\\*)\", re.S)\n",
        "        if rx.search(css_text):\n",
        "            css_text = rx.sub(block_header + \"\\n\" + block_rules, css_text, count=1)\n",
        "        else:\n",
        "            # Edge case: header string present but regex did not match; append once.\n",
        "            css_text = css_text.rstrip() + \"\\n\\n\" + block_header + \"\\n\" + block_rules\n",
        "    else:\n",
        "        css_text = css_text.rstrip() + \"\\n\\n\" + block_header + \"\\n\" + block_rules\n",
        "    return css_text\n",
        "\n",
        "def refresh_css_cache_buster(html_text: str, ts: str) -> str:\n",
        "    # Replace any existing query (?v=...) on /partials/dna_tree_styles.css with the new timestamp.\n",
        "    # Handles single/double-quoted attributes.\n",
        "    def _repl(m):\n",
        "        prefix = m.group(1)  # href=\" or href='\n",
        "        path   = m.group(2)  # /partials/dna_tree_styles.css\n",
        "        return f'{prefix}{path}?v={ts}\"'\n",
        "    # Normalize to double-quote for replacement safety.\n",
        "    html_text = html_text.replace(\"'\", '\"')\n",
        "    pat = re.compile(r'(href=\")(/partials/dna_tree_styles\\.css)(?:\\?[^\"]*)?\"', re.I)\n",
        "    if pat.search(html_text):\n",
        "        html_text = pat.sub(_repl, html_text)\n",
        "    else:\n",
        "        # If a link tag exists without direct match, try to inject one in <head> (rare fallback).\n",
        "        head_pat = re.compile(r\"<head[^>]*>\", re.I)\n",
        "        if head_pat.search(html_text):\n",
        "            link_tag = f'<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/dna_tree_styles.css?v={ts}\" />'\n",
        "            html_text = head_pat.sub(lambda m: m.group(0) + \"\\n\" + link_tag, html_text, count=1)\n",
        "    return html_text\n",
        "\n",
        "# ---------- 3) Execute patch + uploads ----------\n",
        "def run_fix():\n",
        "    if not all(os.environ.get(k) for k in ['FTP_HOST','FTP_USER','FTP_PASS']):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "\n",
        "    ts = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
        "    print(\"[INFO] Cache-buster timestamp:\", ts)\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        # Patch CSS\n",
        "        print(\"[STEP] Download existing CSS:\", \"/\" + _remote_path(CSS_REMOTE))\n",
        "        css_bytes = ftp_download_if_exists(ftps, _remote_path(CSS_REMOTE))\n",
        "        try:\n",
        "            css_text = css_bytes.decode(\"utf-8\")\n",
        "        except Exception:\n",
        "            css_text = css_bytes.decode(\"iso-8859-15\", errors=\"ignore\")\n",
        "\n",
        "        patched = patch_css_for_ios(css_text)\n",
        "        # Keep CSS ASCII to maintain ISO-8859-15 safety; rules are ASCII-only.\n",
        "        patched_bytes = patched.encode(\"ascii\", errors=\"ignore\")\n",
        "        ftp_upload_overwrite(ftps, patched_bytes, _remote_path(CSS_REMOTE))\n",
        "        print(\"[OK] Uploaded patched CSS -> /\" + _remote_path(CSS_REMOTE))\n",
        "\n",
        "        # Touch each HTML to refresh the cache-buster on the CSS link.\n",
        "        for rel in HTML_TARGETS:\n",
        "            rp = _remote_path(rel)\n",
        "            try:\n",
        "                html_bytes = ftp_download(ftps, rp)\n",
        "            except Exception as e:\n",
        "                print(f\"[MISS] {rp} ({e}); skipping.\")\n",
        "                continue\n",
        "            # Decode as ISO-8859-15 first (your pages are authored that way), fallback to utf-8.\n",
        "            try:\n",
        "                html_text = html_bytes.decode(\"iso-8859-15\")\n",
        "            except Exception:\n",
        "                html_text = html_bytes.decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "            updated = refresh_css_cache_buster(html_text, ts)\n",
        "            # Write back in ISO-8859-15 with entity fallback.\n",
        "            out_bytes = updated.encode(\"iso-8859-15\", errors=\"xmlcharrefreplace\")\n",
        "            ftp_upload_overwrite(ftps, out_bytes, rp)\n",
        "            print(\"[PUT]  cache-busted:\", \"/\" + rp)\n",
        "\n",
        "        # Size check (best-effort)\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [ _remote_path(CSS_REMOTE) ] + [ _remote_path(h) for h in HTML_TARGETS ]:\n",
        "            try:\n",
        "                sz = ftp_size(ftps, p)\n",
        "                print(f\"/{p} : {sz if sz is not None else '(SIZE unsupported)'}\")\n",
        "            except Exception:\n",
        "                print(f\"/{p} : (check skipped)\")\n",
        "\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "\n",
        "        print(\"\\n--- Open a couple of pages (manual hard-refresh optional) ---\")\n",
        "        print(\"DNA Register:          https://yates.one-name.net/partials/yates_ancestor_register.htm\")\n",
        "        print(\"Register PLUS:         https://yates.one-name.net/partials/yates_ancestor_register_plus.htm\")\n",
        "        print(\"Trees:                 https://yates.one-name.net/partials/just-trees.htm\")\n",
        "        print(\"Shared stylesheet:     https://yates.one-name.net/partials/dna_tree_styles.css\")\n",
        "        print(\"\\nIf an iPhone still shows old spacing, try a hard-refresh (Safari: tap-reload or clear cache).\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] Session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "run_fix()\n",
        "# ====== CUT STOP  [1/1] CELL ====================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Inck5IHqXqQu",
        "outputId": "be610741-668e-46de-bc45-e384de77478c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3198153314.py:154: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  ts = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Cache-buster timestamp: 20251107221012\n",
            "[STEP] Download existing CSS: /partials/dna_tree_styles.css\n",
            "[OK] Uploaded patched CSS -> /partials/dna_tree_styles.css\n",
            "[PUT]  cache-busted: /partials/yates_ancestor_register.htm\n",
            "[PUT]  cache-busted: /partials/ons_yates_dna_register.htm\n",
            "[PUT]  cache-busted: /partials/justdna.htm\n",
            "[PUT]  cache-busted: /partials/just-trees.htm\n",
            "[PUT]  cache-busted: /partials/yates_ancestor_register_plus.htm\n",
            "[PUT]  cache-busted: /partials/work_plus.htm\n",
            "[PUT]  cache-busted: /partials/match_count.htm\n",
            "[PUT]  cache-busted: /partials/lineage_count.htm\n",
            "[PUT]  cache-busted: /partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "/partials/dna_tree_styles.css : 3814\n",
            "/partials/yates_ancestor_register.htm : 1267364\n",
            "/partials/ons_yates_dna_register.htm : 1267364\n",
            "/partials/justdna.htm : 1267364\n",
            "/partials/just-trees.htm : 794896\n",
            "/partials/yates_ancestor_register_plus.htm : 1343103\n",
            "/partials/work_plus.htm : 1267364\n",
            "/partials/match_count.htm : 18955\n",
            "/partials/lineage_count.htm : 47782\n",
            "/partials/cousin_list_print.htm : 499776\n",
            "\n",
            "--- Open a couple of pages (manual hard-refresh optional) ---\n",
            "DNA Register:          https://yates.one-name.net/partials/yates_ancestor_register.htm\n",
            "Register PLUS:         https://yates.one-name.net/partials/yates_ancestor_register_plus.htm\n",
            "Trees:                 https://yates.one-name.net/partials/just-trees.htm\n",
            "Shared stylesheet:     https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "If an iPhone still shows old spacing, try a hard-refresh (Safari: tap-reload or clear cache).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "JvOlmbj91AGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 1 — GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ===\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.09)\n",
        "# • Complete & runnable Colab Cell — one contiguous block (no fragments, no cross-refs).\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; typography via /partials/dna_tree_styles.css (HTML export only).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.09 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes; DECLARED_LINES printed at start.\n",
        "# ==============================================================================================\n",
        "\n",
        "import os, re, glob, logging, functools, socket, traceback, hashlib\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "from string import Template\n",
        "\n",
        "CELL_NAME = \"Cell1_FTPS_Explicit\"\n",
        "VERSION   = \"2025.11.09\"\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=%s | Version=%s | Encoding=ISO-8859-15\" % (CELL_NAME, VERSION))\n",
        "\n",
        "# ---------- Logging ----------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(CELL_NAME)\n",
        "\n",
        "# ---------- Secrets (env or userdata) ----------\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\",\"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\",\"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\",\"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\",\"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\",\"\") or \"\").strip().strip(\"/\")\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s: return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep*2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\" %\n",
        "      (_mask(FTP_HOST), _mask(FTP_USER,2), \"***\", FTP_PORT, (\"/\"+FTP_DIR) if FTP_DIR else \"(root)\"))\n",
        "\n",
        "# ---------- FTPS (Explicit AUTH TLS) ----------\n",
        "def _ftps_connect():\n",
        "    if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "        raise RuntimeError(\"Missing FTP_HOST/FTP_USER/FTP_PASS.\")\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()                 # Explicit FTPS: AUTH TLS before login\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try: ftps.prot_p()          # Encrypt data channel\n",
        "    except Exception: pass\n",
        "    try: ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception: pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path: return\n",
        "    for p in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(p)\n",
        "        except all_errors:\n",
        "            try: ftps.mkd(p)\n",
        "            except all_errors: pass\n",
        "            ftps.cwd(p)\n",
        "\n",
        "def _ftps_upload(ftps, local_path, remote_name):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR \" + remote_name, fh)\n",
        "    print(\"[OK] Uploaded: %s -> %s/%s\" % (local_path, ftps.pwd().rstrip(\"/\"), remote_name))\n",
        "\n",
        "# ---------- Outputs / Paths ----------\n",
        "REMOTE_DIR       = \"partials\"\n",
        "CSV_OUT_LOCAL    = \"final_combined_df_with_value_labels.csv\"\n",
        "HTML_OUT_LOCAL   = \"cell1_work_table.htm\"\n",
        "ABS_CSV_URL      = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(CSV_OUT_LOCAL))\n",
        "ABS_HOME_URL     = \"/index.htm\"\n",
        "\n",
        "# ---------- Minimal GEDCOM parse helpers ----------\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '') or ''\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0] if parts else \"\"\n",
        "        last_name  = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1; anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "    def get_extractable_NPFX(self): return self.extractable_detail.get('NPFX','') or ''\n",
        "    def get_extractable_cm(self):\n",
        "        v = self.extractable_detail.get('NPFX','') or ''\n",
        "        if '&' in v: cm = v.split('&')[0].strip()\n",
        "        elif '**' in v: cm = v.split('**')[0].strip()\n",
        "        else: cm = v.strip()\n",
        "        try: int(cm); return cm\n",
        "        except Exception: return ''\n",
        "    def get_extractable_sort(self):\n",
        "        v = self.extractable_detail.get('NPFX','') or ''\n",
        "        if '&' in v:\n",
        "            s = v.split('&')[1]\n",
        "            return (s.split('**')[0] if '**' in s else s).strip()\n",
        "        return ''\n",
        "    def get_extractable_YDNA(self):\n",
        "        v = self.extractable_detail.get('NPFX','') or ''\n",
        "        return v.split('**')[1].strip() if '**' in v else ''\n",
        "    def get_extractable_FAMC(self): return (self.extractable_detail.get('FAMC','') or '').strip('@')\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "        current = None\n",
        "        npfx_count = ydna_count = total = 0\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0]); tag = parts[1]; value = parts[2] if len(parts) > 2 else None\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total += 1; current = GedcomDataset(tag); self.gedcom_datasets.append(current)\n",
        "            elif current is not None:\n",
        "                if level == 1 and tag in ['NAME','FAMC']:\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1; current.add_extractable_detail(tag, value)\n",
        "                    if value and '**' in value: ydna_count += 1\n",
        "        autosomal = npfx_count - ydna_count\n",
        "        print(\"GEDCOM contained %d total records\" % total)\n",
        "        print(\"Records tagged and filtered by NPFX: %d\" % npfx_count)\n",
        "        print(\"Records with YDNA information: %d\" % ydna_count)\n",
        "        print(\"Autosomal matches: %d\" % autosomal)\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "        try:\n",
        "            df_filter = pd.read_excel('filtered_ids.xlsx')\n",
        "            manual_ids = set(str(x) for x in df_filter['ID'])\n",
        "            self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_ids]\n",
        "            print(\"After manual filter, total records: %d\" % len(self.filter_pool))\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "        return autosomal\n",
        "\n",
        "def _chunks(lst, n):\n",
        "    for i in range(0, len(lst), n): yield lst[i:i+n]\n",
        "\n",
        "def _quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"): idx = 0\n",
        "        else: return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start); end = len(full_text) if end == -1 else end\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line: return name_line[:10].replace(\" \",\"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \",\"\") + first_name[:10].replace(\" \",\"\")\n",
        "\n",
        "def _find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map: return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id: return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id: _find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id: _find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def _find_distant(individual_id, parents_map, path=None):\n",
        "    if path is None: path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map: return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id: return [path]\n",
        "    paths = []\n",
        "    if father_id: paths.extend(_find_distant(father_id, parents_map, path[:]))\n",
        "    if mother_id: paths.extend(_find_distant(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "def _filter_lineage(winning_ids, gen_table, names_map):\n",
        "    matching = []\n",
        "    for generation, pair in gen_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_ids or id2 in winning_ids:\n",
        "            matching.append((generation, pair))\n",
        "    matching.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for _, pair in matching:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(\"%s&%s\" % (name_pair[0], name_pair[1]))\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "def _process_record(individual_id, ged, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []; visited_pairs = set()\n",
        "    _find_parents(individual_id, 1, parents_map)\n",
        "    paths = _find_distant(individual_id, parents_map)\n",
        "    best_score, best_path = None, None\n",
        "    for path in paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx+1) for idx, nm in enumerate(name_path) if 'Yates' in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score, best_path = score, path\n",
        "    best_path = best_path or []\n",
        "    best_ids  = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str  = _filter_lineage(set(best_ids), generation_table, names_map)\n",
        "    cm_value=''; sort_value=''; ydna_value=''\n",
        "    for ds in ged.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value  = ds.get_extractable_cm()\n",
        "            sort_value= ds.get_extractable_sort()\n",
        "            ydna_value= ds.get_extractable_YDNA()\n",
        "            break\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "# ---------- Main build ----------\n",
        "def main():\n",
        "    # Audit: DECLARED_LINES\n",
        "    import inspect\n",
        "    try:\n",
        "        declared = len(inspect.getsource(main).splitlines())\n",
        "    except Exception:\n",
        "        declared = -1\n",
        "    print(\"[AUDIT] DECLARED_LINES=%s\" % str(declared))\n",
        "\n",
        "    files = glob.glob(\"*.ged\")\n",
        "    if not files:\n",
        "        print(\"No GEDCOM files found.\"); return False\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    gedcom_path = files[0]\n",
        "\n",
        "    ged = Gedcom(gedcom_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "\n",
        "    with open(\"autosomal_count.txt\", \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "\n",
        "    with open(gedcom_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk: continue\n",
        "        flend = blk.find('\\n'); flend = len(blk) if flend == -1 else flend\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            s = first_line.find('@') + 1\n",
        "            e = first_line.find('@', s)\n",
        "            rec_id = first_line[s:e].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map, names_map, families = {}, {}, {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            husb_id = txt[father_idx+len('1 HUSB @'):txt.find('@', father_idx+len('1 HUSB @'))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            wife_id = txt[wife_idx+len('1 WIFE @'):txt.find('@', wife_idx+len('1 WIFE @'))] if wife_idx != -1 else None\n",
        "            kids = [ln.split('@')[1] for ln in txt.split('\\n') if ln.strip().startswith('1 CHIL @')]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "    for rec_id, txt in all_records.items():\n",
        "        names_map[rec_id] = _quick_extract_name(\"\\n\" + txt)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(\"Processing %d individuals with chunk-based parallel...\" % len(individual_ids))\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    from functools import partial\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as ex, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in _chunks(individual_ids, chunk_size):\n",
        "            func = partial(_process_record, ged=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(ex.map(func, chunk))\n",
        "            combined_rows.extend(results); pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def _trim_prefix(row):\n",
        "        prefix = \"YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&FauconerElizabeth~~~\"\n",
        "        s = str(row[\"Yates DNA Ancestral Line\"])\n",
        "        if s.startswith(prefix): row[\"Yates DNA Ancestral Line\"] = s[len(prefix):]\n",
        "        return row\n",
        "    df = df.apply(_trim_prefix, axis=1)\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "\n",
        "    # CSV (ISO-8859-15 as required)\n",
        "    with open(CSV_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df.to_csv(index=False))\n",
        "    logger.info(\"Exported CSV -> %s\", CSV_OUT_LOCAL)\n",
        "\n",
        "    # HTML (XHTML 1.0 Transitional; Times via external CSS is implied; inline minimal styles ok)\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Yates DNA Ancestral Line\"]\n",
        "    table_html = df.to_html(index=False, columns=final_cols, escape=False, border=1)\n",
        "    page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Cell 1 Working Table</title>\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/dna_tree_styles.css\" />\n",
        "<style type=\"text/css\">\n",
        "  html { scroll-behavior: smooth; }\n",
        "  body { background:#ffffff; color:#222; margin:0; padding:20px; }\n",
        "  h1 { margin:0 0 8px 0; font-size:24px; line-height:1.2; text-align:center; }\n",
        "  .meta { text-align:center; font-size:12px; color:#555; margin:2px 0 12px 0; }\n",
        "  .downloads { text-align:center; margin:4px 0 12px 0; font-size:13px; }\n",
        "  a { color:#154b8b; text-decoration:none; } a:hover { text-decoration:underline; }\n",
        "  table { width:100%; border-collapse:collapse; }\n",
        "  th, td { border:1px solid #333; padding:6px 8px; vertical-align:top; }\n",
        "  th { background:#e3eaf8; text-align:left; }\n",
        "  td:nth-child(5) { text-align:left; white-space:normal; }\n",
        "</style>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){ function z(n){return (n<10?'0':'')+n;}\n",
        "document.addEventListener('DOMContentLoaded', function(){\n",
        "  var el = document.getElementById('last-updated');\n",
        "  if(el){ var d=new Date(document.lastModified||new Date());\n",
        "    el.innerHTML = d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes()); }\n",
        "}, false); })();\n",
        "//]]>\n",
        "</script>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>Cell 1 Working Table</h1>\n",
        "  <div class=\"meta\">\n",
        "    <a href=\"$HOME\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "    &nbsp;|&nbsp; Last updated: <span id=\"last-updated\"></span>\n",
        "    &nbsp;|&nbsp; Download: <a href=\"$CSV\">$CSV</a>\n",
        "  </div>\n",
        "  <div class=\"downloads\"><a href=\"$CSV\">/partials/$CSV_NAME</a></div>\n",
        "  $TABLE\n",
        "</body>\n",
        "</html>\"\"\")\n",
        "    page = page_tpl.safe_substitute(HOME=ABS_HOME_URL, CSV=ABS_CSV_URL, CSV_NAME=os.path.basename(ABS_CSV_URL), TABLE=table_html)\n",
        "    with open(HTML_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(page)\n",
        "    logger.info(\"Exported HTML -> %s\", HTML_OUT_LOCAL)\n",
        "\n",
        "    return True\n",
        "\n",
        "ok = main()\n",
        "\n",
        "# ---------- Upload to /partials (Explicit FTPS AUTH TLS) ----------\n",
        "if ok and FTP_HOST and FTP_USER and FTP_PASS:\n",
        "    print(\"[INFO] Uploading artifacts to /partials/ ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        _ftps_ensure_dir(ftps, \"partials\")\n",
        "        try: _ftps_upload(ftps, CSV_OUT_LOCAL, os.path.basename(CSV_OUT_LOCAL))\n",
        "        except Exception as e: print(\"[ERROR] CSV upload failed:\", e)\n",
        "        try: _ftps_upload(ftps, HTML_OUT_LOCAL, os.path.basename(HTML_OUT_LOCAL))\n",
        "        except Exception as e: print(\"[ERROR] HTML upload failed:\", e)\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "        print(\"[OK] Uploads complete to /partials/\")\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing creds or build failed).\")\n",
        "\n",
        "print(\"\\n--- Cell 1 Complete: CSV + HTML built and handled with ISO-8859-15; explicit FTPS used. ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 1 — GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ===\n"
      ],
      "metadata": {
        "id": "qWEuviY1aQQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b0373d-7087-4f88-d24c-d3e87f34d0d0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.09 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[AUDIT] DECLARED_LINES=133\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 62312 total records\n",
            "Records tagged and filtered by NPFX: 1572\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches: 1572\n",
            "After manual filter, total records: 7\n",
            "Processing 7 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 7/7 [00:03<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploads complete to /partials/\n",
            "\n",
            "--- Cell 1 Complete: CSV + HTML built and handled with ISO-8859-15; explicit FTPS used. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "s1Oa3qUz0_Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2 — Build + Publish DNA Register (All styling via stylesheet) ======\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.12)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; ALL styling comes from /partials/dna_tree_styles.css (this cell writes it).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes; DECLARED_LINES printed at run start.\n",
        "\n",
        "DECLARED_LINES = 9999\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\")\n",
        "print(\"DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "import os, re, posixpath, socket, traceback, urllib.parse as _u\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try: os.environ['FTP_DIR']  = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR','')\n",
        "    try: os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception: os.environ.setdefault('FTP_PORT','21')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST',''); os.environ.setdefault('FTP_USER',''); os.environ.setdefault('FTP_PASS','')\n",
        "    os.environ.setdefault('FTP_DIR','');  os.environ.setdefault('FTP_PORT','21')\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "LOCAL_HTML = \"yates_ancestor_register.htm\"\n",
        "REMOTE_HTML_CANON = posixpath.join(\"partials\",\"yates_ancestor_register.htm\")\n",
        "REMOTE_HTML_LEG   = posixpath.join(\"partials\",\"ons_yates_dna_register.htm\")\n",
        "REMOTE_HTML_SIMPLE= posixpath.join(\"partials\",\"justdna.htm\")\n",
        "\n",
        "DNA_REGISTER_ABS = \"https://yates.one-name.net/partials/justdna.htm\"\n",
        "TREES_ABS        = \"https://yates.one-name.net/partials/just-trees.htm\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV  = f\"{EXPORT_BASENAME}.csv\"\n",
        "LOCAL_XLSX = f\"{EXPORT_BASENAME}.xlsx\"\n",
        "REMOTE_CSV = posixpath.join(\"partials\", os.path.basename(LOCAL_CSV))\n",
        "REMOTE_XLSX= posixpath.join(\"partials\", os.path.basename(LOCAL_XLSX))\n",
        "\n",
        "WORK_PLUS_LOCAL  = os.path.join(\"partials\",\"work_plus.htm\")\n",
        "WORK_PLUS_REMOTE = posixpath.join(\"partials\",\"work_plus.htm\")\n",
        "\n",
        "LOCAL_COUNT_FILE = \"/content/autosomal_count.txt\"\n",
        "REMOTE_COUNT_NAME= \"autosomal_count.txt\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\",\"\") or \"\").strip()\n",
        "TNG_BASE=\"https://yates.one-name.net/tng\"; TNG_TREE=\"tree1\"\n",
        "HOME_URL=\"https://yates.one-name.net/partials/yates_ancestor_register.htm\"\n",
        "REMOTE_NAME_ABS = HOME_URL\n",
        "COUNT_PUBLIC_URL = (f\"/{FTP_DIR}/{REMOTE_COUNT_NAME}\" if FTP_DIR else f\"/{REMOTE_COUNT_NAME}\")\n",
        "\n",
        "# Layout knobs (used by CSS variables below)\n",
        "TABLE_WIDTH_PX=5550; COL_A_PX=900; FIND_COL_PX=25\n",
        "ARROW_ENTITY=\"&rarr;\"; REMOVE_PERIOD_AT_END=True\n",
        "\n",
        "SERVER_PARTIALS_DIR=\"partials\"\n",
        "SERVER_MAPPING_BASENAME=\"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE=posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE=\"match_to_unmasked.server.csv\"\n",
        "\n",
        "# Shared stylesheet (this cell WRITES it; all styles live here)\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "STYLESHEET_LOCAL = os.path.join(\"partials\", STYLESHEET_BASENAME)\n",
        "STYLESHEET_REMOTE = posixpath.join(\"partials\", STYLESHEET_BASENAME)\n",
        "CSS_VERSION = \"v2025-11-12-max\"  # cache-bust\n",
        "STYLESHEET_HREF=f\"/partials/{STYLESHEET_BASENAME}?{CSS_VERSION}\"\n",
        "HEAD_LINK = f'<link rel=\"stylesheet\" type=\"text/css\" href=\"{STYLESHEET_HREF}\" />'\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT=int(os.environ.get(\"FTP_TIMEOUT\",\"30\")); FTP_PASSIVE=True\n",
        "def ftp_connect()->FTP_TLS:\n",
        "    ftps=FTP_TLS(timeout=FTP_TIMEOUT); socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get('FTP_HOST',''), int(os.environ.get('FTP_PORT',21)))\n",
        "    ftps.login(os.environ.get('FTP_USER',''), os.environ.get('FTP_PASS',''))\n",
        "    try: ftps.prot_p()\n",
        "    except Exception: pass\n",
        "    try: ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception: pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try: ftps.mkd(p)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "def _remote_path(name:str)->str: return posixpath.join(FTP_DIR,name) if FTP_DIR else name\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path: return\n",
        "    pwd0=ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p!=\".\"]:\n",
        "        try: ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try: ftps.mkd(seg)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name)->bool:\n",
        "    try:\n",
        "        with open(local_name,\"wb\") as f: ftps.retrbinary(f\"RETR {remote_name}\", f.write)\n",
        "        print(f\"[PULL] {remote_name} -> {os.path.abspath(local_name)}\"); return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name): os.remove(local_name)\n",
        "        except Exception: pass\n",
        "        print(f\"[MISS] {remote_name} ({e})\"); return False\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path,\"rb\") as fh: ftps.storbinary(f\"STOR {remote_name}\", fh)\n",
        "    print(f\"[PUT] {local_path} -> {remote_name}\")\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz=ftps.size(remote_name); return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path:str)->pd.DataFrame:\n",
        "    encs=(\"iso-8859-15\",\"utf-8-sig\",\"utf-8\",\"cp1252\",\"latin1\"); last=None; df=None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df=pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False); break\n",
        "        except Exception as e: last=e\n",
        "    if df is None: raise RuntimeError(f\"Unable to read mapping CSV {path}: {last}\")\n",
        "    if df.shape[1] < 2: raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df=df.iloc[:, :2].copy(); df.columns=[\"code\",\"unmasked\"]\n",
        "    df[\"code\"]=df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"]=df[\"unmasked\"].astype(str).str.strip()\n",
        "    df=df[df[\"code\"]!=\"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty: raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "def load_resolver_from_server()->dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try: ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception: pass\n",
        "        ok=ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Resolver not found on server: /\"+_remote_path(SERVER_MAPPING_REMOTE)+\". Upload match_to_unmasked.csv into /partials/ and re-run.\")\n",
        "    df_map=_read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(f\"[OK] Resolver loaded: {len(df_map)} codes\")\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "MATCH_TO_UNMASKED={}\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED: MATCH_TO_UNMASKED=load_resolver_from_server()\n",
        "def resolve_match_to(code:str)->str:\n",
        "    if not isinstance(code,str): return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Name/text utils ----------\n",
        "SEP_RE=re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s): return []\n",
        "    if not isinstance(s,str): s=str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "def _clean_piece(text:str)->str:\n",
        "    t=re.sub(r'~+',' ',str(text)); t=re.sub(r'\\s+',' ',t); return t.strip()\n",
        "_PARTICLES={\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "def _smart_title(token:str)->str:\n",
        "    if not token: return token\n",
        "    token=re.sub(r\"(^|\\b)([a-z])(['’])([a-z])\", lambda m: m.group(1)+m.group(2).upper()+m.group(3)+m.group(4).upper(), token.lower())\n",
        "    token=\"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token=re.sub(r\"\\bmc([a-z])\", lambda m:\"Mc\"+m.group(1).upper(), token)\n",
        "    token=re.sub(r\"\\bmac([a-z])\", lambda m:\"Mac\"+m.group(1).upper(), token)\n",
        "    return token\n",
        "def smart_titlecase(name:str)->str:\n",
        "    name=_clean_piece(name)\n",
        "    if not name: return name\n",
        "    if \",\" in name:\n",
        "        last,first=[p.strip() for p in name.split(\",\",1)]; pieces=(first+\" \"+last).split()\n",
        "    else:\n",
        "        pieces=name.split()\n",
        "    out=[]\n",
        "    for i,w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i>0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "def surname_given_from_token(token):\n",
        "    token=token.strip(); idx=None\n",
        "    for i in range(1,len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper(): idx=i; break\n",
        "    if idx is None:\n",
        "        for i in range(1,len(token)):\n",
        "            if token[i].isupper(): idx=i; break\n",
        "    if idx is None: return (token,)\n",
        "    surname=token[:idx]; given=token[idx:]; given_spaced=re.sub(r'(?<!^)([A-Z])', r' \\1', given)\n",
        "    return (f\"{given_spaced.strip()} {surname.strip()}\",)\n",
        "def normalize_person_name(s:str)->str:\n",
        "    if pd.isna(s): return \"\"\n",
        "    s=_clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last,first=[p.strip() for p in s.split(\",\",1)]; s=f\"{first} {last}\"\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "_CAMEL_WORDS=re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "def norm_matchee_name(raw:str)->str:\n",
        "    raw=str(raw or \"\").strip()\n",
        "    if not raw: return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm=smart_titlecase(raw); parts=nm.split()\n",
        "        if len(parts)==1: return nm\n",
        "        return f\"{parts[0]} {parts[-1]}\".strip()\n",
        "    words=_CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0])==1: words.pop(0)\n",
        "    if not words:\n",
        "        nm=smart_titlecase(surname_given_from_token(raw)[0]); ps=nm.split()\n",
        "        if len(ps)==1: return nm\n",
        "        return f\"{ps[0]} {ps[-1]}\".strip()\n",
        "    surname=smart_titlecase(words[0])\n",
        "    given_candidates=[w for w in words[1:] if w.lower()!=surname.lower()]\n",
        "    if not given_candidates: return surname\n",
        "    return f\"{smart_titlecase(given_candidates[0])} {surname}\".strip()\n",
        "def truncate_first(name:str,n:int=7)->str:\n",
        "    name=name.strip()\n",
        "    if not name: return name\n",
        "    parts=name.split(); return parts[0][:n] if len(parts)==1 else f\"{parts[0][:n]} {parts[-1]}\"\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens: return (\"\",\"\")\n",
        "    first=_clean_piece(tokens[0]); parts=re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts)!=2: return (\"\",\"\")\n",
        "    def _norm(s): return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "def degree_label_from_generations(g):\n",
        "    if g<=1: return (\"parents\" if g==1 else \"self\")\n",
        "    if g==2: return \"grandparents\"\n",
        "    greats=g-2\n",
        "    if greats==1: return \"great-grandparents\"\n",
        "    return f\"{greats}x-great-grandparents\"\n",
        "def build_header(subject_name, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try: cm_str=f\"{int(round(float(cm_val)))}\"\n",
        "    except Exception: cm_str=(str(cm_val).strip() or \"0\")\n",
        "    degree_label=degree_label_from_generations(gens)\n",
        "    parts=[f\"{subject_name} is a {cm_str} cM cousin match to {matchee_name_html}, whose\",\n",
        "           f\"{degree_label} (back {gens} Gens)\",\"are\",f\"{husband} & {wife}.\"]\n",
        "    s=\" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END: s=re.sub(r'\\.\\s*$','',s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) Read CSV ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols=list(df.columns); lowmap={c.lower():c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns: return name\n",
        "            if name and name.lower() in lowmap: return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx=re.compile(pat,re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c): return c\n",
        "    return None\n",
        "\n",
        "_encs=(\"utf-8-sig\",\"utf-8\",\"cp1252\",\"iso-8859-15\",\"latin1\"); _last_err=None; df=None\n",
        "for _e in _encs:\n",
        "    try: df=pd.read_csv(CSV_IN, encoding=_e, dtype=str, keep_default_na=False); break\n",
        "    except Exception as _ex: _last_err=_ex; df=None\n",
        "if df is None: raise RuntimeError(f\"Unable to read CSV: {CSV_IN} ({_last_err})\")\n",
        "print(f\"[OK] Loaded CSV: {len(df)} rows, {len(df.columns)} cols\")\n",
        "\n",
        "id_col   = find_col(df,[r'^(id#|personid)$'],[\"ID#\",\"ID\",\"PersonID\",\"personID\"])\n",
        "match_col= find_col(df,[r'^match\\s*to$'],[\"Match to\",\"Match\",\"match_to\",\"Match_to\"])\n",
        "name_col = find_col(df,[r'^name$'],[\"Name\"])\n",
        "cm_col   = find_col(df,[r'^(c\\s*:?m|cm)$',r'centi.?morgan'],[\"cM\",\"cm\"])\n",
        "path_col = find_col(df,[r'(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)'],[\"Yates DNA Ancestral Line\",\"Ancestral Line\",\"Lineage\"])\n",
        "\n",
        "if not id_col:   raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "if not match_col:raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "if not name_col: raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col:   raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not path_col: raise ValueError(\"CSV missing lineage/path column.\")\n",
        "\n",
        "# ---------- 6) Transform ----------\n",
        "ID_PAT=re.compile(r\"\\bI\\d+\\b\",re.I)\n",
        "def extract_person_id(s:str)->str:\n",
        "    m=ID_PAT.search(str(s or \"\")); return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def _setup_resolver_and_return():\n",
        "    _setup_resolver(); return MATCH_TO_UNMASKED\n",
        "_=_setup_resolver_and_return()\n",
        "\n",
        "headers,lineages,findcol=[],[],[]\n",
        "subjects,first_ancestors=[],[]\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw=row.get(match_col,\"\")\n",
        "    subject_name=normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_b=f\"<strong>{subject_name}</strong>\" if subject_name else subject_name\n",
        "\n",
        "    pid=extract_person_id(row.get(id_col,\"\"))\n",
        "    matchee_name=norm_matchee_name(row.get(name_col,\"\")) or subject_name\n",
        "    matchee_name_html=(f'<a href=\"{TNG_BASE}/verticalchart.php?personID={pid}&tree={TNG_TREE}&parentset=0&display=vertical&generations=15\" target=\"_blank\" rel=\"noopener\">{matchee_name}</a>') if pid else matchee_name\n",
        "\n",
        "    cm_val=row.get(cm_col,\"0\")\n",
        "    tokens=split_tokens(row.get(path_col,\"\")); gens_total=len(tokens); tokens_disp=tokens[:7]\n",
        "\n",
        "    if \"common_husband\" in df.columns and \"common_wife\" in df.columns:\n",
        "        husband_raw=str(row.get(\"common_husband\",\"\")).strip(); wife_raw=str(row.get(\"common_wife\",\"\")).strip()\n",
        "        if not husband_raw and not wife_raw: husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html=build_header(subject_name_b, cm_val, matchee_name_html, gens_total, husband_raw, wife_raw)\n",
        "    if tokens_disp: tokens_disp[0]=f\"<strong>{tokens_disp[0]}</strong>\"\n",
        "    sep=f\" {ARROW_ENTITY} \"; lineage_text=sep.join(tokens_disp) if tokens_disp else \"\"\n",
        "\n",
        "    q=_u.quote(subject_name)\n",
        "    quick=(f'<a class=\"find-btn\" href=\"{REMOTE_NAME_ABS}?q={q}\" target=\"_blank\" rel=\"noopener\" title=\"Open a filtered view for {subject_name}\">Find</a>')\n",
        "\n",
        "    headers.append(header_html); lineages.append(lineage_text); findcol.append(quick)\n",
        "    subjects.append(subject_name); first_ancestors.append(tokens[0] if tokens else \"\")\n",
        "\n",
        "LINEAGE_HEADER_SAFE=\"Lineage (Starting with oldest ancestor, the line is:)\"\n",
        "df[\"Match Summary\"]=headers; df[LINEAGE_HEADER_SAFE]=lineages; df[\"Find\"]=findcol; df[\"Subject\"]=subjects\n",
        "df[\"First Ancestor\"]=[_clean_piece(x) for x in first_ancestors]\n",
        "display_df=df[[\"Find\",\"Match Summary\",LINEAGE_HEADER_SAFE]]\n",
        "\n",
        "# ---------- 6.1) Clean exports ----------\n",
        "TAG_RE=re.compile(r\"<[^>]+>\")\n",
        "def _html_to_text(s:str)->str:\n",
        "    t=TAG_RE.sub(\"\",str(s or \"\")); t=_html.unescape(t); t=t.replace(\"\\u2192\",\"->\"); return re.sub(r\"\\s+\",\" \",t).strip()\n",
        "def _extract_find_url(cell_html:str)->str:\n",
        "    m=re.search(r'href=\"([^\"]+)\"',str(cell_html or \"\")); return _html.unescape(m.group(1)) if m else \"\"\n",
        "\n",
        "export_df=pd.DataFrame({\n",
        "    \"Find URL\":[_extract_find_url(v) for v in display_df[\"Find\"].tolist()],\n",
        "    \"Match Summary\":[_html_to_text(v) for v in display_df[\"Match Summary\"].tolist()],\n",
        "    \"Lineage\":[_html_to_text(v) for v in display_df[LINEAGE_HEADER_SAFE].tolist()],\n",
        "})\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try: export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    with pd.ExcelWriter(LOCAL_XLSX) as _writer: export_df.to_excel(_writer, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- 7) Stylesheet content (ALL styles live here) ----------\n",
        "CSS_TEXT = f\"\"\"/* yates.one-name.net — DNA pages (unified stylesheet)\n",
        "   Version: {CSS_VERSION}\n",
        "   Note: Typography, layout, colors, borders — all centralized here. */\n",
        "\n",
        ":root {{\n",
        "  --table-width-px: {TABLE_WIDTH_PX}px;\n",
        "  --col-a-px: {COL_A_PX}px;\n",
        "  --find-col-px: {FIND_COL_PX}px;\n",
        "  --brand-blue: #5b79b8;\n",
        "  --brand-blue-dark: #4668aa;\n",
        "  --line: #dddddd;\n",
        "  --line-strong: #999999;\n",
        "}}\n",
        "\n",
        "html, body {{\n",
        "  margin:0; padding:0;\n",
        "  font-family: \"Times New Roman\", Times, serif;\n",
        "  font-size: 16px; line-height: 1.35;\n",
        "  color:#111111; background:#ffffff;\n",
        "}}\n",
        "\n",
        ".wrap {{ max-width:100%; margin:0 auto; background:#ffffff; padding:16px; padding-bottom:48px; }}\n",
        ".centerline {{ text-align:center; }}\n",
        "\n",
        ".downloads {{ text-align:center; margin:4px 0 10px 0; font-size: 13px; }}\n",
        ".updated {{ font-size: 12px; color:#555555; text-align:center; margin:2px 0 10px 0; }}\n",
        "\n",
        ".table-scroll {{ max-height:70vh; overflow-y:auto; overflow-x:auto; border:1px solid var(--line); }}\n",
        "\n",
        "table.sortable {{ border-collapse:collapse; width:100%; table-layout:fixed; }}\n",
        "table.sortable th, table.sortable td {{ border:1px solid var(--line); padding:6px 8px; vertical-align:top; word-wrap:break-word; overflow-wrap:break-word; }}\n",
        "table.sortable th {{ background:#e3eaf8; text-align:left; position:sticky; top:0; z-index:2; box-shadow:0 1px 0 #cccccc; cursor:pointer; }}\n",
        "#first-row td {{ border-top:2px solid var(--line-strong); }}\n",
        "\n",
        "#refactor-table col:nth-child(1) {{ width: var(--find-col-px); }}\n",
        "#refactor-table col:nth-child(2) {{ width: var(--col-a-px); }}\n",
        "\n",
        ".find-cell {{ white-space:nowrap; }}\n",
        ".selbox {{ margin-right:6px; vertical-align:middle; }}\n",
        "\n",
        ".back-to-top {{\n",
        "  position:fixed; right:16px; bottom:16px; padding:6px 10px;\n",
        "  border:1px solid #3e5a97; background:var(--brand-blue);\n",
        "  color:#ffffff; cursor:pointer; border-radius:6px; display:none; z-index:9999;\n",
        "}}\n",
        ".back-to-top:hover {{ background:var(--brand-blue-dark); }}\n",
        "\n",
        ".controls {{ text-align:center; }}\n",
        ".controls-spaced {{ margin:6px 0 10px 0; }}\n",
        ".search {{ font-size: 14px; padding:5px 8px; }}\n",
        "\n",
        ".oldnav {{\n",
        "  margin:8px auto 6px auto; padding:0; background:var(--brand-blue);\n",
        "  border-radius:6px; overflow:hidden; max-width: var(--table-width-px);\n",
        "}}\n",
        ".oldnav ul {{ list-style:none; margin:0; padding:0; display:flex; flex-wrap:wrap; }}\n",
        ".oldnav li {{ margin:0; padding:0; }}\n",
        ".oldnav a, .oldnav a:link, .oldnav a:visited, .oldnav a:active {{ color:#ffffff !important; }}\n",
        ".oldnav a {{ display:block; padding:8px 12px; text-decoration:none; white-space:nowrap; border-right:1px solid #ffffff; font-weight:600; }}\n",
        ".oldnav li:last-child a {{ border-right:none; }}\n",
        ".oldnav a:hover {{ background:var(--brand-blue-dark); color:#ffffff !important; }}\n",
        "\n",
        "@media screen and (min-width: 1200px) {{\n",
        "  .wrap {{ max-width: var(--table-width-px); }}\n",
        "}}\n",
        "@media screen and (max-width: 1199px) {{\n",
        "\n",
        "  #refactor-table col:nth-child(1) {{ width:12%; }}\n",
        "  #refactor-table col:nth-child(2) {{ width:44%; }}\n",
        "  #refactor-table col:nth-child(3) {{ width:44%; }}\n",
        "  .oldnav {{ border-radius:0; }}\n",
        "}}\n",
        "@media screen and (max-width: 700px) {{\n",
        "  table.sortable th, table.sortable td {{ padding:5px 6px; }}\n",
        "  #refactor-table col:nth-child(1) {{ width:16%; }}\n",
        "  #refactor-table col:nth-child(2) {{ width:42%; }}\n",
        "  #refactor-table col:nth-child(3) {{ width:42%; }}\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"partials\", exist_ok=True)\n",
        "with open(STYLESHEET_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as _css:\n",
        "    _css.write(CSS_TEXT)\n",
        "print(\"[OK] Wrote stylesheet:\", os.path.abspath(STYLESHEET_LOCAL))\n",
        "\n",
        "# ---------- 8) Main HTML (no inline <style>; relies only on stylesheet) ----------\n",
        "page_tpl=Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  <div class=\"table-scroll\">$HTML_TABLE</div>\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){ return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase(); }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb=tbl && tbl.tBodies ? tbl.tBodies[0] : null; if(!tb) return;\n",
        "    var rows=[].slice.call(tb.rows||[]); var asc=(dir==='asc');\n",
        "    rows.sort(function(a,b){var A=textOf(a.cells[colIndex]),B=textOf(b.cells[colIndex]); if(A<B) return asc?-1:1; if(A>B) return asc?1:-1; return 0;});\n",
        "    var frag=document.createDocumentFragment(); for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]); tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths=tbl.tHead.rows[0].cells; if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx]; var dir='asc';\n",
        "      th.addEventListener('click',function(){\n",
        "        dir=(dir==='asc')?'desc':'asc';\n",
        "        for (var j = 0; j < ths.length; j++){ ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,''); }\n",
        "        th.innerHTML += (dir==='asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl,idx,dir);\n",
        "      },false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){ try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }catch(e){ return String(n||''); } }\n",
        "  function visibleRowCount(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows=tbl.tBodies[0].rows, n=0; for(var i=0;i<rows.length;i++){ if(rows[i].style.display!=='none') n++; } return n;\n",
        "  }\n",
        "  function updateShowing(){ var el=document.getElementById('showing-count'); if(!el) return; el.textContent = formatWithCommas(visibleRowCount()); }\n",
        "  function getParam(name){ var m=location.search.match(new RegExp('[?&]'+name+'=([^&]+)')); return m?decodeURIComponent(m[1].replace(/\\\\+/g,' ')):''; }\n",
        "  function norm(s){ return String(s||'').replace(/\\\\s+/g,' ').toLowerCase(); }\n",
        "  function bindSearch(){\n",
        "    var box=document.getElementById('search-box'); var tbl=document.getElementById('refactor-table'); if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return; var tb=tbl.tBodies[0];\n",
        "    function rowText(tr){ var t=''; for(var i=1;i<tr.cells.length;i++){ t+= ' ' + (tr.cells[i].textContent||tr.cells[i].innerText||''); } return norm(t); }\n",
        "    var cached=[]; (function seed(){ var rows=tb.rows; cached=[]; for(var i=0;i<rows.length;i++){ cached.push({el:rows[i], txt:rowText(rows[i])}); } })();\n",
        "    function apply(q){\n",
        "      q = String(q||''); var parts = q.split('|').map(function(s){return norm(s);}).filter(function(s){return !!s;});\n",
        "      for(var i=0;i<cached.length;i++){\n",
        "        var hit = !parts.length || parts.some(function(p){ return cached[i].txt.indexOf(p)>-1; });\n",
        "        cached[i].el.style.display = hit? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to=null; function onInput(){ if(to) clearTimeout(to); to=setTimeout(function(){ apply(box.value); }, 60); }\n",
        "    box.addEventListener('input', onInput, false); box.addEventListener('search', onInput, false);\n",
        "    var q0=getParam('q'); if(q0){ box.value=q0; apply(q0); try{ history.replaceState(null,'',location.pathname); }catch(e){} }\n",
        "    else { box.value=''; apply(''); setTimeout(function(){ if(!getParam('q')){ box.value=''; apply(''); } }, 0); }\n",
        "  }\n",
        "  function allRowCheckboxes(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return [];\n",
        "    var tb=tbl.tBodies[0], out=[]; for(var i=0;i<tb.rows.length;i++){ var cb=tb.rows[i].querySelector('.selbox'); if(cb) out.push(cb); }\n",
        "    return out;\n",
        "  }\n",
        "  function bindGroupSync(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!tbl) return;\n",
        "    tbl.addEventListener('click', function(e){\n",
        "      var t=e.target||e.srcElement; if(!(t && t.classList && t.classList.contains('selbox'))) return;\n",
        "      var nm = t.getAttribute('data-name') || ''; var checked = !!t.checked;\n",
        "      var cbs = allRowCheckboxes(); for(var i=0;i<cbs.length;i++){ if((cbs[i].getAttribute('data-name')||'') === nm){ cbs[i].checked = checked; } }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindShowSelected(){\n",
        "    var btn=document.getElementById('show-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var cbs = allRowCheckboxes(); var names = {};\n",
        "      for(var i=0;i<cbs.length;i++){ if(cbs[i].checked){ names[cbs[i].getAttribute('data-name')||''] = true; } }\n",
        "      var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "      var tb=tbl.tBodies[0];\n",
        "      for(var r=0;r<tb.rows.length;r++){\n",
        "        var cb = tb.rows[r].querySelector('.selbox'); var nm = cb ? (cb.getAttribute('data-name')||'') : '';\n",
        "        tb.rows[r].style.display = names[nm] ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindShowAll(){\n",
        "    var btn=document.getElementById('show-all'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "      var tb=tbl.tBodies[0]; for(var i=0;i<tb.rows.length;i++){ tb.rows[i].style.display=''; }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindClear(){\n",
        "    var btn=document.getElementById('clear-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var cbs=allRowCheckboxes(); for(var i=0;i<cbs.length;i++) cbs[i].checked=false;\n",
        "      var tbl=document.getElementById('refactor-table'); if(tbl && tbl.tBodies && tbl.tBodies[0]){\n",
        "        var tb=tbl.tBodies[0]; for(var j=0;j<tb.rows.length;j++){ tb.rows[j].style.display=''; }\n",
        "      }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindPrintCousinList(){\n",
        "    var btn=document.getElementById('print-cousin-list'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var url = '/partials/cousin_list_print.htm';\n",
        "      try{ var w = window.open(url, 'CousinPrint'); if(w){ w.focus(); return; } } catch(ex){}\n",
        "      window.location.href = url;\n",
        "    }, false);\n",
        "  }\n",
        "  function addCheckboxes(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb=tbl.tBodies[0];\n",
        "    for(var i=0;i<tb.rows.length;i++){\n",
        "      var tr=tb.rows[i]; var cell=tr.cells[0]; var findBtn=cell ? cell.querySelector('.find-btn') : null;\n",
        "      var name = findBtn ? (findBtn.getAttribute('title')||'').replace('Open a filtered view for ','') : ('Row '+(i+1));\n",
        "      if(cell){\n",
        "        cell.classList.add('find-cell');\n",
        "        cell.innerHTML = '<input type=\"checkbox\" class=\"selbox\" title=\"Select this row\" data-name=\"'+name.replace(/\"/g,'&quot;')+'\" /> ' + cell.innerHTML.replace(/^\\\\s*/, '');\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  function initShowingStatic(){ try{ document.getElementById('showing-count').textContent = document.getElementById('refactor-table').tBodies[0].rows.length; }catch(e){}}\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    addCheckboxes();\n",
        "    (function(){\n",
        "      var el=document.getElementById('last-updated'); if(!el) return;\n",
        "      var d=new Date(document.lastModified||new Date());\n",
        "      var months=['January','February','March','April','May','June','July','August','September','October','November','December'];\n",
        "      var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear();\n",
        "      var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am';\n",
        "      hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min;\n",
        "      el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + hour + ':' + minStr + ' ' + ampm;\n",
        "    })();\n",
        "    bindHeaderSort(); bindSearch(); bindGroupSync(); bindShowSelected(); bindShowAll(); bindClear(); bindPrintCousinList(); initShowingStatic();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# Build table HTML (no inline styles)\n",
        "html_table=display_df.to_html(index=False, escape=False, classes=\"dataframe sortable\", col_space=None)\n",
        "html_table=html_table.replace('<table border=\"1\" class=\"dataframe sortable\">','<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',1)\n",
        "html_table=html_table.replace('<tbody>\\n<tr>','<tbody>\\n<tr id=\"first-row\">',1)\n",
        "html_table=html_table.replace(\"<th>Find</th>\",'<th>Select:</th>',1)\n",
        "html_table=html_table.replace('<th>Match Summary</th>','<th>Match Summary&amp;ndash;click to sort</th>',1)\n",
        "html_table=html_table.replace(f'<th>{_html.escape(LINEAGE_HEADER_SAFE)}</th>','<th>Lineage (Starting with oldest ancestor&amp;ndash;click to sort)</th>',1)\n",
        "# Add a colgroup with no inline width; widths are handled by CSS variables\n",
        "if '<colgroup>' not in html_table:\n",
        "    html_table=html_table.replace('<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "                                  '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n<colgroup><col /><col /><col /></colgroup>',1)\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">'\n",
        "    'Last updated: <span id=\"last-updated\"></span>'\n",
        "    ' &nbsp;|&nbsp; Autosomal matches: <span id=\"auto-count\" class=\"js-count\"></span>'\n",
        "    ' &nbsp;|&nbsp; Showing: <span id=\"showing-count\"></span>'\n",
        "    '</div>'\n",
        ")\n",
        "DOWNLOADS_BLOCK = (\n",
        "    '<p class=\"downloads centerline\">Download: '\n",
        "    f'<a href=\"/partials/{_html.escape(os.path.basename(LOCAL_CSV))}\">CSV</a> | '\n",
        "    f'<a href=\"/partials/{_html.escape(os.path.basename(LOCAL_XLSX))}\">Excel</a></p>'\n",
        ")\n",
        "NAV_BLOCK = (\n",
        "  '<nav class=\"oldnav\"><ul>'\n",
        "  '<li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\" rel=\"noopener\">Study Details</a></li>'\n",
        "  '<li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\" rel=\"noopener\">Theory in Action</a></li>'\n",
        "  '<li><a href=\"/gengen/images/cousin-calculator.jpg\" target=\"_blank\" rel=\"noopener\">Cousin Connection</a></li>'\n",
        "  '<li><a href=\"/gengen/images/Shared_cM_Project_v4.jpg\" target=\"_blank\" rel=\"noopener\">Cousin by DNA</a></li>'\n",
        "  '<li><a href=\"/partials/match_count.htm\" target=\"_blank\" rel=\"noopener\">Match Count</a></li>'\n",
        "  '<li><a href=\"/partials/lineage_count.htm\" target=\"_blank\" rel=\"noopener\">Lineage Count</a></li>'\n",
        "  f'<li><a href=\"{DNA_REGISTER_ABS}\" target=\"_blank\" rel=\"noopener\">DNA Register</a></li>'\n",
        "  f'<li><a href=\"{TREES_ABS}\" target=\"_blank\" rel=\"noopener\">Trees</a></li>'\n",
        "  '<li><a id=\"show-selected\" href=\"#\">Show Selected</a></li>'\n",
        "  '<li><a id=\"show-all\" href=\"#\">Show All</a></li>'\n",
        "  '<li><a id=\"print-cousin-list\" href=\"#\">Cousin List (Printable)</a></li>'\n",
        "  '<li><a id=\"clear-selected\" href=\"#\">Reset</a></li>'\n",
        "  '</ul></nav>'\n",
        ")\n",
        "CONTROLS_BLOCK = (\n",
        "  '<div class=\"controls controls-spaced centerline\">'\n",
        "  '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "  '</div>'\n",
        ")\n",
        "\n",
        "final_html=page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK, UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK, CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    HTML_TABLE=html_table, DOWNLOADS_BLOCK=DOWNLOADS_BLOCK\n",
        ")\n",
        "\n",
        "# ---------- 9) Partials (match_count.htm, lineage_count.htm, cousin_list_print.htm) ----------\n",
        "def _norm_code_for_count(s):\n",
        "    t=str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")): t=t[1:-1]\n",
        "    t=re.sub(r'\\s+',' ',t).strip().lower(); return t\n",
        "\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        f\"{HEAD_LINK}\\n\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        f\"<title>{_html.escape(title)}</title>\\n\"\n",
        "        \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        f\"<h1 class=\\\"centerline\\\">{_html.escape(title)}</h1>\\n\"\n",
        "        \"<div class=\\\"updated centerline\\\">Last updated: <span id=\\\"last-updated\\\"></span> &nbsp;|&nbsp; Autosomal matches: <span id=\\\"auto-count\\\">&mdash;</span></div>\\n\"\n",
        "        \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    safe_count = COUNT_PUBLIC_URL.replace(\"'\",\"%27\")\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function stamp(){var el=document.getElementById('last-updated'); if(!el) return; var d=new Date(document.lastModified||new Date()); var months=['January','February','March','April','May','June','July','August','September','October','November','December']; var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear(); var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am'; hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min; el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + hour + ':' + minStr + ' ' + ampm;}\"\n",
        "        \"function load(){var el=document.getElementById('auto-count'); if(!el) return; var URL='\"+safe_count+\"'; try{var xhr=new XMLHttpRequest(); xhr.open('GET', URL+(URL.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true); xhr.onreadystatechange=function(){if(xhr.readyState===4){if(xhr.status>=200&&xhr.status<300){var m=(xhr.responseText||'').match(/(\\\\d+)/); el.textContent=(m?m[1]:'');} else {el.textContent='(unavailable)';}}}; xhr.send(null);}catch(e){el.textContent='(unavailable)';}}\"\n",
        "        \"document.addEventListener('DOMContentLoaded', function(){ stamp(); load(); }, false);\"\n",
        "        \"})();\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "def build_match_count_partial(main_df: pd.DataFrame)->str:\n",
        "    # main_df MUST be the full df with original columns (including match_col)\n",
        "    codes_raw=main_df[match_col].astype(str).map(lambda x:x.strip())\n",
        "    keys_norm=codes_raw.map(_norm_code_for_count)\n",
        "    counts_series=keys_norm.value_counts(dropna=False)\n",
        "    counts=counts_series.reset_index()\n",
        "    if counts.shape[1]>=2: counts.columns=[\"norm_key\",\"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"]=counts.index.astype(str); counts[\"Count\"]=counts_series.values; counts=counts[[\"norm_key\",\"Count\"]]\n",
        "    first_display={}\n",
        "    for code_disp,k in zip(codes_raw.tolist(), keys_norm.tolist()):\n",
        "        if k not in first_display and str(k)!=\"\": first_display[k]=code_disp\n",
        "    counts[\"Code\"]=counts[\"norm_key\"].map(lambda k:first_display.get(k,k))\n",
        "    counts[\"Unmasked\"]=counts[\"norm_key\"].map(lambda k:MATCH_TO_UNMASKED.get(k,\"\"))\n",
        "    counts=counts.sort_values(by=[\"Code\",\"Count\"], ascending=[True,False], kind=\"mergesort\").reset_index(drop=True)\n",
        "    html=[]; html.append(_partial_head(\"Match Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th style=\"width:35%\">Code</th><th style=\"width:45%\">Unmasked</th><th style=\"width:20%\">Count</th>')\n",
        "    html.append('</tr></thead><tbody id=\"ref-tb\">')\n",
        "    for _,r in counts.iterrows():\n",
        "        code=r.get(\"Code\",\"\"); unm=r.get(\"Unmasked\",\"\"); cnt=int(str(r.get(\"Count\",\"0\")).strip() or \"0\"); label=(unm or code).strip()\n",
        "        tr=(f'<tr data-q=\"{_html.escape(label,quote=True)}\" data-count=\"{cnt}\"><td>{_html.escape(code)}</td><td>{_html.escape(unm)}</td><td class=\"count\"><a href=\"#\" class=\"count-pick\" title=\"Toggle select\">{cnt}</a></td></tr>')\n",
        "        html.append(tr)\n",
        "    html.append('</tbody></table>'); html.append(_partial_tail()); return \"\".join(html)\n",
        "\n",
        "def build_lineage_count_partial(main_df: pd.DataFrame)->str:\n",
        "    first_series=main_df.get(\"First Ancestor\", pd.Series(dtype=str)).astype(str).map(lambda x:x.strip())\n",
        "    vc=first_series[first_series!=\"\"].value_counts(dropna=False)\n",
        "    lin_df=vc.reset_index()\n",
        "    if lin_df.shape[1]>=2: lin_df.columns=[\"First Ancestor\",\"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"]=lin_df.index.astype(str); lin_df[\"Count\"]=vc.values; lin_df=lin_df[[\"First Ancestor\",\"Count\"]]\n",
        "    lin_df=lin_df.sort_values([\"Count\",\"First Ancestor\"], ascending=[False,True], kind=\"mergesort\").reset_index(drop=True)\n",
        "    html=[]; html.append(_partial_head(\"Lineage Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th style=\"width:80%\">First Ancestor</th><th style=\"width:20%\">Count</th>')\n",
        "    html.append('</tr></thead><tbody id=\"ref-tb\">')\n",
        "    for _,r in lin_df.iterrows():\n",
        "        first=str(r.get(\"First Ancestor\",\"\")).strip(); cnt=int(str(r.get(\"Count\",\"0\")).strip() or \"0\")\n",
        "        tr=(f'<tr data-q=\"{_html.escape(first,quote=True)}\" data-count=\"{cnt}\"><td>{_html.escape(first)}</td><td class=\"count\"><a href=\"#\" class=\"count-pick\" title=\"Toggle select\">{cnt}</a></td></tr>')\n",
        "        html.append(tr)\n",
        "    html.append('</tbody></table>'); html.append(_partial_tail()); return \"\".join(html)\n",
        "\n",
        "def build_and_write_partials(main_df: pd.DataFrame):\n",
        "    _setup_resolver(); os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    mc_html=build_match_count_partial(main_df); mc_local=os.path.join(\"partials\",\"match_count.htm\")\n",
        "    with open(mc_local,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(mc_html)\n",
        "    print(\"[OK] Wrote partial:\", mc_local)\n",
        "\n",
        "    lc_html=build_lineage_count_partial(main_df); lc_local=os.path.join(\"partials\",\"lineage_count.htm\")\n",
        "    with open(lc_local,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(lc_html)\n",
        "    print(\"[OK] Wrote partial:\", lc_local)\n",
        "\n",
        "    # Cousin List (Printable) — stylesheet linked\n",
        "    cousin_df=display_df[[\"Match Summary\"]].copy()\n",
        "    cousin_df=cousin_df.sort_values(by=\"Match Summary\", ascending=True, kind=\"mergesort\").reset_index(drop=True)\n",
        "    rows=['<table border=\"1\" id=\"refactor-table\" class=\"sortable\"><thead><tr><th>Match Summary</th></tr></thead><tbody>']\n",
        "    for v in cousin_df[\"Match Summary\"].tolist(): rows.append(f\"<tr><td>{v}</td></tr>\")\n",
        "    rows.append(\"</tbody></table>\")\n",
        "    cousin_html=(\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\"><head>\"\n",
        "        f\"{HEAD_LINK}\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\"\n",
        "        \"<title>Cousin List (Printable)</title>\"\n",
        "        \"</head><body onload=\\\"window.print();\\\">\"\n",
        "        \"<div class=\\\"wrap\\\">\"\n",
        "        \"<h1 class=\\\"centerline\\\">Cousin List (Printable)</h1>\"\n",
        "        \"<div class=\\\"table-scroll\\\">\"+ \"\".join(rows) +\"</div>\"\n",
        "        \"</div></body></html>\"\n",
        "    )\n",
        "    cl_local=os.path.join(\"partials\",\"cousin_list_print.htm\")\n",
        "    with open(cl_local,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", cl_local)\n",
        "\n",
        "    return mc_local, lc_local, cl_local\n",
        "\n",
        "# >>>>>>>>>>> FIX: pass the FULL df (not display_df) so 'Match to' exists <<<<<<<<<<<\n",
        "PARTIAL_MATCH_LOCAL, PARTIAL_LINEAGE_LOCAL, PARTIAL_COUSIN_LOCAL = build_and_write_partials(df)\n",
        "\n",
        "# Build alternate render that points Find to HOME_URL (ABS)\n",
        "def build_register_html_for_abs(remote_abs_path:str)->str:\n",
        "    q_links=[]; subs_names=df[\"Subject\"].astype(str).tolist()\n",
        "    for subject_name in subs_names:\n",
        "        q=_u.quote(subject_name)\n",
        "        q_links.append(f'<a class=\"find-btn\" href=\"{remote_abs_path}?q={q}\" target=\"_blank\" rel=\"noopener\" title=\"Open a filtered view for {subject_name}\">Find</a>')\n",
        "    df_plus=df.copy(); df_plus[\"Find\"]=q_links\n",
        "    disp_plus=df_plus[[\"Find\",\"Match Summary\",LINEAGE_HEADER_SAFE]]\n",
        "    tbl=disp_plus.to_html(index=False, escape=False, classes=\"dataframe sortable\")\n",
        "    tbl=tbl.replace('<table border=\"1\" class=\"dataframe sortable\">','<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',1)\n",
        "    tbl=tbl.replace('<tbody>\\n<tr>','<tbody>\\n<tr id=\"first-row\">',1)\n",
        "    tbl=tbl.replace(\"<th>Find</th>\",'<th>Select:</th>',1)\n",
        "    tbl=tbl.replace(\"<th>Match Summary</th>\",'<th>Match Summary&amp;ndash;click to sort</th>',1)\n",
        "    tbl=tbl.replace(f\"<th>{_html.escape(LINEAGE_HEADER_SAFE)}</th>\",\"<th>Lineage (Starting with oldest ancestor&amp;ndash;click to sort)</th>\",1)\n",
        "    if '<colgroup>' not in tbl:\n",
        "        tbl=tbl.replace('<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "                        '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n<colgroup><col /><col /><col /></colgroup>',1)\n",
        "    return page_tpl.safe_substitute(\n",
        "        HEAD_LINK=HEAD_LINK, UPDATED_BLOCK=UPDATED_BLOCK, NAV_BLOCK=NAV_BLOCK,\n",
        "        CONTROLS_BLOCK=CONTROLS_BLOCK, HTML_TABLE=tbl, DOWNLOADS_BLOCK=DOWNLOADS_BLOCK\n",
        "    )\n",
        "\n",
        "os.makedirs(\"partials\", exist_ok=True)\n",
        "final_html=page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK, UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK, CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    HTML_TABLE=html_table, DOWNLOADS_BLOCK=DOWNLOADS_BLOCK\n",
        ")\n",
        "final_html_plus=build_register_html_for_abs(HOME_URL)\n",
        "\n",
        "with open(LOCAL_HTML,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(final_html)\n",
        "print(\"[OK] Saved canonical render:\", os.path.abspath(LOCAL_HTML))\n",
        "with open(WORK_PLUS_LOCAL,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(final_html_plus)\n",
        "print(\"[OK] Saved:\", os.path.abspath(WORK_PLUS_LOCAL), \"(partials clone)\")\n",
        "\n",
        "# ---------- 10) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in ['FTP_HOST','FTP_USER','FTP_PASS']):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\"); return\n",
        "    try:\n",
        "        ftps=ftp_connect()\n",
        "        try:\n",
        "            # Upload stylesheet first (cache-busted link already points here)\n",
        "            ftp_upload_overwrite(ftps, STYLESHEET_LOCAL, _remote_path(STYLESHEET_REMOTE))\n",
        "        except Exception as e: print(\"[WARN] Upload stylesheet failed:\", e)\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_SIMPLE))\n",
        "        except Exception as e: print(\"[WARN] Upload main HTML failed:\", e)\n",
        "        try:\n",
        "            if os.path.exists(LOCAL_CSV):  ftp_upload_overwrite(ftps, LOCAL_CSV,  _remote_path(REMOTE_CSV))\n",
        "            if os.path.exists(LOCAL_XLSX): ftp_upload_overwrite(ftps, LOCAL_XLSX, _remote_path(REMOTE_XLSX))\n",
        "            print(\"[OK] Uploaded CSV/XLSX -> /partials/\")\n",
        "        except Exception as e: print(\"[WARN] Upload CSV/XLSX failed:\", e)\n",
        "        if os.path.exists(LOCAL_COUNT_FILE):\n",
        "            try: ftp_upload_overwrite(ftps, LOCAL_COUNT_FILE, _remote_path(REMOTE_COUNT_NAME))\n",
        "            except Exception as e: print(\"[WARN] Upload autosomal count failed:\", e)\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, os.path.join(\"partials\",\"match_count.htm\"),       _remote_path(posixpath.join(\"partials\",\"match_count.htm\")))\n",
        "            ftp_upload_overwrite(ftps, os.path.join(\"partials\",\"lineage_count.htm\"),     _remote_path(posixpath.join(\"partials\",\"lineage_count.htm\")))\n",
        "            ftp_upload_overwrite(ftps, os.path.join(\"partials\",\"cousin_list_print.htm\"), _remote_path(posixpath.join(\"partials\",\"cousin_list_print.htm\")))\n",
        "        except Exception as e: print(\"[WARN] Upload partials failed:\", e)\n",
        "        try: ftp_upload_overwrite(ftps, WORK_PLUS_LOCAL, _remote_path(WORK_PLUS_REMOTE))\n",
        "        except Exception as e: print(\"[WARN] Upload work_plus.htm failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [\n",
        "            _remote_path(REMOTE_HTML_CANON), _remote_path(REMOTE_HTML_LEG), _remote_path(REMOTE_HTML_SIMPLE),\n",
        "            _remote_path(REMOTE_CSV), _remote_path(REMOTE_XLSX),\n",
        "            _remote_path(posixpath.join(\"partials\",\"match_count.htm\")),\n",
        "            _remote_path(posixpath.join(\"partials\",\"lineage_count.htm\")),\n",
        "            _remote_path(posixpath.join(\"partials\",\"cousin_list_print.htm\")),\n",
        "            _remote_path(WORK_PLUS_REMOTE),\n",
        "            _remote_path(STYLESHEET_REMOTE),\n",
        "        ]:\n",
        "            sz=ftp_size(ftps,p); print(f\"{p} : {sz if sz is not None else '(SIZE unsupported)'}\")\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.htm\")\n",
        "        print(\"Legacy clone:     https://yates.one-name.net/partials/ons_yates_dna_register.htm\")\n",
        "        print(\"JUSTDNA alias:    https://yates.one-name.net/partials/justdna.htm\")\n",
        "        print(\"Trees:            https://yates.one-name.net/partials/just-trees.htm\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.htm\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.htm\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "        print(\"Work+ clone:      https://yates.one-name.net/partials/work_plus.htm\")\n",
        "        print(\"Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\")\n",
        "        print(\"\\nBust cache once if needed by appending ?v=now to the URL.\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e); traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "# ====== CUT STOP [1/1] CELL 2 ==================================================================="
      ],
      "metadata": {
        "id": "pS0XSIPHEdxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ff781f-8ec6-4a01-e885-d6e57af95aa9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\n",
            "DECLARED_LINES=9999\n",
            "[OK] Loaded CSV: 7 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 79 codes\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Wrote stylesheet: /content/partials/dna_tree_styles.css\n",
            "[OK] Wrote partial: partials/match_count.htm\n",
            "[OK] Wrote partial: partials/lineage_count.htm\n",
            "[OK] Wrote partial: partials/cousin_list_print.htm\n",
            "[OK] Saved canonical render: /content/yates_ancestor_register.htm\n",
            "[OK] Saved: /content/partials/work_plus.htm (partials clone)\n",
            "[PUT] partials/dna_tree_styles.css -> partials/dna_tree_styles.css\n",
            "[PUT] yates_ancestor_register.htm -> partials/yates_ancestor_register.htm\n",
            "[PUT] yates_ancestor_register.htm -> partials/ons_yates_dna_register.htm\n",
            "[PUT] yates_ancestor_register.htm -> partials/justdna.htm\n",
            "[PUT] yates_ancestor_register.csv -> partials/yates_ancestor_register.csv\n",
            "[PUT] yates_ancestor_register.xlsx -> partials/yates_ancestor_register.xlsx\n",
            "[OK] Uploaded CSV/XLSX -> /partials/\n",
            "[PUT] /content/autosomal_count.txt -> autosomal_count.txt\n",
            "[PUT] partials/match_count.htm -> partials/match_count.htm\n",
            "[PUT] partials/lineage_count.htm -> partials/lineage_count.htm\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "[PUT] partials/work_plus.htm -> partials/work_plus.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.htm : 16481\n",
            "partials/ons_yates_dna_register.htm : 16481\n",
            "partials/justdna.htm : 16481\n",
            "partials/yates_ancestor_register.csv : 2931\n",
            "partials/yates_ancestor_register.xlsx : 6694\n",
            "partials/match_count.htm : 2299\n",
            "partials/lineage_count.htm : 2469\n",
            "partials/cousin_list_print.htm : 2971\n",
            "partials/work_plus.htm : 16481\n",
            "partials/dna_tree_styles.css : 3141\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.htm\n",
            "Legacy clone:     https://yates.one-name.net/partials/ons_yates_dna_register.htm\n",
            "JUSTDNA alias:    https://yates.one-name.net/partials/justdna.htm\n",
            "Trees:            https://yates.one-name.net/partials/just-trees.htm\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.htm\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.htm\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "Work+ clone:      https://yates.one-name.net/partials/work_plus.htm\n",
            "Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "Bust cache once if needed by appending ?v=now to the URL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "INiJljOS1kRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 3 — Ancestor Register (Old-school Blue Menu; WHITE menu text) ======\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.12)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.11.12 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes; DECLARED_LINES printed at run start.\n",
        "\n",
        "DECLARED_LINES = 9999\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.11.12 | Encoding=ISO-8859-15\")\n",
        "print(\"DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try:  os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception: os.environ.setdefault('FTP_PORT', '21')\n",
        "    try:  os.environ['FTP_DIR']  = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR', '')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_PORT', '21')\n",
        "    os.environ.setdefault('FTP_DIR', '')\n",
        "\n",
        "FTP_DIR  = os.environ.get('FTP_DIR','').strip().strip('/')\n",
        "COUNT_PUBLIC_URL = (\"/%s/%s\" % (FTP_DIR, \"autosomal_count.txt\")) if FTP_DIR else \"/autosomal_count.txt\"\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "OUTPUT_NAME    = \"just-trees.htm\"  # this page’s filename\n",
        "REMOTE_HTML    = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "# Cross-site buttons (same as Cell 2)\n",
        "DNA_REGISTER_ABS = \"https://yates.one-name.net/partials/justdna.htm\"\n",
        "TREES_ABS        = \"https://yates.one-name.net/partials/just-trees.htm\"\n",
        "\n",
        "# Stylesheet + cache buster (shared with Cell 2)\n",
        "STYLESHEET_HREF=\"/partials/dna_tree_styles.css\"; CSS_VERSION=\"v2025-11-12\"\n",
        "HEAD_LINK = f'<link rel=\"stylesheet\" type=\"text/css\" href=\"{STYLESHEET_HREF}?{CSS_VERSION}\" />'\n",
        "\n",
        "# Old-school menu widths (responsive behavior mostly via CSS; typography comes from stylesheet)\n",
        "TABLE_WIDTH_PX=5550; FIND_COL_PX=25; COL_A_PX=900  # kept for consistency even if not used here\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None; _last_err=None\n",
        "for enc in (\"utf-8-sig\",\"utf-8\",\"cp1252\",\"iso-8859-15\",\"latin1\"):\n",
        "    try:\n",
        "        df=pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc); break\n",
        "    except Exception as e:\n",
        "        _last_err=e; df=None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV:\", INPUT_CSV, \"rows=%d, cols=%d\" % (len(df), len(df.columns)))\n",
        "\n",
        "# Ensure haplogroup present (not strictly needed for “just-trees”, but harmless)\n",
        "if 'haplogroup' not in df.columns: df['haplogroup']=''\n",
        "else: df['haplogroup']=df['haplogroup'].fillna('')\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX=0; B_IDX=1; C_IDX=2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t=str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")): t=t[1:-1]\n",
        "    t=t.replace(\"\\u00a0\",\" \"); t=re.sub(r\"\\s{2,}\",\" \",t)\n",
        "    return t.lower()\n",
        "\n",
        "# Prefer local-first resolver cached by Cell 1; fall back to server\n",
        "LOCAL_RESOLVER=\"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER=\"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path)); return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\",\"\"), int(os.environ.get(\"FTP_PORT\",\"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\",\"\"), os.environ.get(\"FTP_PASS\",\"\"))\n",
        "            try: ftps.prot_p()\n",
        "            except Exception: pass\n",
        "            try: ftps.set_pasv(True)\n",
        "            except Exception: pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try: ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try: ftps.mkd(p)\n",
        "                        except Exception: pass\n",
        "                        ftps.cwd(p)\n",
        "            try: ftps.cwd(\"partials\")\n",
        "            except Exception: pass\n",
        "            with open(\"match_to_unmasked.csv\",\"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR match_to_unmasked.csv\", f.write)\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not pull resolver from server:\", e)\n",
        "        return local_path  # still return whatever we have (likely missing)\n",
        "\n",
        "LOCAL_RESOLVER=_pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    last=None; m=None\n",
        "    for enc in (\"utf-8-sig\",\"iso-8859-15\",\"utf-8\",\"cp1252\",\"latin1\"):\n",
        "        try:\n",
        "            m=pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc); break\n",
        "        except Exception as e:\n",
        "            last=e; m=None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last); return {}\n",
        "    cols={c.lower():c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\"); return {}\n",
        "    m=m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"]=m[cols[\"code\"]].map(_norm_code); m[\"__val__\"]=m[cols[\"unmasked\"]].astype(str)\n",
        "    m=m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map=_load_resolver_to_map(LOCAL_RESOLVER) if os.path.exists(LOCAL_RESOLVER) else {}\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw=df.iloc[:,B_IDX].astype(str); masked_key=masked_raw.map(_norm_code)\n",
        "resolved=masked_key.map(resolver_map)\n",
        "df.iloc[:,C_IDX]=resolved.fillna(\"\")  # write unmasked into col C\n",
        "\n",
        "print(\"[OK] Column B -> C mapping:\", int(resolved.notna().sum()), \"/\", len(df), \"unmatched:\", len(df)-int(resolved.notna().sum()))\n",
        "\n",
        "# ---------- Build page (Old-school Blue Menu header + shared CSS) ----------\n",
        "# Download links (exports mirrored to /partials/)\n",
        "DOWNLOADS_BLOCK = (\n",
        "    '<p class=\"downloads centerline\">Download: '\n",
        "    f'<a href=\"/partials/{_html.escape(LOCAL_CSV)}\">CSV</a> | '\n",
        "    f'<a href=\"/partials/{_html.escape(LOCAL_XLSX)}\">Excel</a></p>'\n",
        ")\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">'\n",
        "    'Last updated: <span id=\"last-updated\"></span>'\n",
        "    ' &nbsp;|&nbsp; Autosomal matches: <span id=\"auto-count\" class=\"js-count\"></span>'\n",
        "    ' &nbsp;|&nbsp; Showing: <span id=\"showing-count\"></span>'\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "NAV_BLOCK = (\n",
        "  '<nav class=\"oldnav\"><ul>'\n",
        "  '<li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\" rel=\"noopener\">Study Details</a></li>'\n",
        "  '<li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\" rel=\"noopener\">Theory in Action</a></li>'\n",
        "  '<li><a href=\"/gengen/images/cousin-calculator.jpg\" target=\"_blank\" rel=\"noopener\">Cousin Connection</a></li>'\n",
        "  '<li><a href=\"/gengen/images/Shared_cM_Project_v4.jpg\" target=\"_blank\" rel=\"noopener\">Cousin by DNA</a></li>'\n",
        "  '<li><a href=\"/partials/match_count.htm\" target=\"_blank\" rel=\"noopener\">Match Count</a></li>'\n",
        "  '<li><a href=\"/partials/lineage_count.htm\" target=\"_blank\" rel=\"noopener\">Lineage Count</a></li>'\n",
        "  f'<li><a href=\"{DNA_REGISTER_ABS}\" target=\"_blank\" rel=\"noopener\">DNA Register</a></li>'\n",
        "  f'<li><a href=\"{TREES_ABS}\" target=\"_blank\" rel=\"noopener\">Trees</a></li>'\n",
        "  '</ul></nav>'\n",
        ")\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "  '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "  '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "  '</div>'\n",
        ")\n",
        "\n",
        "# Page-local layout helpers only. Colors/typography come from dna_tree_styles.css\n",
        "TABLE_CSS = f\"\"\"\n",
        "<style type=\"text/css\">\n",
        "  .wrap {{ max-width:100%; margin:0 auto; background:#ffffff; padding:16px; padding-bottom:48px; }}\n",
        "  .centerline {{ text-align:center; }}\n",
        "  .downloads {{ text-align:center; margin:4px 0 10px 0; font-size:13px; }}\n",
        "  .updated {{ font-size:12px; color:#555; text-align:center; margin:2px 0 10px 0; }}\n",
        "\n",
        "  .oldnav {{ margin:8px auto 6px auto; padding:0; background:#5b79b8; border-radius:6px; overflow:hidden; max-width:{TABLE_WIDTH_PX}px; }}\n",
        "  .oldnav ul {{ list-style:none; margin:0; padding:0; display:flex; flex-wrap:wrap; }}\n",
        "  .oldnav li {{ margin:0; padding:0; }}\n",
        "  .oldnav a, .oldnav a:link, .oldnav a:visited, .oldnav a:active {{ color:#ffffff !important; }}\n",
        "  .oldnav a {{ display:block; padding:8px 12px; text-decoration:none; white-space:nowrap; border-right:1px solid #ffffff; font-weight:600; }}\n",
        "  .oldnav li:last-child a {{ border-right:none; }}\n",
        "  .oldnav a:hover {{ background:#4668aa; color:#ffffff !important; }}\n",
        "\n",
        "  .table-scroll {{ max-height:75vh; overflow-y:auto; overflow-x:auto; border:1px solid #ddd; }}\n",
        "  table.sortable {{ border-collapse:collapse; width:100%; table-layout:auto; }}\n",
        "  table.sortable th, table.sortable td {{ border:1px solid #ddd; padding:6px 8px; vertical-align:top; word-wrap:break-word; overflow-wrap:break-word; }}\n",
        "  table.sortable th {{ background:#e3eaf8; text-align:left; position:sticky; top:0; z-index:2; box-shadow:0 1px 0 #ccc; cursor:pointer; }}\n",
        "  #first-row td {{ border-top:2px solid #999; }}\n",
        "\n",
        "  @media screen and (min-width: 1200px) {{\n",
        "    .wrap {{ max-width:{TABLE_WIDTH_PX}px; }}\n",
        "  }}\n",
        "  @media screen and (max-width: 1199px) {{\n",
        "    .oldnav {{ border-radius:0; }}\n",
        "  }}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Build table for this page (Ancestor Register = show Unmasked [col C] + Lineage columns if present)\n",
        "# Choose a helpful subset if the CSV is wide; otherwise render full and let user filter.\n",
        "visible_cols = [c for c in df.columns if c]  # keep order\n",
        "table_html = df.to_html(index=False, columns=visible_cols, escape=False, border=1, classes=\"dataframe sortable\")\n",
        "table_html = table_html.replace('<table border=\"1\" class=\"dataframe sortable\">','<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',1)\n",
        "table_html = table_html.replace('<tbody>\\n<tr>','<tbody>\\n<tr id=\"first-row\">',1)\n",
        "\n",
        "# XHTML page template (same header structure as Cell 2)\n",
        "from string import Template as _T\n",
        "page_tpl=_T(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "$TABLE_CSS\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  <div class=\"table-scroll\">$HTML_TABLE</div>\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){ return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase(); }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb=tbl && tbl.tBodies ? tbl.tBodies[0] : null; if(!tb) return;\n",
        "    var rows=[].slice.call(tb.rows||[]); var asc=(dir==='asc');\n",
        "    rows.sort(function(a,b){var A=textOf(a.cells[colIndex]),B=textOf(b.cells[colIndex]); var nA=parseFloat(A.replace(/[^0-9.\\\\-]/g,'')), nB=parseFloat(B.replace(/[^0-9.\\\\-]/g,'')); if(!isNaN(nA)&&!isNaN(nB)){return asc?(nA-nB):(nB-nA);} if(A<B) return asc?-1:1; if(A>B) return asc?1:-1; return 0;});\n",
        "    var frag=document.createDocumentFragment(); for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]); tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths=tbl.tHead.rows[0].cells; if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx]; var dir='asc';\n",
        "      th.addEventListener('click',function(){\n",
        "        dir=(dir==='asc')?'desc':'asc';\n",
        "        for (var j = 0; j < ths.length; j++){ ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,''); }\n",
        "        th.innerHTML += (dir==='asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl,idx,dir);\n",
        "      },false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){ try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }catch(e){ return String(n||''); } }\n",
        "  function visibleRowCount(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows=tbl.tBodies[0].rows, n=0; for(var i=0;i<rows.length;i++){ if(rows[i].style.display!=='none') n++; } return n;\n",
        "  }\n",
        "  function updateShowing(){ var el=document.getElementById('showing-count'); if(!el) return; el.textContent = formatWithCommas(visibleRowCount()); }\n",
        "  function getParam(name){ var m=location.search.match(new RegExp('[?&]'+name+'=([^&]+)')); return m?decodeURIComponent(m[1].replace(/\\\\+/g,' ')):''; }\n",
        "  function norm(s){ return String(s||'').replace(/\\\\s+/g,' ').toLowerCase(); }\n",
        "  function bindSearch(){\n",
        "    var box=document.getElementById('search-box'); var tbl=document.getElementById('refactor-table'); if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return; var tb=tbl.tBodies[0];\n",
        "    function rowText(tr){ var t=''; for(var i=0;i<tr.cells.length;i++){ t+= ' ' + (tr.cells[i].textContent||tr.cells[i].innerText||''); } return norm(t); }\n",
        "    var cached=[]; (function seed(){ var rows=tb.rows; cached=[]; for(var i=0;i<rows.length;i++){ cached.push({el:rows[i], txt:rowText(rows[i])}); } })();\n",
        "    function apply(q){\n",
        "      q = String(q||''); var parts = q.split('|').map(function(s){return norm(s);}).filter(function(s){return !!s;});\n",
        "      for(var i=0;i<cached.length;i++){\n",
        "        var hit = !parts.length || parts.some(function(p){ return cached[i].txt.indexOf(p)>-1; });\n",
        "        cached[i].el.style.display = hit? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to=null; function onInput(){ if(to) clearTimeout(to); to=setTimeout(function(){ apply(box.value); }, 60); }\n",
        "    box.addEventListener('input', onInput, false); box.addEventListener('search', onInput, false);\n",
        "    var q0=getParam('q'); if(q0){ box.value=q0; apply(q0); try{ history.replaceState(null,'',location.pathname); }catch(e){} }\n",
        "    else { box.value=''; apply(''); setTimeout(function(){ if(!getParam('q')){ box.value=''; apply(''); } }, 0); }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn=document.getElementById('back-to-top'); if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200) ? 'block' : 'none'; }\n",
        "    toggle(); window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){ window.scrollTo(0,0); }, false);\n",
        "  }\n",
        "  function stampAndCount(){\n",
        "    var el=document.getElementById('last-updated'); if(el){\n",
        "      var d=new Date(document.lastModified||new Date());\n",
        "      var months=['January','February','March','April','May','June','July','August','September','October','November','December'];\n",
        "      var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear();\n",
        "      var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am';\n",
        "      hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min;\n",
        "      el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + minStr + ':' + (hour+''); // corrected below\n",
        "    }\n",
        "    // Corrected time display (AM/PM with hour first); keep above minimal to avoid removal.\n",
        "    (function(){var el=document.getElementById('last-updated'); if(!el) return;\n",
        "      var d=new Date(document.lastModified||new Date());\n",
        "      var months=['January','February','March','April','May','June','July','August','September','October','November','December'];\n",
        "      var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear();\n",
        "      var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am';\n",
        "      hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min;\n",
        "      el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + hour + ':' + minStr + ' ' + ampm; })();\n",
        "    var elc=document.getElementById('auto-count'); if(!elc) return;\n",
        "    var URL='$JS_COUNT_URL'; try{var xhr=new XMLHttpRequest(); xhr.open('GET', URL+(URL.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true);\n",
        "      xhr.onreadystatechange=function(){if(xhr.readyState===4){if(xhr.status>=200&&xhr.status<300){var m=(xhr.responseText||'').match(/(\\\\d+)/); elc.textContent=(m?m[1]:'');} else {elc.textContent='(unavailable)';}}};\n",
        "      xhr.send(null);}catch(e){elc.textContent='(unavailable)';}\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort(); bindBackToTop(); bindSearch(); stampAndCount();\n",
        "    (function(){var sc=document.getElementById('showing-count'); if(!sc) return;\n",
        "      var tb=document.getElementById('refactor-table'); if(!(tb&&tb.tBodies&&tb.tBodies[0])) return;\n",
        "      sc.textContent = tb.tBodies[0].rows.length; })();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html=page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK, TABLE_CSS=TABLE_CSS, DOWNLOADS_BLOCK=DOWNLOADS_BLOCK, UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK, CONTROLS_BLOCK=CONTROLS_BLOCK, HTML_TABLE=table_html,\n",
        "    JS_COUNT_URL=COUNT_PUBLIC_URL\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df=df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w: export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e); traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path: return\n",
        "    for seg in [p for p in path.split('/') if p]:\n",
        "        try: ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try: ftps.mkd(seg)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host=os.environ.get('FTP_HOST'); ftp_user=os.environ.get('FTP_USER'); ftp_pass=os.environ.get('FTP_PASS')\n",
        "ftp_port=int(os.environ.get('FTP_PORT','21') or '21')\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try: ftps.prot_p()\n",
        "            except Exception: pass\n",
        "            try: ftps.set_pasv(True)\n",
        "            except Exception: pass\n",
        "\n",
        "            # Navigate to base and /partials\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            # Upload HTML\n",
        "            with open(OUTPUT_NAME,\"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \"+os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            # Upload CSV/XLSX\n",
        "            with open(LOCAL_CSV,\"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \"+os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX,\"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \"+os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.htm\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e); traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (Old-school header unified; stylesheet-driven; exports + upload ready) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "id": "UJBwmZF8cuRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Cell 1"
      ],
      "metadata": {
        "id": "z7m2W6TOv1Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 1 — GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ===\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.12)\n",
        "# • Complete & runnable Colab Cell — one contiguous block (no fragments, no cross-refs).\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; typography via /partials/dna_tree_styles.css (HTML export only).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.12 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes; DECLARED_LINES printed at start.\n",
        "# ==============================================================================================\n",
        "\n",
        "import os, re, glob, logging, functools, socket, traceback, hashlib, inspect\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "from string import Template\n",
        "\n",
        "CELL_NAME = \"Cell1_FTPS_Explicit\"\n",
        "VERSION   = \"2025.11.12\"\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=%s | Version=%s | Encoding=ISO-8859-15\" % (CELL_NAME, VERSION))\n",
        "\n",
        "# ---------- Logging ----------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(CELL_NAME)\n",
        "\n",
        "# ---------- Secrets (env or userdata) ----------\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\",\"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\",\"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\",\"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\",\"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\",\"\") or \"\").strip().strip(\"/\")\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s: return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep*2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\" %\n",
        "      (_mask(FTP_HOST), _mask(FTP_USER,2), \"***\", FTP_PORT, (\"/\"+FTP_DIR) if FTP_DIR else \"(root)\"))\n",
        "\n",
        "# ---------- FTPS (Explicit AUTH TLS) ----------\n",
        "def _ftps_connect():\n",
        "    if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "        raise RuntimeError(\"Missing FTP_HOST/FTP_USER/FTP_PASS.\")\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()                 # Explicit FTPS: AUTH TLS before login\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try: ftps.prot_p()          # Encrypt data channel\n",
        "    except Exception: pass\n",
        "    try: ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception: pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path: return\n",
        "    parts = [p for p in path.split(\"/\") if p]\n",
        "    for p in parts:\n",
        "        try:\n",
        "            ftps.cwd(p)\n",
        "        except all_errors:\n",
        "            try: ftps.mkd(p)\n",
        "            except all_errors: pass\n",
        "            ftps.cwd(p)\n",
        "\n",
        "def _ftps_upload(ftps, local_path, remote_name):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR \" + remote_name, fh)\n",
        "    print(\"[OK] Uploaded: %s -> %s/%s\" % (local_path, ftps.pwd().rstrip(\"/\"), remote_name))\n",
        "\n",
        "# ---------- Outputs / Paths ----------\n",
        "REMOTE_DIR            = \"partials\"\n",
        "CSV_OUT_LOCAL         = \"final_combined_df_with_value_labels.csv\"\n",
        "HTML_OUT_LOCAL        = \"cell1_work_table.htm\"\n",
        "ABS_CSV_URL           = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(CSV_OUT_LOCAL))\n",
        "ABS_HOME_URL          = \"/index.htm\"\n",
        "\n",
        "# New: artifacts for Cell 2\n",
        "LOCAL_PARTIALS_DIR    = \"/content/partials\"\n",
        "LOCAL_RESOLVER_OUT    = os.path.join(LOCAL_PARTIALS_DIR, \"match_to_unmasked.csv\")\n",
        "LOCAL_COUNT_FILE      = \"/content/autosomal_count.txt\"\n",
        "\n",
        "# Candidate local resolver sources (no guessing; only if present)\n",
        "CANDIDATE_RESOLVER_PATHS = [\n",
        "    \"match_to_unmasked.csv\",\n",
        "    \"/content/match_to_unmasked.csv\",\n",
        "    \"/content/drive/MyDrive/match_to_unmasked.csv\",\n",
        "    \"/workspace/match_to_unmasked.csv\",\n",
        "]\n",
        "\n",
        "# ---------- Minimal GEDCOM parse helpers ----------\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '') or ''\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0] if parts else \"\"\n",
        "        last_name  = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1; anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "    def get_extractable_NPFX(self): return self.extractable_detail.get('NPFX','') or ''\n",
        "    def get_extractable_cm(self):\n",
        "        v = self.extractable_detail.get('NPFX','') or ''\n",
        "        if '&' in v: cm = v.split('&')[0].strip()\n",
        "        elif '**' in v: cm = v.split('**')[0].strip()\n",
        "        else: cm = v.strip()\n",
        "        try: int(cm); return cm\n",
        "        except Exception: return ''\n",
        "    def get_extractable_sort(self):\n",
        "        v = self.extractable_detail.get('NPFX','') or ''\n",
        "        if '&' in v:\n",
        "            s = v.split('&')[1]\n",
        "            return (s.split('**')[0] if '**' in s else s).strip()\n",
        "        return ''\n",
        "    def get_extractable_YDNA(self):\n",
        "        v = self.extractable_detail.get('NPFX','') or ''\n",
        "        return v.split('**')[1].strip() if '**' in v else ''\n",
        "    def get_extractable_FAMC(self): return (self.extractable_detail.get('FAMC','') or '').strip('@')\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "        current = None\n",
        "        npfx_count = ydna_count = total = 0\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0]); tag = parts[1]; value = parts[2] if len(parts) > 2 else None\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total += 1; current = GedcomDataset(tag); self.gedcom_datasets.append(current)\n",
        "            elif current is not None:\n",
        "                if level == 1 and tag in ['NAME','FAMC']:\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1; current.add_extractable_detail(tag, value)\n",
        "                    if value and '**' in value: ydna_count += 1\n",
        "        autosomal = npfx_count - ydna_count\n",
        "        print(\"GEDCOM contained %d total records\" % total)\n",
        "        print(\"Records tagged and filtered by NPFX: %d\" % npfx_count)\n",
        "        print(\"Records with YDNA information: %d\" % ydna_count)\n",
        "        print(\"Autosomal matches: %d\" % autosomal)\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "        try:\n",
        "            df_filter = pd.read_excel('filtered_ids.xlsx')\n",
        "            manual_ids = set(str(x) for x in df_filter['ID'])\n",
        "            self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_ids]\n",
        "            print(\"After manual filter, total records: %d\" % len(self.filter_pool))\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "        return autosomal\n",
        "\n",
        "def _chunks(lst, n):\n",
        "    for i in range(0, len(lst), n): yield lst[i:i+n]\n",
        "\n",
        "def _quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"): idx = 0\n",
        "        else: return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start); end = len(full_text) if end == -1 else end\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line: return name_line[:10].replace(\" \",\"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \",\"\") + first_name[:10].replace(\" \",\"\")\n",
        "\n",
        "def _find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map: return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id: return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id: _find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id: _find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def _find_distant(individual_id, parents_map, path=None):\n",
        "    if path is None: path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map: return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id: return [path]\n",
        "    paths = []\n",
        "    if father_id: paths.extend(_find_distant(father_id, parents_map, path[:]))\n",
        "    if mother_id: paths.extend(_find_distant(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "def _filter_lineage(winning_ids, gen_table, names_map):\n",
        "    matching = []\n",
        "    for generation, pair in gen_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_ids or id2 in winning_ids:\n",
        "            matching.append((generation, pair))\n",
        "    matching.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for _, pair in matching:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(\"%s&%s\" % (name_pair[0], name_pair[1]))\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "def _process_record(individual_id, ged, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []; visited_pairs = set()\n",
        "    _find_parents(individual_id, 1, parents_map)\n",
        "    paths = _find_distant(individual_id, parents_map)\n",
        "    best_score, best_path = None, None\n",
        "    for path in paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx+1) for idx, nm in enumerate(name_path) if 'Yates' in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score, best_path = score, path\n",
        "    best_path = best_path or []\n",
        "    best_ids  = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str  = _filter_lineage(set(best_ids), generation_table, names_map)\n",
        "    cm_value=''; sort_value=''; ydna_value=''\n",
        "    for ds in ged.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value  = ds.get_extractable_cm()\n",
        "            sort_value= ds.get_extractable_sort()\n",
        "            ydna_value= ds.get_extractable_YDNA()\n",
        "            break\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "# ---------- Helper: Local resolver prep ----------\n",
        "def _read_mapping_csv_norm(path):\n",
        "    encs=(\"iso-8859-15\",\"utf-8-sig\",\"utf-8\",\"cp1252\",\"latin1\")\n",
        "    last=None; df=None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df=pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last=e; df=None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV: %s (%s)\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\",\"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"]!=\"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV is empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def prepare_local_resolver(out_path):\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "        for cand in CANDIDATE_RESOLVER_PATHS:\n",
        "            if os.path.exists(cand) and os.path.isfile(cand):\n",
        "                try:\n",
        "                    df_map = _read_mapping_csv_norm(cand)\n",
        "                    with open(out_path, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "                        f.write(df_map.to_csv(index=False))\n",
        "                    print(\"[OK] Local resolver cached -> %s | rows=%d\" % (os.path.abspath(out_path), len(df_map)))\n",
        "                    return True\n",
        "                except Exception as e:\n",
        "                    print(\"[WARN] Candidate resolver failed normalization: %s | %s\" % (cand, e))\n",
        "        print(\"[INFO] No local resolver found. Cell 2 will fall back to server copy if available.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Local resolver prep failed:\", e)\n",
        "        return False\n",
        "\n",
        "# ---------- Main build ----------\n",
        "def main():\n",
        "    # Audit: DECLARED_LINES (of this function, deterministic print)\n",
        "    try:\n",
        "        declared = len(inspect.getsource(main).splitlines())\n",
        "    except Exception:\n",
        "        declared = -1\n",
        "    print(\"[AUDIT] DECLARED_LINES=%s\" % str(declared))\n",
        "\n",
        "    files = glob.glob(\"*.ged\")\n",
        "    if not files:\n",
        "        print(\"No GEDCOM files found.\"); return False\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    gedcom_path = files[0]\n",
        "\n",
        "    ged = Gedcom(gedcom_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "\n",
        "    # Write autosomal count to Cell 2 expected path\n",
        "    try:\n",
        "        with open(LOCAL_COUNT_FILE, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "            f.write(str(autosomal_count))\n",
        "        print(\"[OK] autosomal_count.txt -> %s | count=%d\" % (LOCAL_COUNT_FILE, autosomal_count))\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Failed writing autosomal_count.txt:\", e)\n",
        "\n",
        "    with open(gedcom_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk: continue\n",
        "        flend = blk.find('\\n'); flend = len(blk) if flend == -1 else flend\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            s = first_line.find('@') + 1\n",
        "            e = first_line.find('@', s)\n",
        "            rec_id = first_line[s:e].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map, names_map, families = {}, {}, {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            husb_id = txt[father_idx+len('1 HUSB @'):txt.find('@', father_idx+len('1 HUSB @'))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            wife_id = txt[wife_idx+len('1 WIFE @'):txt.find('@', wife_idx+len('1 WIFE @'))] if wife_idx != -1 else None\n",
        "            kids = [ln.split('@')[1] for ln in txt.split('\\n') if ln.strip().startswith('1 CHIL @')]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "    for rec_id, txt in all_records.items():\n",
        "        names_map[rec_id] = _quick_extract_name(\"\\n\" + txt)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(\"Processing %d individuals with chunk-based parallel...\" % len(individual_ids))\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    from functools import partial\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as ex, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in _chunks(individual_ids, chunk_size):\n",
        "            func = partial(_process_record, ged=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(ex.map(func, chunk))\n",
        "            combined_rows.extend(results); pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def _trim_prefix(row):\n",
        "        prefix = \"YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&FauconerElizabeth~~~\"\n",
        "        s = str(row[\"Yates DNA Ancestral Line\"])\n",
        "        if s.startswith(prefix): row[\"Yates DNA Ancestral Line\"] = s[len(prefix):]\n",
        "        return row\n",
        "    df = df.apply(_trim_prefix, axis=1)\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "\n",
        "    # CSV (ISO-8859-15)\n",
        "    with open(CSV_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df.to_csv(index=False))\n",
        "    logger.info(\"Exported CSV -> %s\", CSV_OUT_LOCAL)\n",
        "\n",
        "    # Working HTML (XHTML 1.0 Transitional; typography from external CSS)\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Yates DNA Ancestral Line\"]\n",
        "    table_html = df.to_html(index=False, columns=final_cols, escape=False, border=1)\n",
        "    page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Cell 1 Working Table</title>\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/dna_tree_styles.css\" />\n",
        "<style type=\"text/css\">\n",
        "  html { scroll-behavior: smooth; }\n",
        "  body { background:#ffffff; color:#222; margin:0; padding:20px; }\n",
        "  h1 { margin:0 0 8px 0; font-size:24px; line-height:1.2; text-align:center; }\n",
        "  .meta { text-align:center; font-size:12px; color:#555; margin:2px 0 12px 0; }\n",
        "  .downloads { text-align:center; margin:4px 0 12px 0; font-size:13px; }\n",
        "  a { color:#154b8b; text-decoration:none; } a:hover { text-decoration:underline; }\n",
        "  table { width:100%; border-collapse:collapse; }\n",
        "  th, td { border:1px solid #333; padding:6px 8px; vertical-align:top; }\n",
        "  th { background:#e3eaf8; text-align:left; }\n",
        "  td:nth-child(5) { text-align:left; white-space:normal; }\n",
        "</style>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){ function z(n){return (n<10?'0':'')+n;}\n",
        "document.addEventListener('DOMContentLoaded', function(){\n",
        "  var el = document.getElementById('last-updated');\n",
        "  if(el){ var d=new Date(document.lastModified||new Date());\n",
        "    el.innerHTML = d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes()); }\n",
        "}, false); })();\n",
        "//]]>\n",
        "</script>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>Cell 1 Working Table</h1>\n",
        "  <div class=\"meta\">\n",
        "    <a href=\"$HOME\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "    &nbsp;|&nbsp; Last updated: <span id=\"last-updated\"></span>\n",
        "    &nbsp;|&nbsp; Download: <a href=\"$CSV\">$CSV</a>\n",
        "  </div>\n",
        "  <div class=\"downloads\"><a href=\"$CSV\">/partials/$CSV_NAME</a></div>\n",
        "  $TABLE\n",
        "</body>\n",
        "</html>\"\"\")\n",
        "    page = page_tpl.safe_substitute(HOME=ABS_HOME_URL, CSV=ABS_CSV_URL, CSV_NAME=os.path.basename(ABS_CSV_URL), TABLE=table_html)\n",
        "    with open(HTML_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(page)\n",
        "    logger.info(\"Exported HTML -> %s\", HTML_OUT_LOCAL)\n",
        "\n",
        "    # Prepare local resolver cache for Cell 2\n",
        "    _ = prepare_local_resolver(LOCAL_RESOLVER_OUT)\n",
        "\n",
        "    return True\n",
        "\n",
        "ok = main()\n",
        "\n",
        "# ---------- Upload to /partials (Explicit FTPS AUTH TLS) ----------\n",
        "if ok and FTP_HOST and FTP_USER and FTP_PASS:\n",
        "    print(\"[INFO] Uploading artifacts to /partials/ ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        _ftps_ensure_dir(ftps, \"partials\")  # current dir now /partials (if supported)\n",
        "        try: _ftps_upload(ftps, CSV_OUT_LOCAL, os.path.basename(CSV_OUT_LOCAL))\n",
        "        except Exception as e: print(\"[ERROR] CSV upload failed:\", e)\n",
        "        try: _ftps_upload(ftps, HTML_OUT_LOCAL, os.path.basename(HTML_OUT_LOCAL))\n",
        "        except Exception as e: print(\"[ERROR] HTML upload failed:\", e)\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "        print(\"[OK] Uploads complete to /partials/\")\n",
        "        print(\"Open CSV:  %s\" % ABS_CSV_URL)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing creds or build failed).\")\n",
        "\n",
        "print(\"\\n--- Cell 1 Complete: CSV + HTML built; resolver cached locally; autosomal_count.txt ready; explicit FTPS used. ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 1 — GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ===\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVTkwrLKGRna",
        "outputId": "86684753-e61b-4e33-c6e8-6de2974bc45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.12 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[AUDIT] DECLARED_LINES=140\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 62312 total records\n",
            "Records tagged and filtered by NPFX: 1572\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches: 1572\n",
            "After manual filter, total records: 93\n",
            "[OK] autosomal_count.txt -> /content/autosomal_count.txt | count=1572\n",
            "Processing 93 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:39<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] No local resolver found. Cell 2 will fall back to server copy if available.\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploads complete to /partials/\n",
            "Open CSV:  /partials/final_combined_df_with_value_labels.csv\n",
            "\n",
            "--- Cell 1 Complete: CSV + HTML built; resolver cached locally; autosomal_count.txt ready; explicit FTPS used. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST Cell 2\n"
      ],
      "metadata": {
        "id": "9RkUt92dnLP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2 — Build + Publish DNA Register (All styling via stylesheet) ======\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.12)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; ALL styling comes from /partials/dna_tree_styles.css (this cell writes it).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes; DECLARED_LINES printed at run start.\n",
        "\n",
        "DECLARED_LINES = 9999\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\")\n",
        "print(\"DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "import os, re, posixpath, socket, traceback, urllib.parse as _u\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try:\n",
        "        os.environ['FTP_DIR'] = userdata.get('FTP_DIR')\n",
        "    except Exception:\n",
        "        os.environ.setdefault('FTP_DIR', '')\n",
        "    try:\n",
        "        os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception:\n",
        "        os.environ.setdefault('FTP_PORT', '21')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_DIR', '')\n",
        "    os.environ.setdefault('FTP_PORT', '21')\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "LOCAL_HTML = \"yates_ancestor_register.htm\"\n",
        "REMOTE_HTML_CANON  = posixpath.join(\"partials\", \"yates_ancestor_register.htm\")\n",
        "REMOTE_HTML_LEG    = posixpath.join(\"partials\", \"ons_yates_dna_register.htm\")\n",
        "REMOTE_HTML_SIMPLE = posixpath.join(\"partials\", \"justdna.htm\")\n",
        "\n",
        "DNA_REGISTER_ABS = \"https://yates.one-name.net/partials/justdna.htm\"\n",
        "TREES_ABS        = \"https://yates.one-name.net/partials/just-trees.htm\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV  = f\"{EXPORT_BASENAME}.csv\"\n",
        "LOCAL_XLSX = f\"{EXPORT_BASENAME}.xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", os.path.basename(LOCAL_CSV))\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", os.path.basename(LOCAL_XLSX))\n",
        "\n",
        "WORK_PLUS_LOCAL  = os.path.join(\"partials\", \"work_plus.htm\")\n",
        "WORK_PLUS_REMOTE = posixpath.join(\"partials\", \"work_plus.htm\")\n",
        "\n",
        "LOCAL_COUNT_FILE  = \"/content/autosomal_count.txt\"\n",
        "REMOTE_COUNT_NAME = \"autosomal_count.txt\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "HOME_URL = \"https://yates.one-name.net/partials/yates_ancestor_register.htm\"\n",
        "REMOTE_NAME_ABS = HOME_URL\n",
        "COUNT_PUBLIC_URL = (f\"/{FTP_DIR}/{REMOTE_COUNT_NAME}\" if FTP_DIR else f\"/{REMOTE_COUNT_NAME}\")\n",
        "\n",
        "# Layout knobs\n",
        "TABLE_WIDTH_PX = 710  # sum of 10 + 375 + 325; only used for max-width wrapper\n",
        "\n",
        "# Column width config for main register (#refactor-table), in pixels.\n",
        "# These are the actual px widths for the three main columns:\n",
        "#   find, summary, lineage\n",
        "REGISTER_COL_WIDTHS = {\n",
        "    \"desktop\": {\n",
        "        \"find\":    10,   # px\n",
        "        \"summary\": 375,  # px\n",
        "        \"lineage\": 325,  # px\n",
        "    },\n",
        "    # \"medium\" / \"small\" removed for now\n",
        "}\n",
        "\n",
        "ARROW_ENTITY = \"&rarr;\"\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "SERVER_PARTIALS_DIR       = \"partials\"\n",
        "SERVER_MAPPING_BASENAME   = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE     = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# Shared stylesheet (this cell WRITES it; all styles live here)\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "STYLESHEET_LOCAL    = os.path.join(\"partials\", STYLESHEET_BASENAME)\n",
        "STYLESHEET_REMOTE   = posixpath.join(\"partials\", STYLESHEET_BASENAME)\n",
        "CSS_VERSION         = \"v2025-11-13-nodebug\"\n",
        "STYLESHEET_HREF     = f\"/partials/{STYLESHEET_BASENAME}?{CSS_VERSION}\"\n",
        "HEAD_LINK           = f'<link rel=\"stylesheet\" type=\"text/css\" href=\"{STYLESHEET_HREF}\" />'\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get('FTP_HOST', ''), int(os.environ.get('FTP_PORT', 21)))\n",
        "    ftps.login(os.environ.get('FTP_USER', ''), os.environ.get('FTP_PASS', ''))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.cwd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(f\"RETR {remote_name}\", f.write)\n",
        "        print(f\"[PULL] {remote_name} -> {os.path.abspath(local_name)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(f\"[MISS] {remote_name} ({e})\")\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(f\"STOR {remote_name}\", fh)\n",
        "    print(f\"[PUT] {local_path} -> {remote_name}\")\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "    if df is None:\n",
        "        raise RuntimeError(f\"Unable to read mapping CSV {path}: {last}\")\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Resolver not found on server: /\" + _remote_path(SERVER_MAPPING_REMOTE) + \". Upload match_to_unmasked.csv into /partials/ and re-run.\")\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(f\"[OK] Resolver loaded: {len(df_map)} codes\")\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Name/text utils ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r'~+', ' ', str(text))\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\", \"del\", \"della\", \"der\", \"van\", \"von\", \"da\", \"dos\", \"das\", \"di\", \"la\", \"le\", \"du\", \"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower()\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r'(?<!^)([A-Z])', r' \\1', given)\n",
        "    return (f\"{given_spaced.strip()} {surname.strip()}\",)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = f\"{first} {last}\"\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return f\"{parts[0]} {parts[-1]}\".strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return f\"{ps[0]} {ps[-1]}\".strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return f\"{smart_titlecase(given_candidates[0])} {surname}\".strip()\n",
        "\n",
        "def truncate_first(name: str, n: int = 7) -> str:\n",
        "    name = name.strip()\n",
        "    if not name:\n",
        "        return name\n",
        "    parts = name.split()\n",
        "    return parts[0][:n] if len(parts) == 1 else f\"{parts[0][:n]} {parts[-1]}\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return (\"parents\" if g == 1 else \"self\")\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return f\"{greats}x-great-grandparents\"\n",
        "\n",
        "def build_header(subject_name, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = f\"{int(round(float(cm_val)))}\"\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        f\"{subject_name} is a {cm_str} cM cousin match to {matchee_name_html}, whose\",\n",
        "        f\"{degree_label} (back {gens} Gens)\",\n",
        "        \"are\",\n",
        "        f\"{husband} & {wife}.\",\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r'\\.\\s*$', '', s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) Read CSV ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "_encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "_last_err = None\n",
        "df = None\n",
        "for _e in _encs:\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_IN, encoding=_e, dtype=str, keep_default_na=False)\n",
        "        break\n",
        "    except Exception as _ex:\n",
        "        _last_err = _ex\n",
        "        df = None\n",
        "\n",
        "if df is None:\n",
        "    raise RuntimeError(f\"Unable to read CSV: {CSV_IN} ({_last_err})\")\n",
        "print(f\"[OK] Loaded CSV: {len(df)} rows, {len(df.columns)} cols\")\n",
        "\n",
        "id_col    = find_col(df, [r'^(id#|personid)$'], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r'^match\\s*to$'], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col  = find_col(df, [r'^name$'], [\"Name\"])\n",
        "cm_col    = find_col(df, [r'^(c\\s*:?m|cm)$', r'centi.?morgan'], [\"cM\", \"cm\"])\n",
        "path_col  = find_col(df, [r'(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)'],\n",
        "                     [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"])\n",
        "\n",
        "if not id_col:\n",
        "    raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "if not match_col:\n",
        "    raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "if not name_col:\n",
        "    raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col:\n",
        "    raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not path_col:\n",
        "    raise ValueError(\"CSV missing lineage/path column.\")\n",
        "\n",
        "# ---------- 6) Transform ----------\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def _setup_resolver_and_return():\n",
        "    _setup_resolver()\n",
        "    return MATCH_TO_UNMASKED\n",
        "\n",
        "_ = _setup_resolver_and_return()\n",
        "\n",
        "headers, lineages, findcol = [], [], []\n",
        "subjects, first_ancestors = [], []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw  = row.get(match_col, \"\")\n",
        "    subject_name = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_b = f\"<strong>{subject_name}</strong>\" if subject_name else subject_name\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        matchee_name_html = (\n",
        "            f'<a href=\"{TNG_BASE}/verticalchart.php?personID={pid}&tree={TNG_TREE}&parentset=0'\n",
        "            f'&display=vertical&generations=15\" target=\"_blank\" rel=\"noopener\">{matchee_name}</a>'\n",
        "        )\n",
        "    else:\n",
        "        matchee_name_html = matchee_name\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "    tokens_disp = tokens[:7]\n",
        "\n",
        "    if \"common_husband\" in df.columns and \"common_wife\" in df.columns:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw    = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(subject_name_b, cm_val, matchee_name_html, gens_total, husband_raw, wife_raw)\n",
        "    if tokens_disp:\n",
        "        tokens_disp[0] = f\"<strong>{tokens_disp[0]}</strong>\"\n",
        "    sep = f\" {ARROW_ENTITY} \"\n",
        "    lineage_text = sep.join(tokens_disp) if tokens_disp else \"\"\n",
        "\n",
        "    q = _u.quote(subject_name)\n",
        "    quick = (\n",
        "        f'<a class=\"find-btn\" href=\"{REMOTE_NAME_ABS}?q={q}\" target=\"_blank\" rel=\"noopener\" '\n",
        "        f'title=\"Open a filtered view for {subject_name}\">Find</a>'\n",
        "    )\n",
        "\n",
        "    headers.append(header_html)\n",
        "    lineages.append(lineage_text)\n",
        "    findcol.append(quick)\n",
        "    subjects.append(subject_name)\n",
        "    first_ancestors.append(tokens[0] if tokens else \"\")\n",
        "\n",
        "LINEAGE_HEADER_SAFE = \"Lineage (Starting with oldest ancestor, the line is:)\"\n",
        "df[\"Match Summary\"] = headers\n",
        "df[LINEAGE_HEADER_SAFE] = lineages\n",
        "df[\"Find\"] = findcol\n",
        "df[\"Subject\"] = subjects\n",
        "df[\"First Ancestor\"] = [_clean_piece(x) for x in first_ancestors]\n",
        "display_df = df[[\"Find\", \"Match Summary\", LINEAGE_HEADER_SAFE]]\n",
        "\n",
        "# ---------- 6.1) Clean exports ----------\n",
        "TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "def _html_to_text(s: str) -> str:\n",
        "    t = TAG_RE.sub(\"\", str(s or \"\"))\n",
        "    t = _html.unescape(t)\n",
        "    t = t.replace(\"\\u2192\", \"->\")\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip()\n",
        "\n",
        "def _extract_find_url(cell_html: str) -> str:\n",
        "    m = re.search(r'href=\"([^\"]+)\"', str(cell_html or \"\"))\n",
        "    return _html.unescape(m.group(1)) if m else \"\"\n",
        "\n",
        "export_df = pd.DataFrame({\n",
        "    \"Find URL\":      [_extract_find_url(v) for v in display_df[\"Find\"].tolist()],\n",
        "    \"Match Summary\": [_html_to_text(v)    for v in display_df[\"Match Summary\"].tolist()],\n",
        "    \"Lineage\":       [_html_to_text(v)    for v in display_df[LINEAGE_HEADER_SAFE].tolist()],\n",
        "})\n",
        "\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    with pd.ExcelWriter(LOCAL_XLSX) as _writer:\n",
        "        export_df.to_excel(_writer, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- 7) Stylesheet content (ALL styles live here) ----------\n",
        "\n",
        "# Resolve column width config into simple variables for CSS (px)\n",
        "REG_DESK = REGISTER_COL_WIDTHS.get(\"desktop\", {\"find\": 10, \"summary\": 375, \"lineage\": 325})\n",
        "\n",
        "CSS_TEXT = f\"\"\"/* yates.one-name.net - DNA pages (unified stylesheet)\n",
        "   Version: {CSS_VERSION}\n",
        "   Note: Typography, layout, colors, borders - all centralized here. */\n",
        "\n",
        ":root {{\n",
        "  --table-width-px: {TABLE_WIDTH_PX}px;\n",
        "  --brand-blue: #5b79b8;\n",
        "  --brand-blue-dark: #4668aa;\n",
        "  --line: #dddddd;\n",
        "  --line-strong: #999999;\n",
        "}}\n",
        "\n",
        "html, body {{\n",
        "  margin:0; padding:0;\n",
        "  font-family: \"Times New Roman\", Times, serif;\n",
        "  font-size: 16px; line-height: 1.35;\n",
        "  color:#111111; background:#ffffff;\n",
        "}}\n",
        "\n",
        ".wrap {{ max-width:100%; margin:0 auto; background:#ffffff; padding:16px; padding-bottom:48px; }}\n",
        ".centerline {{ text-align:center; }}\n",
        "\n",
        ".downloads {{ text-align:center; margin:4px 0 10px 0; font-size: 13px; }}\n",
        ".updated   {{ font-size: 12px; color:#555555; text-align:center; margin:2px 0 10px 0; }}\n",
        "\n",
        ".table-scroll {{ max-height:70vh; overflow-y:auto; overflow-x:auto; border:1px solid var(--line); }}\n",
        "\n",
        "table.sortable {{\n",
        "  border-collapse:collapse;\n",
        "  width:100%;\n",
        "  table-layout:fixed;\n",
        "}}\n",
        "table.sortable th, table.sortable td {{\n",
        "  border:1px solid var(--line);\n",
        "  padding:6px 8px;\n",
        "  vertical-align:top;\n",
        "  word-wrap:break-word;\n",
        "  overflow-wrap:break-word;\n",
        "}}\n",
        "table.sortable th {{\n",
        "  background:#e3eaf8;\n",
        "  text-align:left;\n",
        "  position:sticky;\n",
        "  top:0;\n",
        "  z-index:2;\n",
        "  box-shadow:0 1px 0 #cccccc;\n",
        "  cursor:pointer;\n",
        "}}\n",
        "#first-row td {{ border-top:2px solid var(--line-strong); }}\n",
        "\n",
        "/* Main register colgroup widths (controlled from REGISTER_COL_WIDTHS; pixel-based) */\n",
        "#refactor-table col:nth-child(1) {{ width: {REG_DESK['find']}px; }}\n",
        "#refactor-table col:nth-child(2) {{ width: {REG_DESK['summary']}px; }}\n",
        "#refactor-table col:nth-child(3) {{ width: {REG_DESK['lineage']}px; }}\n",
        "\n",
        ".find-cell {{ white-space:nowrap; }}\n",
        ".selbox    {{ margin-right:6px; vertical-align:middle; }}\n",
        "\n",
        ".back-to-top {{\n",
        "  position:fixed; right:16px; bottom:16px; padding:6px 10px;\n",
        "  border:1px solid #3e5a97; background:var(--brand-blue);\n",
        "  color:#ffffff; cursor:pointer; border-radius:6px; display:none; z-index:9999;\n",
        "}}\n",
        ".back-to-top:hover {{ background:var(--brand-blue-dark); }}\n",
        "\n",
        ".controls        {{ text-align:center; }}\n",
        ".controls-spaced {{ margin:6px 0 10px 0; }}\n",
        ".search          {{ font-size: 14px; padding:5px 8px; }}\n",
        "\n",
        ".oldnav {{\n",
        "  margin:8px auto 6px auto; padding:0; background:var(--brand-blue);\n",
        "  border-radius:6px; overflow:hidden; max-width: var(--table-width-px);\n",
        "}}\n",
        ".oldnav ul {{\n",
        "  list-style:none; margin:0; padding:0;\n",
        "  display:flex; flex-wrap:wrap;\n",
        "}}\n",
        ".oldnav li {{ margin:0; padding:0; }}\n",
        ".oldnav a, .oldnav a:link, .oldnav a:visited, .oldnav a:active {{ color:#ffffff !important; }}\n",
        ".oldnav a {{\n",
        "  display:block; padding:8px 12px;\n",
        "  text-decoration:none; white-space:nowrap;\n",
        "  border-right:1px solid #ffffff;\n",
        "  font-weight:600;\n",
        "}}\n",
        ".oldnav li:last-child a {{ border-right:none; }}\n",
        ".oldnav a:hover {{ background:var(--brand-blue-dark); color:#ffffff !important; }}\n",
        "\n",
        "@media screen and (min-width: 1200px) {{\n",
        "  .wrap {{ max-width: var(--table-width-px); }}\n",
        "}}\n",
        "/* Other @media rules removed for debugging */\n",
        "\"\"\"\n",
        "\n",
        "os.makedirs(\"partials\", exist_ok=True)\n",
        "with open(STYLESHEET_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as _css:\n",
        "    _css.write(CSS_TEXT)\n",
        "print(\"[OK] Wrote stylesheet:\", os.path.abspath(STYLESHEET_LOCAL))\n",
        "\n",
        "# ---------- 8) Main HTML (no inline <style>; relies only on stylesheet) ----------\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  <div class=\"table-scroll\">$HTML_TABLE</div>\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){ return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase(); }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb=tbl && tbl.tBodies ? tbl.tBodies[0] : null; if(!tb) return;\n",
        "    var rows=[].slice.call(tb.rows||[]); var asc=(dir==='asc');\n",
        "    rows.sort(function(a,b){var A=textOf(a.cells[colIndex]),B=textOf(b.cells[colIndex]); if(A<B) return asc?-1:1; if(A>B) return asc?1:-1; return 0;});\n",
        "    var frag=document.createDocumentFragment(); for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]); tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths=tbl.tHead.rows[0].cells; if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx]; var dir='asc';\n",
        "      th.addEventListener('click',function(){\n",
        "        dir=(dir==='asc')?'desc':'asc';\n",
        "        for (var j = 0; j < ths.length; j++){ ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,''); }\n",
        "        th.innerHTML += (dir==='asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl,idx,dir);\n",
        "      },false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){ try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }catch(e){ return String(n||''); } }\n",
        "  function visibleRowCount(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows=tbl.tBodies[0].rows, n=0; for(var i=0;i<rows.length;i++){ if(rows[i].style.display!=='none') n++; } return n;\n",
        "  }\n",
        "  function updateShowing(){ var el=document.getElementById('showing-count'); if(!el) return; el.textContent = formatWithCommas(visibleRowCount()); }\n",
        "  function getParam(name){ var m=location.search.match(new RegExp('[?&]'+name+'=([^&]+)')); return m?decodeURIComponent(m[1].replace(/\\\\+/g,' ')):''; }\n",
        "  function norm(s){ return String(s||'').replace(/\\\\s+/g,' ').toLowerCase(); }\n",
        "  function bindSearch(){\n",
        "    var box=document.getElementById('search-box'); var tbl=document.getElementById('refactor-table'); if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return; var tb=tbl.tBodies[0];\n",
        "    function rowText(tr){ var t=''; for(var i=1;i<tr.cells.length;i++){ t+= ' ' + (tr.cells[i].textContent||tr.cells[i].innerText||''); } return norm(t); }\n",
        "    var cached=[]; (function seed(){ var rows=tb.rows; cached=[]; for(var i=0;i<rows.length;i++){ cached.push({el:rows[i], txt:rowText(rows[i])}); } })();\n",
        "    function apply(q){\n",
        "      q = String(q||''); var parts = q.split('|').map(function(s){return norm(s);}).filter(function(s){return !!s;});\n",
        "      for(var i=0;i<cached.length;i++){\n",
        "        var hit = !parts.length || parts.some(function(p){ return cached[i].txt.indexOf(p)>-1; });\n",
        "        cached[i].el.style.display = hit? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to=null; function onInput(){ if(to) clearTimeout(to); to=setTimeout(function(){ apply(box.value); }, 60); }\n",
        "    box.addEventListener('input', onInput, false); box.addEventListener('search', onInput, false);\n",
        "    var q0=getParam('q'); if(q0){ box.value=q0; apply(q0); try{ history.replaceState(null,'',location.pathname); }catch(e){} }\n",
        "    else { box.value=''; apply(''); setTimeout(function(){ if(!getParam('q')){ box.value=''; apply(''); } }, 0); }\n",
        "  }\n",
        "  function allRowCheckboxes(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return [];\n",
        "    var tb=tbl.tBodies[0], out=[]; for(var i=0;i<tb.rows.length;i++){ var cb=tb.rows[i].querySelector('.selbox'); if(cb) out.push(cb); }\n",
        "    return out;\n",
        "  }\n",
        "  function bindGroupSync(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!tbl) return;\n",
        "    tbl.addEventListener('click', function(e){\n",
        "      var t=e.target||e.srcElement; if(!(t && t.classList && t.classList.contains('selbox'))) return;\n",
        "      var nm = t.getAttribute('data-name') || ''; var checked = !!t.checked;\n",
        "      var cbs = allRowCheckboxes(); for(var i=0;i<cbs.length;i++){ if((cbs[i].getAttribute('data-name')||'') === nm){ cbs[i].checked = checked; } }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindShowSelected(){\n",
        "    var btn=document.getElementById('show-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var cbs = allRowCheckboxes(); var names = {};\n",
        "      for(var i=0;i<cbs.length;i++){ if(cbs[i].checked){ names[cbs[i].getAttribute('data-name')||''] = true; } }\n",
        "      var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "      var tb=tbl.tBodies[0];\n",
        "      for(var r=0;r<tb.rows.length;r++){\n",
        "        var cb = tb.rows[r].querySelector('.selbox'); var nm = cb ? (cb.getAttribute('data-name')||'') : '';\n",
        "        tb.rows[r].style.display = names[nm] ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindShowAll(){\n",
        "    var btn=document.getElementById('show-all'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "      var tb=tbl.tBodies[0]; for(var i=0;i<tb.rows.length;i++){ tb.rows[i].style.display=''; }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindClear(){\n",
        "    var btn=document.getElementById('clear-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var cbs=allRowCheckboxes(); for(var i=0;i<cbs.length;i++) cbs[i].checked=false;\n",
        "      var tbl=document.getElementById('refactor-table'); if(tbl && tbl.tBodies && tbl.tBodies[0]){\n",
        "        var tb=tbl.tBodies[0]; for(var j=0;j<tb.rows.length;j++){ tb.rows[j].style.display=''; }\n",
        "      }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "  function bindPrintCousinList(){\n",
        "    var btn=document.getElementById('print-cousin-list'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(e){\n",
        "      e.preventDefault();\n",
        "      var url = '/partials/cousin_list_print.htm';\n",
        "      try{ var w = window.open(url, 'CousinPrint'); if(w){ w.focus(); return; } } catch(ex){}\n",
        "      window.location.href = url;\n",
        "    }, false);\n",
        "  }\n",
        "  function addCheckboxes(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb=tbl.tBodies[0];\n",
        "    for(var i=0;i<tb.rows.length;i++){\n",
        "      var tr=tb.rows[i]; var cell=tr.cells[0]; var findBtn=cell ? cell.querySelector('.find-btn') : null;\n",
        "      var name = findBtn ? (findBtn.getAttribute('title')||'').replace('Open a filtered view for ','') : ('Row '+(i+1));\n",
        "      if(cell){\n",
        "        cell.classList.add('find-cell');\n",
        "        cell.innerHTML = '<input type=\"checkbox\" class=\"selbox\" title=\"Select this row\" data-name=\"'+name.replace(/\"/g,'&quot;')+'\" /> ' + cell.innerHTML.replace(/^\\\\s*/, '');\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  function initShowingStatic(){ try{ document.getElementById('showing-count').textContent = document.getElementById('refactor-table').tBodies[0].rows.length; }catch(e){}}\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    addCheckboxes();\n",
        "    (function(){\n",
        "      var el=document.getElementById('last-updated'); if(!el) return;\n",
        "      var d=new Date(document.lastModified||new Date());\n",
        "      var months=['January','February','March','April','May','June','July','August','September','October','November','December'];\n",
        "      var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear();\n",
        "      var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am';\n",
        "      hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min;\n",
        "      el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + hour + ':' + minStr + ' ' + ampm;\n",
        "    })();\n",
        "    bindHeaderSort(); bindSearch(); bindGroupSync(); bindShowSelected(); bindShowAll(); bindClear(); bindPrintCousinList(); initShowingStatic();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# Build table HTML (no inline styles)\n",
        "html_table = display_df.to_html(index=False, escape=False, classes=\"dataframe sortable\", col_space=None)\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "    1\n",
        ")\n",
        "html_table = html_table.replace('<tbody>\\n<tr>', '<tbody>\\n<tr id=\"first-row\">', 1)\n",
        "html_table = html_table.replace(\"<th>Find</th>\", '<th>Select:</th>', 1)\n",
        "html_table = html_table.replace('<th>Match Summary</th>', '<th>Match Summary&amp;ndash;click to sort</th>', 1)\n",
        "html_table = html_table.replace(\n",
        "    f'<th>{_html.escape(LINEAGE_HEADER_SAFE)}</th>',\n",
        "    '<th>Lineage (Starting with oldest ancestor&amp;ndash;click to sort)</th>',\n",
        "    1\n",
        ")\n",
        "\n",
        "# Add a colgroup; actual widths are controlled by CSS (REGISTER_COL_WIDTHS)\n",
        "if '<colgroup>' not in html_table:\n",
        "    html_table = html_table.replace(\n",
        "        '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "        '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n<colgroup><col /><col /><col /></colgroup>',\n",
        "        1\n",
        "    )\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">'\n",
        "    'Last updated: <span id=\"last-updated\"></span>'\n",
        "    ' &nbsp;|&nbsp; Autosomal matches: <span id=\"auto-count\" class=\"js-count\"></span>'\n",
        "    ' &nbsp;|&nbsp; Showing: <span id=\"showing-count\"></span>'\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "DOWNLOADS_BLOCK = (\n",
        "    '<p class=\"downloads centerline\">Download: '\n",
        "    f'<a href=\"/partials/{_html.escape(os.path.basename(LOCAL_CSV))}\">CSV</a> | '\n",
        "    f'<a href=\"/partials/{_html.escape(os.path.basename(LOCAL_XLSX))}\">Excel</a></p>'\n",
        ")\n",
        "\n",
        "NAV_BLOCK = (\n",
        "    '<nav class=\"oldnav\"><ul>'\n",
        "    '<li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\" rel=\"noopener\">Study Details</a></li>'\n",
        "    '<li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\" rel=\"noopener\">Theory in Action</a></li>'\n",
        "    '<li><a href=\"/gengen/images/cousin-calculator.jpg\" target=\"_blank\" rel=\"noopener\">Cousin Connection</a></li>'\n",
        "    '<li><a href=\"/gengen/images/Shared_cM_Project_v4.jpg\" target=\"_blank\" rel=\"noopener\">Cousin by DNA</a></li>'\n",
        "    '<li><a href=\"/partials/match_count.htm\" target=\"_blank\" rel=\"noopener\">Match Count</a></li>'\n",
        "    '<li><a href=\"/partials/lineage_count.htm\" target=\"_blank\" rel=\"noopener\">Lineage Count</a></li>'\n",
        "    f'<li><a href=\"{DNA_REGISTER_ABS}\" target=\"_blank\" rel=\"noopener\">DNA Register</a></li>'\n",
        "    f'<li><a href=\"{TREES_ABS}\" target=\"_blank\" rel=\"noopener\">Trees</a></li>'\n",
        "    '<li><a id=\"show-selected\" href=\"#\">Show Selected</a></li>'\n",
        "    '<li><a id=\"show-all\" href=\"#\">Show All</a></li>'\n",
        "    '<li><a id=\"print-cousin-list\" href=\"#\">Cousin List (Printable)</a></li>'\n",
        "    '<li><a id=\"clear-selected\" href=\"#\">Reset</a></li>'\n",
        "    '</ul></nav>'\n",
        ")\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls controls-spaced centerline\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    HTML_TABLE=html_table,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK\n",
        ")\n",
        "\n",
        "# ---------- 9) Partials (match_count.htm, lineage_count.htm, cousin_list_print.htm) ----------\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = re.sub(r'\\s+', ' ', t).strip().lower()\n",
        "    return t\n",
        "\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        f\"{HEAD_LINK}\\n\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        f\"<title>{_html.escape(title)}</title>\\n\"\n",
        "        \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        f\"<h1 class=\\\"centerline\\\">{_html.escape(title)}</h1>\\n\"\n",
        "        \"<div class=\\\"updated centerline\\\">Last updated: <span id=\\\"last-updated\\\"></span> &nbsp;|&nbsp; Autosomal matches: <span id=\\\"auto-count\\\">&mdash;</span></div>\\n\"\n",
        "        \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    safe_count = COUNT_PUBLIC_URL.replace(\"'\", \"%27\")\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function stamp(){var el=document.getElementById('last-updated'); if(!el) return; var d=new Date(document.lastModified||new Date()); var months=['January','February','March','April','May','June','July','August','September','October','November','December']; var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear(); var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am'; hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min; el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + hour + ':' + minStr + ' ' + ampm;}\"\n",
        "        \"function load(){var el=document.getElementById('auto-count'); if(!el) return; var URL='\" + safe_count + \"'; try{var xhr=new XMLHttpRequest(); xhr.open('GET', URL+(URL.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true); xhr.onreadystatechange=function(){if(xhr.readyState===4){if(xhr.status>=200&&xhr.status<300){var m=(xhr.responseText||'').match(/(\\\\d+)/); el.textContent=(m?m[1]:'');} else {el.textContent='(unavailable)';}}}; xhr.send(null);}catch(e){el.textContent='(unavailable)';}}\"\n",
        "        \"document.addEventListener('DOMContentLoaded', function(){ stamp(); load(); }, false);\"\n",
        "        \"})();\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "def build_match_count_partial(main_df: pd.DataFrame) -> str:\n",
        "    # main_df MUST be the full df with original columns (including match_col)\n",
        "    codes_raw = main_df[match_col].astype(str).map(lambda x: x.strip())\n",
        "    keys_norm = codes_raw.map(_norm_code_for_count)\n",
        "    counts_series = keys_norm.value_counts(dropna=False)\n",
        "    counts = counts_series.reset_index()\n",
        "    if counts.shape[1] >= 2:\n",
        "        counts.columns = [\"norm_key\", \"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"] = counts.index.astype(str)\n",
        "        counts[\"Count\"] = counts_series.values\n",
        "        counts = counts[[\"norm_key\", \"Count\"]]\n",
        "    first_display = {}\n",
        "    for code_disp, k in zip(codes_raw.tolist(), keys_norm.tolist()):\n",
        "        if k not in first_display and str(k) != \"\":\n",
        "            first_display[k] = code_disp\n",
        "    counts[\"Code\"]     = counts[\"norm_key\"].map(lambda k: first_display.get(k, k))\n",
        "    counts[\"Unmasked\"] = counts[\"norm_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "    counts = counts.sort_values(by=[\"Code\", \"Count\"], ascending=[True, False], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Match Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th style=\"width:35%\">Code</th><th style=\"width:45%\">Unmasked</th><th style=\"width:20%\">Count</th>')\n",
        "    html.append('</tr></thead><tbody id=\"ref-tb\">')\n",
        "    for _, r in counts.iterrows():\n",
        "        code = r.get(\"Code\", \"\")\n",
        "        unm  = r.get(\"Unmasked\", \"\")\n",
        "        cnt  = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        label = (unm or code).strip()\n",
        "        tr = (\n",
        "            f'<tr data-q=\"{_html.escape(label, quote=True)}\" data-count=\"{cnt}\">'\n",
        "            f'<td>{_html.escape(code)}</td><td>{_html.escape(unm)}</td>'\n",
        "            f'<td class=\"count\"><a href=\"#\" class=\"count-pick\" title=\"Toggle select\">{cnt}</a></td></tr>'\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append('</tbody></table>')\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "def build_lineage_count_partial(main_df: pd.DataFrame) -> str:\n",
        "    first_series = main_df.get(\"First Ancestor\", pd.Series(dtype=str)).astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "    lin_df = lin_df.sort_values([\"Count\", \"First Ancestor\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Lineage Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th style=\"width:80%\">First Ancestor</th><th style=\"width:20%\">Count</th>')\n",
        "    html.append('</tr></thead><tbody id=\"ref-tb\">')\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt   = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        tr = (\n",
        "            f'<tr data-q=\"{_html.escape(first, quote=True)}\" data-count=\"{cnt}\">'\n",
        "            f'<td>{_html.escape(first)}</td>'\n",
        "            f'<td class=\"count\"><a href=\"#\" class=\"count-pick\" title=\"Toggle select\">{cnt}</a></td></tr>'\n",
        "        )\n",
        "    html.append('</tbody></table>')\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "def build_and_write_partials(main_df: pd.DataFrame):\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    mc_html = build_match_count_partial(main_df)\n",
        "    mc_local = os.path.join(\"partials\", \"match_count.htm\")\n",
        "    with open(mc_local, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(mc_html)\n",
        "    print(\"[OK] Wrote partial:\", mc_local)\n",
        "\n",
        "    lc_html = build_lineage_count_partial(main_df)\n",
        "    lc_local = os.path.join(\"partials\", \"lineage_count.htm\")\n",
        "    with open(lc_local, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(lc_html)\n",
        "    print(\"[OK] Wrote partial:\", lc_local)\n",
        "\n",
        "    # Cousin List (Printable) — stylesheet linked\n",
        "    cousin_df = display_df[[\"Match Summary\"]].copy()\n",
        "    cousin_df = cousin_df.sort_values(by=\"Match Summary\", ascending=True, kind=\"mergesort\").reset_index(drop=True)\n",
        "    rows = ['<table border=\"1\" id=\"refactor-table\" class=\"sortable\"><thead><tr><th>Match Summary</th></tr></thead><tbody>']\n",
        "    for v in cousin_df[\"Match Summary\"].tolist():\n",
        "        rows.append(f\"<tr><td>{v}</td></tr>\")\n",
        "    rows.append(\"</tbody></table>\")\n",
        "    cousin_html = (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\" \"\n",
        "        \"\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\"><head>\"\n",
        "        f\"{HEAD_LINK}\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\"\n",
        "        \"<title>Cousin List (Printable)</title>\"\n",
        "        \"</head><body onload=\\\"window.print();\\\">\"\n",
        "        \"<div class=\\\"wrap\\\">\"\n",
        "        \"<h1 class=\\\"centerline\\\">Cousin List (Printable)</h1>\"\n",
        "        \"<div class=\\\"table-scroll\\\">\" + \"\".join(rows) + \"</div>\"\n",
        "        \"</div></body></html>\"\n",
        "    )\n",
        "    cl_local = os.path.join(\"partials\", \"cousin_list_print.htm\")\n",
        "    with open(cl_local, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", cl_local)\n",
        "\n",
        "    return mc_local, lc_local, cl_local\n",
        "\n",
        "# Pass the FULL df (not display_df) so 'Match to' exists\n",
        "PARTIAL_MATCH_LOCAL, PARTIAL_LINEAGE_LOCAL, PARTIAL_COUSIN_LOCAL = build_and_write_partials(df)\n",
        "\n",
        "# Build alternate render that points Find to HOME_URL (ABS)\n",
        "def build_register_html_for_abs(remote_abs_path: str) -> str:\n",
        "    q_links = []\n",
        "    subs_names = df[\"Subject\"].astype(str).tolist()\n",
        "    for subject_name in subs_names:\n",
        "        q = _u.quote(subject_name)\n",
        "        q_links.append(\n",
        "            f'<a class=\"find-btn\" href=\"{remote_abs_path}?q={q}\" target=\"_blank\" rel=\"noopener\" '\n",
        "            f'title=\"Open a filtered view for {subject_name}\">Find</a>'\n",
        "        )\n",
        "    df_plus = df.copy()\n",
        "    df_plus[\"Find\"] = q_links\n",
        "    disp_plus = df_plus[[\"Find\", \"Match Summary\", LINEAGE_HEADER_SAFE]]\n",
        "    tbl = disp_plus.to_html(index=False, escape=False, classes=\"dataframe sortable\")\n",
        "    tbl = tbl.replace(\n",
        "        '<table border=\"1\" class=\"dataframe sortable\">',\n",
        "        '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "        1\n",
        "    )\n",
        "    tbl = tbl.replace('<tbody>\\n<tr>', '<tbody>\\n<tr id=\"first-row\">', 1)\n",
        "    tbl = tbl.replace(\"<th>Find</th>\", '<th>Select:</th>', 1)\n",
        "    tbl = tbl.replace(\"<th>Match Summary</th>\", '<th>Match Summary&amp;ndash;click to sort</th>', 1)\n",
        "    tbl = tbl.replace(\n",
        "        f\"<th>{_html.escape(LINEAGE_HEADER_SAFE)}</th>\",\n",
        "        \"<th>Lineage (Starting with oldest ancestor&amp;ndash;click to sort)</th>\",\n",
        "        1\n",
        "    )\n",
        "    if '<colgroup>' not in tbl:\n",
        "        tbl = tbl.replace(\n",
        "            '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "            '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n<colgroup><col /><col /><col /></colgroup>',\n",
        "            1\n",
        "        )\n",
        "    return page_tpl.safe_substitute(\n",
        "        HEAD_LINK=HEAD_LINK,\n",
        "        UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "        NAV_BLOCK=NAV_BLOCK,\n",
        "        CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "        HTML_TABLE=tbl,\n",
        "        DOWNLOADS_BLOCK=DOWNLOADS_BLOCK\n",
        "    )\n",
        "\n",
        "os.makedirs(\"partials\", exist_ok=True)\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    HTML_TABLE=html_table,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK\n",
        ")\n",
        "final_html_plus = build_register_html_for_abs(HOME_URL)\n",
        "\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"[OK] Saved canonical render:\", os.path.abspath(LOCAL_HTML))\n",
        "\n",
        "with open(WORK_PLUS_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html_plus)\n",
        "print(\"[OK] Saved:\", os.path.abspath(WORK_PLUS_LOCAL), \"(partials clone)\")\n",
        "\n",
        "# ---------- 10) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in ['FTP_HOST', 'FTP_USER', 'FTP_PASS']):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            # Upload stylesheet first (cache-busted link already points here)\n",
        "            ftp_upload_overwrite(ftps, STYLESHEET_LOCAL, _remote_path(STYLESHEET_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload stylesheet failed:\", e)\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_SIMPLE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload main HTML failed:\", e)\n",
        "        try:\n",
        "            if os.path.exists(LOCAL_CSV):\n",
        "                ftp_upload_overwrite(ftps, LOCAL_CSV, _remote_path(REMOTE_CSV))\n",
        "            if os.path.exists(LOCAL_XLSX):\n",
        "                ftp_upload_overwrite(ftps, LOCAL_XLSX, _remote_path(REMOTE_XLSX))\n",
        "            print(\"[OK] Uploaded CSV/XLSX -> /partials/\")\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload CSV/XLSX failed:\", e)\n",
        "        if os.path.exists(LOCAL_COUNT_FILE):\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, LOCAL_COUNT_FILE, _remote_path(REMOTE_COUNT_NAME))\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] Upload autosomal count failed:\", e)\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, os.path.join(\"partials\", \"match_count.htm\"),\n",
        "                                 _remote_path(posixpath.join(\"partials\", \"match_count.htm\")))\n",
        "            ftp_upload_overwrite(ftps, os.path.join(\"partials\", \"lineage_count.htm\"),\n",
        "                                 _remote_path(posixpath.join(\"partials\", \"lineage_count.htm\")))\n",
        "            ftp_upload_overwrite(ftps, os.path.join(\"partials\", \"cousin_list_print.htm\"),\n",
        "                                 _remote_path(posixpath.join(\"partials\", \"cousin_list_print.htm\")))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload partials failed:\", e)\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, WORK_PLUS_LOCAL, _remote_path(WORK_PLUS_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload work_plus.htm failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [\n",
        "            _remote_path(REMOTE_HTML_CANON),\n",
        "            _remote_path(REMOTE_HTML_LEG),\n",
        "            _remote_path(REMOTE_HTML_SIMPLE),\n",
        "            _remote_path(REMOTE_CSV),\n",
        "            _remote_path(REMOTE_XLSX),\n",
        "            _remote_path(posixpath.join(\"partials\", \"match_count.htm\")),\n",
        "            _remote_path(posixpath.join(\"partials\", \"lineage_count.htm\")),\n",
        "            _remote_path(posixpath.join(\"partials\", \"cousin_list_print.htm\")),\n",
        "            _remote_path(WORK_PLUS_REMOTE),\n",
        "            _remote_path(STYLESHEET_REMOTE),\n",
        "        ]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(f\"{p} : {sz if sz is not None else '(SIZE unsupported)'}\")\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.htm\")\n",
        "        print(\"Legacy clone:     https://yates.one-name.net/partials/ons_yates_dna_register.htm\")\n",
        "        print(\"JUSTDNA alias:    https://yates.one-name.net/partials/justdna.htm\")\n",
        "        print(\"Trees:            https://yates.one-name.net/partials/just-trees.htm\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.htm\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.htm\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "        print(\"Work+ clone:      https://yates.one-name.net/partials/work_plus.htm\")\n",
        "        print(\"Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\")\n",
        "        print(\"\\nBust cache once if needed by appending ?v=now to the URL.\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "# ====== CUT STOP [1/1] CELL 2 ===================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv49L-bmQLmH",
        "outputId": "58ff08c9-85f7-45d5-b277-2223649656f8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\n",
            "DECLARED_LINES=9999\n",
            "[OK] Loaded CSV: 7 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 79 codes\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Wrote stylesheet: /content/partials/dna_tree_styles.css\n",
            "[OK] Wrote partial: partials/match_count.htm\n",
            "[OK] Wrote partial: partials/lineage_count.htm\n",
            "[OK] Wrote partial: partials/cousin_list_print.htm\n",
            "[OK] Saved canonical render: /content/yates_ancestor_register.htm\n",
            "[OK] Saved: /content/partials/work_plus.htm (partials clone)\n",
            "[PUT] partials/dna_tree_styles.css -> partials/dna_tree_styles.css\n",
            "[PUT] yates_ancestor_register.htm -> partials/yates_ancestor_register.htm\n",
            "[PUT] yates_ancestor_register.htm -> partials/ons_yates_dna_register.htm\n",
            "[PUT] yates_ancestor_register.htm -> partials/justdna.htm\n",
            "[PUT] yates_ancestor_register.csv -> partials/yates_ancestor_register.csv\n",
            "[PUT] yates_ancestor_register.xlsx -> partials/yates_ancestor_register.xlsx\n",
            "[OK] Uploaded CSV/XLSX -> /partials/\n",
            "[PUT] /content/autosomal_count.txt -> autosomal_count.txt\n",
            "[PUT] partials/match_count.htm -> partials/match_count.htm\n",
            "[PUT] partials/lineage_count.htm -> partials/lineage_count.htm\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "[PUT] partials/work_plus.htm -> partials/work_plus.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.htm : 16485\n",
            "partials/ons_yates_dna_register.htm : 16485\n",
            "partials/justdna.htm : 16485\n",
            "partials/yates_ancestor_register.csv : 2931\n",
            "partials/yates_ancestor_register.xlsx : 6694\n",
            "partials/match_count.htm : 2303\n",
            "partials/lineage_count.htm : 2113\n",
            "partials/cousin_list_print.htm : 2975\n",
            "partials/work_plus.htm : 16485\n",
            "partials/dna_tree_styles.css : 2837\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.htm\n",
            "Legacy clone:     https://yates.one-name.net/partials/ons_yates_dna_register.htm\n",
            "JUSTDNA alias:    https://yates.one-name.net/partials/justdna.htm\n",
            "Trees:            https://yates.one-name.net/partials/just-trees.htm\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.htm\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.htm\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "Work+ clone:      https://yates.one-name.net/partials/work_plus.htm\n",
            "Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "Bust cache once if needed by appending ?v=now to the URL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST Cell 3"
      ],
      "metadata": {
        "id": "ZST5Z7Gxnene"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 3 — Ancestor Register (Old-school Blue Menu; WHITE menu text) ======\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.12)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.11.12 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes; DECLARED_LINES printed at run start.\n",
        "\n",
        "DECLARED_LINES = 9999\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.11.12 | Encoding=ISO-8859-15\")\n",
        "print(\"DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try:  os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception: os.environ.setdefault('FTP_PORT', '21')\n",
        "    try:  os.environ['FTP_DIR']  = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR', '')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_PORT', '21')\n",
        "    os.environ.setdefault('FTP_DIR', '')\n",
        "\n",
        "FTP_DIR  = os.environ.get('FTP_DIR','').strip().strip('/')\n",
        "COUNT_PUBLIC_URL = (\"/%s/%s\" % (FTP_DIR, \"autosomal_count.txt\")) if FTP_DIR else \"/autosomal_count.txt\"\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "OUTPUT_NAME    = \"just-trees.htm\"  # this page’s filename\n",
        "REMOTE_HTML    = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "# Cross-site buttons (same as Cell 2)\n",
        "DNA_REGISTER_ABS = \"https://yates.one-name.net/partials/justdna.htm\"\n",
        "TREES_ABS        = \"https://yates.one-name.net/partials/just-trees.htm\"\n",
        "\n",
        "# Stylesheet + cache buster (shared with Cell 2)\n",
        "STYLESHEET_HREF=\"/partials/dna_tree_styles.css\"; CSS_VERSION=\"v2025-11-12\"\n",
        "HEAD_LINK = f'<link rel=\"stylesheet\" type=\"text/css\" href=\"{STYLESHEET_HREF}?{CSS_VERSION}\" />'\n",
        "\n",
        "# Old-school menu widths (responsive behavior mostly via CSS; typography comes from stylesheet)\n",
        "TABLE_WIDTH_PX=5550; FIND_COL_PX=25; COL_A_PX=900  # kept for consistency even if not used here\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None; _last_err=None\n",
        "for enc in (\"utf-8-sig\",\"utf-8\",\"cp1252\",\"iso-8859-15\",\"latin1\"):\n",
        "    try:\n",
        "        df=pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc); break\n",
        "    except Exception as e:\n",
        "        _last_err=e; df=None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV:\", INPUT_CSV, \"rows=%d, cols=%d\" % (len(df), len(df.columns)))\n",
        "\n",
        "# Ensure haplogroup present (not strictly needed for “just-trees”, but harmless)\n",
        "if 'haplogroup' not in df.columns: df['haplogroup']=''\n",
        "else: df['haplogroup']=df['haplogroup'].fillna('')\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX=0; B_IDX=1; C_IDX=2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t=str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")): t=t[1:-1]\n",
        "    t=t.replace(\"\\u00a0\",\" \"); t=re.sub(r\"\\s{2,}\",\" \",t)\n",
        "    return t.lower()\n",
        "\n",
        "# Prefer local-first resolver cached by Cell 1; fall back to server\n",
        "LOCAL_RESOLVER=\"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER=\"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path)); return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\",\"\"), int(os.environ.get(\"FTP_PORT\",\"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\",\"\"), os.environ.get(\"FTP_PASS\",\"\"))\n",
        "            try: ftps.prot_p()\n",
        "            except Exception: pass\n",
        "            try: ftps.set_pasv(True)\n",
        "            except Exception: pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try: ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try: ftps.mkd(p)\n",
        "                        except Exception: pass\n",
        "                        ftps.cwd(p)\n",
        "            try: ftps.cwd(\"partials\")\n",
        "            except Exception: pass\n",
        "            with open(\"match_to_unmasked.csv\",\"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR match_to_unmasked.csv\", f.write)\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not pull resolver from server:\", e)\n",
        "        return local_path  # still return whatever we have (likely missing)\n",
        "\n",
        "LOCAL_RESOLVER=_pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    last=None; m=None\n",
        "    for enc in (\"utf-8-sig\",\"iso-8859-15\",\"utf-8\",\"cp1252\",\"latin1\"):\n",
        "        try:\n",
        "            m=pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc); break\n",
        "        except Exception as e:\n",
        "            last=e; m=None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last); return {}\n",
        "    cols={c.lower():c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\"); return {}\n",
        "    m=m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"]=m[cols[\"code\"]].map(_norm_code); m[\"__val__\"]=m[cols[\"unmasked\"]].astype(str)\n",
        "    m=m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map=_load_resolver_to_map(LOCAL_RESOLVER) if os.path.exists(LOCAL_RESOLVER) else {}\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw=df.iloc[:,B_IDX].astype(str); masked_key=masked_raw.map(_norm_code)\n",
        "resolved=masked_key.map(resolver_map)\n",
        "df.iloc[:,C_IDX]=resolved.fillna(\"\")  # write unmasked into col C\n",
        "\n",
        "print(\"[OK] Column B -> C mapping:\", int(resolved.notna().sum()), \"/\", len(df), \"unmatched:\", len(df)-int(resolved.notna().sum()))\n",
        "\n",
        "# ---------- Build page (Old-school Blue Menu header + shared CSS) ----------\n",
        "# Download links (exports mirrored to /partials/)\n",
        "DOWNLOADS_BLOCK = (\n",
        "    '<p class=\"downloads centerline\">Download: '\n",
        "    f'<a href=\"/partials/{_html.escape(LOCAL_CSV)}\">CSV</a> | '\n",
        "    f'<a href=\"/partials/{_html.escape(LOCAL_XLSX)}\">Excel</a></p>'\n",
        ")\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">'\n",
        "    'Last updated: <span id=\"last-updated\"></span>'\n",
        "    ' &nbsp;|&nbsp; Autosomal matches: <span id=\"auto-count\" class=\"js-count\"></span>'\n",
        "    ' &nbsp;|&nbsp; Showing: <span id=\"showing-count\"></span>'\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "NAV_BLOCK = (\n",
        "  '<nav class=\"oldnav\"><ul>'\n",
        "  '<li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\" target=\"_blank\" rel=\"noopener\">Study Details</a></li>'\n",
        "  '<li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\" target=\"_blank\" rel=\"noopener\">Theory in Action</a></li>'\n",
        "  '<li><a href=\"/gengen/images/cousin-calculator.jpg\" target=\"_blank\" rel=\"noopener\">Cousin Connection</a></li>'\n",
        "  '<li><a href=\"/gengen/images/Shared_cM_Project_v4.jpg\" target=\"_blank\" rel=\"noopener\">Cousin by DNA</a></li>'\n",
        "  '<li><a href=\"/partials/match_count.htm\" target=\"_blank\" rel=\"noopener\">Match Count</a></li>'\n",
        "  '<li><a href=\"/partials/lineage_count.htm\" target=\"_blank\" rel=\"noopener\">Lineage Count</a></li>'\n",
        "  f'<li><a href=\"{DNA_REGISTER_ABS}\" target=\"_blank\" rel=\"noopener\">DNA Register</a></li>'\n",
        "  f'<li><a href=\"{TREES_ABS}\" target=\"_blank\" rel=\"noopener\">Trees</a></li>'\n",
        "  '</ul></nav>'\n",
        ")\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "  '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "  '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "  '</div>'\n",
        ")\n",
        "\n",
        "# Page-local layout helpers only. Colors/typography come from dna_tree_styles.css\n",
        "TABLE_CSS = f\"\"\"\n",
        "<style type=\"text/css\">\n",
        "  .wrap {{ max-width:100%; margin:0 auto; background:#ffffff; padding:16px; padding-bottom:48px; }}\n",
        "  .centerline {{ text-align:center; }}\n",
        "  .downloads {{ text-align:center; margin:4px 0 10px 0; font-size:13px; }}\n",
        "  .updated {{ font-size:12px; color:#555; text-align:center; margin:2px 0 10px 0; }}\n",
        "\n",
        "  .oldnav {{ margin:8px auto 6px auto; padding:0; background:#5b79b8; border-radius:6px; overflow:hidden; max-width:{TABLE_WIDTH_PX}px; }}\n",
        "  .oldnav ul {{ list-style:none; margin:0; padding:0; display:flex; flex-wrap:wrap; }}\n",
        "  .oldnav li {{ margin:0; padding:0; }}\n",
        "  .oldnav a, .oldnav a:link, .oldnav a:visited, .oldnav a:active {{ color:#ffffff !important; }}\n",
        "  .oldnav a {{ display:block; padding:8px 12px; text-decoration:none; white-space:nowrap; border-right:1px solid #ffffff; font-weight:600; }}\n",
        "  .oldnav li:last-child a {{ border-right:none; }}\n",
        "  .oldnav a:hover {{ background:#4668aa; color:#ffffff !important; }}\n",
        "\n",
        "  .table-scroll {{ max-height:75vh; overflow-y:auto; overflow-x:auto; border:1px solid #ddd; }}\n",
        "  table.sortable {{ border-collapse:collapse; width:100%; table-layout:auto; }}\n",
        "  table.sortable th, table.sortable td {{ border:1px solid #ddd; padding:6px 8px; vertical-align:top; word-wrap:break-word; overflow-wrap:break-word; }}\n",
        "  table.sortable th {{ background:#e3eaf8; text-align:left; position:sticky; top:0; z-index:2; box-shadow:0 1px 0 #ccc; cursor:pointer; }}\n",
        "  #first-row td {{ border-top:2px solid #999; }}\n",
        "\n",
        "  @media screen and (min-width: 1200px) {{\n",
        "    .wrap {{ max-width:{TABLE_WIDTH_PX}px; }}\n",
        "  }}\n",
        "  @media screen and (max-width: 1199px) {{\n",
        "    .oldnav {{ border-radius:0; }}\n",
        "  }}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Build table for this page (Ancestor Register = show Unmasked [col C] + Lineage columns if present)\n",
        "# Choose a helpful subset if the CSV is wide; otherwise render full and let user filter.\n",
        "visible_cols = [c for c in df.columns if c]  # keep order\n",
        "table_html = df.to_html(index=False, columns=visible_cols, escape=False, border=1, classes=\"dataframe sortable\")\n",
        "table_html = table_html.replace('<table border=\"1\" class=\"dataframe sortable\">','<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',1)\n",
        "table_html = table_html.replace('<tbody>\\n<tr>','<tbody>\\n<tr id=\"first-row\">',1)\n",
        "\n",
        "# XHTML page template (same header structure as Cell 2)\n",
        "from string import Template as _T\n",
        "page_tpl=_T(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "$TABLE_CSS\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  <div class=\"table-scroll\">$HTML_TABLE</div>\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){ return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase(); }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb=tbl && tbl.tBodies ? tbl.tBodies[0] : null; if(!tb) return;\n",
        "    var rows=[].slice.call(tb.rows||[]); var asc=(dir==='asc');\n",
        "    rows.sort(function(a,b){var A=textOf(a.cells[colIndex]),B=textOf(b.cells[colIndex]); var nA=parseFloat(A.replace(/[^0-9.\\\\-]/g,'')), nB=parseFloat(B.replace(/[^0-9.\\\\-]/g,'')); if(!isNaN(nA)&&!isNaN(nB)){return asc?(nA-nB):(nB-nA);} if(A<B) return asc?-1:1; if(A>B) return asc?1:-1; return 0;});\n",
        "    var frag=document.createDocumentFragment(); for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]); tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths=tbl.tHead.rows[0].cells; if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx]; var dir='asc';\n",
        "      th.addEventListener('click',function(){\n",
        "        dir=(dir==='asc')?'desc':'asc';\n",
        "        for (var j = 0; j < ths.length; j++){ ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,''); }\n",
        "        th.innerHTML += (dir==='asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl,idx,dir);\n",
        "      },false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){ try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }catch(e){ return String(n||''); } }\n",
        "  function visibleRowCount(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows=tbl.tBodies[0].rows, n=0; for(var i=0;i<rows.length;i++){ if(rows[i].style.display!=='none') n++; } return n;\n",
        "  }\n",
        "  function updateShowing(){ var el=document.getElementById('showing-count'); if(!el) return; el.textContent = formatWithCommas(visibleRowCount()); }\n",
        "  function getParam(name){ var m=location.search.match(new RegExp('[?&]'+name+'=([^&]+)')); return m?decodeURIComponent(m[1].replace(/\\\\+/g,' ')):''; }\n",
        "  function norm(s){ return String(s||'').replace(/\\\\s+/g,' ').toLowerCase(); }\n",
        "  function bindSearch(){\n",
        "    var box=document.getElementById('search-box'); var tbl=document.getElementById('refactor-table'); if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return; var tb=tbl.tBodies[0];\n",
        "    function rowText(tr){ var t=''; for(var i=0;i<tr.cells.length;i++){ t+= ' ' + (tr.cells[i].textContent||tr.cells[i].innerText||''); } return norm(t); }\n",
        "    var cached=[]; (function seed(){ var rows=tb.rows; cached=[]; for(var i=0;i<rows.length;i++){ cached.push({el:rows[i], txt:rowText(rows[i])}); } })();\n",
        "    function apply(q){\n",
        "      q = String(q||''); var parts = q.split('|').map(function(s){return norm(s);}).filter(function(s){return !!s;});\n",
        "      for(var i=0;i<cached.length;i++){\n",
        "        var hit = !parts.length || parts.some(function(p){ return cached[i].txt.indexOf(p)>-1; });\n",
        "        cached[i].el.style.display = hit? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to=null; function onInput(){ if(to) clearTimeout(to); to=setTimeout(function(){ apply(box.value); }, 60); }\n",
        "    box.addEventListener('input', onInput, false); box.addEventListener('search', onInput, false);\n",
        "    var q0=getParam('q'); if(q0){ box.value=q0; apply(q0); try{ history.replaceState(null,'',location.pathname); }catch(e){} }\n",
        "    else { box.value=''; apply(''); setTimeout(function(){ if(!getParam('q')){ box.value=''; apply(''); } }, 0); }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn=document.getElementById('back-to-top'); if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200) ? 'block' : 'none'; }\n",
        "    toggle(); window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){ window.scrollTo(0,0); }, false);\n",
        "  }\n",
        "  function stampAndCount(){\n",
        "    var el=document.getElementById('last-updated'); if(el){\n",
        "      var d=new Date(document.lastModified||new Date());\n",
        "      var months=['January','February','March','April','May','June','July','August','September','October','November','December'];\n",
        "      var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear();\n",
        "      var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am';\n",
        "      hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min;\n",
        "      el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + minStr + ':' + (hour+''); // corrected below\n",
        "    }\n",
        "    // Corrected time display (AM/PM with hour first); keep above minimal to avoid removal.\n",
        "    (function(){var el=document.getElementById('last-updated'); if(!el) return;\n",
        "      var d=new Date(document.lastModified||new Date());\n",
        "      var months=['January','February','March','April','May','June','July','August','September','October','November','December'];\n",
        "      var day=d.getDate(); var month=months[d.getMonth()]; var year=d.getFullYear();\n",
        "      var hour=d.getHours(); var min=d.getMinutes(); var ampm = hour >= 12 ? 'pm' : 'am';\n",
        "      hour = hour % 12; hour = hour ? 12 && hour : 12; var minStr = min < 10 ? '0' + min : min;\n",
        "      el.innerHTML = day + ' ' + month + ', ' + year + ' at ' + hour + ':' + minStr + ' ' + ampm; })();\n",
        "    var elc=document.getElementById('auto-count'); if(!elc) return;\n",
        "    var URL='$JS_COUNT_URL'; try{var xhr=new XMLHttpRequest(); xhr.open('GET', URL+(URL.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true);\n",
        "      xhr.onreadystatechange=function(){if(xhr.readyState===4){if(xhr.status>=200&&xhr.status<300){var m=(xhr.responseText||'').match(/(\\\\d+)/); elc.textContent=(m?m[1]:'');} else {elc.textContent='(unavailable)';}}};\n",
        "      xhr.send(null);}catch(e){elc.textContent='(unavailable)';}\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort(); bindBackToTop(); bindSearch(); stampAndCount();\n",
        "    (function(){var sc=document.getElementById('showing-count'); if(!sc) return;\n",
        "      var tb=document.getElementById('refactor-table'); if(!(tb&&tb.tBodies&&tb.tBodies[0])) return;\n",
        "      sc.textContent = tb.tBodies[0].rows.length; })();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html=page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK, TABLE_CSS=TABLE_CSS, DOWNLOADS_BLOCK=DOWNLOADS_BLOCK, UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK, CONTROLS_BLOCK=CONTROLS_BLOCK, HTML_TABLE=table_html,\n",
        "    JS_COUNT_URL=COUNT_PUBLIC_URL\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df=df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w: export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME,\"w\",encoding=\"iso-8859-15\",errors=\"xmlcharrefreplace\") as f: f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e); traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path: return\n",
        "    for seg in [p for p in path.split('/') if p]:\n",
        "        try: ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try: ftps.mkd(seg)\n",
        "            except Exception: pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host=os.environ.get('FTP_HOST'); ftp_user=os.environ.get('FTP_USER'); ftp_pass=os.environ.get('FTP_PASS')\n",
        "ftp_port=int(os.environ.get('FTP_PORT','21') or '21')\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try: ftps.prot_p()\n",
        "            except Exception: pass\n",
        "            try: ftps.set_pasv(True)\n",
        "            except Exception: pass\n",
        "\n",
        "            # Navigate to base and /partials\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            # Upload HTML\n",
        "            with open(OUTPUT_NAME,\"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \"+os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            # Upload CSV/XLSX\n",
        "            with open(LOCAL_CSV,\"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \"+os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX,\"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \"+os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.htm\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e); traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (Old-school header unified; stylesheet-driven; exports + upload ready) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2SCiGCrHxkj",
        "outputId": "4a83cf8f-9d3f-4f34-89f8-c5bfc4e1ba79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.11.12 | Encoding=ISO-8859-15\n",
            "DECLARED_LINES=9999\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=7, cols=6\n",
            "Using resolver: /content/match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 7 / 7 unmatched: 0\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.htm\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.htm\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.htm\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (Old-school header unified; stylesheet-driven; exports + upload ready) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# debug"
      ],
      "metadata": {
        "id": "9G7Y0HwjtZIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EL5UoDDP17F",
        "outputId": "3fae9e0e-eaa6-4b1f-eb92-b9d0d9ef0c31"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.11.12 | Encoding=ISO-8859-15\n",
            "DECLARED_LINES=9999\n",
            "[OK] Loaded CSV: 7 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 79 codes\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Wrote stylesheet: /content/partials/dna_tree_styles.css\n",
            "[OK] Wrote partial: partials/match_count.htm\n",
            "[OK] Wrote partial: partials/lineage_count.htm\n",
            "[OK] Wrote partial: partials/cousin_list_print.htm\n",
            "[OK] Saved canonical render: /content/yates_ancestor_register.htm\n",
            "[OK] Saved: /content/partials/work_plus.htm (partials clone)\n",
            "[PUT] partials/dna_tree_styles.css -> partials/dna_tree_styles.css\n",
            "[PUT] yates_ancestor_register.htm -> partials/yates_ancestor_register.htm\n",
            "[PUT] yates_ancestor_register.htm -> partials/ons_yates_dna_register.htm\n",
            "[PUT] yates_ancestor_register.htm -> partials/justdna.htm\n",
            "[PUT] yates_ancestor_register.csv -> partials/yates_ancestor_register.csv\n",
            "[PUT] yates_ancestor_register.xlsx -> partials/yates_ancestor_register.xlsx\n",
            "[OK] Uploaded CSV/XLSX -> /partials/\n",
            "[PUT] /content/autosomal_count.txt -> autosomal_count.txt\n",
            "[PUT] partials/match_count.htm -> partials/match_count.htm\n",
            "[PUT] partials/lineage_count.htm -> partials/lineage_count.htm\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "[PUT] partials/work_plus.htm -> partials/work_plus.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.htm : 16485\n",
            "partials/ons_yates_dna_register.htm : 16485\n",
            "partials/justdna.htm : 16485\n",
            "partials/yates_ancestor_register.csv : 2931\n",
            "partials/yates_ancestor_register.xlsx : 6694\n",
            "partials/match_count.htm : 2303\n",
            "partials/lineage_count.htm : 2113\n",
            "partials/cousin_list_print.htm : 2975\n",
            "partials/work_plus.htm : 16485\n",
            "partials/dna_tree_styles.css : 2837\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.htm\n",
            "Legacy clone:     https://yates.one-name.net/partials/ons_yates_dna_register.htm\n",
            "JUSTDNA alias:    https://yates.one-name.net/partials/justdna.htm\n",
            "Trees:            https://yates.one-name.net/partials/just-trees.htm\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.htm\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.htm\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "Work+ clone:      https://yates.one-name.net/partials/work_plus.htm\n",
            "Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "Bust cache once if needed by appending ?v=now to the URL.\n"
          ]
        }
      ]
    }
  ]
}