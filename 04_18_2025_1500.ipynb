{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPuh1ZVRfZp2K5+hVBNi44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/04_18_2025_1500.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "73eaf6bb-3f80-4462-e67a-66d1f23ded0d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.3\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.14.1)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.4.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 03_14_2025_1600\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script using:\n",
        " - Chunk-based Parallel Processing for Speed (Stage 1: genealogical line creation)\n",
        " - A Trie-based approach, then final \"Value\" = 5 * (number of couples with node.count >=2) + (total couples)\n",
        "\n",
        "For ancestral lines where none of the couples are repeated (a one-off line), the Value is still computed.\n",
        "Now, instead of composite scoring, two new columns are added:\n",
        "  - Value Range (the numeric bracket)\n",
        "  - Value Label (a descriptive label)\n",
        "\n",
        "Exports final CSV/HTML sorted by \"Yates DNA Ancestral Line\".\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "import logging\n",
        "import functools\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# Global Variables\n",
        "###############################################################################\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "###############################################################################\n",
        "# Trie Data Structure\n",
        "###############################################################################\n",
        "class TrieNode:\n",
        "    \"\"\"A simple Trie node for storing a couple and counting how many lines pass here.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.children = {}  # maps couple string -> TrieNode\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert_line(self, couples_list):\n",
        "        \"\"\"\n",
        "        Insert a reversed line (list of couples) into the trie.\n",
        "        Increment .count for each node visited.\n",
        "        \"\"\"\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple not in current.children:\n",
        "                current.children[couple] = TrieNode()\n",
        "            current = current.children[couple]\n",
        "            current.count += 1\n",
        "\n",
        "    def get_couple_count(self, couples_list):\n",
        "        \"\"\"\n",
        "        For each couple in this line, retrieve the node.count if it exists.\n",
        "        Returns a list of node.count values, in order.\n",
        "        \"\"\"\n",
        "        counts = []\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple in current.children:\n",
        "                current = current.children[couple]\n",
        "                counts.append(current.count)\n",
        "            else:\n",
        "                counts.append(0)\n",
        "                break\n",
        "        return counts\n",
        "\n",
        "###############################################################################\n",
        "# Utility: chunk generator\n",
        "###############################################################################\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "###############################################################################\n",
        "# GedcomDataset\n",
        "###############################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        \"\"\"\n",
        "        Extract cM from NPFX field. If NPFX has a format like \"175&someSort**someYDNA\",\n",
        "        the cM is '175'. If it doesn't parse cleanly, returns blank.\n",
        "        \"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        \"\"\"\n",
        "        If NPFX has \"xxx&sortVal**ydnaVal\", returns sortVal. If not found, blank.\n",
        "        \"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        \"\"\"\n",
        "        If NPFX has something like \"...**ydnaVal\", return ydnaVal. If not found, blank.\n",
        "        \"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "###############################################################################\n",
        "# Gedcom Class\n",
        "###############################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        # Optional second-level filter\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                self.filter_pool = [\n",
        "                    d for d in self.filter_pool if d.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "###############################################################################\n",
        "# quick_extract_name\n",
        "###############################################################################\n",
        "def quick_extract_name(full_text):\n",
        "    \"\"\"\n",
        "    Minimal function to extract a short name from a GEDCOM chunk.\n",
        "    \"\"\"\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start)\n",
        "    if end == -1:\n",
        "        end = len(full_text)\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "###############################################################################\n",
        "# Parents, Ancestors\n",
        "###############################################################################\n",
        "def find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def find_distant_ancestors(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(find_distant_ancestors(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(find_distant_ancestors(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "###############################################################################\n",
        "# filter_ancestral_line\n",
        "###############################################################################\n",
        "def filter_ancestral_line(winning_path_ids, generation_table_local, names_map):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table_local:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    matching_table.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for gen, pair in matching_table:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(f\"{name_pair[0]}&{name_pair[1]}\")\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "###############################################################################\n",
        "# process_record_wrapper (parallel) - STAGE 1\n",
        "###############################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, parents_map, names_map):\n",
        "    \"\"\"\n",
        "    This is the function used in parallel for 'Processing individuals' stage.\n",
        "    It gathers and builds the 'Yates DNA Ancestral Line' for each ID.\n",
        "    \"\"\"\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, parents_map)\n",
        "    distant_anc_paths = find_distant_ancestors(individual_id, parents_map)\n",
        "\n",
        "    best_score = None\n",
        "    best_path = None\n",
        "    for path in distant_anc_paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = 0\n",
        "        for idx, nm in enumerate(name_path):\n",
        "            if 'Yates' in nm:\n",
        "                score += (idx + 1)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_path = path\n",
        "\n",
        "    if not best_path:\n",
        "        best_path = []\n",
        "\n",
        "    # remove individual's own ID\n",
        "    best_path_cleaned = [pid for pid in best_path if pid != individual_id]\n",
        "\n",
        "    line_str = filter_ancestral_line(set(best_path_cleaned), generation_table, names_map)\n",
        "\n",
        "    cm_value = ''\n",
        "    sort_value = ''\n",
        "    ydna_value = ''\n",
        "    anchor_name = ''\n",
        "    for ds in gedcom_instance.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            anchor_name = ds.get_anchor_gen1()\n",
        "            break\n",
        "\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    # Return columns: ID#, Match to, Name, cM, Yates DNA Ancestral Line\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str]\n",
        "\n",
        "###############################################################################\n",
        "# main()\n",
        "###############################################################################\n",
        "def main():\n",
        "    def select_gedcom():\n",
        "        files = glob.glob(\"*.ged\")\n",
        "        if not files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom()\n",
        "    if not gedcom_file_path:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    # 1) Parse GEDCOM\n",
        "    ged = Gedcom(gedcom_file_path)\n",
        "    ged.parse_gedcom()\n",
        "    filter_count = len(ged.filter_pool)\n",
        "    print(\"Records tagged and filtered by NPFX:\", filter_count)\n",
        "\n",
        "    # 2) Build parents_map, names_map from raw GEDCOM\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    blocks = raw_data.split('\\n0 ')\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find('\\n')\n",
        "        if flend == -1:\n",
        "            flend = len(blk)\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            start = first_line.find('@') + 1\n",
        "            end = first_line.find('@', start)\n",
        "            rec_id = first_line[start:end].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    names_map = {}\n",
        "\n",
        "    for rec_id, txt in all_records.items():\n",
        "        nm = quick_extract_name(\"\\n\" + txt)\n",
        "        names_map[rec_id] = nm\n",
        "\n",
        "    # gather families\n",
        "    families = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            if father_idx != -1:\n",
        "                start = father_idx + len('1 HUSB @')\n",
        "                end = txt.find('@', start)\n",
        "                husb_id = txt[start:end]\n",
        "            else:\n",
        "                husb_id = None\n",
        "\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            if wife_idx != -1:\n",
        "                start = wife_idx + len('1 WIFE @')\n",
        "                end = txt.find('@', start)\n",
        "                wife_id = txt[start:end]\n",
        "            else:\n",
        "                wife_id = None\n",
        "\n",
        "            kids = []\n",
        "            lines_ = txt.split('\\n')\n",
        "            for ln in lines_:\n",
        "                if ln.strip().startswith('1 CHIL @'):\n",
        "                    s2 = ln.strip().split('1 CHIL @')[1]\n",
        "                    kid_id = s2.split('@')[0]\n",
        "                    kids.append(kid_id)\n",
        "\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    # 3) Gather ID list\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(f\"Processing {len(individual_ids)} individuals with chunk-based parallel...\")\n",
        "\n",
        "    # 4) Stage 1: Chunk-based parallel to build lines\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Starting chunk-based parallel processing with %d workers.\", max_workers)\n",
        "\n",
        "    total_records = len(individual_ids)\n",
        "    from functools import partial\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor, \\\n",
        "         tqdm(total=total_records, desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            func = partial(\n",
        "                process_record_wrapper,\n",
        "                gedcom_instance=ged,\n",
        "                parents_map=parents_map,\n",
        "                names_map=names_map\n",
        "            )\n",
        "            results = list(executor.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    # combined_rows now has 5 columns: [ID#, \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "\n",
        "    df.index += 1\n",
        "\n",
        "    # 5) Build a Trie from all reversed lines\n",
        "    logger.info(\"Building Trie from reversed lines...\")\n",
        "    trie = Trie()\n",
        "    num_lines_inserted = 0\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            continue\n",
        "        couples_list = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "        trie.insert_line(couples_list)\n",
        "        num_lines_inserted += 1\n",
        "    logger.info(\"Inserted %d lines into the trie.\", num_lines_inserted)\n",
        "\n",
        "    # 6) Compute final \"Value\" = 5*(#couples with node.count >=2) + (total couples)\n",
        "    values = []\n",
        "    prefix_counts = []  # store the count of couples with node.count >=2 for each line\n",
        "    logger.info(\"Computing 'Value' = 5*(#couples with node.count >=2) + (total couples) ...\")\n",
        "    for idx, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            values.append(0)\n",
        "            prefix_counts.append(0)\n",
        "        else:\n",
        "            couples_list = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "            line_length = len(couples_list)\n",
        "            node_counts = trie.get_couple_count(couples_list)\n",
        "            # Count only couples that appear in at least two lines\n",
        "            prefix_count = sum(1 for c in node_counts if c >= 2)\n",
        "            val = 5 * prefix_count + line_length\n",
        "            values.append(val)\n",
        "            prefix_counts.append(prefix_count)\n",
        "    df[\"Value\"] = values\n",
        "    df[\"PrefixCount\"] = prefix_counts\n",
        "\n",
        "    # 7) Assign Value Range and Value Label based on the calculated Value\n",
        "    def assign_value_range_label(val):\n",
        "      try:\n",
        "          val = float(val)\n",
        "      except (ValueError, TypeError):\n",
        "          return \"\", \"\"\n",
        "\n",
        "      if 60 <= val <= 100:\n",
        "          return \"100~60\", \"1-likely correct\"\n",
        "      elif 47 <= val <= 59:\n",
        "          return \"59~47\", \"2-lines forming\"\n",
        "      elif 34 <= val <= 46:\n",
        "          return \"46~34\", \"3-patterns emerging\"\n",
        "      elif 21 <= val <= 33:\n",
        "          return \"33~21\", \"4-notable patterns\"\n",
        "      elif 8 <= val <= 20:\n",
        "          return \"20~8\", \"5-patterns stable\"\n",
        "      elif 1 <= val <= 7:\n",
        "          return \"\", \"6-need research\"\n",
        "      else:\n",
        "          return \"\", \"\"\n",
        "\n",
        "\n",
        "    value_ranges = []\n",
        "    value_labels = []\n",
        "    for v in df[\"Value\"]:\n",
        "        rng, lbl = assign_value_range_label(v)\n",
        "        value_ranges.append(rng)\n",
        "        value_labels.append(lbl)\n",
        "    df[\"Value Range\"] = value_ranges\n",
        "    df[\"Value Label\"] = value_labels\n",
        "\n",
        "    # 8) Sort final by \"Yates DNA Ancestral Line\"\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], ascending=True, inplace=True)\n",
        "\n",
        "    # Remove the temporary PrefixCount column from the final output\n",
        "    df.drop(\"PrefixCount\", axis=1, inplace=True)\n",
        "\n",
        "    # 9) Final columns (with cM moved right after ID#)\n",
        "    final_cols = [\n",
        "        \"ID#\",\n",
        "        \"cM\",\n",
        "        \"Match to\",\n",
        "        \"Value\",\n",
        "        \"Value Range\",\n",
        "        \"Value Label\",\n",
        "        \"Yates DNA Ancestral Line\"\n",
        "    ]\n",
        "    df = df[final_cols]\n",
        "\n",
        "    logger.info(\"Final DataFrame columns: %s\", df.columns.tolist())\n",
        "    print(df.head(10))\n",
        "\n",
        "    # 10) Export CSV and HTML\n",
        "    csv_name = \"final_combined_df_with_value_labels.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    logger.info(\"Exported final DataFrame to '%s'.\", csv_name)\n",
        "\n",
        "    html_name = \"HTML_combined_df_with_value_labels.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table {\n",
        "      width: 100%;\n",
        "      border-collapse: collapse;\n",
        "      margin: 20px 0;\n",
        "    }\n",
        "    table, th, td {\n",
        "      border: 1px solid #333;\n",
        "    }\n",
        "    th, td {\n",
        "      padding: 8px 12px;\n",
        "      text-align: center;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "    }\n",
        "    /* Left-align the last column */\n",
        "    td:nth-child(7) {\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    html_content = css_style + df.to_html(\n",
        "        index=False,\n",
        "        columns=final_cols,\n",
        "        escape=False\n",
        "    )\n",
        "    with open(html_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Exported HTML to '%s'.\", html_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "\n",
        "# 1,300_12 min_37 sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6K2lze31_pD",
        "outputId": "6893aeb4-2ddc-471e-9cb8-b05efecb559e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:filtered_ids.xlsx not found. Skipping second-level manual filter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEDCOM contained 59511 total records\n",
            "Records tagged and filtered by NPFX: 1406\n",
            "Records with YDNA information: 90\n",
            "Autosomal matches: 1316\n",
            "Records tagged and filtered by NPFX: 1406\n",
            "Processing 1406 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1):   4%|▎         | 50/1406 [00:27<12:17,  1.84it/s]"
          ]
        }
      ]
    }
  ]
}