{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0KasjaJTFek2G4QXNz9yi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/01_30_0930_2025_HTML%2BValue_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "19aea501-99cb-4796-b52b-0bb84a67148d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_28_2033(works!)_2025_stable\n",
        "\n",
        "# Standard Libraries\n",
        "import csv\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# GEDCOM Parsing\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Alignment\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts first and last name from a GEDCOM record.\n",
        "    Handles missing or malformed names gracefully.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "\n",
        "    if name_start == 5 or name_end == -1:  # Meaning '1 NAME ' was not found\n",
        "        return \"UnknownName\"\n",
        "\n",
        "    name = record[name_start:name_end].strip()\n",
        "\n",
        "    # Handle cases where no '/' is present in the name\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")  # Take first 10 characters as default name\n",
        "\n",
        "    # Extract first and last name\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]  # first 10 chars\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}  # Global dictionary to hold name->ID mapping\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # we’ll modify name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0  # Count YDNA occurrences\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1  # YDNA found\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "        print(f'Records with YDNA information: {ydna_count}')\n",
        "        print(f'Autosomal matches: {autosomal_count}')\n",
        "\n",
        "        # First-level filter: only those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Optional second-level filter from an Excel file\n",
        "        manual_filter_activated = True  # or False\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [\n",
        "                    dataset for dataset in self.filter_pool\n",
        "                    if dataset.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    # Just automatically return the first GEDCOM file found\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    # If you want a manual selection, uncomment the below while loop:\n",
        "    #\n",
        "    # while True:\n",
        "    #     for i, file in enumerate(gedcom_files, start=1):\n",
        "    #         print(f\"{i}. {file}\")\n",
        "    #     try:\n",
        "    #         selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "    #         if 1 <= selected_num <= len(gedcom_files):\n",
        "    #             return gedcom_files[selected_num - 1]\n",
        "    #         else:\n",
        "    #             print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "    #     except ValueError:\n",
        "    #         print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "################################################################################\n",
        "#          Execute GEDCOM Parsing & Build Our Filter Pool                      #\n",
        "################################################################################\n",
        "gedcom_file_path = select_gedcom_file()\n",
        "if gedcom_file_path:\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    # Gather individuals (last_name, individual_id) from the filter pool\n",
        "    individuals = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    ################################################################################\n",
        "    # Function: Extract ID from GEDCOM Record\n",
        "    ################################################################################\n",
        "    def extract_id(record):\n",
        "        \"\"\"\n",
        "        Extracts the ID from a GEDCOM record.\n",
        "        A valid ID is enclosed within '@' symbols.\n",
        "        \"\"\"\n",
        "        id_start = record.find('@') + 1\n",
        "        id_end = record.find('@', id_start)\n",
        "\n",
        "        if id_start == 0 or id_end == -1:  # If '@' is missing\n",
        "            return \"UnknownID\"\n",
        "\n",
        "        return record[id_start:id_end].strip()\n",
        "\n",
        "    # Read the GEDCOM file as raw text, parse out records\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "\n",
        "    data = data.split('\\n0 ')  # Split records based on GEDCOM structure\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "else:\n",
        "    print(\"No GEDCOM file selected; exiting.\")\n",
        "    raise SystemExit\n",
        "\n",
        "################################################################################\n",
        "#        Functions to Traverse & Score Ancestors, Build Data for DataFrame     #\n",
        "################################################################################\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # We'll update anchor_gen1 if found\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    # Build generation_table, visited_pairs\n",
        "    find_parents(individual_id, 1, records)\n",
        "\n",
        "    # All possible ancestor paths for that ID\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "\n",
        "    filtered_ancestral_line_names = []\n",
        "\n",
        "    # Gather more info from the dataset\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "\n",
        "    # Build ancestral line (exclude anchor_gen1 itself)\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    # Reverse order\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "\n",
        "    # Check we did not accidentally include anchor_gen1\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(\n",
        "            f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\"\n",
        "        )\n",
        "\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "#         Build Rows for DataFrame from the Filter Pool                        #\n",
        "################################################################################\n",
        "combined_df_rows = []\n",
        "for dataset in gedcom_instance.filter_pool:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(\n",
        "        individual_id, gedcom_instance, records\n",
        "    )\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    individual_name = extract_name(records.get(individual_id, \"\"))\n",
        "\n",
        "    combined_df_rows.append(\n",
        "        [individual_id, sort, individual_name, cm, filtered_ancestral_line_str]\n",
        "    )\n",
        "\n",
        "################################################################################\n",
        "#       NO NEED TO MODIFY ABOVE THIS SECTION 28-1-2025                         #\n",
        "################################################################################\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "################################################################################\n",
        "#       Create and Populate Main DataFrame (combined_df)\n",
        "################################################################################\n",
        "columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "# Initialize the value_store dictionary\n",
        "value_store = {}\n",
        "\n",
        "# Populate value_store with data from the DataFrame, including a placeholder for 'Value'\n",
        "for _, row in combined_df.iterrows():\n",
        "    value_store[row[\"ID#\"]] = {\n",
        "        \"Match to\": row[\"Match to\"],\n",
        "        \"Name\": row[\"Name\"],\n",
        "        \"cM\": row[\"cM\"],\n",
        "        \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "        \"Value\": None  # Placeholder for 'Value'\n",
        "    }\n",
        "\n",
        "################################################################################\n",
        "#       Remove miscellaneous Distant ancestors (combined_df)\n",
        "################################################################################\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "    prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "\n",
        "# Order and clean up columns\n",
        "ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = combined_df[ordered_columns]\n",
        "combined_df.index += 1\n",
        "combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "\n",
        "################################################################################\n",
        "#       Segmentation and Frequency Analysis\n",
        "################################################################################\n",
        "def parse_line_to_pairs(line, delimiter=\"~~~\"):\n",
        "    \"\"\" Splits a given ancestral line into named pairs based on the delimiter. \"\"\"\n",
        "    return line.strip().split(delimiter)\n",
        "\n",
        "def find_largest_common_prefix_at_least_two_pairs(lines_as_pairs):\n",
        "    \"\"\"\n",
        "    Finds the largest common prefix that appears in at least two lines.\n",
        "    Returns the longest shared segment and the indices of lines sharing it.\n",
        "    \"\"\"\n",
        "    if not lines_as_pairs:\n",
        "        return [], []\n",
        "\n",
        "    segment_map = defaultdict(set)\n",
        "\n",
        "    # Build a mapping of segment -> line indices\n",
        "    for idx, pairs in enumerate(lines_as_pairs):\n",
        "        for start in range(len(pairs)):\n",
        "            for end in range(start + 2, len(pairs) + 1):  # Minimum length = 2\n",
        "                segment = \"~~~\".join(pairs[start:end])\n",
        "                segment_map[segment].add(idx)\n",
        "\n",
        "    # Filter to segments shared by at least two lines\n",
        "    shared_segments = {seg: idx_set for seg, idx_set in segment_map.items() if len(idx_set) >= 2}\n",
        "\n",
        "    if not shared_segments:\n",
        "        return [], []\n",
        "\n",
        "    # Sort by segment length (descending) and frequency (descending)\n",
        "    sorted_segments = sorted(\n",
        "        shared_segments.items(),\n",
        "        key=lambda x: (-len(x[0].split(\"~~~\")), -len(x[1]))\n",
        "    )\n",
        "\n",
        "    # Select the most significant segment\n",
        "    seg_str, idx_set = sorted_segments[0]\n",
        "    return seg_str.split(\"~~~\"), list(idx_set)\n",
        "\n",
        "def iterative_segmentation_min_length_2(lines):\n",
        "    \"\"\"\n",
        "    Performs iterative segmentation by extracting the longest common prefix\n",
        "    that appears in at least two lines, until no such prefixes remain.\n",
        "    \"\"\"\n",
        "    lines_as_pairs = [parse_line_to_pairs(line) for line in lines]\n",
        "    segments = []\n",
        "\n",
        "    while True:\n",
        "        prefix_list, shared_indices = find_largest_common_prefix_at_least_two_pairs(lines_as_pairs)\n",
        "        if not prefix_list or len(shared_indices) < 2:\n",
        "            break  # No more valid shared prefixes\n",
        "\n",
        "        seg_str = \"~~~\".join(prefix_list)\n",
        "        segments.append((seg_str, set(shared_indices)))\n",
        "\n",
        "        # Remove this segment from all affected lines\n",
        "        prefix_len = len(prefix_list)\n",
        "        for idx in shared_indices:\n",
        "            lines_as_pairs[idx] = lines_as_pairs[idx][prefix_len:]\n",
        "\n",
        "    # Process leftover segments\n",
        "    leftover_map = {}\n",
        "    for i, pairs in enumerate(lines_as_pairs):\n",
        "        if pairs:\n",
        "            leftover_str = \"~~~\".join(pairs)\n",
        "            leftover_map.setdefault(leftover_str, set()).add(i)\n",
        "\n",
        "    for leftover_str, idx_set in leftover_map.items():\n",
        "        segments.append((leftover_str, idx_set))\n",
        "\n",
        "    return segments, lines_as_pairs\n",
        "\n",
        "def build_presence_absence(segments, original_lines):\n",
        "    \"\"\"\n",
        "    Creates a presence/absence matrix for discovered segments.\n",
        "    Each row represents an ancestral line, and columns represent discovered segments.\n",
        "    \"\"\"\n",
        "    from_substring_sets = []\n",
        "    for line in original_lines:\n",
        "        pairs = parse_line_to_pairs(line)\n",
        "        sset = set(\"~~~\".join(pairs[i:j]) for i in range(len(pairs)) for j in range(i + 1, len(pairs) + 1))\n",
        "        from_substring_sets.append(sset)\n",
        "\n",
        "    seg_strings = [seg[0] for seg in segments]\n",
        "    presence_matrix = []\n",
        "    for sub_set in from_substring_sets:\n",
        "        row = [1 if seg_str in sub_set else 0 for seg_str in seg_strings]\n",
        "        presence_matrix.append(row)\n",
        "\n",
        "    return pd.DataFrame(presence_matrix, columns=seg_strings)\n",
        "\n",
        "################################################################################\n",
        "#       Main Processing\n",
        "################################################################################\n",
        "# Extract all ancestral lines\n",
        "all_lines = combined_df[\"Yates DNA Ancestral Line\"].tolist()\n",
        "\n",
        "# Perform segmentation\n",
        "segments_found, leftover_info = iterative_segmentation_min_length_2(all_lines)\n",
        "\n",
        "# Create a DataFrame for segment frequency analysis\n",
        "seg_df = pd.DataFrame(\n",
        "    [(s, len(idx_set)) for s, idx_set in segments_found],\n",
        "    columns=[\"Segment\", \"Frequency\"]\n",
        ").sort_values(by=\"Frequency\", ascending=False)\n",
        "\n",
        "# Build presence/absence matrix\n",
        "presence_df = build_presence_absence(segments_found, all_lines)\n",
        "\n",
        "# Calculate line values based on segment presence\n",
        "line_values = presence_df.sum(axis=1)\n",
        "combined_df[\"Value\"] = line_values\n",
        "\n",
        "# Create mapping of ID# to Value\n",
        "id_to_value_map = combined_df.set_index(\"ID#\")[\"Value\"].to_dict()\n",
        "\n",
        "################################################################################\n",
        "# Update value_store Without Printing the Debugging Report\n",
        "################################################################################\n",
        "def update_value_store(record_ids, ancestral_lines, segments, value_store):\n",
        "    \"\"\"\n",
        "    Updates value_store with correct 'TOTAL' values for each ID.\n",
        "    \"\"\"\n",
        "    for record_id, line in zip(record_ids, ancestral_lines):\n",
        "        total_contribution = 0  # Reset total per record\n",
        "        for segment, line_indices in segments:\n",
        "            if record_ids.index(record_id) in line_indices:\n",
        "                frequency = len(line_indices)\n",
        "                contribution = frequency\n",
        "                total_contribution += contribution\n",
        "\n",
        "        # Update value_store with the correct 'Value'\n",
        "        if record_id in value_store:\n",
        "            value_store[record_id][\"Value\"] = total_contribution\n",
        "        else:\n",
        "            print(f\"WARNING: ID {record_id} not found in value_store\")\n",
        "\n",
        "################################################################################\n",
        "# Execute Value Calculation\n",
        "################################################################################\n",
        "update_value_store(\n",
        "    record_ids=combined_df[\"ID#\"].tolist(),\n",
        "    ancestral_lines=all_lines,\n",
        "    segments=segments_found,\n",
        "    value_store=value_store\n",
        ")\n",
        "\n",
        "################################################################################\n",
        "# FINAL CHECK: Ensure combined_df is updated correctly before HTML generation\n",
        "################################################################################\n",
        "print(\"\\nFinal Verification of value_store (AFTER update):\\n\")\n",
        "\n",
        "# Define headers\n",
        "headers = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Value\"]\n",
        "row_format = \"{:<10} {:<15} {:<25} {:<5} {:<5}\"  # Adjust spacing based on expected lengths\n",
        "\n",
        "# Print headers (centered)\n",
        "print(row_format.format(*headers))\n",
        "print(\"=\" * 65)  # Separator line\n",
        "\n",
        "# Print each row from value_store (limit to 50)\n",
        "for index, (id_, data) in enumerate(value_store.items()):\n",
        "    if index >= 50:  # Stop after printing 50 records\n",
        "        break\n",
        "    print(row_format.format(\n",
        "        id_,\n",
        "        data['Match to'],\n",
        "        data['Name'],\n",
        "        str(data['cM']),\n",
        "        str(data['Value'])\n",
        "    ))\n",
        "\n",
        "print(\"\\n\")  # Line break after printing\n",
        "\n",
        "\n",
        "print(\"\\n\")  # Line break at the end\n",
        "\n",
        "################################################################################\n",
        "# Ensure combined_df is updated before HTML output\n",
        "################################################################################\n",
        "combined_df[\"Value\"] = combined_df[\"ID#\"].map(lambda id_: value_store[id_][\"Value\"] if id_ in value_store else None)\n",
        "\n",
        "# Create hotlink function\n",
        "def create_hotlink(row):\n",
        "    \"\"\"\n",
        "    Generates an HTML hyperlink for the ID# column.\n",
        "    \"\"\"\n",
        "    url_base = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "    additional_params = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "\n",
        "    if pd.notnull(row[\"ID#\"]):\n",
        "        return f'<a href=\"{url_base}{row[\"ID#\"]}{additional_params}\">{row[\"ID#\"]}</a>'\n",
        "    return \"\"  # Return empty if no valid ID#\n",
        "\n",
        "# Apply the hotlink function and replace \"ID#\" with clickable links\n",
        "if \"ID#\" in combined_df.columns:\n",
        "    combined_df[\"ID#\"] = combined_df.apply(create_hotlink, axis=1)\n",
        "else:\n",
        "    print(\"Error: 'ID#' column not found in DataFrame\")\n",
        "\n",
        "# Reorder columns so \"Value\" appears before \"Yates DNA Ancestral Line\"\n",
        "ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = combined_df[ordered_columns]\n",
        "\n",
        "# Sort dataset by \"Yates DNA Ancestral Line\" in descending order\n",
        "combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "\n",
        "###############################################################################\n",
        "#               Configure these Booleans to Enable/Disable Outputs            #\n",
        "###############################################################################\n",
        "GENERATE_MAIN_HTML = False\n",
        "GENERATE_MINIMAL_HTML = True  # ✅ Ensure this variable is declared!\n",
        "\n",
        "################################################################################\n",
        "#      Conditional Output: MAIN HTML (Use Boolean Switch)\n",
        "################################################################################\n",
        "if GENERATE_MAIN_HTML:\n",
        "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    output_html_path = f\"htmloutput_{current_datetime}.html\"\n",
        "\n",
        "    # CSS for styling\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table, th, td {\n",
        "      border: 1px solid black;\n",
        "      border-collapse: collapse;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "      text-align: center;\n",
        "    }\n",
        "    td {\n",
        "      text-align: center;\n",
        "    }\n",
        "    td:nth-child(6) {  /* Yates DNA Ancestral Line column */\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the HTML table\n",
        "    html_main = css_style + combined_df.to_html(\n",
        "        index=False,\n",
        "        classes=\"sortable\",\n",
        "        escape=False,\n",
        "        border=0  # Let CSS handle borders\n",
        "    )\n",
        "\n",
        "    # Ensure \"Yates DNA Ancestral Line\" remains left-aligned\n",
        "    html_main = html_main.replace(\n",
        "        \"<th>Yates DNA Ancestral Line</th>\",\n",
        "        '<th style=\"text-align:left;\">Yates DNA Ancestral Line</th>'\n",
        "    )\n",
        "\n",
        "    # Save HTML to file\n",
        "    with open(output_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_main)\n",
        "\n",
        "################################################################################\n",
        "#      Minimal 2-Column HTML Output (Optional)\n",
        "################################################################################\n",
        "def generate_html_output_with_headers(dict_data):\n",
        "    \"\"\"\n",
        "    Creates an HTML table containing only:\n",
        "      - 'Match to'\n",
        "      - 'Yates DNA Ancestral Line'\n",
        "    sorted DESC by 'Yates DNA Ancestral Line'.\n",
        "    \"\"\"\n",
        "    sorted_data = sorted(dict_data, key=lambda x: x.get(\"Yates DNA Ancestral Line\", \"\"), reverse=True)\n",
        "\n",
        "    html_head = \"\"\"\n",
        "<style>\n",
        "table, th, td {\n",
        "  border: 1px solid black;\n",
        "  border-collapse: collapse;\n",
        "}\n",
        "th {\n",
        "  background-color: #f2f2f2;\n",
        "  text-align: left;\n",
        "}\n",
        "</style>\n",
        "<table>\n",
        "  <tr>\n",
        "    <th style=\"text-align:left;\">Match to</th>\n",
        "    <th style=\"text-align:left;\">Yates DNA Ancestral Line</th>\n",
        "  </tr>\n",
        "\"\"\"\n",
        "    html_body = []\n",
        "    for row in sorted_data:\n",
        "        match_to = row.get(\"Match to\", \"\")\n",
        "        yates_line = row.get(\"Yates DNA Ancestral Line\", \"\")\n",
        "        html_body.append(f\"  <tr><td>{match_to}</td><td>{yates_line}</td></tr>\")\n",
        "\n",
        "    return \"\\n\".join([html_head] + html_body + [\"</table>\"])\n",
        "\n",
        "if GENERATE_MINIMAL_HTML:\n",
        "    records_for_html = combined_df[[\"Match to\", \"Yates DNA Ancestral Line\"]].to_dict(orient=\"records\")\n",
        "    minimal_html_code = generate_html_output_with_headers(records_for_html)\n",
        "\n",
        "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    minimal_html_path = f\"minimal_htmloutput_{current_datetime}.html\"\n",
        "\n",
        "    with open(minimal_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(minimal_html_code)\n",
        "\n",
        "################################################################################\n",
        "# Export Final Data\n",
        "################################################################################\n",
        "#combined_df.to_csv(\"final_combined_df.csv\", index=False)\n",
        "#seg_df.to_csv(\"segments_discovered.csv\", index=False)\n",
        "#presence_df.to_csv(\"segments_presence_absence.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b017d3-8b92-412a-962b-e4fe5b4be905"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 1\n",
            "After manual filter, total records: 2\n",
            "Records tagged and filtered by NPFX: 2\n",
            "\n",
            "Final Verification of value_store (AFTER update):\n",
            "\n",
            "ID#        Match to        Name                      cM    Value\n",
            "=================================================================\n",
            "I46369     yates,johnh     PerkinsTeresaG            13    3    \n",
            "I46791     yates,ronald    LeavertonMelissaA         11    3    \n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}