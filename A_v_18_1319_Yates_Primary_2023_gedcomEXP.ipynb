{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnN/29Z/mUDztITX6qH635",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/A_v_18_1319_Yates_Primary_2023_gedcomEXP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVWCghMxefi",
        "outputId": "9dd5087d-d37d-4d66-967a-5496600895bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 51006 total records\n",
            "Records tagged and filtered by NPFX: 405\n",
            "Manual filter IDs loaded: 0\n",
            "After manual filter, total records: 1\n",
            "314-Debug - winning_path_ids: ['I50522', 'I50519', 'I50517', 'I50515', 'I50513', 'I44014', 'I21091', 'I21089', 'I18899', 'I11851', 'I11866', 'I9912', 'I9914', 'I11798', 'I24380', 'I24382', 'I24502', 'I47548', 'I47549']\n",
            "314-Debug - winning_path_ids: ['I50522', 'I50519', 'I50517', 'I50515', 'I50513', 'I44014', 'I21091', 'I21089', 'I18899', 'I11851', 'I11866', 'I9912', 'I9914', 'I11798', 'I24380', 'I24382', 'I24502', 'I47548', 'I47549']\n",
            "392-Debug - winning_path_names: ['FehrenbachAngelaDe', 'SwinsonRita', 'SwinsonPaulRoss', 'BennettHazelBen', 'RictorInez', 'YatesJuliaAnn', 'YatesWilliam', 'YatesSimeon', 'YatesWilliam', 'YatesJoshua', 'YatesSamuel', 'YatesGeorge', 'YatesGeorge', 'YatesJohn', 'YatesThomas', 'YatesFrancis', 'YatesThomas', 'YatesJohn', 'YatesRichard']\n",
            "393-Debug - filtered_parent_pairs before sorting: {}\n",
            "Filtered Parent Pairs String for I50522: \n",
            "314-Debug - winning_path_ids: ['I50522', 'I50519', 'I50517', 'I50515', 'I50513', 'I44014', 'I21091', 'I21089', 'I18899', 'I11851', 'I11866', 'I9912', 'I9914', 'I11798', 'I24380', 'I24382', 'I24502', 'I47548', 'I47549']\n",
            "392-Debug - winning_path_names: ['FehrenbachAngelaDe', 'SwinsonRita', 'SwinsonPaulRoss', 'BennettHazelBen', 'RictorInez', 'YatesJuliaAnn', 'YatesWilliam', 'YatesSimeon', 'YatesWilliam', 'YatesJoshua', 'YatesSamuel', 'YatesGeorge', 'YatesGeorge', 'YatesJohn', 'YatesThomas', 'YatesFrancis', 'YatesThomas', 'YatesJohn', 'YatesRichard']\n",
            "393-Debug - filtered_parent_pairs before sorting: {}\n",
            "Filtered Parent Pairs String for I50522: \n",
            "314-Debug - winning_path_ids: ['I50522', 'I50519', 'I50517', 'I50515', 'I50513', 'I44014', 'I21091', 'I21089', 'I18899', 'I11851', 'I11866', 'I9912', 'I9914', 'I11798', 'I24380', 'I24382', 'I24502', 'I47548', 'I47549']\n",
            "392-Debug - winning_path_names: ['FehrenbachAngelaDe', 'SwinsonRita', 'SwinsonPaulRoss', 'BennettHazelBen', 'RictorInez', 'YatesJuliaAnn', 'YatesWilliam', 'YatesSimeon', 'YatesWilliam', 'YatesJoshua', 'YatesSamuel', 'YatesGeorge', 'YatesGeorge', 'YatesJohn', 'YatesThomas', 'YatesFrancis', 'YatesThomas', 'YatesJohn', 'YatesRichard']\n",
            "393-Debug - filtered_parent_pairs before sorting: {}\n",
            "Filtered Parent Pairs String for I50522: \n",
            "      ID#      Match to                Name  cM  \\\n",
            "1  I50522  yates,ronald  FehrenbachAngelaDe  17   \n",
            "\n",
            "                                                LUN# Yates DNA Ancestral Line  \n",
            "1  <a href=\"https://yates.one-name.net/tng/vertic...                           \n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10] # Use slicing syntax to extract the first 10 characters of the first_name variable\n",
        "    last_name = last_name[:10].rstrip('/') # Use slicing syntax to extract the first 10 characters of the last_name variable\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "# Global dictionary to hold name to ID mapping\n",
        "name_to_id = {}\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            import pandas as pd  # Assuming you haven't imported it yet\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list,add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "#    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    # ... (same code, but store individual IDs in visited_pairs and generation_table)\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "#            print(f\"Added to generation_table: {parent_pair} at generation {generation}\")\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "#    print(f\"Visited pairs so far: {visited_pairs}\")\n",
        "\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "#    print(f\"Distant ancestors paths for {individual_id}: {paths}\")\n",
        "\n",
        "    return paths\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "# Additional Function to isolate score calculation logic\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    # ... (same code, but works with individual IDs)\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]  # Get the IDs of the winning path\n",
        "        print(f\"314-Debug - winning_path_ids: {winning_path_ids}\")  # Print the IDs of the winning path\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        print(\"Debug - winning_path_ids: None\")  # Print None if no winning path found\n",
        "\n",
        "    return winning_path_score, winning_path_names\n",
        "\n",
        "\n",
        "# New function to filter the full ancestral line based on the DNA line\n",
        "def filter_ancestral_line(dna_line_ids, full_ancestral_line_ids):\n",
        "    return [pair for pair in full_ancestral_line_ids if any(id in pair for id in dna_line_ids)]\n",
        "\n",
        "# Main Loop\n",
        "for dataset in filtered_datasets:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs = set()\n",
        "    generation_table = []\n",
        "\n",
        "    # Find full ancestral line and distant ancestors\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "\n",
        "    # Calculate score and get DNA line\n",
        "    winning_path_score, winning_dna_line_ids = calculate_score(distant_ancestors_paths, records)\n",
        "\n",
        "    # Filter the full ancestral line based on the DNA line\n",
        "    filtered_ancestral_line_ids = filter_ancestral_line(winning_dna_line_ids, [pair for generation, pair in generation_table])\n",
        "\n",
        "# Main Loop\n",
        "# Assuming name_to_id is populated elsewhere in your code\n",
        "name_to_id = {}  # Placeholder, populate this after reading the GEDCOM file\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "\n",
        "    # Resetting the variables for each individual\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    # Finding parents and distant ancestors\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "\n",
        "    # Calculate the winning path\n",
        "    winning_path_score, winning_path_names = calculate_score(distant_ancestors_paths, records)\n",
        "\n",
        "    # Extract other information for this individual\n",
        "    individual_data = {}\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            individual_data['cM'] = dataset.get_extractable_cm()\n",
        "            individual_data['Sort'] = dataset.get_extractable_sort()\n",
        "            break\n",
        "\n",
        "    # Filter the parent pairs based on the winning path\n",
        "    selected_dna_branch_ids = {name_to_id[name] for name in winning_path_names if name in name_to_id}\n",
        "    filtered_parent_pairs = {}\n",
        "    for generation, parent_pair_tuple in generation_table:\n",
        "        parent1, parent2 = parent_pair_tuple  # Unpack the tuple\n",
        "        for parent in [parent1, parent2]:  # Loop through the unpacked parent IDs\n",
        "            if parent in selected_dna_branch_ids:\n",
        "                filtered_parent_pairs[generation] = f\"{parent1}&{parent2}\"  # Reconstruct the parent_pair string\n",
        "                break\n",
        "\n",
        "    # Sort the filtered_parent_pairs by the generation numbers (keys)\n",
        "    sorted_parent_pairs = sorted(filtered_parent_pairs.items())\n",
        "\n",
        "    # Extract just the parent pairs, discarding the generation numbers\n",
        "    sorted_parent_pairs_str_list = [pair for generation, pair in sorted_parent_pairs]\n",
        "\n",
        "    # Concatenate into the final string using '|' as separator\n",
        "    filtered_parent_pairs_str = '|'.join(sorted_parent_pairs_str_list)\n",
        "\n",
        "#    print(f\"390-Debug - individual_id: {individual_id}\")\n",
        "#    print(f\"391-Debug - distant_ancestors_paths: {distant_ancestors_paths}\")\n",
        "    print(f\"392-Debug - winning_path_names: {winning_path_names}\")\n",
        "\n",
        "\n",
        "    print(f\"393-Debug - filtered_parent_pairs before sorting: {filtered_parent_pairs}\")\n",
        "\n",
        "    print(f\"Filtered Parent Pairs String for {individual_id}: {filtered_parent_pairs_str}\")\n",
        "\n",
        "    individual_data['Filtered Ancestral Line'] = filtered_parent_pairs_str\n",
        "\n",
        "    return individual_data, filtered_parent_pairs_str  # Return both individual_data and the formatted ancestral line string\n",
        "\n",
        "\n",
        "# Initialize your empty combined_df_rows list here, before the main loop\n",
        "combined_df_rows = []\n",
        "\n",
        "# Main Loop\n",
        "for dataset in filtered_datasets:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs = set()\n",
        "    generation_table = []\n",
        "\n",
        "    # Call the refactored process_individual function\n",
        "    individual_data, filtered_parent_pairs_str = process_individual(individual_id, gedcom_instance, records)\n",
        "\n",
        "    cm = individual_data['cM']\n",
        "    sort = individual_data['Sort']\n",
        "\n",
        "    # No need for final_output, we'll use filtered_parent_pairs_str directly\n",
        "    combined_df_rows.append([individual_id, sort, individual_name, cm, filtered_parent_pairs_str])\n",
        "    # Removed the second call to process_individual\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Global variables\n",
        "visited_pairs = set()\n",
        "\n",
        "# Assuming gedcom_instance and path_scores variables are already defined\n",
        "filtered_individuals = [(dataset.get_anchor_gen1(), dataset.get_gen_person()) for dataset in gedcom_instance.filter_pool]\n",
        "\n",
        "combined_df_rows = []  # Initialize your empty combined_df_rows list\n",
        "\n",
        "# Main Loop\n",
        "for dataset in filtered_datasets:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs = set()\n",
        "    generation_table = []\n",
        "\n",
        "    # Call the refactored process_individual function\n",
        "    individual_data = process_individual(individual_id, gedcom_instance, records)\n",
        "\n",
        "    # Now, individual_data will contain 'cM', 'Sort', and 'Filtered Ancestral Line'\n",
        "    # You can use this information for further processing or output.\n",
        "\n",
        "    # Translate IDs to Names\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    formatted_names = [f\"{name}\" for name in winning_path_names]\n",
        "    final_output = \"~\".join(formatted_names)\n",
        "\n",
        "    individual_data, filtered_parent_pairs_str = process_individual(individual_id, gedcom_instance, records)\n",
        "    cm = individual_data['cM']\n",
        "    sort = individual_data['Sort']\n",
        "\n",
        "    # Swapping the positions of 'individual_name' and 'sort' in the output\n",
        "    combined_df_rows.append([individual_id, sort, individual_name, cm, filtered_parent_pairs_str])\n",
        "\n",
        "\n",
        "# Define whether to use hotlinks or not\n",
        "USE_HOTLINK = True  # Set this to False if you don't want to use hotlinks\n",
        "\n",
        "def create_hotlink(row):\n",
        "    if USE_HOTLINK:\n",
        "        url_base = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "        additional_params = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "        person_id = row['ID#']\n",
        "        hotlink_url = f'{url_base}{person_id}{additional_params}'\n",
        "        return f'<a href=\"{hotlink_url}\">{person_id}</a>'\n",
        "    else:\n",
        "        return row['ID#']\n",
        "\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=['ID#', 'Match to', 'Name', 'cM', 'Yates DNA Ancestral Line'])  # Excluded 'Score' from columns\n",
        "\n",
        "combined_df['LUN#'] = combined_df.apply(lambda row: create_hotlink(row), axis=1)\n",
        "\n",
        "ordered_columns = ['ID#', 'Match to', 'Name', 'cM', 'LUN#', 'Yates DNA Ancestral Line']  # Removed 'Score' from ordered columns\n",
        "combined_df = combined_df[ordered_columns]\n",
        "\n",
        "combined_df.index = combined_df.index + 1\n",
        "\n",
        "# Sort the DataFrame as you wanted\n",
        "combined_df = combined_df.sort_values(by=['Match to', 'Yates DNA Ancestral Line'], ascending=[False, True])\n",
        "\n",
        "print(combined_df)\n",
        "\n",
        "combined_df.to_excel('/content/output.xlsx', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik3CzJtgxuXH",
        "outputId": "8db00666-f18c-402b-bcb6-952122e98d91"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: python-gedcom in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    }
  ]
}