{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1RROHlBgtXAYWOqDWQkIX8NCciZBqPcV3",
      "authorship_tag": "ABX9TyPZiukX/SgccIXIJVu4SxBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/New_Gold__Orchestrator_20251130_1200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIP"
      ],
      "metadata": {
        "id": "XtvXRl-lcavJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "403f29a1-98d5-4a94-a1e0-0aaf23424dbf",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "ERROR: unknown command \"caas_jupyter_tools\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n",
        "!pip caas_jupyter_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 0"
      ],
      "metadata": {
        "id": "ydh7RNi7elx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.11.30-G3)\n",
        "# - Complete & runnable Colab cell: one contiguous block, no fragments.\n",
        "# - Source ASCII-only; any file writes must use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Authority:\n",
        "#     * Colab work dir: /content\n",
        "#     * Scripts pulled from: /partials/colab_cells/*.py  (server)\n",
        "#     * Latest GEDCOM pulled from: /tng/gedcom/*.ged     (server)\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2025.11.30-G3 | Encoding=ISO-8859-15\n",
        "# - Execution order (after pulls): cell1.py, cell2.py, cell2b.py, cell2c.py, cell3.py\n",
        "# ====================================================================\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2025.11.30-G3 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os, socket, traceback, hashlib\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "\n",
        "# ---------- 0) Env / secrets ----------\n",
        "\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\", \"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\", \"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\", \"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\", \"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\", \"\") or \"\").strip().strip(\"/\")\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\n",
        "    \"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\"\n",
        "    % (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\")\n",
        ")\n",
        "\n",
        "if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "    raise SystemExit(\"[FATAL] Missing FTP_HOST/FTP_USER/FTP_PASS; cannot run orchestrator.\")\n",
        "\n",
        "# ---------- 1) FTPS helpers ----------\n",
        "\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _ftps_connect():\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()  # Explicit FTPS\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for seg in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.cwd(seg)\n",
        "            except all_errors:\n",
        "                try:\n",
        "                    ftps.mkd(seg)\n",
        "                except all_errors:\n",
        "                    pass\n",
        "                ftps.cwd(seg)\n",
        "    return ftps\n",
        "\n",
        "def _sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "# ---------- 2) Pull authority scripts from /partials/colab_cells ----------\n",
        "\n",
        "SCRIPT_REMOTE_DIR = \"/partials/colab_cells\"\n",
        "SCRIPT_NAMES = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell3.py\"]\n",
        "\n",
        "def pull_authority_scripts():\n",
        "    print(\"[STEP] Pulling authority scripts from server ...\")\n",
        "    pulled = 0\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        try:\n",
        "            pwd0 = ftps.pwd()\n",
        "        except Exception:\n",
        "            pwd0 = \"(unknown)\"\n",
        "        print(\"[OK] Connected via FTPS (explicit AUTH TLS).\")\n",
        "        print(\"[INFO] Initial PWD on server: %s\" % pwd0)\n",
        "\n",
        "        # Move to /partials/colab_cells, independent of FTP_DIR\n",
        "        try:\n",
        "            # go to root, then walk to /partials/colab_cells\n",
        "            try:\n",
        "                ftps.cwd(\"/\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            for seg in [p for p in SCRIPT_REMOTE_DIR.split(\"/\") if p]:\n",
        "                ftps.cwd(seg)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Unable to cwd to %s (%s)\" % (SCRIPT_REMOTE_DIR, e))\n",
        "\n",
        "        print(\"[INFO] Using remote dir for scripts: %s\" % SCRIPT_REMOTE_DIR)\n",
        "\n",
        "        os.chdir(\"/content\")\n",
        "        for name in SCRIPT_NAMES:\n",
        "            try:\n",
        "                local_path = os.path.join(\"/content\", name)\n",
        "                with open(local_path, \"wb\") as f:\n",
        "                    ftps.retrbinary(\"RETR \" + name, f.write)\n",
        "                sz = os.path.getsize(local_path)\n",
        "                sh = _sha256_of_file(local_path)\n",
        "                print(\"[PULL] %s -> %s  size=%d  sha256=%s\" % (name, local_path, sz, sh))\n",
        "                pulled += 1\n",
        "            except Exception as e:\n",
        "                print(\"[MISS] Could not pull %s: %s\" % (name, e))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Pulled %d script(s) from authority shelf.\" % pulled)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Script pull failed:\", e)\n",
        "        traceback.print_exc()\n",
        "    return pulled\n",
        "\n",
        "# ---------- 3) Pull latest GEDCOM from /tng/gedcom ----------\n",
        "\n",
        "GEDCOM_REMOTE_DIR = \"/tng/gedcom\"\n",
        "LOCAL_GED_DIR = \"/content\"\n",
        "\n",
        "def _choose_latest_gedcom(ftps, names):\n",
        "    \"\"\"\n",
        "    Pick the newest *.ged using MDTM if available; fallback = last alphabetically.\n",
        "    \"\"\"\n",
        "    ged_files = [n for n in names if n.lower().endswith(\".ged\")]\n",
        "    if not ged_files:\n",
        "        return None\n",
        "\n",
        "    latest_name = None\n",
        "    latest_ts = None\n",
        "\n",
        "    for nm in ged_files:\n",
        "        ts = None\n",
        "        try:\n",
        "            # MDTM response: '213 YYYYMMDDhhmmss'\n",
        "            resp = ftps.sendcmd(\"MDTM \" + nm)\n",
        "            parts = resp.strip().split()\n",
        "            if len(parts) == 2 and parts[0] == \"213\":\n",
        "                ts = parts[1]\n",
        "        except Exception:\n",
        "            ts = None\n",
        "        if ts is None:\n",
        "            # Fallback: lexical key\n",
        "            ts = \"00000000000000\" + nm\n",
        "        if latest_ts is None or ts > latest_ts:\n",
        "            latest_ts = ts\n",
        "            latest_name = nm\n",
        "    return latest_name\n",
        "\n",
        "def pull_latest_gedcom():\n",
        "    print(\"\\n[STEP] Pulling latest GEDCOM from %s ...\" % GEDCOM_REMOTE_DIR)\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        # Go to absolute /tng/gedcom irrespective of FTP_DIR\n",
        "        try:\n",
        "            ftps.cwd(\"/\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        for seg in [p for p in GEDCOM_REMOTE_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "        names = ftps.nlst()\n",
        "        if not names:\n",
        "            print(\"[WARN] No files listed in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        latest = _choose_latest_gedcom(ftps, names)\n",
        "        if not latest:\n",
        "            print(\"[WARN] No .ged files found in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        print(\"[INFO] Latest GEDCOM on server: %s\" % latest)\n",
        "\n",
        "        # Clean any old local GEDCOMs so Cell 1 can't accidentally pick the wrong one\n",
        "        try:\n",
        "            for fname in os.listdir(LOCAL_GED_DIR):\n",
        "                if fname.lower().endswith(\".ged\"):\n",
        "                    try:\n",
        "                        os.remove(os.path.join(LOCAL_GED_DIR, fname))\n",
        "                        print(\"[CLEAN] Removed old local GEDCOM:\", fname)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] Could not remove %s: %s\" % (fname, e))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Could not scan local GED dir:\", e)\n",
        "\n",
        "        # Download latest into /content with same filename\n",
        "        local_path = os.path.join(LOCAL_GED_DIR, latest)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR \" + latest, f.write)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        sz = os.path.getsize(local_path)\n",
        "        print(\"[OK] Pulled GEDCOM -> %s  size=%d bytes\" % (local_path, sz))\n",
        "        print(\"[INFO] Cell 1 will now see exactly one *.ged in /content.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] GEDCOM pull failed; Cell 1 will use any existing local *.ged instead.\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 4) Run scripts in order ----------\n",
        "\n",
        "def run_script(path):\n",
        "    print(\"\\n[RUN] %s\" % path)\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[SKIP] %s not found in /content.\" % path)\n",
        "        return\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"iso-8859-15\", errors=\"ignore\") as f:\n",
        "            code = f.read()\n",
        "        compiled = compile(code, path, \"exec\")\n",
        "        exec(compiled, globals(), globals())\n",
        "        print(\"[DONE] %s\" % path)\n",
        "    except SystemExit as e:\n",
        "        print(\"[EXIT] %s exited: %s\" % (path, e))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Exception while running %s: %s\" % (path, e))\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    os.chdir(\"/content\")\n",
        "    pulled = pull_authority_scripts()\n",
        "    if pulled == 0:\n",
        "        print(\"[FATAL] No authority scripts pulled; aborting.\")\n",
        "        return\n",
        "\n",
        "    # Always pull the latest GEDCOM snapshot before running Cell 1\n",
        "    pull_latest_gedcom()\n",
        "\n",
        "    script_order = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell3.py\"]\n",
        "    print(\"\\n[STEP] Running scripts in order:\", \", \".join(script_order))\n",
        "    for s in script_order:\n",
        "        run_script(os.path.join(\"/content\", s))\n",
        "\n",
        "main()\n",
        "\n",
        "print(\"\\n--- Cell 0 That's all folks, Orchestrator complete (authority scripts + latest GEDCOM pulled, then Cell1/2/2b/2c/3 executed) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLbYv5vtHvoc",
        "outputId": "8e99d9ce-9008-40da-81c2-78cd5bc9b323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2025.11.30-G3 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[STEP] Pulling authority scripts from server ...\n",
            "[OK] Connected via FTPS (explicit AUTH TLS).\n",
            "[INFO] Initial PWD on server: /\n",
            "[INFO] Using remote dir for scripts: /partials/colab_cells\n",
            "[PULL] cell1.py -> /content/cell1.py  size=25624  sha256=f4a9f8aaa5be4317\n",
            "[PULL] cell2.py -> /content/cell2.py  size=39821  sha256=857ed34e45060d8d\n",
            "[PULL] cell2b.py -> /content/cell2b.py  size=38691  sha256=610b68482191f255\n",
            "[PULL] cell2c.py -> /content/cell2c.py  size=16653  sha256=91c1a583dd9a776d\n",
            "[PULL] cell3.py -> /content/cell3.py  size=23320  sha256=e7091d2d79a4d827\n",
            "[OK] Pulled 5 script(s) from authority shelf.\n",
            "\n",
            "[STEP] Pulling latest GEDCOM from /tng/gedcom ...\n",
            "[INFO] Latest GEDCOM on server: yates_study_2025.ged\n",
            "[CLEAN] Removed old local GEDCOM: yates_study_2025.ged\n",
            "[OK] Pulled GEDCOM -> /content/yates_study_2025.ged  size=38705063 bytes\n",
            "[INFO] Cell 1 will now see exactly one *.ged in /content.\n",
            "\n",
            "[STEP] Running scripts in order: cell1.py, cell2.py, cell2b.py, cell2c.py, cell3.py\n",
            "\n",
            "[RUN] /content/cell1.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.30-G2 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[WARN] MDTM unsupported; using lexicographically last GEDCOM: yates_study_2025.ged\n",
            "[OK] Pulled GEDCOM from /tng/gedcom: yates_study_2025.ged -> /content/yates_study_2025.ged\n",
            "Automatically selected GEDCOM file: /content/yates_study_2025.ged\n",
            "GEDCOM contained 62567 total records\n",
            "Records tagged and filtered by NPFX: 1605\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1605\n",
            "After manual filter, total records: 93\n",
            "[OK] Wrote autosomal_count.txt = 1605\n",
            "[OK] LAST_UPDATED_TEXT for dna_vitals.csv: 2025-12-05 01:27 UTC\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/cell1.py:460: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  last_updated_text = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 93 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rBuilding Yates Lines (Stage 1):   0%|          | 0/93 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "JvOlmbj91AGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.11.27-G2)\n",
        "# - Complete and runnable Colab cell, one contiguous block (no fragments).\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography via /partials/dna_tree_styles.css (HTML export only).\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.27-G2 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes; replace non-Latin with XML entities.\n",
        "# =================================================================================================\n",
        "\n",
        "import os, re, glob, logging, functools, socket, traceback, hashlib\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "from string import Template\n",
        "\n",
        "CELL_NAME = \"Cell1_FTPS_Explicit\"\n",
        "VERSION   = \"2025.11.27-G2\"\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=%s | Version=%s | Encoding=ISO-8859-15\" % (CELL_NAME, VERSION))\n",
        "\n",
        "# ---------- Logging ----------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(CELL_NAME)\n",
        "\n",
        "# ---------- Timezone helper (EST) ----------\n",
        "def _now_est_string():\n",
        "    \"\"\"\n",
        "    Return a human formatted EST timestamp like:\n",
        "    November 27, 2025 8:09 PM\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Python 3.9+ standard library\n",
        "        from zoneinfo import ZoneInfo\n",
        "        tz = ZoneInfo(\"America/New_York\")\n",
        "        now = datetime.now(tz)\n",
        "    except Exception:\n",
        "        # Fallback: naive local time if zoneinfo is unavailable\n",
        "        now = datetime.now()\n",
        "\n",
        "    month_name = now.strftime(\"%B\")\n",
        "    day = now.day\n",
        "    year = now.year\n",
        "    hour_24 = now.hour\n",
        "    minute = now.minute\n",
        "    ampm = \"AM\" if hour_24 < 12 else \"PM\"\n",
        "    hour_12 = hour_24 % 12\n",
        "    if hour_12 == 0:\n",
        "        hour_12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, day, year, hour_12, minute, ampm)\n",
        "\n",
        "# ---------- Secrets (env or userdata) ----------\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\",\"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\",\"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\",\"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\",\"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\",\"\") or \"\").strip().strip(\"/\")\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\" %\n",
        "      (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\"))\n",
        "\n",
        "# ---------- FTPS (Explicit AUTH TLS) ----------\n",
        "def _ftps_connect():\n",
        "    if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "        raise RuntimeError(\"Missing FTP_HOST/FTP_USER/FTP_PASS.\")\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()                 # Explicit FTPS: AUTH TLS before login\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()           # Encrypt data channel\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for p in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(p)\n",
        "        except all_errors:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except all_errors:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "\n",
        "def _ftps_upload(ftps, local_path, remote_name):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR \" + remote_name, fh)\n",
        "    print(\"[OK] Uploaded: %s -> %s/%s\" % (local_path, ftps.pwd().rstrip(\"/\"), remote_name))\n",
        "\n",
        "# ---------- Outputs / Paths ----------\n",
        "REMOTE_DIR        = \"partials\"\n",
        "CSV_OUT_LOCAL     = \"final_combined_df_with_value_labels.csv\"\n",
        "HTML_OUT_LOCAL    = \"cell1_work_table.htm\"\n",
        "ABS_CSV_URL       = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(CSV_OUT_LOCAL))\n",
        "ABS_HOME_URL      = \"/index.htm\"\n",
        "\n",
        "# vitals file used by Cell 2 / Cell 3\n",
        "VITALS_CSV_PATH        = \"dna_vitals.csv\"\n",
        "AUTOSOMAL_COUNT_TXT    = \"autosomal_count.txt\"  # legacy count for Cell 3 JS\n",
        "\n",
        "# ---------- Minimal GEDCOM parse helpers ----------\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get(\"NAME\", \"\") or \"\"\n",
        "        parts = name.split(\"/\", 1)\n",
        "        first_name = parts[0].split(\" \")[0] if parts else \"\"\n",
        "        last_name  = parts[1].rstrip(\"/\") if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip(\"@\")\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            cm = v.split(\"&\")[0].strip()\n",
        "        elif \"**\" in v:\n",
        "            cm = v.split(\"**\")[0].strip()\n",
        "        else:\n",
        "            cm = v.strip()\n",
        "        try:\n",
        "            int(cm)\n",
        "            return cm\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            s = v.split(\"&\")[1]\n",
        "            return (s.split(\"**\")[0] if \"**\" in s else s).strip()\n",
        "        return \"\"\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        return v.split(\"**\")[1].strip() if \"**\" in v else \"\"\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return (self.extractable_detail.get(\"FAMC\", \"\") or \"\").strip(\"@\")\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "        # Counters / vitals\n",
        "        self.total_records = 0\n",
        "        self.npfx_count = 0\n",
        "        self.ydna_count = 0\n",
        "        self.autosomal_count = 0\n",
        "        self.after_manual_filter_total = 0\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(\" \", 2)\n",
        "            if not parts or not parts[0].isdigit():\n",
        "                continue\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith(\"@\") and tag.endswith(\"@\") and value == \"INDI\":\n",
        "                total += 1\n",
        "                current = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current)\n",
        "            elif current is not None:\n",
        "                if level == 1 and tag in [\"NAME\", \"FAMC\"]:\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == \"NPFX\":\n",
        "                    npfx_count += 1\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                    if value and \"**\" in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal = npfx_count - ydna_count\n",
        "\n",
        "        # Store vitals on the instance\n",
        "        self.total_records = total\n",
        "        self.npfx_count = npfx_count\n",
        "        self.ydna_count = ydna_count\n",
        "        self.autosomal_count = autosomal\n",
        "\n",
        "        print(\"GEDCOM contained %d total records\" % total)\n",
        "        print(\"Records tagged and filtered by NPFX: %d\" % npfx_count)\n",
        "        print(\"Records with YDNA information: %d\" % ydna_count)\n",
        "        print(\"Autosomal matches (NPFX minus YDNA): %d\" % autosomal)\n",
        "\n",
        "        # First-level filter: keep only records with NPFX\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        # Second-level manual filter via filtered_ids.xlsx (if present)\n",
        "        try:\n",
        "            df_filter = pd.read_excel(\"filtered_ids.xlsx\")\n",
        "            manual_ids = set(str(x) for x in df_filter[\"ID\"])\n",
        "            self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_ids]\n",
        "            print(\"After manual filter, total records: %d\" % len(self.filter_pool))\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "\n",
        "        # Final: record the post-filter count as a vital\n",
        "        self.after_manual_filter_total = len(self.filter_pool)\n",
        "\n",
        "        return autosomal\n",
        "\n",
        "def _chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "def _quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find(\"\\n\", start)\n",
        "    end = len(full_text) if end == -1 else end\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if \"/\" not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split(\"/\", 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "def _find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        _find_parents(father_id, generation + 1, parents_map)\n",
        "    if mother_id:\n",
        "        _find_parents(mother_id, generation + 1, parents_map)\n",
        "\n",
        "def _find_distant(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(_find_distant(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(_find_distant(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "def _filter_lineage(winning_ids, gen_table, names_map):\n",
        "    matching = []\n",
        "    for generation, pair in gen_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_ids or id2 in winning_ids:\n",
        "            matching.append((generation, pair))\n",
        "    matching.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for _, pair in matching:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(\"%s&%s\" % (name_pair[0], name_pair[1]))\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "def _process_record(individual_id, ged, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    _find_parents(individual_id, 1, parents_map)\n",
        "    paths = _find_distant(individual_id, parents_map)\n",
        "    best_score, best_path = None, None\n",
        "    for path in paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx + 1) for idx, nm in enumerate(name_path) if \"Yates\" in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score, best_path = score, path\n",
        "    best_path = best_path or []\n",
        "    best_ids  = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str  = _filter_lineage(set(best_ids), generation_table, names_map)\n",
        "    cm_value = \"\"\n",
        "    sort_value = \"\"\n",
        "    ydna_value = \"\"\n",
        "    for ds in ged.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value   = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "# ---------- Main build ----------\n",
        "def main():\n",
        "    files = glob.glob(\"*.ged\")\n",
        "    if not files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return False\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    gedcom_path = files[0]\n",
        "\n",
        "    # Parse GEDCOM, build datasets, and compute vitals\n",
        "    ged = Gedcom(gedcom_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "\n",
        "    # Legacy autosomal_count.txt (used by Cell 3 JS)\n",
        "    with open(AUTOSOMAL_COUNT_TXT, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "    print(\"[OK] Wrote %s = %d\" % (AUTOSOMAL_COUNT_TXT, autosomal_count))\n",
        "\n",
        "    # EST-based LAST_UPDATED_TEXT for dna_vitals and HTML\n",
        "    last_updated_text = _now_est_string()\n",
        "    print(\"[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: %s\" % last_updated_text)\n",
        "\n",
        "    npfx_count = int(ged.npfx_count)\n",
        "    after_manual_filter_total = int(ged.after_manual_filter_total or len(ged.filter_pool))\n",
        "\n",
        "    vitals_lines = [\n",
        "        \"Records tagged and filtered by NPFX: %d\" % npfx_count,\n",
        "        \"After manual filter, total records: %d\" % after_manual_filter_total,\n",
        "        \"LAST_UPDATED_TEXT: %s\" % last_updated_text,\n",
        "    ]\n",
        "    vitals_df = pd.DataFrame({\"line\": vitals_lines})\n",
        "    vitals_df.to_csv(\n",
        "        VITALS_CSV_PATH,\n",
        "        index=False,\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    )\n",
        "    print(\"[OK] Wrote dna_vitals.csv -> %s\" % os.path.abspath(VITALS_CSV_PATH))\n",
        "\n",
        "    # Re-read GEDCOM raw text for ancestor-building\n",
        "    with open(gedcom_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find(\"\\n\")\n",
        "        flend = len(blk) if flend == -1 else flend\n",
        "        first_line = blk[:flend]\n",
        "        if \"@\" in first_line:\n",
        "            s = first_line.find(\"@\") + 1\n",
        "            e = first_line.find(\"@\", s)\n",
        "            rec_id = first_line[s:e].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map, names_map, families = {}, {}, {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if \"FAM\" in txt[:50]:\n",
        "            father_idx = txt.find(\"1 HUSB @\")\n",
        "            husb_id = txt[father_idx + len(\"1 HUSB @\"):txt.find(\"@\", father_idx + len(\"1 HUSB @\"))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find(\"1 WIFE @\")\n",
        "            wife_id = txt[wife_idx + len(\"1 WIFE @\"):txt.find(\"@\", wife_idx + len(\"1 WIFE @\"))] if wife_idx != -1 else None\n",
        "            kids = [ln.split(\"@\")[1] for ln in txt.split(\"\\n\") if ln.strip().startswith(\"1 CHIL @\")]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    for rec_id, txt in all_records.items():\n",
        "        names_map[rec_id] = _quick_extract_name(\"\\n\" + txt)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(\"Processing %d individuals with chunk-based parallel...\" % len(individual_ids))\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    from functools import partial as _partial\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as ex, tqdm(\n",
        "        total=len(individual_ids),\n",
        "        desc=\"Building Yates Lines (Stage 1)\"\n",
        "    ) as pbar:\n",
        "        for chunk in _chunks(individual_ids, chunk_size):\n",
        "            func = _partial(_process_record, ged=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(ex.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def _trim_prefix(row):\n",
        "        prefix = (\n",
        "            \"YatesJohn&SearchingStill~~~\"\n",
        "            \"YatesWilliam&SearchingStill~~~\"\n",
        "            \"YatesWilliam&SearchingStill~~~\"\n",
        "            \"YatesEdmund&CornellMargaret~~~\"\n",
        "            \"YatesRichard&AshendonJoan~~~\"\n",
        "            \"YatesJohn&HydeAlice~~~\"\n",
        "            \"YatesThomas&FauconerElizabeth~~~\"\n",
        "        )\n",
        "        s = str(row[\"Yates DNA Ancestral Line\"])\n",
        "        if s.startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = s[len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(_trim_prefix, axis=1)\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "\n",
        "    # CSV (ISO-8859-15 as required)\n",
        "    with open(CSV_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df.to_csv(index=False))\n",
        "    logger.info(\"Exported CSV -> %s\", CSV_OUT_LOCAL)\n",
        "\n",
        "    # HTML (XHTML 1.0 Transitional; Times via external CSS is implied; inline minimal styles ok)\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Yates DNA Ancestral Line\"]\n",
        "    table_html = df.to_html(index=False, columns=final_cols, escape=False, border=1)\n",
        "\n",
        "    page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Cell 1 Working Table</title>\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/dna_tree_styles.css\" />\n",
        "<style type=\"text/css\">\n",
        "  html { scroll-behavior: smooth; }\n",
        "  body { background:#ffffff; color:#222222; margin:0; padding:20px; }\n",
        "  h1 { margin:0 0 8px 0; font-size:24px; line-height:1.2; text-align:center; }\n",
        "  .meta { text-align:center; font-size:12px; color:#555555; margin:2px 0 12px 0; }\n",
        "  .downloads { text-align:center; margin:4px 0 12px 0; font-size:13px; }\n",
        "  a { color:#154b8b; text-decoration:none; }\n",
        "  a:hover { text-decoration:underline; }\n",
        "  table { width:100%%; border-collapse:collapse; }\n",
        "  th, td { border:1px solid #333333; padding:6px 8px; vertical-align:top; }\n",
        "  th { background:#e3eaf8; text-align:left; }\n",
        "  td:nth-child(5) { text-align:left; white-space:normal; }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>Cell 1 Working Table</h1>\n",
        "  <div class=\"meta\">\n",
        "    <a href=\"$HOME\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "    &nbsp;|&nbsp; Last updated: $LAST_UPDATED_TEXT\n",
        "    &nbsp;|&nbsp; Download: <a href=\"$CSV\">$CSV</a>\n",
        "  </div>\n",
        "  <div class=\"downloads\"><a href=\"$CSV\">/partials/$CSV_NAME</a></div>\n",
        "  $TABLE\n",
        "</body>\n",
        "</html>\"\"\")\n",
        "\n",
        "    page = page_tpl.safe_substitute(\n",
        "        HOME=ABS_HOME_URL,\n",
        "        CSV=ABS_CSV_URL,\n",
        "        CSV_NAME=os.path.basename(ABS_CSV_URL),\n",
        "        TABLE=table_html,\n",
        "        LAST_UPDATED_TEXT=last_updated_text,\n",
        "    )\n",
        "\n",
        "    with open(HTML_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(page)\n",
        "    logger.info(\"Exported HTML -> %s\", HTML_OUT_LOCAL)\n",
        "\n",
        "    print(\"[SUMMARY] GEDCOM total records: %d\" % ged.total_records)\n",
        "    print(\"[SUMMARY] NPFX-tagged records: %d\" % ged.npfx_count)\n",
        "    print(\"[SUMMARY] Autosomal matches (NPFX minus YDNA): %d\" % ged.autosomal_count)\n",
        "    print(\"[SUMMARY] After manual filter, total records: %d\" % ged.after_manual_filter_total)\n",
        "\n",
        "    return True\n",
        "\n",
        "ok = main()\n",
        "\n",
        "# ---------- Upload to /partials (Explicit FTPS AUTH TLS) ----------\n",
        "if ok and FTP_HOST and FTP_USER and FTP_PASS:\n",
        "    print(\"[INFO] Uploading artifacts to /partials/ ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        _ftps_ensure_dir(ftps, \"partials\")\n",
        "        try:\n",
        "            _ftps_upload(ftps, CSV_OUT_LOCAL, os.path.basename(CSV_OUT_LOCAL))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] CSV upload failed:\", e)\n",
        "        try:\n",
        "            _ftps_upload(ftps, HTML_OUT_LOCAL, os.path.basename(HTML_OUT_LOCAL))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] HTML upload failed:\", e)\n",
        "\n",
        "        # Also upload dna_vitals.csv and autosomal_count.txt so they are in the same authority path\n",
        "        try:\n",
        "            _ftps_upload(ftps, VITALS_CSV_PATH, os.path.basename(VITALS_CSV_PATH))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] dna_vitals.csv upload failed:\", e)\n",
        "        try:\n",
        "            _ftps_upload(ftps, AUTOSOMAL_COUNT_TXT, os.path.basename(AUTOSOMAL_COUNT_TXT))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] autosomal_count.txt upload failed:\", e)\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Uploads complete to /partials/\")\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing creds or build failed).\")\n",
        "\n",
        "print(\"\\n--- Cell 1 Complete: CSV + HTML + dna_vitals.csv + autosomal_count.txt built with ISO-8859-15; explicit FTPS used. ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGfU-fkHHJ3d",
        "outputId": "1a739653-f548-4967-b813-d47a18409180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.27-G2 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 62553 total records\n",
            "Records tagged and filtered by NPFX: 1604\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1604\n",
            "After manual filter, total records: 7\n",
            "[OK] Wrote autosomal_count.txt = 1604\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: December 2, 2025 7:36 PM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 7 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 7/7 [00:03<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUMMARY] GEDCOM total records: 62553\n",
            "[SUMMARY] NPFX-tagged records: 1604\n",
            "[SUMMARY] Autosomal matches (NPFX minus YDNA): 1604\n",
            "[SUMMARY] After manual filter, total records: 7\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploads complete to /partials/\n",
            "\n",
            "--- Cell 1 Complete: CSV + HTML + dna_vitals.csv + autosomal_count.txt built with ISO-8859-15; explicit FTPS used. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "TKAmqiIIDaxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2 - Build + Publish DNA Register (All styling via stylesheet) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.01-G1)\n",
        "# - Complete and runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout via /partials/dna_tree_styles.css (this cell writes it).\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os, re, posixpath, socket, traceback, urllib.parse as _u\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# ---------- A) LAYOUT CONTROL BLOCK (ONE PLACE TO TUNE WIDTHS) ----------\n",
        "# Col 1 = Match to\n",
        "# Col 2 = Name\n",
        "# Col 3 = cM\n",
        "# Col 4 = Match Summary\n",
        "# Col 5 = Website\n",
        "# Col 6 = Yates DNA Ancestral Lines\n",
        "\n",
        "COL_1_PX = 80\n",
        "COL_2_PX = 220\n",
        "COL_3_PX = 60\n",
        "COL_4_PX = 1200\n",
        "COL_5_PX = 120\n",
        "COL_6_PX = 1800\n",
        "\n",
        "COL_WIDTHS = [COL_1_PX, COL_2_PX, COL_3_PX, COL_4_PX, COL_5_PX, COL_6_PX]\n",
        "TABLE_TOTAL_WIDTH_PX = sum(COL_WIDTHS)\n",
        "\n",
        "# Hide these column indices (zero-based) in the rendered table: 2,3,5\n",
        "HIDE_COLS_ZERO_BASED = {1, 2, 4}\n",
        "\n",
        "print(\"[LAYOUT] TABLE_TOTAL_WIDTH_PX=%d\" % TABLE_TOTAL_WIDTH_PX)\n",
        "print(\"[LAYOUT] Column widths (px): 1=%d 2=%d 3=%d 4=%d 5=%d 6=%d\" %\n",
        "      (COL_1_PX, COL_2_PX, COL_3_PX, COL_4_PX, COL_5_PX, COL_6_PX))\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# NOTE: main register pages now .shtml (SSI pages only)\n",
        "LOCAL_HTML        = \"yates_ancestor_register.shtml\"\n",
        "REMOTE_HTML_CANON = posixpath.join(\"partials\", \"yates_ancestor_register.shtml\")\n",
        "REMOTE_HTML_LEG   = posixpath.join(\"partials\", \"ons_yates_dna_register.shtml\")\n",
        "\n",
        "DNA_REGISTER_ABS = \"https://yates.one-name.net/partials/ons_yates_dna_register.shtml\"\n",
        "TREES_ABS        = \"https://yates.one-name.net/partials/just-trees.htm\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV  = \"%s.csv\"  % EXPORT_BASENAME\n",
        "LOCAL_XLSX = \"%s.xlsx\" % EXPORT_BASENAME\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", os.path.basename(LOCAL_CSV))\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", os.path.basename(LOCAL_XLSX))\n",
        "\n",
        "FTP_DIR  = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "HOME_URL        = \"https://yates.one-name.net/partials/yates_ancestor_register.shtml\"\n",
        "REMOTE_NAME_ABS = HOME_URL\n",
        "\n",
        "ARROW_ENTITY         = \"&rarr;\"\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "SERVER_PARTIALS_DIR        = \"partials\"\n",
        "SERVER_MAPPING_BASENAME    = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE      = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# Stylesheet\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "STYLESHEET_LOCAL    = os.path.join(\"partials\", STYLESHEET_BASENAME)\n",
        "STYLESHEET_REMOTE   = posixpath.join(\"partials\", STYLESHEET_BASENAME)\n",
        "CSS_VERSION         = \"v2025-12-01-g1\"\n",
        "STYLESHEET_HREF     = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK           = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "# Path for vitals from Cell 1\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"]     = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Name/text utils ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\", \"del\", \"della\", \"der\", \"van\", \"von\", \"da\", \"dos\", \"das\", \"di\", \"la\", \"le\", \"du\", \"of\"\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"  + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        if i > 0 and w.lower() in _PARTICLES:\n",
        "            out.append(w.lower())\n",
        "        else:\n",
        "            out.append(_smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname      = token[:idx]\n",
        "    given        = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm    = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname          = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        if \" \" in s:\n",
        "            return smart_titlecase(s)\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "# ---------- 5) Read CSV ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols   = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "_encs     = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "_last_err = None\n",
        "df        = None\n",
        "for _e in _encs:\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_IN, encoding=_e, dtype=str, keep_default_na=False)\n",
        "        break\n",
        "    except Exception as _ex:\n",
        "        _last_err = _ex\n",
        "        df        = None\n",
        "if df is None:\n",
        "    raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, _last_err))\n",
        "print(\"[OK] Loaded CSV: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "id_col    = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col  = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "cm_col    = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "path_col  = find_col(\n",
        "    df,\n",
        "    [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "    [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        ")\n",
        "\n",
        "if not id_col:\n",
        "    raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "if not match_col:\n",
        "    raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "if not name_col:\n",
        "    raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col:\n",
        "    raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not path_col:\n",
        "    raise ValueError(\"CSV missing lineage/path column.\")\n",
        "\n",
        "# ---------- 5.1) Read vitals from dna_vitals.csv ----------\n",
        "AUTOSOMAL_MATCHES = \"\"\n",
        "SHOWING_STATIC    = \"\"\n",
        "LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a UTC timestamp like\n",
        "      'YYYY-MM-DD HH:MM UTC'\n",
        "      'YYYY-MM-DD HH:MM:SS UTC'\n",
        "      'YYYY-MM-DDTHH:MM UTC'\n",
        "      'YYYY-MM-DDTHH:MM:SS UTC'\n",
        "    to 'Month D, YYYY H:MM AM/PM' in approx. US Eastern (UTC-5).\n",
        "    If parsing fails, return the original string.\n",
        "    \"\"\"\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _format_int_with_commas(s):\n",
        "    \"\"\"\n",
        "    Take a numeric-like string, strip non-digits, and return with thousands\n",
        "    separators (e.g., '1604' -> '1,604'). On failure, return the input.\n",
        "    \"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"[^0-9\\-]\", \"\", str(s))\n",
        "    if not t:\n",
        "        return str(s)\n",
        "    try:\n",
        "        n = int(t)\n",
        "        return \"{:,}\".format(n)\n",
        "    except Exception:\n",
        "        return str(s)\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global AUTOSOMAL_MATCHES, SHOWING_STATIC, LAST_UPDATED_TEXT\n",
        "    AUTOSOMAL_MATCHES = \"\"\n",
        "    SHOWING_STATIC    = \"\"\n",
        "    LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will omit counts and last-updated text.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        vdf = pd.read_csv(path, dtype=str, encoding=\"iso-8859-15\", keep_default_na=False)\n",
        "    except Exception:\n",
        "        encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "        last = None\n",
        "        vdf  = None\n",
        "        for enc in encs:\n",
        "            try:\n",
        "                vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "        if vdf is None:\n",
        "            print(\"[WARN] Unable to read dna_vitals.csv: %s\" % last)\n",
        "            return\n",
        "\n",
        "    flat = []\n",
        "    for row in vdf.astype(str).values.tolist():\n",
        "        for cell in row:\n",
        "            flat.append(str(cell))\n",
        "\n",
        "    autosomal = None\n",
        "    showing   = None\n",
        "    last_text = None\n",
        "\n",
        "    for cell in flat:\n",
        "        s = str(cell)\n",
        "        if \"Records tagged and filtered by NPFX\" in s and autosomal is None:\n",
        "            m = re.search(r\"(\\d+)\", s)\n",
        "            if m:\n",
        "                autosomal = m.group(1)\n",
        "        if \"After manual filter, total records\" in s and showing is None:\n",
        "            m = re.search(r\"(\\d+)\", s)\n",
        "            if m:\n",
        "                showing = m.group(1)\n",
        "        if \"LAST_UPDATED_TEXT\" in s and last_text is None:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", s)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "\n",
        "    if autosomal is None or showing is None or last_text is None:\n",
        "        all_text = \" \".join(flat)\n",
        "        nums = re.findall(r\"\\d+\", all_text)\n",
        "        if autosomal is None and len(nums) >= 1:\n",
        "            autosomal = nums[0]\n",
        "        if showing is None and len(nums) >= 2:\n",
        "            showing = nums[1]\n",
        "\n",
        "    if last_text is not None:\n",
        "        last_text = _friendly_ts_from_utc(last_text)\n",
        "\n",
        "    AUTOSOMAL_MATCHES = _format_int_with_commas(autosomal) if autosomal else \"\"\n",
        "    SHOWING_STATIC    = _format_int_with_commas(showing)   if showing   else \"\"\n",
        "    LAST_UPDATED_TEXT = last_text or \"\"\n",
        "\n",
        "    print(\"[OK] Loaded vitals from %s -> autosomal=%s, showing=%s, last_updated_text=%s\"\n",
        "          % (path,\n",
        "             AUTOSOMAL_MATCHES or \"?\",\n",
        "             SHOWING_STATIC or \"?\",\n",
        "             LAST_UPDATED_TEXT or \"(blank)\"))\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "if SHOWING_STATIC:\n",
        "    try:\n",
        "        show_raw = re.sub(r\"[^0-9\\-]\", \"\", SHOWING_STATIC)\n",
        "        csv_rows = len(df)\n",
        "        if show_raw:\n",
        "            if int(show_raw) != csv_rows:\n",
        "                print(\"[WARN] dna_vitals showing (%s) != CSV rows (%d)\" %\n",
        "                      (show_raw, csv_rows))\n",
        "    except Exception as _e:\n",
        "        print(\"[WARN] Unable to compare showing from dna_vitals.csv to CSV rows: %s\" % _e)\n",
        "\n",
        "# ---------- 6) Transform ----------\n",
        "_setup_resolver()\n",
        "\n",
        "headers          = []\n",
        "lineages         = []\n",
        "subjects         = []\n",
        "first_ancestors  = []\n",
        "display_match_to = []\n",
        "display_name     = []\n",
        "\n",
        "LINEAGE_HEADER_SAFE = \"Yates DNA Ancestral Lines\"\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw    = row.get(match_col, \"\")\n",
        "    subject_name   = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_b = \"<strong>%s</strong>\" % subject_name if subject_name else subject_name\n",
        "\n",
        "    pid          = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_raw  = row.get(name_col, \"\")\n",
        "    matchee_name = norm_matchee_name(matchee_raw) or subject_name\n",
        "\n",
        "    if pid:\n",
        "        matchee_url = (\n",
        "            \"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\"\n",
        "            % (TNG_BASE, pid, TNG_TREE)\n",
        "        )\n",
        "        matchee_name_html = '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (matchee_url, matchee_name)\n",
        "    else:\n",
        "        matchee_name_html = matchee_name\n",
        "\n",
        "    cm_val      = row.get(cm_col, \"0\")\n",
        "    tokens      = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total  = len(tokens)\n",
        "    tokens_disp = tokens[:7]\n",
        "\n",
        "    if \"common_husband\" in df.columns and \"common_wife\" in df.columns:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw    = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(subject_name_b or subject_name, cm_val, matchee_name_html, gens_total, husband_raw, wife_raw)\n",
        "    if tokens_disp:\n",
        "        tokens_disp[0] = \"<strong>%s</strong>\" % tokens_disp[0]\n",
        "    sep          = \" %s \" % ARROW_ENTITY\n",
        "    lineage_text = sep.join(tokens_disp) if tokens_disp else \"\"\n",
        "\n",
        "    headers.append(header_html)\n",
        "    lineages.append(lineage_text)\n",
        "    subjects.append(subject_name)\n",
        "    first_ancestors.append(tokens[0] if tokens else \"\")\n",
        "\n",
        "    display_match_to.append(subject_name)\n",
        "    display_name.append(matchee_name_html)\n",
        "\n",
        "df[\"Match Summary\"]      = headers\n",
        "df[LINEAGE_HEADER_SAFE]  = lineages\n",
        "df[\"Subject\"]            = subjects\n",
        "df[\"First Ancestor\"]     = [_clean_piece(x) for x in first_ancestors]\n",
        "\n",
        "# ---------- 6.1) Clean exports ----------\n",
        "TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "def _html_to_text(s: str) -> str:\n",
        "    t = TAG_RE.sub(\"\", str(s or \"\"))\n",
        "    t = _html.unescape(t)\n",
        "    t = t.replace(\"\\u2192\", \"->\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "def _extract_find_url(subject_name: str) -> str:\n",
        "    if not subject_name:\n",
        "        return \"\"\n",
        "    q = _u.quote(subject_name)\n",
        "    return \"%s?q=%s\" % (REMOTE_NAME_ABS, q)\n",
        "\n",
        "website_urls = [_extract_find_url(subj) for subj in df[\"Subject\"].tolist()]\n",
        "\n",
        "export_df = pd.DataFrame({\n",
        "    \"Match to\"      : df[match_col].tolist(),\n",
        "    \"Name\"          : df[name_col].tolist(),\n",
        "    \"cM\"            : df[cm_col].tolist(),\n",
        "    \"Match Summary\" : [_html_to_text(v) for v in df[\"Match Summary\"].tolist()],\n",
        "    \"Website URL\"   : website_urls,\n",
        "    \"Lineage\"       : [_html_to_text(v) for v in df[LINEAGE_HEADER_SAFE].tolist()],\n",
        "})\n",
        "\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    with pd.ExcelWriter(LOCAL_XLSX) as _writer:\n",
        "        export_df.to_excel(_writer, index=False)\n",
        "print(\"[OK] Wrote exports: %s and %s\" % (os.path.abspath(LOCAL_CSV), os.path.abspath(LOCAL_XLSX)))\n",
        "\n",
        "# ---------- 7) Stylesheet content (includes print fix for scroll containers) ----------\n",
        "CSS_TEXT = \"\"\"/* yates.one-name.net - DNA pages (unified stylesheet)\n",
        "   Version: %s\n",
        "   Note: Typography, layout, colors, borders - centralized here. */\n",
        "\n",
        ":root {\n",
        "  --table-width-px: %dpx;\n",
        "  --brand-blue: #5b79b8;\n",
        "  --brand-blue-dark: #4668aa;\n",
        "  --line: #dddddd;\n",
        "  --line-strong: #999999;\n",
        "}\n",
        "\n",
        "html, body {\n",
        "  margin:0; padding:0;\n",
        "  font-family: \"Times New Roman\", Times, serif;\n",
        "  font-size: 16px; line-height: 1.35;\n",
        "  color:#111111; background:#ffffff;\n",
        "}\n",
        "\n",
        ".wrap {\n",
        "  max-width:100%%;\n",
        "  margin:0 auto;\n",
        "  background:#ffffff;\n",
        "  padding:16px;\n",
        "  padding-bottom:48px;\n",
        "}\n",
        ".centerline { text-align:center; }\n",
        "\n",
        ".downloads { text-align:center; margin:4px 0 10px 0; font-size: 13px; }\n",
        ".updated   { font-size: 12px; color:#555555; text-align:center; margin:2px 0 10px 0; }\n",
        "\n",
        ".left-align { text-align:left; }\n",
        "\n",
        "/* Simple header alignment helpers */\n",
        "th.center-header { text-align:center; }\n",
        "th.left-header   { text-align:left; }\n",
        "\n",
        "/* Wrapper for scroll container */\n",
        ".table-scroll-wrapper {\n",
        "  width:100%%;\n",
        "  max-width:100%%;\n",
        "  margin:0 auto;\n",
        "}\n",
        "\n",
        "/* Single scroll container (optimized: no fake top scrollbar) */\n",
        ".table-scroll {\n",
        "  width:100%%;\n",
        "  max-width:100%%;\n",
        "  max-height:80vh;\n",
        "  overflow-x:auto;\n",
        "  overflow-y:auto;\n",
        "  border:1px solid var(--line);\n",
        "  position:relative;\n",
        "  -webkit-overflow-scrolling:touch;\n",
        "  scrollbar-width:auto;\n",
        "  -ms-overflow-style:auto;\n",
        "}\n",
        "\n",
        "/* Table: let widths and content drive horizontal size */\n",
        "table.sortable {\n",
        "  border-collapse:separate;\n",
        "  border-spacing:0;\n",
        "}\n",
        "\n",
        "table.sortable th,\n",
        "table.sortable td {\n",
        "  border:1px solid var(--line);\n",
        "  padding:6px 8px;\n",
        "  vertical-align:top;\n",
        "  white-space:nowrap;\n",
        "}\n",
        "\n",
        "/* Sticky header row */\n",
        "table.sortable th {\n",
        "  background:#e3eaf8;\n",
        "  position:-webkit-sticky;\n",
        "  position:sticky;\n",
        "  top:0;\n",
        "  z-index:5;\n",
        "  box-shadow:0 1px 0 #cccccc;\n",
        "  cursor:pointer;\n",
        "}\n",
        "\n",
        "/* Sticky first column (Match to) for DNA Register table */\n",
        "table.dna-register-table th:nth-child(1),\n",
        "table.dna-register-table td:nth-child(1) {\n",
        "  position:-webkit-sticky;\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "table.dna-register-table th:nth-child(1) {\n",
        "  z-index:7;\n",
        "}\n",
        "\n",
        "/* First data row marker */\n",
        "#first-row td { border-top:2px solid var(--line-strong); }\n",
        "\n",
        "/* Back-to-top button */\n",
        ".back-to-top {\n",
        "  position:fixed; right:16px; bottom:16px; padding:6px 10px;\n",
        "  border:1px solid #3e5a97; background:var(--brand-blue);\n",
        "  color:#ffffff; cursor:pointer; border-radius:6px; display:none; z-index:9999;\n",
        "}\n",
        ".back-to-top:hover { background:var(--brand-blue-dark); }\n",
        "\n",
        "/* Controls */\n",
        ".controls { text-align:center; }\n",
        ".controls-spaced { margin:6px 0 10px 0; }\n",
        ".search { font-size: 14px; padding:5px 8px; }\n",
        "\n",
        "/* Old-school blue nav menu */\n",
        ".oldnav {\n",
        "  margin:8px auto 6px auto; padding:0; background:var(--brand-blue);\n",
        "  border-radius:6px; overflow:hidden; max-width: var(--table-width-px);\n",
        "}\n",
        ".oldnav ul { list-style:none; margin:0; padding:0; display:flex; flex-wrap:wrap; }\n",
        ".oldnav li { margin:0; padding:0; }\n",
        ".oldnav a, .oldnav a:link, .oldnav a:visited, .oldnav a:active { color:#ffffff !important; }\n",
        ".oldnav a {\n",
        "  display:block;\n",
        "  padding:8px 12px;\n",
        "  text-decoration:none;\n",
        "  white-space:nowrap;\n",
        "  border-right:1px solid #ffffff;\n",
        "  font-weight:600;\n",
        "}\n",
        ".oldnav li:last-child a { border-right:none; }\n",
        ".oldnav a:hover { background:var(--brand-blue-dark); color:#ffffff !important; }\n",
        "\n",
        "/* Responsive tweaks */\n",
        "@media screen and (min-width: 1200px) {\n",
        "  .wrap { max-width: var(--table-width-px); }\n",
        "}\n",
        "@media screen and (max-width: 1199px) {\n",
        "  .oldnav { border-radius:0; }\n",
        "}\n",
        "@media screen and (max-width: 700px) {\n",
        "  table.sortable th, table.sortable td { padding:5px 6px; }\n",
        "}\n",
        "\n",
        "/* Print: avoid scroll clipping for long tables (e.g., Cousin List, DNA register) */\n",
        "@media print {\n",
        "  .table-scroll-wrapper,\n",
        "  .table-scroll {\n",
        "    max-height:none !important;\n",
        "    overflow:visible !important;\n",
        "    border:none !important;\n",
        "  }\n",
        "  html, body {\n",
        "    overflow:visible !important;\n",
        "  }\n",
        "}\n",
        "\"\"\" % (\n",
        "    CSS_VERSION,\n",
        "    TABLE_TOTAL_WIDTH_PX,\n",
        ")\n",
        "\n",
        "os.makedirs(\"partials\", exist_ok=True)\n",
        "with open(STYLESHEET_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as _css:\n",
        "    _css.write(CSS_TEXT)\n",
        "print(\"[OK] Wrote stylesheet: %s\" % os.path.abspath(STYLESHEET_LOCAL))\n",
        "\n",
        "# ---------- 8) Main HTML ----------\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<!-- Cell2 build: Version=2025.12.01-G1 | HIDE_COLS=2,3,5 | CousinListFullPrint=on | Scroll=single-bottom -->\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '')\n",
        "      .replace(/\\\\s+/g,' ')\n",
        "      .trim()\n",
        "      .toLowerCase();\n",
        "  }\n",
        "\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[colIndex]),\n",
        "          B = textOf(b.cells[colIndex]);\n",
        "\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "\n",
        "      if(!isNaN(nA) && !isNaN(nB)){\n",
        "        return asc ? (nA - nB) : (nB - nA);\n",
        "      }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      frag.appendChild(rows[i]);\n",
        "    }\n",
        "    tb.appendChild(frag);\n",
        "  }\n",
        "\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "\n",
        "    for(var i=0;i<ths.length;i++){\n",
        "      (function(idx){\n",
        "        var th  = ths[idx];\n",
        "        var dir = 'asc';\n",
        "        th.addEventListener('click', function(){\n",
        "          dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "\n",
        "          for (var j = 0; j < ths.length; j++){\n",
        "            ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "          }\n",
        "          th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "          sortTable(tbl, idx, dir);\n",
        "        }, false);\n",
        "      })(i);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "\n",
        "    var tb   = tbl.tBodies[0];\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt  = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "    }\n",
        "\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "\n",
        "    box.addEventListener('input',  onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "\n",
        "    window.addEventListener('scroll', function(){\n",
        "      btn.style.display = (window.pageYOffset > 200 ? 'block' : 'none');\n",
        "    }, false);\n",
        "\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  function bindScrollSync(){\n",
        "    // No-op in the optimized version (no separate top-scroll container).\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindSearch();\n",
        "    bindBackToTop();\n",
        "    bindScrollSync();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# ---------- 8.1) Build HTML table manually with inline widths ----------\n",
        "website_links = []\n",
        "for subj in df[\"Subject\"].tolist():\n",
        "    url = _extract_find_url(subj)\n",
        "    if url:\n",
        "        website_links.append('<a href=\"%s\" target=\"_blank\" rel=\"noopener\">Website</a>' %\n",
        "                             _html.escape(url, quote=True))\n",
        "    else:\n",
        "        website_links.append(\"\")\n",
        "\n",
        "col_headers = [\n",
        "    (\"Match to\", \"center\"),\n",
        "    (\"Name\", \"center\"),\n",
        "    (\"cM\", \"center\"),\n",
        "    (\"Match Summary\", \"center\"),\n",
        "    (\"Website\", \"center\"),\n",
        "    (\"Yates DNA Ancestral Lines\", \"left\"),\n",
        "]\n",
        "\n",
        "col_data = [\n",
        "    display_match_to,\n",
        "    display_name,\n",
        "    df[cm_col].tolist(),\n",
        "    df[\"Match Summary\"].tolist(),\n",
        "    website_links,\n",
        "    df[LINEAGE_HEADER_SAFE].tolist(),\n",
        "]\n",
        "\n",
        "thead_cells = []\n",
        "for (idx, (hdr, align)) in enumerate(col_headers):\n",
        "    wpx = COL_WIDTHS[idx]\n",
        "    style_bits = [\"width:%dpx\" % wpx]\n",
        "    if idx in HIDE_COLS_ZERO_BASED:\n",
        "        style_bits.append(\"display:none\")\n",
        "    style_attr = \"; \".join(style_bits)\n",
        "    if align == \"center\":\n",
        "        cell_html = '<th class=\"center-header\" style=\"%s;\">%s</th>' % (style_attr, hdr)\n",
        "    else:\n",
        "        cell_html = '<th class=\"left-header\" style=\"%s;\">%s</th>' % (style_attr, hdr)\n",
        "    thead_cells.append(cell_html)\n",
        "thead_html = \"<thead>\\n  <tr>\" + \"\".join(thead_cells) + \"</tr>\\n</thead>\"\n",
        "\n",
        "nrows = len(df)\n",
        "tbody_lines = [\"<tbody>\"]\n",
        "for r in range(nrows):\n",
        "    tr_open = '  <tr id=\"first-row\">' if r == 0 else '  <tr>'\n",
        "    cells = []\n",
        "    for c in range(len(col_headers)):\n",
        "        wpx = COL_WIDTHS[c]\n",
        "        val = col_data[c][r]\n",
        "        val_str = \"\" if val is None else str(val)\n",
        "        style_bits = [\"width:%dpx\" % wpx]\n",
        "        if c in HIDE_COLS_ZERO_BASED:\n",
        "            style_bits.append(\"display:none\")\n",
        "        style_attr = \"; \".join(style_bits)\n",
        "        cells.append('<td style=\"%s;\">%s</td>' % (style_attr, val_str))\n",
        "    tbody_lines.append(tr_open + \"\".join(cells) + \"</tr>\")\n",
        "tbody_lines.append(\"</tbody>\")\n",
        "tbody_html = \"\\n\".join(tbody_lines)\n",
        "\n",
        "html_table = (\n",
        "    '<table border=\"1\" class=\"dataframe sortable dna-register-table\" id=\"refactor-table\">'\n",
        "    + thead_html +\n",
        "    \"\\n\" +\n",
        "    tbody_html +\n",
        "    \"</table>\"\n",
        ")\n",
        "\n",
        "# Optimized: single bottom scroll container (no fake top scroll shim)\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div class=\"table-scroll\" id=\"bottom-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (html_table,)\n",
        "\n",
        "# ---------- 8.2) Page assembly ----------\n",
        "if LAST_UPDATED_TEXT:\n",
        "    updated_label = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT)\n",
        "else:\n",
        "    updated_label = 'Last updated: <span id=\"last-updated\"></span>'\n",
        "\n",
        "_updated_parts = [updated_label]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "if SHOWING_STATIC:\n",
        "    _updated_parts.append('Showing: %s' % _html.escape(SHOWING_STATIC))\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">' +\n",
        "    ' &nbsp;|&nbsp; '.join(_updated_parts) +\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "# No \"Download:\" paragraph in the body; downloads live in nav\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "  '<div class=\"controls controls-spaced centerline\">'\n",
        "  '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "  '</div>'\n",
        ")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK       = HEAD_LINK,\n",
        "    UPDATED_BLOCK   = UPDATED_BLOCK,\n",
        "    NAV_BLOCK       = NAV_BLOCK,\n",
        "    CONTROLS_BLOCK  = CONTROLS_BLOCK,\n",
        "    DOWNLOADS_BLOCK = DOWNLOADS_BLOCK,\n",
        "    SCROLL_WRAPPER  = SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"[OK] Saved canonical render: %s\" % os.path.abspath(LOCAL_HTML))\n",
        "\n",
        "# ---------- 9) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, STYLESHEET_LOCAL, _remote_path(STYLESHEET_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload stylesheet failed: %s\" % e)\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload main HTML failed: %s\" % e)\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(LOCAL_CSV):\n",
        "                ftp_upload_overwrite(ftps, LOCAL_CSV, _remote_path(REMOTE_CSV))\n",
        "            if os.path.exists(LOCAL_XLSX):\n",
        "                ftp_upload_overwrite(ftps, LOCAL_XLSX, _remote_path(REMOTE_XLSX))\n",
        "            print(\"[OK] Uploaded CSV/XLSX -> /partials/\")\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload CSV/XLSX failed: %s\" % e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [\n",
        "            _remote_path(REMOTE_HTML_CANON),\n",
        "            _remote_path(REMOTE_HTML_LEG),\n",
        "            _remote_path(REMOTE_CSV),\n",
        "            _remote_path(REMOTE_XLSX),\n",
        "            _remote_path(STYLESHEET_REMOTE),\n",
        "        ]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.shtml\")\n",
        "        print(\"Legacy (ons_):    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.shtml\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "        print(\"Trees (Cell 3):   https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "        print(\"Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\")\n",
        "        print(\"\\nBust cache once if needed by appending ?v=%s to the URL.\" % CSS_VERSION)\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session: %s\" % e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "# ====== CUT STOP [1/1] CELL 2 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN77LDV7bAjE",
        "outputId": "f288010d-dad4-45aa-d6a9-87c0f6fe91b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=3480\n",
            "[LAYOUT] Column widths (px): 1=80 2=220 3=60 4=1200 5=120 6=1800\n",
            "[OK] Loaded CSV: 93 rows, 6 cols\n",
            "[OK] Loaded vitals from dna_vitals.csv -> autosomal=1,605, showing=93, last_updated_text=December 4, 2025 8:00 PM\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Wrote stylesheet: /content/partials/dna_tree_styles.css\n",
            "[OK] Saved canonical render: /content/yates_ancestor_register.shtml\n",
            "[PUT] partials/dna_tree_styles.css -> partials/dna_tree_styles.css\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "[PUT] yates_ancestor_register.csv -> partials/yates_ancestor_register.csv\n",
            "[PUT] yates_ancestor_register.xlsx -> partials/yates_ancestor_register.xlsx\n",
            "[OK] Uploaded CSV/XLSX -> /partials/\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 107517\n",
            "partials/ons_yates_dna_register.shtml : 107517\n",
            "partials/yates_ancestor_register.csv : 40278\n",
            "partials/yates_ancestor_register.xlsx : 17969\n",
            "partials/dna_tree_styles.css : 3855\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy (ons_):    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "Trees (Cell 3):   https://yates.one-name.net/partials/just-trees.shtml\n",
            "Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "Bust cache once if needed by appending ?v=v2025-12-01-g1 to the URL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2b"
      ],
      "metadata": {
        "id": "5i6GjL2cwd2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2b — Build + Upload Match/Lineage Count Partials (Counts only) ======\n",
        "# RON GOLDEN RULES -- CLIFF NOTES (v2025.11.30-G3)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; ALL typography/layout via /partials/dna_tree_styles.css (linked only).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.11.30-G3 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes;\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.11.30-G3 | Encoding=ISO-8859-15\")\n",
        "\n",
        "DOWNLOADS_BLOCK = \"\"  # moved into nav_block.shtml\n",
        "\n",
        "import os, re, posixpath, socket, traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "# Shared stylesheet link (must already be present on server from Cell 2)\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "# Shared nav include (SSI)\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "# NEW: vitals authority (built by Cell 1)\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "# TNG settings for cousin links (match vertical chart behavior in main register)\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "# Local partial paths\n",
        "MATCH_COUNT_LOCAL   = os.path.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_LOCAL = os.path.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_LOCAL  = os.path.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# Remote partial paths (server-side)\n",
        "MATCH_COUNT_REMOTE   = posixpath.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_REMOTE = posixpath.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_REMOTE  = posixpath.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# ---------- 1a) Load vitals authority from dna_vitals.csv ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a UTC timestamp like\n",
        "      'YYYY-MM-DD HH:MM UTC'\n",
        "      'YYYY-MM-DD HH:MM:SS UTC'\n",
        "      'YYYY-MM-DDTHH:MM UTC'\n",
        "      'YYYY-MM-DDTHH:MM:SS UTC'\n",
        "    to 'Month D, YYYY H:MM AM/PM' in approx. US Eastern (UTC-5).\n",
        "    If parsing fails, return the original string.\n",
        "    \"\"\"\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "\n",
        "    # Strip trailing 'UTC' (any case) and extra spaces\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "\n",
        "    # Try several common formats\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "\n",
        "    if dt_utc is None:\n",
        "        # Could not parse; return as-is\n",
        "        return raw\n",
        "\n",
        "    # Approximate Eastern as UTC-5; DST handling can be layered later if needed.\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            # Allow optional commas in the stored number and normalize\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw  # fallback\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ---------- 3) Resolver (match_to_unmasked.csv on server) ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "\n",
        "# ---------- 4) CSV + name helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\",\n",
        "    \"del\",\n",
        "    \"della\",\n",
        "    \"der\",\n",
        "    \"van\",\n",
        "    \"von\",\n",
        "    \"da\",\n",
        "    \"dos\",\n",
        "    \"das\",\n",
        "    \"di\",\n",
        "    \"la\",\n",
        "    \"le\",\n",
        "    \"du\",\n",
        "    \"of\",\n",
        "}\n",
        "\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) Count helpers + partial HTML shells ----------\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
        "    return t\n",
        "\n",
        "\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        + \"%s\\n\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        + \"<title>%s</title>\\n\" % _html.escape(title)\n",
        "        + \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        + \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title)\n",
        "        + \"<div class=\\\"updated centerline\\\">\"\n",
        "          \"Last updated: %s &nbsp;|&nbsp; \"\n",
        "          \"Showing: %s\"\n",
        "          \"</div>\\n\" % (\n",
        "              _html.escape(LAST_UPDATED_DISPLAY),\n",
        "              _html.escape(AUTOSOMAL_MATCHES_TEXT),\n",
        "          )\n",
        "        + NAV_BLOCK + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "          \"Showing: \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "          \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def _partial_tail():\n",
        "    # JS now only handles row selection; no date stamping or XHR autosomal fetch.\n",
        "    # Selection color is enforced INLINE for robustness and iOS compatibility.\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function ySelEachRow(tb, cb){\"\n",
        "        \" if(!tb) return;\"\n",
        "        \" var rows=tb.getElementsByTagName('tr');\"\n",
        "        \" for(var i=0;i<rows.length;i++){cb(rows[i]);}\"\n",
        "        \"}\"\n",
        "        \"function ySelClear(tr){\"\n",
        "        \" if(!tr) return;\"\n",
        "        \" tr.removeAttribute('data-selected');\"\n",
        "        \" var cls=tr.className||'';\"\n",
        "        \" cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,'');\"\n",
        "        \" tr.className=cls;\"\n",
        "        \" tr.style.backgroundColor='';\"\n",
        "        \"}\"\n",
        "        \"function ySelToggle(a){\"\n",
        "        \" var tr=a;\"\n",
        "        \" while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;}\"\n",
        "        \" if(!tr) return false;\"\n",
        "        \" var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \" if(sel){\"\n",
        "        \"  ySelClear(tr);\"\n",
        "        \" }else{\"\n",
        "        \"  tr.setAttribute('data-selected','1');\"\n",
        "        \"  var cls=tr.className||'';\"\n",
        "        \"  if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';}\"\n",
        "        \"  tr.style.backgroundColor='#fff2cc';\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelGetTBody(tbodyId){\"\n",
        "        \" var tb=document.getElementById(tbodyId);\"\n",
        "        \" if(tb) return tb;\"\n",
        "        \" var t=document.getElementById('ref-table');\"\n",
        "        \" if(!t) return null;\"\n",
        "        \" if(t.tBodies&&t.tBodies.length){return t.tBodies[0];}\"\n",
        "        \" return t;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowSelected(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){\"\n",
        "        \"  var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \"  tr.style.display=sel?'':'none';\"\n",
        "        \" });\"\n",
        "        \" var rl=document.getElementById('reg-list');\"\n",
        "        \" if(rl){\"\n",
        "        \"  var selVals=[];\"\n",
        "        \"  ySelEachRow(tb,function(tr){\"\n",
        "        \"    if(tr.getAttribute('data-selected')==='1'){\"\n",
        "        \"      var v=tr.getAttribute('data-filter')\"\n",
        "        \"        || tr.getAttribute('data-lineage')\"\n",
        "        \"        || tr.getAttribute('data-code')\"\n",
        "        \"        || tr.getAttribute('data-q')\"\n",
        "        \"        || '';\"\n",
        "        \"      if(v){selVals.push(v);}\"\n",
        "        \"    }\"\n",
        "        \"  });\"\n",
        "        \"  if(selVals.length===0){\"\n",
        "        \"    return false;\"\n",
        "        \"  }\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    var r=rows[i];\"\n",
        "        \"    var lv=r.getAttribute('data-filter')\"\n",
        "        \"      || r.getAttribute('data-lineage')\"\n",
        "        \"      || r.getAttribute('data-code')\"\n",
        "        \"      || '';\"\n",
        "        \"    var show=false;\"\n",
        "        \"    for(var j=0;j<selVals.length;j++){\"\n",
        "        \"      if(lv===selVals[j]){show=true; break;}\"\n",
        "        \"    }\"\n",
        "        \"    r.style.display=show?'':'none';\"\n",
        "        \"  }\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowAll(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display='';});\"\n",
        "        \" var rl=document.getElementById('reg-list');\"\n",
        "        \" if(rl){\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){rows[i].style.display='';}\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelReset(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);});\"\n",
        "        \" var rl=document.getElementById('reg-list');\"\n",
        "        \" if(rl){\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){rows[i].style.display='';}\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"window.ySelToggle=ySelToggle;\"\n",
        "        \"window.ySelShowSelected=ySelShowSelected;\"\n",
        "        \"window.ySelShowAll=ySelShowAll;\"\n",
        "        \"window.ySelReset=ySelReset;\"\n",
        "        \"})();\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(\n",
        "    row,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    # Unmask subject if possible, then normalize to same style as main register\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        name_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        name_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return subject_name_html, name_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Match Count partial ----------\n",
        "def build_match_count_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    codes_raw = main_df[match_col].astype(str).map(lambda x: x.strip())\n",
        "    keys_norm = codes_raw.map(_norm_code_for_count)\n",
        "\n",
        "    counts_series = keys_norm.value_counts(dropna=False)\n",
        "    counts = counts_series.reset_index()\n",
        "    if counts.shape[1] >= 2:\n",
        "        counts.columns = [\"norm_key\", \"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"] = counts.index.astype(str)\n",
        "        counts[\"Count\"] = counts_series.values\n",
        "        counts = counts[[\"norm_key\", \"Count\"]]\n",
        "\n",
        "    first_display = {}\n",
        "    raw_list = codes_raw.tolist()\n",
        "    norm_list = keys_norm.tolist()\n",
        "    for code_disp, k in zip(raw_list, norm_list):\n",
        "        if k not in first_display and str(k) != \"\":\n",
        "            first_display[k] = code_disp\n",
        "\n",
        "    counts[\"Code\"] = counts[\"norm_key\"].map(lambda k: first_display.get(k, k))\n",
        "    counts[\"Unmasked\"] = counts[\"norm_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "\n",
        "    counts = counts.sort_values(\n",
        "        by=[\"Code\", \"Count\"],\n",
        "        ascending=[True, False],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Match Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append(\n",
        "        '<th style=\"width:35%\">Code</th>'\n",
        "        '<th style=\"width:35%\">Unmasked</th>'\n",
        "        '<th style=\"width:30%\">Showing</th>'\n",
        "    )\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in counts.iterrows():\n",
        "        code = r.get(\"Code\", \"\")\n",
        "        unm = r.get(\"Unmasked\", \"\")\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        norm_key = _norm_code_for_count(code)\n",
        "        label = (unm or code).strip()\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td><td>%s</td>\"\n",
        "            \"<td class=\\\"count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td></tr>\"\n",
        "            % (\n",
        "                _html.escape(label, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(code),\n",
        "                _html.escape(unm),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    # DNA Register-style table under Match Count\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected code(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>Name</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        code_raw = str(row.get(match_col, \"\")).strip()\n",
        "        if not code_raw:\n",
        "            continue\n",
        "        norm_key = _norm_code_for_count(code_raw)\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Lineage Count partial ----------\n",
        "def build_lineage_count_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    first_series = (\n",
        "        main_df.get(\"First Ancestor\", pd.Series(dtype=str))\n",
        "        .astype(str)\n",
        "        .map(lambda x: x.strip())\n",
        "    )\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values(\n",
        "        [\"Count\", \"First Ancestor\"],\n",
        "        ascending=[False, True],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Lineage Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append(\n",
        "        '<th style=\"width:80%\">First Ancestor</th>'\n",
        "        '<th style=\"width:20%\">Showing</th>'\n",
        "    )\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td class=\\\"count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    # DNA Register-style table under Lineage Count\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected lineage(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>Name</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        first = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        if not first:\n",
        "            continue\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 9) Cousin printable partial ----------\n",
        "def build_cousin_print_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    rows = []\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        subject_raw = row.get(match_col, \"\")\n",
        "        subject_name = normalize_person_name(MATCH_TO_UNMASKED.get(str(subject_raw).strip().lower(), subject_raw))\n",
        "        subject_name_html = \"<strong>%s</strong>\" % subject_name if subject_name else \"\"\n",
        "\n",
        "        pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "        matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "        if pid:\n",
        "            matchee_name_html = (\n",
        "                '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "                'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "                % (TNG_BASE, pid, TNG_TREE, matchee_name)\n",
        "            )\n",
        "        else:\n",
        "            matchee_name_html = matchee_name\n",
        "\n",
        "        cm_val = row.get(cm_col, \"0\")\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        gens_total = len(tokens)\n",
        "\n",
        "        if \"common_husband\" in main_df.columns and \"common_wife\" in main_df.columns:\n",
        "            husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "            wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "            if not husband_raw and not wife_raw:\n",
        "                husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "        else:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "        header_html = build_header(\n",
        "            subject_name_html or subject_name,\n",
        "            cm_val,\n",
        "            matchee_name_html,\n",
        "            gens_total,\n",
        "            husband_raw,\n",
        "            wife_raw,\n",
        "        )\n",
        "        rows.append(header_html)\n",
        "\n",
        "    rows_sorted = sorted(rows)\n",
        "\n",
        "    html_rows = [\n",
        "        '<table border=\"1\" id=\"refactor-table\" class=\"sortable\"><thead><tr><th>Match Summary</th></tr></thead><tbody>'\n",
        "    ]\n",
        "    for v in rows_sorted:\n",
        "        html_rows.append(\"<tr><td>%s</td></tr>\" % v)\n",
        "    html_rows.append(\"</tbody></table>\")\n",
        "\n",
        "    cousin_html = (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\" \"\n",
        "        \"\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\"><head>\"\n",
        "        \"%s\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\"\n",
        "        \"<title>Cousin List (Printable)</title>\"\n",
        "        \"</head><body onload=\\\"window.print();\\\">\"\n",
        "        \"<div class=\\\"wrap\\\">\"\n",
        "        \"<h1 class=\\\"centerline\\\">Cousin List (Printable)</h1>\"\n",
        "        \"<div class=\\\"table-scroll\\\">%s</div>\"\n",
        "        \"</div></body></html>\"\n",
        "        % \"\".join(html_rows)\n",
        "    )\n",
        "    return cousin_html\n",
        "\n",
        "# ---------- 10) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for counts: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column for First Ancestor.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    # Rebuild First Ancestor column from lineage path\n",
        "    first_ancestors = []\n",
        "    for _, row in df.iterrows():\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    mc_html = build_match_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        MATCH_COUNT_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(mc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(MATCH_COUNT_LOCAL))\n",
        "\n",
        "    lc_html = build_lineage_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        LINEAGE_COUNT_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(lc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(LINEAGE_COUNT_LOCAL))\n",
        "\n",
        "    cousin_html = build_cousin_print_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        COUSIN_PRINT_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(COUSIN_PRINT_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, MATCH_COUNT_LOCAL, _remote_path(MATCH_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, LINEAGE_COUNT_LOCAL, _remote_path(LINEAGE_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, COUSIN_PRINT_LOCAL, _remote_path(COUSIN_PRINT_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload partials failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [\n",
        "            _remote_path(MATCH_COUNT_REMOTE),\n",
        "            _remote_path(LINEAGE_COUNT_REMOTE),\n",
        "            _remote_path(COUSIN_PRINT_REMOTE),\n",
        "        ]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.shtml\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2b ================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvqS4BLVQ8xR",
        "outputId": "4a7eb7cf-f461-4315-b967-f5b40859be7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.11.30-G3 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): December 4, 2025 10:46 AM\n",
            "[OK] Loaded CSV for counts: 93 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 65480\n",
            "partials/lineage_count.shtml : 69825\n",
            "partials/cousin_list_print.htm : 31260\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell2c"
      ],
      "metadata": {
        "id": "l_ilL_Uwj3kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2c — Seed-based DNA Network (Match Count companion) ======\n",
        "# RON GOLDEN RULES — CLIFF NOTES (v2025.11.30-G3)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; ALL typography/layout via /partials/dna_tree_styles.css (linked only).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2c_SeedNetwork | Version=2025.12.04-G3 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2c_SeedNetwork | Version=2025.12.04-G3 | Encoding=ISO-8859-15\")\n",
        "DECLARED_LINES = 999\n",
        "print(f\"[AUDIT] DECLARED_LINES={DECLARED_LINES}\")\n",
        "\n",
        "import os, re, html as _html, traceback, posixpath, socket\n",
        "import pandas as pd\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ---------- 0) Secrets (FTP) ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 1) Config: use the SAME CSV as Cell 2b ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Where to write outputs\n",
        "NETWORK_LINEAGE_CSV = \"dna_network_lineages.csv\"\n",
        "NETWORK_MATCHES_CSV = \"dna_network_matches.csv\"\n",
        "NETWORK_PARTIAL     = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "# Remote path for the partial\n",
        "SERVER_PARTIALS_DIR    = \"partials\"\n",
        "REMOTE_NETWORK_PARTIAL = posixpath.join(SERVER_PARTIALS_DIR, \"dna_network.shtml\")\n",
        "\n",
        "# Stylesheet used by the proof system (already on server from Cell 2)\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "\n",
        "# ---------- 2) Seed list: codes exactly as shown in Match Count ----------\n",
        "SEED_CODES = [\n",
        "    \"girtain,alma\",\n",
        "    \"girtain,andy\",\n",
        "    \"girtain,kathryn\",\n",
        "    \"girtain,theresa\",\n",
        "]\n",
        "\n",
        "NETWORK_TITLE = \"DNA Network by Lineage (Seed-based from Match Count)\"\n",
        "NETWORK_SUBTITLE = \"Seeds (by code): \" + \", \".join(SEED_CODES)\n",
        "\n",
        "# ---------- 3) Small helpers (matching Cell 2b behavior where needed) ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
        "    return t\n",
        "\n",
        "def safe_write(path, text):\n",
        "    dirpath = os.path.dirname(path)\n",
        "    if dirpath:\n",
        "        os.makedirs(dirpath, exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "def safe_to_csv(df, path):\n",
        "    dirpath = os.path.dirname(path)\n",
        "    if dirpath:\n",
        "        os.makedirs(dirpath, exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        df.to_csv(f, index=False)\n",
        "\n",
        "def escape_txt(x):\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    return _html.escape(str(x), quote=True)\n",
        "\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ---------- 4) Load the SAME working CSV as Cell 2b ----------\n",
        "encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "last_err = None\n",
        "df = None\n",
        "for enc in encs:\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "        break\n",
        "    except Exception as ex:\n",
        "        last_err = ex\n",
        "        df = None\n",
        "\n",
        "if df is None:\n",
        "    raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "print(\"[OK] Loaded CSV for network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "path_col = find_col(\n",
        "    df,\n",
        "    [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "    [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        ")\n",
        "\n",
        "if not match_col:\n",
        "    raise ValueError(\"CSV missing 'Match to' column (tried headings like 'Match to', 'Match').\")\n",
        "if not path_col:\n",
        "    raise ValueError(\"CSV missing lineage/path column (same one Cell 2b uses).\")\n",
        "if not name_col:\n",
        "    raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col:\n",
        "    raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not id_col:\n",
        "    raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "print(\"[COLS] id_col=%r, match_col=%r, name_col=%r, cm_col=%r, path_col=%r\"\n",
        "      % (id_col, match_col, name_col, cm_col, path_col))\n",
        "\n",
        "# ---------- 5) Define \"lineage\" and normalized codes ----------\n",
        "df[\"_lineage_key\"] = df[path_col].map(lambda x: _clean_piece(x))\n",
        "df[\"_code_norm\"]   = df[match_col].map(_norm_code_for_count)\n",
        "\n",
        "seed_norm_set = {_norm_code_for_count(c) for c in SEED_CODES}\n",
        "print(\"[SEEDS] Raw:\", SEED_CODES)\n",
        "print(\"[SEEDS] Normalized:\", \", \".join(sorted(seed_norm_set)))\n",
        "\n",
        "if not seed_norm_set:\n",
        "    print(\"[ERROR] No seeds defined; nothing to do.\")\n",
        "    raise SystemExit(0)\n",
        "\n",
        "# ---------- 6) Batch 1: rows for seed matches, and their lineages ----------\n",
        "df_seed_rows = df[df[\"_code_norm\"].isin(seed_norm_set)].copy()\n",
        "print(\"[INFO] Rows in CSV matching seed codes:\", len(df_seed_rows))\n",
        "\n",
        "if df_seed_rows.empty:\n",
        "    print(\"[WARN] No rows matched the seed codes. Network will be empty.\")\n",
        "    df_lineage = pd.DataFrame()\n",
        "    df_net = pd.DataFrame()\n",
        "else:\n",
        "    lineage_keys = sorted(\n",
        "        x for x in df_seed_rows[\"_lineage_key\"].unique()\n",
        "        if isinstance(x, str) and x.strip() != \"\"\n",
        "    )\n",
        "    print(\"[INFO] Unique lineage keys in Batch 1:\", len(lineage_keys))\n",
        "\n",
        "    if lineage_keys:\n",
        "        df_net = df[df[\"_lineage_key\"].isin(lineage_keys)].copy()\n",
        "    else:\n",
        "        df_net = df.iloc[0:0].copy()\n",
        "    print(\"[INFO] Total rows (matches) in Batch 1 + Batch 2 universe:\", len(df_net))\n",
        "\n",
        "    rows_summary = []\n",
        "    for lin in lineage_keys:\n",
        "        sub_all  = df_net[df_net[\"_lineage_key\"] == lin]\n",
        "        sub_seed = sub_all[sub_all[\"_code_norm\"].isin(seed_norm_set)]\n",
        "\n",
        "        total_count = len(sub_all)\n",
        "        seed_count  = len(sub_seed)\n",
        "        pct_seed = round(100.0 * float(seed_count) / float(total_count), 2) if total_count else 0.0\n",
        "\n",
        "        tokens = split_tokens(lin)\n",
        "        first_ancestor = _clean_piece(tokens[0]) if tokens else \"\"\n",
        "\n",
        "        rows_summary.append({\n",
        "            \"lineage_key\": lin,\n",
        "            \"first_ancestor\": first_ancestor,\n",
        "            \"seed_matches_in_lineage\": seed_count,\n",
        "            \"total_matches_in_lineage\": total_count,\n",
        "            \"percent_seed_in_lineage\": pct_seed,\n",
        "        })\n",
        "\n",
        "    df_lineage = pd.DataFrame(rows_summary)\n",
        "    if not df_lineage.empty:\n",
        "        df_lineage = df_lineage.sort_values(\n",
        "            by=[\"seed_matches_in_lineage\", \"total_matches_in_lineage\", \"first_ancestor\"],\n",
        "            ascending=[False, False, True],\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "# ---------- 8) Match-level listing with Batch flags ----------\n",
        "if not df_net.empty:\n",
        "    df_net = df_net.copy()\n",
        "    df_net[\"batch_flag\"] = df_net[\"_code_norm\"].apply(\n",
        "        lambda v: \"Batch1_Seed\" if v in seed_norm_set else \"Batch2_Extended\"\n",
        "    )\n",
        "\n",
        "    df_matches = df_net[[\n",
        "        \"batch_flag\",\n",
        "        \"_code_norm\",\n",
        "        match_col,\n",
        "        name_col,\n",
        "        cm_col,\n",
        "        \"_lineage_key\",\n",
        "    ]].copy()\n",
        "\n",
        "    df_matches = df_matches.rename(columns={\n",
        "        \"_code_norm\": \"code_norm\",\n",
        "        match_col: \"match_code_raw\",\n",
        "        name_col: \"match_name\",\n",
        "        cm_col: \"cM\",\n",
        "        \"_lineage_key\": \"lineage_key\",\n",
        "    })\n",
        "\n",
        "    df_matches = df_matches.sort_values(\n",
        "        by=[\"lineage_key\", \"batch_flag\", \"code_norm\", \"match_name\"],\n",
        "        ascending=[True, True, True, True],\n",
        "    ).reset_index(drop=True)\n",
        "else:\n",
        "    df_matches = df_net.iloc[0:0].copy()\n",
        "\n",
        "# ---------- 9) Write CSV outputs ----------\n",
        "print(\"[WRITE] Lineage summary CSV:\", NETWORK_LINEAGE_CSV)\n",
        "safe_to_csv(df_lineage, NETWORK_LINEAGE_CSV)\n",
        "\n",
        "print(\"[WRITE] Match-level network CSV:\", NETWORK_MATCHES_CSV)\n",
        "safe_to_csv(df_matches, NETWORK_MATCHES_CSV)\n",
        "\n",
        "# ---------- 10) Build XHTML partial (dna_network.shtml) ----------\n",
        "lineage_rows_html = []\n",
        "if df_lineage.empty:\n",
        "    lineage_rows_html.append(\n",
        "        \"<tr><td colspan=\\\"5\\\" class=\\\"dna-empty\\\">No lineages found for the selected seed codes.</td></tr>\"\n",
        "    )\n",
        "else:\n",
        "    for _, r in df_lineage.iterrows():\n",
        "        lineage_rows_html.append(\n",
        "            \"<tr>\"\n",
        "            + \"<td class=\\\"dna-cell-first-ancestor\\\">\" + escape_txt(r.get(\"first_ancestor\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-lineage-path\\\">\"   + escape_txt(r.get(\"lineage_key\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-count-seed\\\">\"     + escape_txt(r.get(\"seed_matches_in_lineage\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-count-total\\\">\"    + escape_txt(r.get(\"total_matches_in_lineage\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-pct\\\">\"            + escape_txt(r.get(\"percent_seed_in_lineage\", \"\")) + \"</td>\"\n",
        "            + \"</tr>\"\n",
        "        )\n",
        "lineage_table_html = \"\\n\".join(lineage_rows_html)\n",
        "\n",
        "match_rows_html = []\n",
        "if df_matches.empty:\n",
        "    match_rows_html.append(\n",
        "        \"<tr><td colspan=\\\"6\\\" class=\\\"dna-empty\\\">No matches found in the Batch-1/Batch-2 universe.</td></tr>\"\n",
        "    )\n",
        "else:\n",
        "    for _, r in df_matches.iterrows():\n",
        "        match_rows_html.append(\n",
        "            \"<tr>\"\n",
        "            + \"<td class=\\\"dna-cell-batch-flag\\\">\"   + escape_txt(r.get(\"batch_flag\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-code-norm\\\">\"    + escape_txt(r.get(\"code_norm\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-code-raw\\\">\"     + escape_txt(r.get(\"match_code_raw\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-name\\\">\"         + escape_txt(r.get(\"match_name\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-cm\\\">\"           + escape_txt(r.get(\"cM\", \"\")) + \"</td>\"\n",
        "            + \"<td class=\\\"dna-cell-lineage-path\\\">\" + escape_txt(r.get(\"lineage_key\", \"\")) + \"</td>\"\n",
        "            + \"</tr>\"\n",
        "        )\n",
        "match_table_html = \"\\n\".join(match_rows_html)\n",
        "\n",
        "xhtml = \"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        "  \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "  <title>DNA Network (Seed-based)</title>\n",
        "  <link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"wrap dna-network\">\n",
        "  <h1 class=\"centerline\">%s</h1>\n",
        "  <div class=\"centerline dna-subheading\">%s</div>\n",
        "\n",
        "  <h2 class=\"centerline\">Lineage overview (Batch 1 universe)</h2>\n",
        "  <div class=\"table-scroll\">\n",
        "    <table class=\"sortable dna-table dna-network-lineage\" border=\"1\">\n",
        "      <thead>\n",
        "        <tr>\n",
        "          <th>First Ancestor (token 1)</th>\n",
        "          <th>Lineage line (full path)</th>\n",
        "          <th>Seed matches in lineage</th>\n",
        "          <th>Total matches in lineage</th>\n",
        "          <th>Percent seed in lineage</th>\n",
        "        </tr>\n",
        "      </thead>\n",
        "      <tbody>\n",
        "%s\n",
        "      </tbody>\n",
        "    </table>\n",
        "  </div>\n",
        "\n",
        "  <h2 class=\"centerline\">Batch 1 and Batch 2 matches in those lineages</h2>\n",
        "  <div class=\"table-scroll\">\n",
        "    <table class=\"sortable dna-table dna-network-matches\" border=\"1\">\n",
        "      <thead>\n",
        "        <tr>\n",
        "          <th>Batch</th>\n",
        "          <th>Code (normalized)</th>\n",
        "          <th>Code (raw from CSV)</th>\n",
        "          <th>Name</th>\n",
        "          <th>cM</th>\n",
        "          <th>Lineage line (full path)</th>\n",
        "        </tr>\n",
        "      </thead>\n",
        "      <tbody>\n",
        "%s\n",
        "      </tbody>\n",
        "    </table>\n",
        "  </div>\n",
        "\n",
        "  <p class=\"dna-footnote\">\n",
        "    Batch 1 (Seed) = rows where the normalized code is one of: %s.<br />\n",
        "    Batch 2 (Extended) = all other matches sharing any lineage path with Batch 1.\n",
        "  </p>\n",
        "</div>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\" % (\n",
        "    STYLESHEET_HREF,\n",
        "    _html.escape(NETWORK_TITLE),\n",
        "    _html.escape(NETWORK_SUBTITLE),\n",
        "    lineage_table_html,\n",
        "    match_table_html,\n",
        "    _html.escape(\", \".join(sorted(seed_norm_set))),\n",
        ")\n",
        "\n",
        "print(\"[WRITE] XHTML partial:\", NETWORK_PARTIAL)\n",
        "safe_write(NETWORK_PARTIAL, xhtml)\n",
        "\n",
        "# ---------- 11) Upload partial to server ----------\n",
        "def upload_network_partial():\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, NETWORK_PARTIAL, _remote_path(REMOTE_NETWORK_PARTIAL))\n",
        "            print(\"[OK] Uploaded dna_network partial ->\", _remote_path(REMOTE_NETWORK_PARTIAL))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network partial failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p = _remote_path(REMOTE_NETWORK_PARTIAL)\n",
        "        sz = ftp_size(ftps, p)\n",
        "        print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\nOpen URL: https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "upload_network_partial()\n",
        "\n",
        "print(\"[DONE] Seed-based DNA network CSVs and partial generated + uploaded successfully.\")\n",
        "# ====== CUT STOP [1/1] CELL 2c — Seed-based DNA Network (Match Count companion) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcIsrGKNGcrP",
        "outputId": "75f11b31-2c22-44c1-857d-88777aa0c2e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2c_SeedNetwork | Version=2025.12.04-G3 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=999\n",
            "[OK] Loaded CSV for network: 93 rows, 6 cols\n",
            "[COLS] id_col='ID#', match_col='Match to', name_col='Name', cm_col='cM', path_col='Yates DNA Ancestral Line'\n",
            "[SEEDS] Raw: ['girtain,alma', 'girtain,andy', 'girtain,kathryn', 'girtain,theresa']\n",
            "[SEEDS] Normalized: girtain,alma, girtain,andy, girtain,kathryn, girtain,theresa\n",
            "[INFO] Rows in CSV matching seed codes: 11\n",
            "[INFO] Unique lineage keys in Batch 1: 10\n",
            "[INFO] Total rows (matches) in Batch 1 + Batch 2 universe: 11\n",
            "[WRITE] Lineage summary CSV: dna_network_lineages.csv\n",
            "[WRITE] Match-level network CSV: dna_network_matches.csv\n",
            "[WRITE] XHTML partial: partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[OK] Uploaded dna_network partial -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 14483\n",
            "\n",
            "Open URL: https://yates.one-name.net/partials/dna_network.shtml\n",
            "[DONE] Seed-based DNA network CSVs and partial generated + uploaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "INiJljOS1kRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 3 - Ancestor Register (Old-school Blue Menu; WHITE menu text; .shtml + SSI) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.01-G1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "from string import Template as _T\n",
        "\n",
        "# Downloads paragraph is now suppressed (links live in nav_block.shtml)\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip().strip(\"/\")\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "# This page is now .shtml so Apache will parse SSI\n",
        "OUTPUT_NAME = \"just-trees.shtml\"\n",
        "REMOTE_HTML = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "# Stylesheet + cache buster (shared with Cell 2)\n",
        "STYLESHEET_HREF = \"/partials/dna_tree_styles.css\"\n",
        "CSS_VERSION     = \"v2025-11-23-g3\"\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />' % (STYLESHEET_HREF, CSS_VERSION)\n",
        "\n",
        "# Layout knob (used for top-scroll inner width)\n",
        "TABLE_WIDTH_PX = 5550\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV: %s rows=%d, cols=%d\" % (INPUT_CSV, len(df), len(df.columns)))\n",
        "\n",
        "# Ensure haplogroup present (harmless for this view)\n",
        "if \"haplogroup\" not in df.columns:\n",
        "    df[\"haplogroup\"] = \"\"\n",
        "else:\n",
        "    df[\"haplogroup\"] = df[\"haplogroup\"].fillna(\"\")\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX = 0\n",
        "B_IDX = 1\n",
        "C_IDX = 2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\", \" \")\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
        "    return t.lower()\n",
        "\n",
        "# Prefer local-first resolver cached by Cell 1; fall back to server\n",
        "LOCAL_RESOLVER = \"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER = \"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path))\n",
        "        return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", \"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try:\n",
        "                        ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            ftps.mkd(p)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        ftps.cwd(p)\n",
        "            try:\n",
        "                ftps.cwd(\"partials\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            with open(\"match_to_unmasked.csv\", \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR match_to_unmasked.csv\", f.write)\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not pull resolver from server:\", e)\n",
        "        return local_path\n",
        "\n",
        "LOCAL_RESOLVER = _pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    last = None\n",
        "    m = None\n",
        "    for enc in (\"utf-8-sig\", \"iso-8859-15\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            m = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            m = None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last)\n",
        "        return {}\n",
        "    cols = {c.lower(): c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\")\n",
        "        return {}\n",
        "    m = m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"] = m[cols[\"code\"]].map(_norm_code)\n",
        "    m[\"__val__\"] = m[cols[\"unmasked\"]].astype(str)\n",
        "    m = m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map = _load_resolver_to_map(LOCAL_RESOLVER) if os.path.exists(LOCAL_RESOLVER) else {}\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw = df.iloc[:, B_IDX].astype(str)\n",
        "masked_key = masked_raw.map(_norm_code)\n",
        "resolved   = masked_key.map(resolver_map)\n",
        "df.iloc[:, C_IDX] = resolved.fillna(\"\")\n",
        "\n",
        "print(\n",
        "    \"[OK] Column B -> C mapping: %d / %d  unmatched: %d\"\n",
        "    % (int(resolved.notna().sum()), len(df), len(df) - int(resolved.notna().sum()))\n",
        ")\n",
        "\n",
        "# ---------- Load vitals from dna_vitals.csv (friendly date + autosomal count) ----------\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "LAST_UPDATED_TEXT  = \"\"\n",
        "AUTOSOMAL_MATCHES  = \"\"\n",
        "SHOWING_STATIC     = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    \"\"\"\n",
        "    Convert a UTC timestamp like\n",
        "      'YYYY-MM-DD HH:MM UTC'\n",
        "      'YYYY-MM-DD HH:MM:SS UTC'\n",
        "      'YYYY-MM-DDTHH:MM UTC'\n",
        "      'YYYY-MM-DDTHH:MM:SS UTC'\n",
        "    to 'Month D, YYYY H:MM AM/PM' in approximate US Eastern (UTC-5).\n",
        "    On failure, return the original string.\n",
        "    \"\"\"\n",
        "    s = str(raw or \"\").strip()\n",
        "    if not s:\n",
        "        return \"(unknown)\"\n",
        "    s = s.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(s, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24  = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12  = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _format_num_with_commas(raw_val):\n",
        "    \"\"\"Normalize a numeric string (allowing commas) and format with thousands commas.\"\"\"\n",
        "    if raw_val is None:\n",
        "        return \"\"\n",
        "    s = str(raw_val)\n",
        "    # Strip everything except digits and optional leading minus\n",
        "    s_digits = re.sub(r\"[^0-9\\-]\", \"\", s)\n",
        "    if not s_digits:\n",
        "        return \"\"\n",
        "    try:\n",
        "        n = int(s_digits)\n",
        "        return \"{:,}\".format(n)\n",
        "    except Exception:\n",
        "        return s_digits\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global LAST_UPDATED_TEXT, AUTOSOMAL_MATCHES, SHOWING_STATIC\n",
        "    LAST_UPDATED_TEXT = \"\"\n",
        "    AUTOSOMAL_MATCHES = \"\"\n",
        "    SHOWING_STATIC    = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; LAST_UPDATED_TEXT and AUTOSOMAL_MATCHES will be blank.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        vdf = pd.read_csv(path, dtype=str, encoding=\"iso-8859-15\", keep_default_na=False)\n",
        "    except Exception:\n",
        "        encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "        last = None\n",
        "        vdf  = None\n",
        "        for enc in encs:\n",
        "            try:\n",
        "                vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "        if vdf is None:\n",
        "            print(\"[WARN] Unable to read dna_vitals.csv: %s\" % last)\n",
        "            return\n",
        "\n",
        "    flat = []\n",
        "    for row in vdf.astype(str).values.tolist():\n",
        "        for cell in row:\n",
        "            flat.append(str(cell))\n",
        "\n",
        "    autosomal_raw = None\n",
        "    showing_raw   = None\n",
        "    last_text     = None\n",
        "\n",
        "    for cell in flat:\n",
        "        s = str(cell)\n",
        "        if \"Records tagged and filtered by NPFX\" in s and autosomal_raw is None:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", s)\n",
        "            if m:\n",
        "                autosomal_raw = m.group(1)\n",
        "        if \"After manual filter, total records\" in s and showing_raw is None:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", s)\n",
        "            if m:\n",
        "                showing_raw = m.group(1)\n",
        "        if \"LAST_UPDATED_TEXT\" in s and last_text is None:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", s)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "\n",
        "    # Fallback: grab first two numeric-looking tokens if labels were not found\n",
        "    if autosomal_raw is None or showing_raw is None or last_text is None:\n",
        "        all_text = \" \".join(flat)\n",
        "        nums = re.findall(r\"\\d[\\d,]*\", all_text)\n",
        "        if autosomal_raw is None and len(nums) >= 1:\n",
        "            autosomal_raw = nums[0]\n",
        "        if showing_raw is None and len(nums) >= 2:\n",
        "            showing_raw = nums[1]\n",
        "\n",
        "    if last_text is not None:\n",
        "        last_text = _friendly_ts_from_utc(last_text)\n",
        "\n",
        "    AUTOSOMAL_MATCHES = _format_num_with_commas(autosomal_raw)\n",
        "    SHOWING_STATIC    = _format_num_with_commas(showing_raw)\n",
        "    LAST_UPDATED_TEXT = last_text or \"\"\n",
        "\n",
        "    print(\"[OK] Vitals from %s -> autosomal=%s, showing=%s, last_updated_text=%s\"\n",
        "          % (path,\n",
        "             AUTOSOMAL_MATCHES or \"?\",\n",
        "             SHOWING_STATIC or \"?\",\n",
        "             LAST_UPDATED_TEXT or \"(blank)\"))\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "# ---------- Blocks (updated, nav via SSI, controls) ----------\n",
        "if LAST_UPDATED_TEXT:\n",
        "    updated_str = (\n",
        "        'Last updated: <span id=\"last-updated\">%s</span>' %\n",
        "        _html.escape(LAST_UPDATED_TEXT)\n",
        "    )\n",
        "else:\n",
        "    updated_str = 'Last updated: <span id=\"last-updated\"></span>'\n",
        "\n",
        "_updated_parts = [updated_str]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "# Showing is still the live, on-page filtered count\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">' +\n",
        "    ' &nbsp;|&nbsp; '.join(_updated_parts) +\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" '\n",
        "    'placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ---------- HTML table ----------\n",
        "visible_cols = [c for c in df.columns if c]\n",
        "\n",
        "table_html = df.to_html(\n",
        "    index=False,\n",
        "    columns=visible_cols,\n",
        "    escape=False,\n",
        "    border=1,\n",
        "    classes=\"dataframe sortable\"\n",
        ")\n",
        "\n",
        "if 'id=\"refactor-table\"' not in table_html:\n",
        "    table_html = re.sub(r\"<table([^>]*)>\", r'<table\\1 id=\"refactor-table\">', table_html, count=1)\n",
        "\n",
        "if 'class=\"dataframe sortable\"' not in table_html and \"sortable\" not in table_html:\n",
        "    table_html = table_html.replace('class=\"dataframe\"', 'class=\"dataframe sortable\"', 1)\n",
        "\n",
        "table_html = table_html.replace(\"<tbody>\\n<tr>\", \"<tbody>\\n<tr id=\\\"first-row\\\">\", 1)\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div id=\"top-scroll\" class=\"scroll-sync-top\">'\n",
        "    '<div class=\"scroll-sync-top-inner\" style=\"width:%dpx;\"></div>'\n",
        "    '</div>'\n",
        "    '<div id=\"bottom-scroll\" class=\"table-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (TABLE_WIDTH_PX, table_html)\n",
        "\n",
        "# ---------- XHTML page template ----------\n",
        "page_tpl = _T(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "<style type=\"text/css\">\n",
        "/* Sticky second column (index 2) for Trees table */\n",
        "#refactor-table th:nth-child(2),\n",
        "#refactor-table td:nth-child(2){\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "#refactor-table th:nth-child(2){\n",
        "  z-index:7;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase();\n",
        "  }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[colIndex]), B = textOf(b.cells[colIndex]);\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "      if(!isNaN(nA) && !isNaN(nB)){ return asc ? (nA-nB) : (nB-nA); }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dir = 'asc';\n",
        "      th.addEventListener('click', function(){\n",
        "        dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "        for (var j = 0; j < ths.length; j++){\n",
        "          ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "        }\n",
        "        th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl, idx, dir);\n",
        "      }, false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){ return String(n||''); }\n",
        "  }\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb = tbl.tBodies[0];\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200 ? 'block' : 'none'); }\n",
        "    toggle();\n",
        "    window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "  function bindSyncedScrollbars(){\n",
        "    var topScroll    = document.getElementById('top-scroll');\n",
        "    var bottomScroll = document.getElementById('bottom-scroll');\n",
        "    if(!(topScroll && bottomScroll)) return;\n",
        "    var syncing = false;\n",
        "    topScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      bottomScroll.scrollLeft = topScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "    bottomScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      topScroll.scrollLeft = bottomScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindBackToTop();\n",
        "    bindSearch();\n",
        "    bindSyncedScrollbars();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER=SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df = df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w:\n",
        "        export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for seg in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host = os.environ.get(\"FTP_HOST\")\n",
        "ftp_user = os.environ.get(\"FTP_USER\")\n",
        "ftp_pass = os.environ.get(\"FTP_PASS\")\n",
        "ftp_port = int(os.environ.get(\"FTP_PORT\", \"21\") or \"21\")\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            with open(OUTPUT_NAME, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            with open(LOCAL_CSV, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (.shtml + SSI nav; top+bottom scroll; sticky col 2; sortable/searchable with live 'Showing' count; LAST_UPDATED_TEXT + Autosomal matches from dna_vitals.csv, now comma-formatted; exports + upload ready) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SPp7EfhUfkE",
        "outputId": "2cfbc717-10bd-40af-a5ee-5fbfe42b19e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=1604, cols=6\n",
            "Using resolver: /content/match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 1604 / 1604  unmatched: 0\n",
            "[OK] Vitals from dna_vitals.csv -> autosomal=1,604, showing=1,604, last_updated_text=December 1, 2025 8:39 AM\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (.shtml + SSI nav; top+bottom scroll; sticky col 2; sortable/searchable with live 'Showing' count; LAST_UPDATED_TEXT + Autosomal matches from dna_vitals.csv, now comma-formatted; exports + upload ready) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# debug"
      ],
      "metadata": {
        "id": "9G7Y0HwjtZIt"
      }
    }
  ]
}