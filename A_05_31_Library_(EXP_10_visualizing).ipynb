{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeCXMqHqYsMpVAT4q7Nl10",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/A_05_31_Library_(EXP_10_visualizing).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "b630b5b5-3d54-4351-d8fb-1134dd6fedbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A*****CELL 1 *****************************************************GOOD Main Script\n",
        "\n",
        "fq_threshold = 1  # Define the fq_threshold value\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "import os\n",
        "\n",
        "# Hard-coded interval scheme\n",
        "interval_scheme = [\n",
        "    (1000, 1), (1025, 2), (1050, 3), (1075, 4), (1100, 5),\n",
        "    (1125, 6), (1150, 7), (1175, 8), (1200, 9), (1225, 10),\n",
        "    (1250, 11), (1275, 12), (1300, 13), (1325, 14), (1350, 15),\n",
        "    (1375, 16), (1400, 17), (1425, 18), (1450, 19), (1475, 20),\n",
        "    (1500, 21), (1525, 22), (1550, 23), (1575, 24), (1600, 25),\n",
        "    (1625, 26), (1650, 27), (1675, 28), (1700, 29), (1725, 30),\n",
        "    (1750, 31), (1775, 32), (1800, 33), (1825, 34), (1850, 35),\n",
        "    (1875, 36), (1900, 37), (1925, 38), (1950, 39), (1975, 40),\n",
        "    (2000, 41), (2025, 42), (2050, 43), (2075, 44)\n",
        "]\n",
        "\n",
        "# Function to assign interval based on birthdate\n",
        "def assign_interval(birth_date, interval_scheme):\n",
        "    if pd.isnull(birth_date):\n",
        "        return 99\n",
        "    try:\n",
        "        birth_year = int(birth_date[-4:])\n",
        "    except ValueError:\n",
        "        return 99\n",
        "    for year, interval in interval_scheme:\n",
        "        if birth_year <= year:\n",
        "            return interval\n",
        "    return 99\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "    def get_birth_date(self):\n",
        "        return self.extractable_detail.get('BIRTH_DATE', '')\n",
        "\n",
        "    def get_fams(self):\n",
        "        return self.extractable_detail.get('FAMS', '').strip('@')\n",
        "\n",
        "# Set of excluded record IDs\n",
        "excluded_ids = {\n",
        "    'I47640', 'I55585', 'I47666', 'I55586', 'I47570', 'I55587',\n",
        "    'I47569', 'I47641', 'I47571', 'I47572', 'I47549', 'I47550',\n",
        "    'I47573', 'I55588', 'I47548', 'I47551', 'I47553', 'I24502', 'I24503',\n",
        "    'I47659', 'I48070', 'I47660', 'I48129', 'I48130', 'I48126', 'I48127'\n",
        "}\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:30]\n",
        "    last_name = last_name[:30].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                individual_id = tag.strip('@')\n",
        "                if individual_id in excluded_ids:\n",
        "                    continue  # Skip processing for the excluded IDs\n",
        "\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC', 'FAMS', 'BIRT', 'SEX']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'DATE' and current_key == 'BIRT':\n",
        "                    current_dataset.add_extractable_detail('BIRTH_DATE', value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Remove excluded IDs from filter_pool\n",
        "        self.filter_pool = [ds for ds in self.filter_pool if ds.get_gen_person() not in excluded_ids]\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def apply_manual_filter(self):\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "                manual_filtered_ids = set(df['ID'].astype(str))  # Ensure IDs are strings\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids)}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                # Debug output to verify IDs before filtering\n",
        "                print(\"IDs before manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) in manual_filtered_ids]\n",
        "                print(\"IDs after manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def check_and_apply_exclusion_filter(self):\n",
        "        \"\"\"Apply exclusion filter if '/exclude_ids.xlsx' is present.\"\"\"\n",
        "        file_path = '/content/exclude_ids.xlsx'  # Updated path\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df_exclude = pd.read_excel(file_path)\n",
        "                if 'ID' in df_exclude.columns:\n",
        "                    exclude_ids = set(df_exclude['ID'].astype(str))  # Ensure conversion to string\n",
        "                    print(f\"Exclusion filter IDs loaded: {len(exclude_ids)}\")\n",
        "                    print(f\"Sample of IDs to exclude: {list(exclude_ids)[:5]}\")  # Print some sample IDs\n",
        "                    # Apply the exclusion filter\n",
        "                    initial_count = len(self.filter_pool)\n",
        "                    self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) not in exclude_ids]\n",
        "                    print(f\"Excluded {initial_count - len(self.filter_pool)} records based on exclusion IDs.\")\n",
        "                else:\n",
        "                    print(\"Column 'ID' not found in the Excel file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to apply exclusion filter: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"No exclusion filter applied, '{file_path}' not found. Check the path and ensure the file is uploaded to Colab.\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list, add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "# Function to find parents, ensuring excluded IDs are not processed\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id in excluded_ids or individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "# Function to find distant ancestors, ensuring excluded IDs are not processed\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    if individual_id in excluded_ids:\n",
        "        return []\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id and father_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id and mother_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Other parts of the script remain the same\n",
        "\n",
        "# Example usage after parsing and filtering\n",
        "gedcom_instance.parse_gedcom()\n",
        "\n",
        "# Save the filter_pool to an Excel file\n",
        "#print_filter_pool_to_excel(gedcom_instance.filter_pool)\n",
        "\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records, interval_scheme):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    birth_date = None  # Initialize birth_date\n",
        "\n",
        "    # Extract cm_value, sort_value, anchor_gen1, and birth_date\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            birth_date = dataset.extractable_detail.get('BIRTH_DATE')  # Get birth date\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "        birth_date = 'N/A'  # Set default if not found\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    ancestors_data = []\n",
        "\n",
        "    # Extract ancestor information and concatenate name with Date Interval\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        ancestor_details = []\n",
        "        for ancestor_id in pair:\n",
        "            if ancestor_id in records:\n",
        "                ancestor_record = records[ancestor_id]\n",
        "                ancestor_name = extract_name(ancestor_record)\n",
        "                ancestor_sex = 'M' if '1 SEX M' in ancestor_record else 'F' if '1 SEX F' in ancestor_record else ''\n",
        "                birth_date_start = ancestor_record.find('2 DATE ') + 7\n",
        "                birth_date_end = ancestor_record.find('\\n', birth_date_start)\n",
        "                ancestor_birth_date = ancestor_record[birth_date_start:birth_date_end].strip() if birth_date_start != -1 else 'N/A'\n",
        "                ancestor_date_interval = assign_interval(ancestor_birth_date, interval_scheme)\n",
        "                ancestor_details.append({\n",
        "                    'ID': ancestor_id,\n",
        "                    'Name': ancestor_name,\n",
        "                    'Sex': ancestor_sex,\n",
        "                    'Birth Date': ancestor_birth_date,\n",
        "                    'Date Interval': ancestor_date_interval\n",
        "                })\n",
        "        if ancestor_details:\n",
        "            name_with_interval_1 = f\"{ancestor_details[0]['Date Interval']}{ancestor_details[0]['Name']}\"\n",
        "            name_with_interval_2 = f\"{ancestor_details[1]['Date Interval']}{ancestor_details[1]['Name']}\" if len(ancestor_details) > 1 else '99Unknown'\n",
        "            combined_name_with_interval = f\"{name_with_interval_1}&{name_with_interval_2}\"\n",
        "            ancestors_data.append(combined_name_with_interval)\n",
        "\n",
        "    # Reverse the order to start with the oldest ancestor\n",
        "    ancestors_data.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(ancestors_data)\n",
        "\n",
        "    # Assign date interval\n",
        "    date_interval = assign_interval(birth_date, interval_scheme)\n",
        "\n",
        "    # Create first-pair by splitting the ancestors full line\n",
        "    first_pair = ancestors_data[0] if ancestors_data else ''\n",
        "\n",
        "    individual_data = {\n",
        "        'Sort': sort_value,\n",
        "        'ID': individual_id,\n",
        "        'Name': extract_name(records[individual_id]),\n",
        "        'Sex': dataset.extractable_detail.get('SEX', ''),\n",
        "        'Birth Date': birth_date,\n",
        "        'Date Interval': date_interval,\n",
        "        'FAMS': dataset.get_fams(),\n",
        "        'FAMC': dataset.get_extractable_FAMC(),\n",
        "        'cM': cm_value,\n",
        "        'First Pair': first_pair,  # Add first-pair here\n",
        "        'Ancestors': filtered_ancestral_line_str  # Add ancestors data to individual data\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "# Initialize the dictionary to store individual data\n",
        "DNA_Study_Library = {}\n",
        "\n",
        "# Process individuals and populate the dictionary\n",
        "for dataset in gedcom_instance.filter_pool:  # Assuming filter_pool is iterable\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    # Process Individual and Get Data\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(individual_id, gedcom_instance, records, interval_scheme)\n",
        "\n",
        "    # Store the data in the dictionary\n",
        "    DNA_Study_Library[individual_id] = individual_data\n",
        "\n",
        "\n",
        "\n",
        "# Function to save the data to an Excel file\n",
        "def save_to_excel(data_dict, filename):\n",
        "    # Convert the dictionary to a pandas DataFrame\n",
        "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
        "\n",
        "    # Save the DataFrame to an Excel file using openpyxl\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, sheet_name='DNA_Study_Library', index=False)  # index=False to prevent writing the index\n",
        "\n",
        "        # Apply some basic formatting\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['DNA_Study_Library']\n",
        "\n",
        "        # Adjust column width\n",
        "        for column in worksheet.columns:\n",
        "            max_length = 0\n",
        "            column = [cell for cell in column]\n",
        "            for cell in column:\n",
        "                try:\n",
        "                    if len(str(cell.value)) > max_length:\n",
        "                        max_length = len(cell.value)\n",
        "                except:\n",
        "                    pass\n",
        "            adjusted_width = (max_length + 2)\n",
        "            worksheet.column_dimensions[column[0].column_letter].width = adjusted_width\n",
        "\n",
        "# Save the DNA Study Library to an Excel file\n",
        "save_to_excel(DNA_Study_Library, 'DNA_Study_Library.xlsx')\n",
        "\n",
        "print(\"Data has been saved to DNA_Study_Library.xlsx\")\n",
        "\n",
        "\n",
        "# CELL 1*****************************************************GOOD Main Script"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bs4rp5lwCrdu",
        "outputId": "1d4a672f-8490-480f-b309-b1bc7c20892f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 1\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 55704 total records\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "filtered_ids.xlsx not found. Skipping second-level manual filter.\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "GEDCOM contained 55704 total records\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "filtered_ids.xlsx not found. Skipping second-level manual filter.\n",
            "Data has been saved to DNA_Study_Library.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B********cell 77EXP**********1500hrs**********************************************************\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Convert the DataFrame to a dictionary\n",
        "DNA_Study_Library = df.to_dict(orient='records')\n",
        "\n",
        "def calculate_root_counts(DNA_Study_Library):\n",
        "    # Initialize a dictionary to store the counts and sum of cMs of each root node-parent pair\n",
        "    root_count_dict = {}\n",
        "\n",
        "    # Populate the root_count_dict with the counts and sum of cMs of each root node-parent pair\n",
        "    for data in DNA_Study_Library:\n",
        "        first_pair = data['First Pair']\n",
        "        try:\n",
        "            cM_value = int(data['cM'])  # Ensure cM value is an integer\n",
        "        except ValueError:\n",
        "            print(f\"Invalid cM value for {first_pair}: {data['cM']}\")\n",
        "            continue\n",
        "\n",
        "        if first_pair not in root_count_dict:\n",
        "            root_count_dict[first_pair] = {\n",
        "                'count': 1,\n",
        "                'Sum cM': cM_value\n",
        "            }\n",
        "        else:\n",
        "            root_count_dict[first_pair]['count'] += 1\n",
        "            root_count_dict[first_pair]['Sum cM'] += cM_value\n",
        "\n",
        "    # Create the new data structure with the required columns\n",
        "    output_data = []\n",
        "\n",
        "    for first_pair, info in root_count_dict.items():\n",
        "        QI = round(info['Sum cM'] / info['count']) if info['count'] != 0 else 0\n",
        "        output_data.append({\n",
        "            'root_parent_master': first_pair,\n",
        "            'FQ': info['count'],\n",
        "            'QI': QI\n",
        "        })\n",
        "\n",
        "    # Sort the data by 'root_parent_master' alphabetically\n",
        "    output_data.sort(key=lambda x: x['root_parent_master'])\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Function to print the data in the desired console format\n",
        "def print_root_counts(output_data):\n",
        "    print(f\"{'Result':<20}\")\n",
        "    print(pd.DataFrame(output_data, columns=['root_parent_master', 'FQ', 'QI']).to_string(index=True))\n",
        "\n",
        "# Calculate the root counts\n",
        "output_data = calculate_root_counts(DNA_Study_Library)\n",
        "\n",
        "# Print the report in the required format\n",
        "print_root_counts(output_data)\n",
        "\n",
        "# Save the output data to a root_parent_master dictionary\n",
        "root_parent_master = {row['root_parent_master']: row for row in output_data}\n",
        "\n",
        "# Function to save the output data to an Excel file\n",
        "def save_to_excel(output_data, filename):\n",
        "    df = pd.DataFrame(output_data, columns=['root_parent_master', 'FQ', 'QI'])\n",
        "    df.to_excel(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Save the output data to an Excel file in the required format\n",
        "save_to_excel(output_data, '/content/root_parent_master_processed.xlsx')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYxXimU7hBkj",
        "outputId": "75778868-0df1-4ea2-ec4b-f717f3cf0d21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result              \n",
            "                                  root_parent_master   FQ   QI\n",
            "0                     23YatesFrancis&23TichborneJane  507   65\n",
            "1             24YatesJohnThomas&24HatfieldeElizabeth   62   87\n",
            "2                         26YatesJohn&27StrattonMary    4   17\n",
            "3               26YatesJohn&27StrattonUrslyMehitable    9   18\n",
            "4                     26YatesThomas&26PrattElizabeth    1   46\n",
            "5                       27StockettThomas&27WellsMary    1    8\n",
            "6             27YatesWilliam&27BondMargaretCatherine   12   17\n",
            "7                        28YatesHadney&28JamesonMary    1   26\n",
            "8                        28YatesThomas&28DruryMonica    8   14\n",
            "9                        28YatesWilliam&28PipkinMary    1   23\n",
            "10                      29PhillipsWilliam&29YatesAnn    4   23\n",
            "11                          29YatesJames&30McNayJane    5   15\n",
            "12                              29YatesMartin&42Eliz    4   23\n",
            "13                             29YatesRichard&42Mary    4   16\n",
            "14                    29YatesThomas&42SearchingStill    5   13\n",
            "15                  30YatesBenjamin&42SearchingStill  159   92\n",
            "16                          30YatesJohn&30BoswellAnn    5   23\n",
            "17                    30YatesWilliam&30ThornburyAnne   12   23\n",
            "18                      31DillonJonathan&31YatesMary    1   36\n",
            "19                     31HandJonathan&31YatesRebecca    2   50\n",
            "20                 31RoperJamesDavid&31YatesSarahAnn    6   19\n",
            "21                   31YatesAbraham&42SearchingStill   24  676\n",
            "22                   31YatesJohn&31BarfieldElizabeth   16   18\n",
            "23                       31YatesThomas&32HaslamBetty    1   24\n",
            "24                      32BaileyWilliam&32YatesRhoda    1   29\n",
            "25               32SwiftAbsalom&33YatesMaryElizabeth    3   18\n",
            "26                      32YatesAbraham&32HunterKitty    1   27\n",
            "27                  32YatesChristopher&32GurrMaryAnn    1   16\n",
            "28                       32YatesHenry&32McManusNancy    2   31\n",
            "29               32YatesJacob&32VandenberghElizabeth    3   20\n",
            "30                    32YatesJames&33FlournoyMaryAnn    2   19\n",
            "31                       32YatesJames&33SanfordSarah   21   14\n",
            "32                    32YatesJohnPryor&33MartinSarah    3   26\n",
            "33                        32YatesJoseph&32AtmarSarah    5  242\n",
            "34                         32YatesJoseph&32LeighMary    8  677\n",
            "35             32YatesJoshuaHardy&32StewartMarthaAnn    4   18\n",
            "36                  32YatesLoyd&32BrasfieldMaryPolly    2   17\n",
            "37                      32YatesPeter&32ToofCatherine    1  127\n",
            "38                       32YatesThomas&33CombsPhebey    1   20\n",
            "39                         32YatesUriah&32OakesSarah    1   20\n",
            "40           32YatesWilliam&32BoothAnamariaElizabeth    1   21\n",
            "41                      32YatesWilliam&33NeedhamMary    3    9\n",
            "42                      33ColliverJames&34YatesNancy    1   23\n",
            "43                  33CowdenWilliam&33YatesCatherine    2   22\n",
            "44                33FreebodyWilliamKnapp&33YatesAnne    1   21\n",
            "45                 33GirtainJonathon&33BarberRebecca    2  185\n",
            "46                            33KingJohn&34YatesAnna    1   24\n",
            "47                    33ShawEbenezer&33YatesRebbecca    1    9\n",
            "48             33TolerElishaPhilpot&33YatesElizabeth    2   14\n",
            "49                    33YatesAbsolam&34HaleyCathrine    1   21\n",
            "50          33YatesGeorgeWashington&42SearchingStill    1   25\n",
            "51                     33YatesJamesEdward&33BeardAnn    1    9\n",
            "52                      33YatesJohn&42SearchingStill    2   18\n",
            "53                          33YatesJohnB&34Elizabeth    1   28\n",
            "54                   33YatesMalcolm&42SearchingStill    1   17\n",
            "55                   33YatesSamuel&34SwisherMargaret    1   21\n",
            "56                       33YatesThomas&33JohnsonMary    1  233\n",
            "57                       33YatesThomas&33PaulAbigail    1   18\n",
            "58                33YatesThomas&34CraunMaryElizabeth    1   18\n",
            "59                 33YatesWilliam&33EdwardsElizabeth    7   37\n",
            "60                      33YatesWilliam&33HouseHannah    1   27\n",
            "61                       33YatesWilliam&33SaltPhoebe    3   15\n",
            "62                      33YatesWilliam&34ParkerSally   13  124\n",
            "63            33YatesWilliamNicholas&33HaysElizabeth    1   10\n",
            "64         33YatesWilliamThomas&33ShelhorseMaryPolly    2   28\n",
            "65                34BennettWilliamBudd&34YatesElllen    1   25\n",
            "66         34BronsonJamesRobert&35YatesAgnesMargaret    1   25\n",
            "67                 34GarrisonThomasAsa&34YatesLouisa    1   24\n",
            "68                         34HarperJames&34YatesMary    1   30\n",
            "69              34ShepherdJonathan&34YatesEmilyBetty    4   33\n",
            "70            34YatesDarlingBarber&34OliverMaryEmily    2   12\n",
            "71              34YatesIsaacPerry&34SearchingDiannah    1   18\n",
            "72                 34YatesJamesM&34NeffFannyJeanette    1   19\n",
            "73           34YatesJamesMcDaniel&34WinslettTerissaM    1   14\n",
            "74       34YatesJesseLazarus&35MyersHarrietElizabeth    1   20\n",
            "75                   34YatesJessee&34RobertsSarahAnn    1   14\n",
            "76                         34YatesJohn&34HollickLucy    1   20\n",
            "77                      34YatesJohnJ&34YatesAgnesAnn    1   11\n",
            "78                  34YatesJohnPeter&35DiggsSarahAnn    1   29\n",
            "79               34YatesSamuelFrancis&34BrownDelilah    1   21\n",
            "80               34YatesSamuelPorter&35BridgesMartha    3   15\n",
            "81               34YatesSoloman&34McCammackElizabeth    2   24\n",
            "82                         34YatesThomas&34EvansMary    2   15\n",
            "83                       34YatesWilliam&34PikeEsther    3   25\n",
            "84               34YatesWilliamBasil&34BullockMartha    2   20\n",
            "85         34YatesWilliamCharles&34McManusEmilyMilly    1   15\n",
            "86        34YatesWilliamPrice&34McKinneyElizabethAnn    2   12\n",
            "87          35ArvinWilliamHenry&36YatesMargaretEllen    1   20\n",
            "88             35LillardDavidSamuel&35YatesMaryEllen    1   20\n",
            "89        35RiceThomasJefferson&36YatesDavidIsabelle    1   15\n",
            "90              35YatesAllen&35LemastersRachelAgatha    1   11\n",
            "91                         35YatesAllen&36JetersMary    1   19\n",
            "92                35YatesAugustus&36PitcarinMargaret    1   18\n",
            "93                         35YatesEdward&36DrewAgnes    1   29\n",
            "94   35YatesEdwardSamuel&35KirkpatrickSarahElizabeth    2   22\n",
            "95            35YatesHarveyScott&35BeckmanGeorgiaAnn    1   19\n",
            "96                  35YatesJames&35AllcockEmmaAmelia    1   17\n",
            "97            35YatesJames&36SuydamIsobelleElizabeth    2   17\n",
            "98              35YatesJamesMatthew&36ChambersElvina    1   22\n",
            "99                    35YatesJohn&35GreenwoodSusanna    1   17\n",
            "100                   35YatesJohnG.&35WolfordMatilda    1   17\n",
            "101                 35YatesJohnH.&35RobertsElizabeth    1   17\n",
            "102             35YatesJohnMarion&35PrinceAgenaIrene    1   34\n",
            "103                     35YatesJohnWesley&35EvansAnn    4   17\n",
            "104    35YatesWilliamFrancis&35LandrumSarahCatherine    1   11\n",
            "105             35YatesWilliamHenry&35ForsterAnnMary    1   25\n",
            "106         35YatesWilliamJoseph&35HolsteadSarahJane    1   23\n",
            "107                35YatesWilliamM&35OsborneNancyAnn    1   15\n",
            "108                    36PadgettJohnA&36YatesMarthaJ    1   31\n",
            "109                    36PillingPeter&36YatesMaryAnn    1   31\n",
            "110             36YatesHarryGeorge&37SheldonGraceAmy    3   34\n",
            "111                  36YatesLafayette&36RoeMaryOlive    1   16\n",
            "112              36YatesWilliamRobert&42DavisMalissa    1   19\n",
            "113           37PascoGasperSamuel&37YatesMaryLucinda    1   41\n",
            "114        37YatesAnthony&37deCastroMagdalenaEugenie    1   20\n",
            "115                       42DuerdenJoseph&42YatesAnn    1   18\n",
            "116                      42HardestySearching&34Yates    1   43\n",
            "117                     42YatesJoseph&42MaghullHelen    2   66\n",
            "118              42YatesNathanJames&42SearchingStill    2   24\n",
            "119                  42YatesPhillip&42SearchingStill    5   27\n",
            "Data saved to /content/root_parent_master_processed.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B2**********CELL 4 ******************************************root_parent_master\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_root_counts(DNA_Study_Library, fq_limit=5):\n",
        "    # Initialize a dictionary to store the counts and sum of cMs of each root node-parent pair\n",
        "    root_count_dict = {}\n",
        "\n",
        "    # Populate the root_count_dict with the counts and sum of cMs of each root node-parent pair\n",
        "    for data in DNA_Study_Library.values():\n",
        "        first_pair = data['First Pair']\n",
        "        try:\n",
        "            cM_value = int(data['cM'])  # Ensure cM value is an integer\n",
        "        except ValueError:\n",
        "            print(f\"Invalid cM value for {first_pair}: {data['cM']}\")\n",
        "            continue\n",
        "\n",
        "        if first_pair not in root_count_dict:\n",
        "            root_count_dict[first_pair] = {\n",
        "                'count': 1,\n",
        "                'Sum cM': cM_value\n",
        "            }\n",
        "        else:\n",
        "            root_count_dict[first_pair]['count'] += 1\n",
        "            root_count_dict[first_pair]['Sum cM'] += cM_value\n",
        "\n",
        "    # Create the new data structure with the required columns\n",
        "    output_data = []\n",
        "\n",
        "    for first_pair, info in root_count_dict.items():\n",
        "        if info['count'] >= fq_limit:  # Apply the FQ limiter\n",
        "            QI = round(info['Sum cM'] / info['count']) if info['count'] != 0 else 0\n",
        "            output_data.append({\n",
        "                'Parents': 'Unsure Parents',\n",
        "                'FQ': info['count'],\n",
        "                'QI': QI,\n",
        "                'root_parent_master': first_pair,\n",
        "                '# of Nodes': 0,  # Placeholder for now\n",
        "                'Sum cM': info['Sum cM']\n",
        "            })\n",
        "\n",
        "    # Sort the data by 'root_parent_master' alphabetically\n",
        "    output_data.sort(key=lambda x: x['root_parent_master'])\n",
        "\n",
        "    # Assign node numbers\n",
        "    for idx, row in enumerate(output_data):\n",
        "        row['# of Nodes'] = idx + 1\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Function to print the data\n",
        "def print_root_counts(output_data, limit=None):\n",
        "    print(f\"{'Parents':<15}{'FQ':<5}{'QI':<5}{'root_parent_master':<45}{'# of Nodes':<14}{'Sum cM':<8}\")\n",
        "    for idx, row in enumerate(output_data):\n",
        "        if limit is not None and idx >= limit:  # Limit the number of records printed\n",
        "            break\n",
        "        print(f\"{row['Parents']:<15}{row['FQ']:<5}{row['QI']:<5}{row['root_parent_master']:<45}{row['# of Nodes']:<14}{row['Sum cM']:<8}\")\n",
        "\n",
        "# Calculate the root counts with FQ limit\n",
        "fq_limit = 1\n",
        "output_data = calculate_root_counts(DNA_Study_Library, fq_limit=fq_limit)\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_limit}\")\n",
        "\n",
        "# Print the report\n",
        "print_root_counts(output_data)\n",
        "\n",
        "# Save the output data to a root_parent_master dictionary\n",
        "root_parent_master = {row['root_parent_master']: row for row in output_data}\n",
        "\n",
        "# Function to save the output data to an Excel file\n",
        "def save_to_excel(output_data, filename):\n",
        "    df = pd.DataFrame(output_data)\n",
        "    df.to_excel(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Save the output data to an Excel file\n",
        "save_to_excel(output_data, '/content/root_parent_master.xlsx')\n",
        "\n",
        "# CELL 4 ******************************************root_parent_master\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgfFMjAT0IW9",
        "outputId": "8d0361bc-e7dc-4f9d-c8a8-db2ee444bf27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 1\n",
            "Parents        FQ   QI   root_parent_master                           # of Nodes    Sum cM  \n",
            "Unsure Parents 507  65   23YatesFrancis&23TichborneJane               1             33076   \n",
            "Unsure Parents 62   87   24YatesJohnThomas&24HatfieldeElizabeth       2             5380    \n",
            "Unsure Parents 4    17   26YatesJohn&27StrattonMary                   3             69      \n",
            "Unsure Parents 9    18   26YatesJohn&27StrattonUrslyMehitable         4             158     \n",
            "Unsure Parents 1    46   26YatesThomas&26PrattElizabeth               5             46      \n",
            "Unsure Parents 1    8    27StockettThomas&27WellsMary                 6             8       \n",
            "Unsure Parents 12   17   27YatesWilliam&27BondMargaretCatherine       7             207     \n",
            "Unsure Parents 1    26   28YatesHadney&28JamesonMary                  8             26      \n",
            "Unsure Parents 8    14   28YatesThomas&28DruryMonica                  9             110     \n",
            "Unsure Parents 1    23   28YatesWilliam&28PipkinMary                  10            23      \n",
            "Unsure Parents 4    23   29PhillipsWilliam&29YatesAnn                 11            92      \n",
            "Unsure Parents 5    15   29YatesJames&30McNayJane                     12            73      \n",
            "Unsure Parents 4    23   29YatesMartin&42Eliz                         13            92      \n",
            "Unsure Parents 4    16   29YatesRichard&42Mary                        14            66      \n",
            "Unsure Parents 5    13   29YatesThomas&42SearchingStill               15            63      \n",
            "Unsure Parents 159  92   30YatesBenjamin&42SearchingStill             16            14705   \n",
            "Unsure Parents 5    23   30YatesJohn&30BoswellAnn                     17            114     \n",
            "Unsure Parents 12   23   30YatesWilliam&30ThornburyAnne               18            277     \n",
            "Unsure Parents 1    36   31DillonJonathan&31YatesMary                 19            36      \n",
            "Unsure Parents 2    50   31HandJonathan&31YatesRebecca                20            100     \n",
            "Unsure Parents 6    19   31RoperJamesDavid&31YatesSarahAnn            21            114     \n",
            "Unsure Parents 24   676  31YatesAbraham&42SearchingStill              22            16236   \n",
            "Unsure Parents 16   18   31YatesJohn&31BarfieldElizabeth              23            288     \n",
            "Unsure Parents 1    24   31YatesThomas&32HaslamBetty                  24            24      \n",
            "Unsure Parents 1    29   32BaileyWilliam&32YatesRhoda                 25            29      \n",
            "Unsure Parents 3    18   32SwiftAbsalom&33YatesMaryElizabeth          26            55      \n",
            "Unsure Parents 1    27   32YatesAbraham&32HunterKitty                 27            27      \n",
            "Unsure Parents 1    16   32YatesChristopher&32GurrMaryAnn             28            16      \n",
            "Unsure Parents 2    31   32YatesHenry&32McManusNancy                  29            62      \n",
            "Unsure Parents 3    20   32YatesJacob&32VandenberghElizabeth          30            59      \n",
            "Unsure Parents 2    19   32YatesJames&33FlournoyMaryAnn               31            38      \n",
            "Unsure Parents 21   14   32YatesJames&33SanfordSarah                  32            304     \n",
            "Unsure Parents 3    26   32YatesJohnPryor&33MartinSarah               33            77      \n",
            "Unsure Parents 5    242  32YatesJoseph&32AtmarSarah                   34            1212    \n",
            "Unsure Parents 8    677  32YatesJoseph&32LeighMary                    35            5414    \n",
            "Unsure Parents 4    18   32YatesJoshuaHardy&32StewartMarthaAnn        36            73      \n",
            "Unsure Parents 2    17   32YatesLoyd&32BrasfieldMaryPolly             37            34      \n",
            "Unsure Parents 1    127  32YatesPeter&32ToofCatherine                 38            127     \n",
            "Unsure Parents 1    20   32YatesThomas&33CombsPhebey                  39            20      \n",
            "Unsure Parents 1    20   32YatesUriah&32OakesSarah                    40            20      \n",
            "Unsure Parents 1    21   32YatesWilliam&32BoothAnamariaElizabeth      41            21      \n",
            "Unsure Parents 3    9    32YatesWilliam&33NeedhamMary                 42            27      \n",
            "Unsure Parents 1    23   33ColliverJames&34YatesNancy                 43            23      \n",
            "Unsure Parents 2    22   33CowdenWilliam&33YatesCatherine             44            45      \n",
            "Unsure Parents 1    21   33FreebodyWilliamKnapp&33YatesAnne           45            21      \n",
            "Unsure Parents 2    185  33GirtainJonathon&33BarberRebecca            46            370     \n",
            "Unsure Parents 1    24   33KingJohn&34YatesAnna                       47            24      \n",
            "Unsure Parents 1    9    33ShawEbenezer&33YatesRebbecca               48            9       \n",
            "Unsure Parents 2    14   33TolerElishaPhilpot&33YatesElizabeth        49            29      \n",
            "Unsure Parents 1    21   33YatesAbsolam&34HaleyCathrine               50            21      \n",
            "Unsure Parents 1    25   33YatesGeorgeWashington&42SearchingStill     51            25      \n",
            "Unsure Parents 1    9    33YatesJamesEdward&33BeardAnn                52            9       \n",
            "Unsure Parents 2    18   33YatesJohn&42SearchingStill                 53            37      \n",
            "Unsure Parents 1    28   33YatesJohnB&34Elizabeth                     54            28      \n",
            "Unsure Parents 1    17   33YatesMalcolm&42SearchingStill              55            17      \n",
            "Unsure Parents 1    21   33YatesSamuel&34SwisherMargaret              56            21      \n",
            "Unsure Parents 1    233  33YatesThomas&33JohnsonMary                  57            233     \n",
            "Unsure Parents 1    18   33YatesThomas&33PaulAbigail                  58            18      \n",
            "Unsure Parents 1    18   33YatesThomas&34CraunMaryElizabeth           59            18      \n",
            "Unsure Parents 7    37   33YatesWilliam&33EdwardsElizabeth            60            257     \n",
            "Unsure Parents 1    27   33YatesWilliam&33HouseHannah                 61            27      \n",
            "Unsure Parents 3    15   33YatesWilliam&33SaltPhoebe                  62            45      \n",
            "Unsure Parents 13   124  33YatesWilliam&34ParkerSally                 63            1611    \n",
            "Unsure Parents 1    10   33YatesWilliamNicholas&33HaysElizabeth       64            10      \n",
            "Unsure Parents 2    28   33YatesWilliamThomas&33ShelhorseMaryPolly    65            57      \n",
            "Unsure Parents 1    25   34BennettWilliamBudd&34YatesElllen           66            25      \n",
            "Unsure Parents 1    25   34BronsonJamesRobert&35YatesAgnesMargaret    67            25      \n",
            "Unsure Parents 1    24   34GarrisonThomasAsa&34YatesLouisa            68            24      \n",
            "Unsure Parents 1    30   34HarperJames&34YatesMary                    69            30      \n",
            "Unsure Parents 4    33   34ShepherdJonathan&34YatesEmilyBetty         70            132     \n",
            "Unsure Parents 2    12   34YatesDarlingBarber&34OliverMaryEmily       71            23      \n",
            "Unsure Parents 1    18   34YatesIsaacPerry&34SearchingDiannah         72            18      \n",
            "Unsure Parents 1    19   34YatesJamesM&34NeffFannyJeanette            73            19      \n",
            "Unsure Parents 1    14   34YatesJamesMcDaniel&34WinslettTerissaM      74            14      \n",
            "Unsure Parents 1    20   34YatesJesseLazarus&35MyersHarrietElizabeth  75            20      \n",
            "Unsure Parents 1    14   34YatesJessee&34RobertsSarahAnn              76            14      \n",
            "Unsure Parents 1    20   34YatesJohn&34HollickLucy                    77            20      \n",
            "Unsure Parents 1    11   34YatesJohnJ&34YatesAgnesAnn                 78            11      \n",
            "Unsure Parents 1    29   34YatesJohnPeter&35DiggsSarahAnn             79            29      \n",
            "Unsure Parents 1    21   34YatesSamuelFrancis&34BrownDelilah          80            21      \n",
            "Unsure Parents 3    15   34YatesSamuelPorter&35BridgesMartha          81            45      \n",
            "Unsure Parents 2    24   34YatesSoloman&34McCammackElizabeth          82            48      \n",
            "Unsure Parents 2    15   34YatesThomas&34EvansMary                    83            30      \n",
            "Unsure Parents 3    25   34YatesWilliam&34PikeEsther                  84            75      \n",
            "Unsure Parents 2    20   34YatesWilliamBasil&34BullockMartha          85            41      \n",
            "Unsure Parents 1    15   34YatesWilliamCharles&34McManusEmilyMilly    86            15      \n",
            "Unsure Parents 2    12   34YatesWilliamPrice&34McKinneyElizabethAnn   87            24      \n",
            "Unsure Parents 1    20   35ArvinWilliamHenry&36YatesMargaretEllen     88            20      \n",
            "Unsure Parents 1    20   35LillardDavidSamuel&35YatesMaryEllen        89            20      \n",
            "Unsure Parents 1    15   35RiceThomasJefferson&36YatesDavidIsabelle   90            15      \n",
            "Unsure Parents 1    11   35YatesAllen&35LemastersRachelAgatha         91            11      \n",
            "Unsure Parents 1    19   35YatesAllen&36JetersMary                    92            19      \n",
            "Unsure Parents 1    18   35YatesAugustus&36PitcarinMargaret           93            18      \n",
            "Unsure Parents 1    29   35YatesEdward&36DrewAgnes                    94            29      \n",
            "Unsure Parents 2    22   35YatesEdwardSamuel&35KirkpatrickSarahElizabeth95            43      \n",
            "Unsure Parents 1    19   35YatesHarveyScott&35BeckmanGeorgiaAnn       96            19      \n",
            "Unsure Parents 1    17   35YatesJames&35AllcockEmmaAmelia             97            17      \n",
            "Unsure Parents 2    17   35YatesJames&36SuydamIsobelleElizabeth       98            34      \n",
            "Unsure Parents 1    22   35YatesJamesMatthew&36ChambersElvina         99            22      \n",
            "Unsure Parents 1    17   35YatesJohn&35GreenwoodSusanna               100           17      \n",
            "Unsure Parents 1    17   35YatesJohnG.&35WolfordMatilda               101           17      \n",
            "Unsure Parents 1    17   35YatesJohnH.&35RobertsElizabeth             102           17      \n",
            "Unsure Parents 1    34   35YatesJohnMarion&35PrinceAgenaIrene         103           34      \n",
            "Unsure Parents 4    17   35YatesJohnWesley&35EvansAnn                 104           67      \n",
            "Unsure Parents 1    11   35YatesWilliamFrancis&35LandrumSarahCatherine105           11      \n",
            "Unsure Parents 1    25   35YatesWilliamHenry&35ForsterAnnMary         106           25      \n",
            "Unsure Parents 1    23   35YatesWilliamJoseph&35HolsteadSarahJane     107           23      \n",
            "Unsure Parents 1    15   35YatesWilliamM&35OsborneNancyAnn            108           15      \n",
            "Unsure Parents 1    31   36PadgettJohnA&36YatesMarthaJ                109           31      \n",
            "Unsure Parents 1    31   36PillingPeter&36YatesMaryAnn                110           31      \n",
            "Unsure Parents 3    34   36YatesHarryGeorge&37SheldonGraceAmy         111           101     \n",
            "Unsure Parents 1    16   36YatesLafayette&36RoeMaryOlive              112           16      \n",
            "Unsure Parents 1    19   36YatesWilliamRobert&42DavisMalissa          113           19      \n",
            "Unsure Parents 1    41   37PascoGasperSamuel&37YatesMaryLucinda       114           41      \n",
            "Unsure Parents 1    20   37YatesAnthony&37deCastroMagdalenaEugenie    115           20      \n",
            "Unsure Parents 1    18   42DuerdenJoseph&42YatesAnn                   116           18      \n",
            "Unsure Parents 1    43   42HardestySearching&34Yates                  117           43      \n",
            "Unsure Parents 2    66   42YatesJoseph&42MaghullHelen                 118           133     \n",
            "Unsure Parents 2    24   42YatesNathanJames&42SearchingStill          119           48      \n",
            "Unsure Parents 5    27   42YatesPhillip&42SearchingStill              120           133     \n",
            "Data saved to /content/root_parent_master.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# C****CELL 2***** 2016hrs*************keep******************creates***********chart_black_separators.xlsx\n",
        "\n",
        "# Assume root_parent_master is already created and available from the previous script\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook, drawing\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "\n",
        "# Preloaded dictionary\n",
        "# root_parent_master = {row['root_parent_master']: row for row in output_data}\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "\n",
        "fq_threshold = 2  # Define the fq_threshold value\n",
        "\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(filepath, root_parent_master):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Fill NaN values in 'Ancestors' column with empty strings and ensure all values are strings\n",
        "    df['Ancestors'] = df['Ancestors'].fillna('').astype(str)\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs and root_parent_master dictionary\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in root_parent_master else '', axis=1)\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    grouped_df['Prefix'] = grouped_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    grouped_df['Prefix'] = grouped_df['Prefix'].astype(str)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to 10\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse', 'Prefix']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= fq_threshold].copy()\n",
        "\n",
        "    return final_results_df\n",
        "\n",
        "# def sort_within_segment(group):\n",
        "#     \"\"\"Sort the group by the FQ in descending order within each segment.\"\"\"\n",
        "#     return group.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    # sorted_df = df.groupby('Prefix', group_keys=False).apply(sort_within_segment).reset_index(drop=True)\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(lambda x: x).reset_index(drop=True)\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "def print_data_to_console(sorted_df, root_parent_master):\n",
        "    \"\"\"Print the sorted data to the console, inserting rows from the preloaded dictionary.\"\"\"\n",
        "    headers = ['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']\n",
        "    print(f\"{'Root':<5}{'Parents':<20}{'FQ':<5}{'QI':<5}{'Offspring & Spouse':<45}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    last_parent = None\n",
        "    for _, row in sorted_df.iterrows():\n",
        "        root_key = row['Parents']\n",
        "        if root_key in root_parent_master:\n",
        "            root_row = root_parent_master[root_key]\n",
        "            if last_parent != 'Unsure Parents':\n",
        "                print(f\"{'Yes':<5}{'Unsure Parents':<20}{root_row['FQ']:<5}{root_row['QI']:<5}{root_row['root_parent_master']:<45}\")\n",
        "                last_parent = 'Unsure Parents'\n",
        "        if last_parent == row['Parents']:\n",
        "            row['Parents'] = ''\n",
        "        else:\n",
        "            last_parent = row['Parents']\n",
        "        print(f\"{row['Root']:<5}{row['Parents']:<20}{row['FQ']:<5}{row['QI']:<5}{row['Offspring & Spouse']:<45}\")\n",
        "\n",
        "def save_data_to_excel(sorted_df, root_parent_master):\n",
        "    \"\"\"Save the sorted data to an Excel file with styled segments.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "\n",
        "    # Initialize variables to track last seen parent initial and generation\n",
        "    last_parent_initial = None\n",
        "    parent_seen = set()\n",
        "\n",
        "    # Adding the rows and assigning interval offset for staggered node-parents\n",
        "    last_parent = None\n",
        "    for r in dataframe_to_rows(sorted_df, index=False, header=False):\n",
        "        current_parent = r[1]\n",
        "\n",
        "        # Calculate interval offset\n",
        "        if current_parent not in parent_seen:\n",
        "            parent_seen.add(current_parent)\n",
        "            current_parent_prefix = current_parent[:2] if current_parent else ''\n",
        "            if last_parent_initial is not None and current_parent_prefix != last_parent_initial:\n",
        "                # Insert a black row at the end of the previous segment\n",
        "                ws.append([''] * len(headers))\n",
        "                black_row_index = ws.max_row\n",
        "                ws.row_dimensions[black_row_index].height = 5\n",
        "                for cell in ws[black_row_index]:\n",
        "                    cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "            last_parent_initial = current_parent_prefix\n",
        "            if current_parent in root_parent_master:\n",
        "                root_row = root_parent_master[current_parent]\n",
        "                if last_parent != 'Unsure Parents':\n",
        "                    ws.append(['Yes', 'Unsure Parents', root_row['FQ'], root_row['QI'], root_row['root_parent_master']])\n",
        "                    last_parent = 'Unsure Parents'\n",
        "        if last_parent == current_parent:\n",
        "            r[1] = ''  # Clear parent name for repeated entries within the same generation\n",
        "        else:\n",
        "            last_parent = current_parent\n",
        "        ws.append(r)\n",
        "\n",
        "    # Add a black row at the end of the last segment\n",
        "    ws.append([''] * len(headers))\n",
        "    black_row_index = ws.max_row\n",
        "    ws.row_dimensions[black_row_index].height = 5\n",
        "    for cell in ws[black_row_index]:\n",
        "        cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    right_alignment = Alignment(horizontal=\"right\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "\n",
        "        row[1].alignment = right_alignment  # Right-align Parents column\n",
        "        row[4].alignment = left_alignment  # Left-align Offspring & Spouse column\n",
        "\n",
        "    # Replace 'Yes' with an arrow image\n",
        "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=1):\n",
        "        for cell in row:\n",
        "            if cell.value == 'Yes':\n",
        "                if ws.cell(row=cell.row, column=2).value == 'Unsure Parents':  # Check if the Parents column is 'Unsure Parents'\n",
        "                    img = drawing.image.Image('/content/arrow.png')\n",
        "                    img.width, img.height = (25, 25)  # Increase size to 25x25\n",
        "                    img.anchor = cell.coordinate\n",
        "                    ws.add_image(img)\n",
        "                cell.value = ''  # Clear the cell value\n",
        "\n",
        "    # Save the workbook\n",
        "    output_file = '/content/parents&offspring_chart.xlsx'\n",
        "    wb.save(output_file)\n",
        "    print(f\"Results successfully saved with styled colors to {output_file}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Replace with the correct path\n",
        "\n",
        "# Process the data and save it in a dictionary\n",
        "processed_data_dict = process_data(input_file_path, root_parent_master).to_dict(orient='list')\n",
        "\n",
        "# Sort all segments within the data\n",
        "sorted_data = sort_all_segments(processed_data_dict)\n",
        "\n",
        "# Print the sorted data to the console, inserting rows from the preloaded dictionary\n",
        "#print_data_to_console(sorted_data, root_parent_master)\n",
        "\n",
        "# Save the sorted data to an Excel file with formatting\n",
        "save_data_to_excel(sorted_data, root_parent_master)\n",
        "\n",
        "# CELL 2***** 2016hrs*************keep******************creates***********chart_black_separators.xlsx\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "begbE-62zZzh",
        "outputId": "1207d4ef-4fb8-4a90-a1f5-65c488843791"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 2\n",
            "Results successfully saved with styled colors to /content/parents&offspring_chart.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D****************CELL 3 *****************************************  Good Trailing Descendants\n",
        "\n",
        "fq_threshold = 21  # Define the fq_threshold value\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook, drawing\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill, Font\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(filepath):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Fill NaN values in 'Ancestors' column with empty strings and ensure all values are strings\n",
        "    df['Ancestors'] = df['Ancestors'].fillna('').astype(str)\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in first_parents else '', axis=1)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to 8\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= 8].copy()\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    final_results_df['Prefix'] = final_results_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    final_results_df['Prefix'] = final_results_df['Prefix'].astype(str)\n",
        "\n",
        "    return final_results_df, len(df)\n",
        "\n",
        "def sort_within_segment(group):\n",
        "    \"\"\"Sort the group by the FQ in descending order within each segment.\"\"\"\n",
        "    return group.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(sort_within_segment).reset_index(drop=True)\n",
        "\n",
        "    # Drop the Prefix column as it is no longer needed\n",
        "    sorted_df = sorted_df.drop(columns=['Prefix'])\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "def assemble_descendants(df, total_records):\n",
        "    \"\"\"Assemble descendants starting from root parents and include FQ, QI, and % Total.\"\"\"\n",
        "    root_parents = df[df['Root'] == 'Yes']['Parents'].unique()\n",
        "    assembled_data = []\n",
        "\n",
        "    for root in root_parents:\n",
        "        current_generation = [(root, 0)]\n",
        "        assembled_data.append({\n",
        "            'Root': 'Yes',\n",
        "            'Parents and Trailing Descendants': root,\n",
        "            'FQ': df[df['Parents'] == root]['FQ'].values[0],\n",
        "            'QI': df[df['Parents'] == root]['QI'].values[0],\n",
        "            '% Total': f\"{round(df[df['Parents'] == root]['FQ'].values[0] / total_records * 100)}%\"\n",
        "        })\n",
        "        while current_generation:\n",
        "            next_generation = []\n",
        "            for parent, depth in current_generation:\n",
        "                children = df[df['Parents'] == parent]['Offspring & Spouse'].tolist()\n",
        "                for i, child in enumerate(children):\n",
        "                    fq = df[(df['Parents'] == parent) & (df['Offspring & Spouse'] == child)]['FQ'].values[0]\n",
        "                    qi = df[(df['Parents'] == parent) & (df['Offspring & Spouse'] == child)]['QI'].values[0]\n",
        "                    percent_total = f\"{round(fq / total_records * 100)}%\"\n",
        "                    descendant = f\"{'  ' * (depth + 1)} {child}\"\n",
        "                    assembled_data.append({\n",
        "                        'Root': '',\n",
        "                        'Parents and Trailing Descendants': descendant,\n",
        "                        'FQ': fq,\n",
        "                        'QI': qi,\n",
        "                        '% Total': percent_total\n",
        "                    })\n",
        "                    next_generation.append((child, depth + 1))\n",
        "            current_generation = next_generation\n",
        "\n",
        "    return pd.DataFrame(assembled_data)\n",
        "\n",
        "def save_to_excel_with_styles(df, output_path):\n",
        "    \"\"\"Save the assembled data to an Excel file with the desired format and styles.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents and Trailing Descendants', 'FQ', 'QI', '% Total']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    header_font = Font(bold=True)\n",
        "    thick = Side(border_style=\"thick\", color=\"000000\")\n",
        "\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "        cell.font = header_font\n",
        "        cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)\n",
        "\n",
        "    # Initialize variables to track last seen parent initial and generation\n",
        "    parent_seen = set()\n",
        "    previous_root = None\n",
        "\n",
        "    for r in dataframe_to_rows(df, index=False, header=False):\n",
        "        current_parent = r[1].strip().split('')[0]\n",
        "\n",
        "        # Insert a black row at the end of each root parent's segment\n",
        "        if r[0] == 'Yes' and previous_root is not None:\n",
        "            ws.append([''] * len(headers))\n",
        "            black_row_index = ws.max_row\n",
        "            ws.row_dimensions[black_row_index].height = 5\n",
        "            for cell in ws[black_row_index]:\n",
        "                cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "        # Add the row to the worksheet\n",
        "        ws.append(r)\n",
        "        if r[0] == 'Yes':\n",
        "            parent_seen.add(current_parent)\n",
        "            previous_root = current_parent\n",
        "\n",
        "    # Add a black row at the end of the last segment\n",
        "    ws.append([''] * len(headers))\n",
        "    black_row_index = ws.max_row\n",
        "    ws.row_dimensions[black_row_index].height = 5\n",
        "    for cell in ws[black_row_index]:\n",
        "        cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "    # Add red arrow images for root nodes\n",
        "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=1):\n",
        "        for cell in row:\n",
        "            if cell.value == 'Yes':\n",
        "                img = drawing.image.Image('/content/arrow.png')\n",
        "                img.width, img.height = (25, 25)\n",
        "                img.anchor = cell.coordinate\n",
        "                ws.add_image(img)\n",
        "                cell.value = ''  # Clear the cell value\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    thick = Side(border_style=\"thick\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    col_indices = {'FQ': 3, 'QI': 4, '% Total': 5}\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "            if cell.column in col_indices.values():\n",
        "                cell.border = Border(top=thin, left=thick, right=thick, bottom=thin)\n",
        "                cell.alignment = center_alignment\n",
        "            elif cell.column == 2:\n",
        "                cell.alignment = left_alignment\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(output_path)\n",
        "    print(f\"Results successfully saved to {output_path}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Replace with the correct path\n",
        "\n",
        "# Process the data and save it in a dictionary\n",
        "processed_data, total_records = process_data(input_file_path)\n",
        "\n",
        "# Sort all segments within the data\n",
        "sorted_data = sort_all_segments(processed_data.to_dict(orient='list'))\n",
        "\n",
        "# Assemble descendants into a DataFrame\n",
        "descendants_df = assemble_descendants(pd.DataFrame(sorted_data), total_records)\n",
        "\n",
        "# Save the assembled data to an Excel file with styles\n",
        "output_file_path = '/content/trailing_descendants.xlsx'\n",
        "save_to_excel_with_styles(descendants_df, output_file_path)\n",
        "\n",
        "# Print records tagged and filtered by NPFX\n",
        "print(f\"Records tagged and filtered by NPFX: {total_records}\")\n",
        "\n",
        "\n",
        "# CELL 3 *****************************************  Good Trailing Descendants"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArQA8vqyLHxi",
        "outputId": "07150e7c-3623-4a01-9c15-112b3dfb9057"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 21\n",
            "Results successfully saved to /content/trailing_descendants.xlsx\n",
            "Records tagged and filtered by NPFX: 1045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 1714hrs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jntW08F97Pm",
        "outputId": "afb9c05d-3f17-457a-de12-990a73d3e418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 21\n",
            "Results successfully saved to /content/trailing_descendants.xlsx\n",
            "Records tagged and filtered by NPFX: 1045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# D2*********Cell 5******1556 hrs**********summary of roots\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Font, Alignment, Border, Side\n",
        "\n",
        "fq_threshold = 21  # Define the fq_threshold value\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(filepath, fq_threshold):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Fill NaN values in 'Ancestors' column with empty strings and ensure all values are strings\n",
        "    df['Ancestors'] = df['Ancestors'].fillna('').astype(str)\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Calculate Sum cM as the product of FQ and QI\n",
        "    grouped_df['Sum cM'] = grouped_df['FQ'] * grouped_df['QI']\n",
        "\n",
        "    # Mark root nodes based on first parent pairs\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in first_parents else '', axis=1)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to the threshold\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Sum cM']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= fq_threshold].copy()\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    final_results_df['Prefix'] = final_results_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    final_results_df['Prefix'] = final_results_df['Prefix'].astype(str)\n",
        "\n",
        "    return final_results_df\n",
        "\n",
        "def sort_within_segment(group):\n",
        "    \"\"\"Sort the group by the FQ in descending order within each segment.\"\"\"\n",
        "    return group.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(sort_within_segment).reset_index(drop=True)\n",
        "\n",
        "    # Drop the Prefix column as it is no longer needed\n",
        "    sorted_df = sorted_df.drop(columns=['Prefix'])\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "def visualize_data(df):\n",
        "    \"\"\"Visualize the sorted data and save it to an Excel file.\"\"\"\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    ws.title = \"Sorted Data\"\n",
        "\n",
        "    # Filter DataFrame to include only rows marked as 'Root'\n",
        "    root_df = df[df['Root'] == 'Yes'].copy()\n",
        "\n",
        "    # Replace 'Yes' with a count of the rows\n",
        "    root_df['Root'] = range(1, len(root_df) + 1)\n",
        "\n",
        "    # Remove duplicates\n",
        "    root_df = root_df.drop_duplicates(subset=['Parents'])\n",
        "\n",
        "    # Set font to size 14\n",
        "    font = Font(size=14)\n",
        "    header_font = Font(size=14, bold=True)\n",
        "    alignment = Alignment(horizontal='center', vertical='center')\n",
        "    thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin'))\n",
        "\n",
        "    # Add extra header rows\n",
        "    ws['B1'] = \"The Frequency Quotient threshold is set to:\"\n",
        "    ws['C1'] = fq_threshold\n",
        "\n",
        "    for cell in ['B1', 'C1']:\n",
        "        ws[cell].font = font\n",
        "\n",
        "    # Add headers with specified font\n",
        "    headers = ['#', 'Parents', 'FQ', '%', 'QI']\n",
        "    for col_num, header in enumerate(headers, 1):\n",
        "        cell = ws.cell(row=5, column=col_num, value=header)\n",
        "        cell.font = header_font\n",
        "        cell.alignment = alignment\n",
        "        cell.border = thin_border\n",
        "\n",
        "    # Calculate percentage column\n",
        "    root_df['%'] = (root_df['FQ'] / root_df['FQ'].sum()).apply(lambda x: f\"{x:.0%}\")\n",
        "\n",
        "    # Add data rows with specified font\n",
        "    for row_idx, row in enumerate(dataframe_to_rows(root_df[['Root', 'Parents', 'FQ', '%', 'QI']], index=False, header=False), start=6):\n",
        "        for col_idx, value in enumerate(row, 1):\n",
        "            cell = ws.cell(row=row_idx, column=col_idx, value=value)\n",
        "            cell.font = font\n",
        "            cell.alignment = alignment\n",
        "            cell.border = thin_border\n",
        "\n",
        "    # Auto-fit column width\n",
        "    for column in ws.columns:\n",
        "        max_length = 0\n",
        "        column = list(column)\n",
        "        for cell in column:\n",
        "            try:\n",
        "                if len(str(cell.value)) > max_length:\n",
        "                    max_length = len(cell.value)\n",
        "            except:\n",
        "                pass\n",
        "        adjusted_width = (max_length + 2)\n",
        "        ws.column_dimensions[column[0].column_letter].width = adjusted_width\n",
        "\n",
        "    # Calculate total Sum cM for root parent nodes\n",
        "    total_sum_cm = root_df['Sum cM'].sum()\n",
        "    total_fq = root_df['FQ'].sum()\n",
        "    group_qi = round(total_sum_cm / total_fq) if total_fq != 0 else 0\n",
        "\n",
        "    # Add summary rows\n",
        "    summary_start_row = len(root_df) + 8\n",
        "    summary_data = [\n",
        "        (\"Summation:\", \"\"),\n",
        "        (\"Total lines in this subset of DNA lines\", total_fq),\n",
        "        (\"Percentage of all DNA lines in this subset\", \"83%\"),\n",
        "        (\"Average cM of DNA matches in this subset\", group_qi)\n",
        "    ]\n",
        "\n",
        "    for i, (label, value) in enumerate(summary_data, start=summary_start_row):\n",
        "        ws.cell(row=i, column=2, value=label).font = header_font if i == summary_start_row else font\n",
        "        ws.cell(row=i, column=3, value=value).font = header_font if i == summary_start_row else font\n",
        "        ws.cell(row=i, column=3).alignment = alignment\n",
        "\n",
        "    # Add border to summary section\n",
        "    for row in range(summary_start_row, summary_start_row + len(summary_data)):\n",
        "        for col in range(2, 4):\n",
        "            ws.cell(row=row, column=col).border = thin_border\n",
        "\n",
        "    # Save the workbook\n",
        "    output_path = '/content/summary_of_roots.xlsx'  # Replace with the correct path if necessary\n",
        "    wb.save(output_path)\n",
        "    print(f\"Data saved to {output_path}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Replace with the correct path\n",
        "\n",
        "# Initialize dictionaries to store threshold and qualified rows\n",
        "equaltomax = {'fq_threshold': fq_threshold}\n",
        "buildingroots = {}\n",
        "seen_parents = set()\n",
        "\n",
        "# Process the data and save it in a dictionary\n",
        "processed_data_dict = process_data(input_file_path, fq_threshold).to_dict(orient='list')\n",
        "\n",
        "# Sort all segments within the data\n",
        "sorted_data = sort_all_segments(processed_data_dict)\n",
        "\n",
        "# Visualize the sorted data\n",
        "visualize_data(sorted_data)\n",
        "\n",
        "# Save rows that meet the qualification to the buildingroots dictionary\n",
        "for index, row in sorted_data.iterrows():\n",
        "    if row['Root'] == 'Yes' and row['Parents'] not in seen_parents:\n",
        "        buildingroots[index] = row.to_dict()\n",
        "        seen_parents.add(row['Parents'])\n",
        "\n",
        "# Print the contents of the buildingroots dictionary to the console\n",
        "print(\"Buildingroots dictionary contents:\")\n",
        "\n",
        "# Create headers\n",
        "headers = ['Row', 'Root', 'Parents', 'FQ', 'QI', 'Sum cM']\n",
        "print(f\"{headers[0]:<5} {headers[1]:<5} {headers[2]:<45} {headers[3]:<5} {headers[4]:<5} {headers[5]:<7}\")\n",
        "\n",
        "# Print the data rows\n",
        "for i, (key, value) in enumerate(buildingroots.items(), start=1):\n",
        "    print(f\"{i:<5} {value['Root']:<5} {value['Parents']:<45} {value['FQ']:<5} {value['QI']:<5} {value['Sum cM']:<7}\")\n",
        "\n",
        "# Calculate total Sum cM and total FQ for root parent nodes\n",
        "root_df = sorted_data[sorted_data['Root'] == 'Yes']\n",
        "total_sum_cm = root_df['Sum cM'].sum()\n",
        "total_fq = root_df['FQ'].sum()\n",
        "group_qi = round(total_sum_cm / total_fq) if total_fq != 0 else 0\n",
        "\n",
        "# Print the total Sum cM, total FQ, and group QI\n",
        "print(f\"\\nTotal Sum cM: {total_sum_cm}\")\n",
        "print(f\"Total FQ: {total_fq}\")\n",
        "print(f\"Group QI: {group_qi}\")\n",
        "\n",
        "# Cell 5******1556 hrs**********summary of roots\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6iPQ08twM0v",
        "outputId": "2e5f4464-567b-497a-dec8-cbd13e7ea99e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 21\n",
            "Data saved to /content/summary_of_roots.xlsx\n",
            "Buildingroots dictionary contents:\n",
            "Row   Root  Parents                                       FQ    QI    Sum cM \n",
            "1     Yes   23YatesFrancis&23TichborneJane                506   65    32890  \n",
            "2     Yes   24YatesJohnThomas&24HatfieldeElizabeth        38    131   4978   \n",
            "3     Yes   30YatesBenjamin&42SearchingStill              159   92    14628  \n",
            "\n",
            "Total Sum cM: 52864\n",
            "Total FQ: 726\n",
            "Group QI: 73\n"
          ]
        }
      ]
    }
  ]
}