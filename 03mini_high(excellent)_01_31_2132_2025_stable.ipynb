{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtevVGKN/cSOzleou9JErh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/03mini_high(excellent)_01_31_2132_2025_stable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "ff8767a8-bd75-4d41-fdcf-b0740bead432",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 03mini-high_01_31_2029_2025_stable\n",
        "\n",
        "# Standard Libraries\n",
        "import csv\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# GEDCOM Parsing\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Alignment\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts first and last name from a GEDCOM record.\n",
        "    Handles missing or malformed names gracefully.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    if name_start == 5 or name_end == -1:  # Meaning '1 NAME ' was not found\n",
        "        return \"UnknownName\"\n",
        "    name = record[name_start:name_end].strip()\n",
        "    # Handle cases where no '/' is present in the name\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")  # Take first 10 characters as default name\n",
        "    # Extract first and last name\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]  # first 10 chars\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}  # Global dictionary to hold name->ID mapping\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # we’ll modify name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0  # Count YDNA occurrences\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1  # YDNA found\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "        print(f'Records with YDNA information: {ydna_count}')\n",
        "        print(f'Autosomal matches: {autosomal_count}')\n",
        "\n",
        "        # First-level filter: only those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Optional second-level filter from an Excel file\n",
        "        manual_filter_activated = True  # or False\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "                self.filter_pool = [\n",
        "                    dataset for dataset in self.filter_pool\n",
        "                    if dataset.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "    # Automatically select the first GEDCOM file found.\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "################################################################################\n",
        "#          Execute GEDCOM Parsing & Build Our Filter Pool                      #\n",
        "################################################################################\n",
        "gedcom_file_path = select_gedcom_file()\n",
        "if gedcom_file_path:\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "    # Gather individuals (last_name, individual_id) from the filter pool\n",
        "    individuals = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    ################################################################################\n",
        "    # Function: Extract ID from GEDCOM Record\n",
        "    ################################################################################\n",
        "    def extract_id(record):\n",
        "        \"\"\"\n",
        "        Extracts the ID from a GEDCOM record.\n",
        "        A valid ID is enclosed within '@' symbols.\n",
        "        \"\"\"\n",
        "        id_start = record.find('@') + 1\n",
        "        id_end = record.find('@', id_start)\n",
        "        if id_start == 0 or id_end == -1:\n",
        "            return \"UnknownID\"\n",
        "        return record[id_start:id_end].strip()\n",
        "\n",
        "    # Read the GEDCOM file as raw text and split into records\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "else:\n",
        "    print(\"No GEDCOM file selected; exiting.\")\n",
        "    raise SystemExit\n",
        "\n",
        "################################################################################\n",
        "#        Functions to Traverse & Score Ancestors, Build Data for DataFrame     #\n",
        "################################################################################\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # We'll update anchor_gen1 if found\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    # Build generation_table and visited_pairs\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "\n",
        "    # Gather more info from the dataset\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "\n",
        "    # Build ancestral line (exclude anchor_gen1 itself)\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\")\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "#         Build Rows for DataFrame from the Filter Pool                        #\n",
        "################################################################################\n",
        "combined_df_rows = []\n",
        "for dataset in gedcom_instance.filter_pool:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(\n",
        "        individual_id, gedcom_instance, records\n",
        "    )\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    individual_name = extract_name(records.get(individual_id, \"\"))\n",
        "    combined_df_rows.append(\n",
        "        [individual_id, sort, individual_name, cm, filtered_ancestral_line_str]\n",
        "    )\n",
        "\n",
        "################################################################################\n",
        "#       NO NEED TO MODIFY ABOVE THIS SECTION 28-1-2025                         #\n",
        "################################################################################\n",
        "\n",
        "# Create and Populate Main DataFrame (combined_df) using all desired columns\n",
        "columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "print(\"\\nDEBUG: 481-Initial DataFrame created from combined_df_rows\")\n",
        "print(\"Columns:\", combined_df.columns.tolist())\n",
        "print(combined_df.head(5))\n",
        "\n",
        "# Initialize the value_store dictionary (if needed)\n",
        "value_store = {}\n",
        "for _, row in combined_df.iterrows():\n",
        "    value_store[row[\"ID#\"]] = {\n",
        "        \"Match to\": row[\"Match to\"],\n",
        "        \"Name\": row[\"Name\"],\n",
        "        \"cM\": row[\"cM\"],\n",
        "        \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "        \"Value\": None  # Placeholder for 'Value'\n",
        "    }\n",
        "\n",
        "################################################################################\n",
        "#       Remove miscellaneous Distant ancestors (combined_df)\n",
        "################################################################################\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "    prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "print(\"\\nDEBUG: 517-DataFrame after removing prefix\")\n",
        "print(\"Columns:\", combined_df.columns.tolist())\n",
        "print(combined_df.head(5))\n",
        "\n",
        "# Order and clean up columns (do not drop 'Match to' and 'Name')\n",
        "ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = combined_df[ordered_columns]\n",
        "combined_df.index += 1\n",
        "combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "print(\"\\nDEBUG: 530-DataFrame after reordering and sorting\")\n",
        "print(\"Columns:\", combined_df.columns.tolist())\n",
        "print(combined_df.head(5))\n",
        "\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "\n",
        "################################################################################\n",
        "# Ensure Required Columns Exist in Memory (for processing)\n",
        "################################################################################\n",
        "expected_columns = [\"ID#\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "for col in expected_columns:\n",
        "    if col not in combined_df.columns:\n",
        "        if col == \"Value\":\n",
        "            combined_df[col] = 0\n",
        "        else:\n",
        "            combined_df[col] = \"Unknown\"\n",
        "print(\"✅ Required columns for processing are now present in combined_df.\")\n",
        "\n",
        "################################################################################\n",
        "#       Segmentation and Frequency Analysis\n",
        "################################################################################\n",
        "def parse_line_to_pairs(line, delimiter=\"~~~\"):\n",
        "    return line.strip().split(delimiter) if pd.notna(line) else []\n",
        "\n",
        "def identify_all_shared_segments(df, ancestral_col, min_shared=2, min_size=2):\n",
        "    segment_counts = defaultdict(int)\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.isna(row[ancestral_col]):\n",
        "            continue\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "        for size in range(min_size, len(lines) + 1):\n",
        "            for subset in combinations(sorted(lines), size):\n",
        "                segment_counts[subset] += 1\n",
        "    shared_segments = {segment: count for segment, count in segment_counts.items() if count >= min_shared}\n",
        "    return dict(sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "def calculate_value(df, ancestral_col, shared_segments):\n",
        "    sorted_segments = sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "    df[\"Value\"] = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.isna(row[ancestral_col]):\n",
        "            df.at[idx, \"Value\"] = 0\n",
        "            continue\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "        value = 0\n",
        "        lines_copy = lines.copy()\n",
        "        for segment, freq in sorted_segments:\n",
        "            segment_list = list(segment)\n",
        "            if all(line in lines_copy for line in segment_list):\n",
        "                value += freq\n",
        "                for line in segment_list:\n",
        "                    lines_copy.remove(line)\n",
        "        value += len(lines_copy)\n",
        "        df.at[idx, \"Value\"] = value\n",
        "    return df\n",
        "\n",
        "################################################################################\n",
        "#       Main Processing for Segmentation & Value Calculation\n",
        "################################################################################\n",
        "# Create a separate DataFrame for processing so that we do not lose \"Match to\" and \"Name\"\n",
        "df_process = combined_df[[\"ID#\", \"Yates DNA Ancestral Line\"]].copy()\n",
        "df_process[\"Value\"] = 0\n",
        "print(\"\\nDEBUG: Processing DataFrame (df_process) created for segmentation\")\n",
        "print(\"Columns:\", df_process.columns.tolist())\n",
        "print(df_process.head(5))\n",
        "\n",
        "ancestral_col = \"Yates DNA Ancestral Line\"\n",
        "shared_segments_found = identify_all_shared_segments(df_process, ancestral_col, min_shared=2, min_size=2)\n",
        "seg_df = pd.DataFrame(\n",
        "    [(\"~~~\".join(seg), freq) for seg, freq in shared_segments_found.items()],\n",
        "    columns=[\"Segment\", \"Frequency\"]\n",
        ").sort_values(by=\"Frequency\", ascending=False)\n",
        "print(\"\\n✅ All Shared Segments and Frequencies:\")\n",
        "print(seg_df)\n",
        "\n",
        "df_process = calculate_value(df_process, ancestral_col, shared_segments_found)\n",
        "print(\"\\n✅ Final Processed DataFrame with Value (df_process):\")\n",
        "print(df_process[[\"ID#\", \"Value\"]])\n",
        "id_to_value_map = df_process.set_index(\"ID#\")[\"Value\"].to_dict()\n",
        "print(\"\\n✅ Mapping of ID# to Value:\")\n",
        "print(id_to_value_map)\n",
        "\n",
        "# Merge the calculated \"Value\" column back into combined_df.\n",
        "merged_df = combined_df.merge(df_process[[\"ID#\", \"Value\"]], on=\"ID#\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
        "if \"Value\" not in merged_df.columns and \"Value_new\" in merged_df.columns:\n",
        "    merged_df.rename(columns={\"Value_new\": \"Value\"}, inplace=True)\n",
        "else:\n",
        "    # If both exist, choose the one from the processing DataFrame.\n",
        "    if \"Value_new\" in merged_df.columns:\n",
        "        merged_df[\"Value\"] = merged_df[\"Value_new\"]\n",
        "        merged_df.drop(columns=[\"Value_new\"], inplace=True)\n",
        "combined_df = merged_df\n",
        "\n",
        "combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "print(\"\\n✅ Processing Complete! 🚀\")\n",
        "print(\"Combined DataFrame now (first 5 rows):\")\n",
        "print(combined_df.head(5))\n",
        "\n",
        "################################################################################\n",
        "#       Generate HTML Output (Optional)\n",
        "################################################################################\n",
        "GENERATE_HTML = False  # Set to True if HTML output is needed\n",
        "if GENERATE_HTML:\n",
        "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    output_html_path = f\"html_output_{current_datetime}.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table, th, td {\n",
        "      border: 1px solid black;\n",
        "      border-collapse: collapse;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "      text-align: center;\n",
        "    }\n",
        "    td {\n",
        "      text-align: center;\n",
        "    }\n",
        "    td:nth-child(6) {\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    html_content = css_style + combined_df.to_html(index=False, escape=False, border=0)\n",
        "    with open(output_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    print(f\"✅ HTML output generated: {output_html_path}\")\n",
        "\n",
        "################################################################################\n",
        "# Export Final Data (Optional)\n",
        "################################################################################\n",
        "EXPORT_CSV = False  # Change to True if needed\n",
        "if EXPORT_CSV:\n",
        "    combined_df.to_csv(\"final_combined_df.csv\", index=False)\n",
        "    seg_df.to_csv(\"segments_discovered.csv\", index=False)\n",
        "print(\"✅ Data processing complete!\")\n",
        "\n",
        "################################################################################\n",
        "# Detailed Report Generation for Segment Breakdown\n",
        "################################################################################\n",
        "sorted_segments = sorted(shared_segments_found.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "report_data = []\n",
        "for idx, row in combined_df.iterrows():\n",
        "    individual_id = row[\"ID#\"]\n",
        "    ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "    if pd.isna(ancestral_line) or ancestral_line.strip() == \"\":\n",
        "        continue\n",
        "    segments = [seg.strip() for seg in ancestral_line.split(\"~~~\") if seg.strip()]\n",
        "    segments_copy = segments.copy()\n",
        "    cumulative_value = 0\n",
        "    breakdown_entries = []\n",
        "    for segment_tuple, freq in sorted_segments:\n",
        "        segment_list = list(segment_tuple)\n",
        "        if all(seg in segments_copy for seg in segment_list):\n",
        "            cumulative_value += freq\n",
        "            breakdown_entries.append({\n",
        "                \"Individual ID\": individual_id,\n",
        "                \"Segment\": \"&\".join(segment_list),\n",
        "                \"Segment Type\": \"Shared\",\n",
        "                \"Frequency\": freq,\n",
        "                \"Contribution\": freq,\n",
        "                \"Cumulative Value\": cumulative_value\n",
        "            })\n",
        "            for seg in segment_list:\n",
        "                segments_copy.remove(seg)\n",
        "    for leftover in segments_copy:\n",
        "        cumulative_value += 1\n",
        "        breakdown_entries.append({\n",
        "            \"Individual ID\": individual_id,\n",
        "            \"Segment\": leftover,\n",
        "            \"Segment Type\": \"Leftover\",\n",
        "            \"Frequency\": 1,\n",
        "            \"Contribution\": 1,\n",
        "            \"Cumulative Value\": cumulative_value\n",
        "        })\n",
        "    breakdown_entries.append({\n",
        "        \"Individual ID\": individual_id,\n",
        "        \"Segment\": \"Final Calculated Value\",\n",
        "        \"Segment Type\": \"\",\n",
        "        \"Frequency\": \"\",\n",
        "        \"Contribution\": \"\",\n",
        "        \"Cumulative Value\": cumulative_value\n",
        "    })\n",
        "    report_data.extend(breakdown_entries)\n",
        "    report_data.append({\n",
        "        \"Individual ID\": \"\",\n",
        "        \"Segment\": \"\",\n",
        "        \"Segment Type\": \"\",\n",
        "        \"Frequency\": \"\",\n",
        "        \"Contribution\": \"\",\n",
        "        \"Cumulative Value\": \"\"\n",
        "    })\n",
        "report_df = pd.DataFrame(report_data, columns=[\n",
        "    \"Individual ID\", \"Segment\", \"Segment Type\", \"Frequency\", \"Contribution\", \"Cumulative Value\"\n",
        "])\n",
        "report_csv_filename = \"detailed_segments_report.csv\"\n",
        "report_df.to_csv(report_csv_filename, index=False)\n",
        "print(f\"Detailed segment breakdown report has been exported to '{report_csv_filename}'.\")\n",
        "\n",
        "################################################################################\n",
        "# Calculate Additional Scores: Standard Z-Score, Robust Z-Score, and Percentile Rank\n",
        "################################################################################\n",
        "combined_df[\"Value\"] = pd.to_numeric(combined_df[\"Value\"], errors='coerce')\n",
        "mean_value = combined_df[\"Value\"].mean()\n",
        "std_value = combined_df[\"Value\"].std()\n",
        "combined_df[\"Z-Score\"] = (combined_df[\"Value\"] - mean_value) / std_value\n",
        "median_value = combined_df[\"Value\"].median()\n",
        "mad_value = np.median(np.abs(combined_df[\"Value\"] - median_value))\n",
        "if mad_value == 0:\n",
        "    combined_df[\"Robust Z-Score\"] = 0\n",
        "else:\n",
        "    combined_df[\"Robust Z-Score\"] = (combined_df[\"Value\"] - median_value) / (mad_value * 1.4826)\n",
        "combined_df[\"Percentile Rank\"] = combined_df[\"Value\"].rank(pct=True) * 100\n",
        "\n",
        "################################################################################\n",
        "# Repair DataFrame Columns: Set to \"ID#\", \"Match to\", \"Name\", \"Value\",\n",
        "# \"Z-Score\", \"Robust Z-Score\", \"Percentile Rank\", \"Yates DNA Ancestral Line\"\n",
        "################################################################################\n",
        "final_columns = [\"ID#\", \"Match to\", \"Name\", \"Value\", \"Z-Score\", \"Robust Z-Score\", \"Percentile Rank\", \"Yates DNA Ancestral Line\"]\n",
        "for col in final_columns:\n",
        "    if col not in combined_df.columns:\n",
        "        combined_df[col] = None\n",
        "combined_df = combined_df[final_columns]\n",
        "print(\"\\nDEBUG: Final DataFrame after repairing columns\")\n",
        "print(\"Columns:\", combined_df.columns.tolist())\n",
        "print(combined_df.head(10))\n",
        "\n",
        "################################################################################\n",
        "# Export the final DataFrame with the additional scores to CSV\n",
        "################################################################################\n",
        "output_filename = \"final_combined_df_with_scores.csv\"\n",
        "combined_df.to_csv(output_filename, index=False)\n",
        "print(f\"Final DataFrame with Z-Scores, Robust Z-Scores, and Percentile Ranks exported to '{output_filename}'.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa20790-bab7-4dfe-c382-b9ebbdb95300"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 7\n",
            "After manual filter, total records: 8\n",
            "Records tagged and filtered by NPFX: 8\n",
            "\n",
            "DEBUG: 481-Initial DataFrame created from combined_df_rows\n",
            "Columns: ['ID#', 'Match to', 'Name', 'cM', 'Yates DNA Ancestral Line']\n",
            "      ID#        Match to               Name   cM  \\\n",
            "0  I47130          leedon  CrabtreeChadEdmun   25   \n",
            "1  I52559  yates,timothyj       LeeDonnaStel   12   \n",
            "2  I52798          leedon        HoukJeffrey  448   \n",
            "3  I52802          leedon              HoukW  369   \n",
            "4  I52807          leedon     RileyRonldPaul  173   \n",
            "\n",
            "                            Yates DNA Ancestral Line  \n",
            "0  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "1  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "2  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "3  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "4  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "\n",
            "DEBUG: 517-DataFrame after removing prefix\n",
            "Columns: ['ID#', 'Match to', 'Name', 'cM', 'Yates DNA Ancestral Line']\n",
            "      ID#        Match to               Name   cM  \\\n",
            "0  I47130          leedon  CrabtreeChadEdmun   25   \n",
            "1  I52559  yates,timothyj       LeeDonnaStel   12   \n",
            "2  I52798          leedon        HoukJeffrey  448   \n",
            "3  I52802          leedon              HoukW  369   \n",
            "4  I52807          leedon     RileyRonldPaul  173   \n",
            "\n",
            "                            Yates DNA Ancestral Line  \n",
            "0  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "1  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "2  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "3  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "4  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "\n",
            "DEBUG: 530-DataFrame after reordering and sorting\n",
            "Columns: ['ID#', 'Match to', 'Name', 'cM', 'Yates DNA Ancestral Line']\n",
            "      ID#        Match to               Name   cM  \\\n",
            "2  I52559  yates,timothyj       LeeDonnaStel   12   \n",
            "3  I52798          leedon        HoukJeffrey  448   \n",
            "4  I52802          leedon              HoukW  369   \n",
            "6  I52810          leedon  LewallenDonaldChr  154   \n",
            "5  I52807          leedon     RileyRonldPaul  173   \n",
            "\n",
            "                            Yates DNA Ancestral Line  \n",
            "2  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "3  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "4  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "6  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "5  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "✅ Required columns for processing are now present in combined_df.\n",
            "\n",
            "DEBUG: Processing DataFrame (df_process) created for segmentation\n",
            "Columns: ['ID#', 'Yates DNA Ancestral Line', 'Value']\n",
            "      ID#                           Yates DNA Ancestral Line  Value\n",
            "2  I52559  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      0\n",
            "3  I52798  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      0\n",
            "4  I52802  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      0\n",
            "6  I52810  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      0\n",
            "5  I52807  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      0\n",
            "\n",
            "✅ All Shared Segments and Frequencies:\n",
            "                                              Segment  Frequency\n",
            "60  YatesAmbroseDu&CooleyElizabeth~~~YatesLevi&Coo...          8\n",
            "31  YatesAmbroseDu&CooleyElizabeth~~~YatesLevi&Coo...          8\n",
            "61  YatesAmbroseDu&CooleyElizabeth~~~YatesWilliam&...          8\n",
            "62  YatesLevi&CooleyRebecca~~~YatesWilliam&ParkerS...          8\n",
            "48  WheelerRobertJef&YatesRosaAlafa~~~YatesAmbrose...          4\n",
            "..                                                ...        ...\n",
            "26  RileyAlveEdwin&WheelerWinifred~~~WheelerRobert...          2\n",
            "25  RileyAlveEdwin&WheelerWinifred~~~WheelerRobert...          2\n",
            "24  RileyAlveEdwin&WheelerWinifred~~~WheelerRobert...          2\n",
            "22  HoukGeorgeEdw&AdamsShirley~~~YatesAmbroseDu&Co...          2\n",
            "78  RileyAlveEdwin&WheelerWinifred~~~YatesWilliam&...          2\n",
            "\n",
            "[79 rows x 2 columns]\n",
            "\n",
            "✅ Final Processed DataFrame with Value (df_process):\n",
            "      ID#  Value\n",
            "2  I52559      4\n",
            "3  I52798      2\n",
            "4  I52802      2\n",
            "6  I52810      5\n",
            "5  I52807      3\n",
            "8  I52836      3\n",
            "1  I47130      6\n",
            "7  I52816     11\n",
            "\n",
            "✅ Mapping of ID# to Value:\n",
            "{'I52559': 4, 'I52798': 2, 'I52802': 2, 'I52810': 5, 'I52807': 3, 'I52836': 3, 'I47130': 6, 'I52816': 11}\n",
            "\n",
            "✅ Processing Complete! 🚀\n",
            "Combined DataFrame now (first 5 rows):\n",
            "      ID# Match to               Name   cM  \\\n",
            "7  I52816   leedon         DucksonPam  116   \n",
            "6  I47130   leedon  CrabtreeChadEdmun   25   \n",
            "5  I52836   leedon        RileySandra   63   \n",
            "4  I52807   leedon     RileyRonldPaul  173   \n",
            "3  I52810   leedon  LewallenDonaldChr  154   \n",
            "\n",
            "                            Yates DNA Ancestral Line  Value  \n",
            "7  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...     11  \n",
            "6  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      6  \n",
            "5  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      3  \n",
            "4  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      3  \n",
            "3  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...      5  \n",
            "✅ Data processing complete!\n",
            "Detailed segment breakdown report has been exported to 'detailed_segments_report.csv'.\n",
            "\n",
            "DEBUG: Final DataFrame after repairing columns\n",
            "Columns: ['ID#', 'Match to', 'Name', 'Value', 'Z-Score', 'Robust Z-Score', 'Percentile Rank', 'Yates DNA Ancestral Line']\n",
            "      ID#        Match to               Name  Value   Z-Score  Robust Z-Score  \\\n",
            "7  I52816          leedon         DucksonPam     11  2.184070        3.372454   \n",
            "6  I47130          leedon  CrabtreeChadEdmun      6  0.504016        1.124151   \n",
            "5  I52836          leedon        RileySandra      3 -0.504016       -0.224830   \n",
            "4  I52807          leedon     RileyRonldPaul      3 -0.504016       -0.224830   \n",
            "3  I52810          leedon  LewallenDonaldChr      5  0.168005        0.674491   \n",
            "0  I52559  yates,timothyj       LeeDonnaStel      4 -0.168005        0.224830   \n",
            "1  I52798          leedon        HoukJeffrey      2 -0.840027       -0.674491   \n",
            "2  I52802          leedon              HoukW      2 -0.840027       -0.674491   \n",
            "\n",
            "   Percentile Rank                           Yates DNA Ancestral Line  \n",
            "7           100.00  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "6            87.50  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "5            43.75  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "4            43.75  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "3            75.00  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "0            62.50  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "1            18.75  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "2            18.75  YatesWilliam&ParkerSally~~~YatesLevi&CooleyReb...  \n",
            "Final DataFrame with Z-Scores, Robust Z-Scores, and Percentile Ranks exported to 'final_combined_df_with_scores.csv'.\n"
          ]
        }
      ]
    }
  ]
}