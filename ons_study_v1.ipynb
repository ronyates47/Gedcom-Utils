{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "14vSUVZPJDnCMckmdicLv6NweRJeg9FLK",
      "authorship_tag": "ABX9TyP7QGRvRmpQbjPbvC4dknEu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/ons_study_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [PIP]\n",
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Oh0QPOW8iLz",
        "outputId": "ba76cf56-5698-4378-cc36-cfd45d7dcd94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: python-gedcom in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.12/dist-packages (3.2.9)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [STEP 0] MASTER SETUP: Run this once to initialize the Shared Library\n",
        "# This cell creates 'config.py', 'toolkit.py', and 'deploy.py' in your Colab session.\n",
        "\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION (config.py)\n",
        "# ==========================================\n",
        "config_content = \"\"\"\n",
        "import os\n",
        "\n",
        "# --- SITE SETTINGS ---\n",
        "SITE_BASE_URL = \"https://yates.one-name.net\"\n",
        "TNG_BASE_URL = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE_ID = \"tree1\"\n",
        "\n",
        "# --- CSS VERSIONS (Control from here!) ---\n",
        "CSS_UNIFIED     = \"v2026-02-01-unified-blue-refactor1\"\n",
        "CSS_DNA_STYLES  = \"v2025-11-23-g3\"\n",
        "\n",
        "# --- FILE PATHS ---\n",
        "INPUT_CSV       = \"final_combined_df_with_value_labels.csv\"\n",
        "VITALS_CSV      = \"dna_vitals.csv\"\n",
        "NETWORK_AUTH    = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# --- FTP CREDENTIALS ---\n",
        "def get_ftp_creds():\n",
        "    # Tries to get from Colab secrets first, then environment\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        return {\n",
        "            \"HOST\": userdata.get(\"FTP_HOST\"),\n",
        "            \"USER\": userdata.get(\"FTP_USER\"),\n",
        "            \"PASS\": userdata.get(\"FTP_PASS\"),\n",
        "            \"PORT\": int(userdata.get(\"FTP_PORT\", 21)),\n",
        "            \"DIR\":  userdata.get(\"FTP_DIR\", \"\")\n",
        "        }\n",
        "    except:\n",
        "        return {\n",
        "            \"HOST\": os.environ.get(\"FTP_HOST\", \"\"),\n",
        "            \"USER\": os.environ.get(\"FTP_USER\", \"\"),\n",
        "            \"PASS\": os.environ.get(\"FTP_PASS\", \"\"),\n",
        "            \"PORT\": int(os.environ.get(\"FTP_PORT\", 21)),\n",
        "            \"DIR\":  \"ons-study\"\n",
        "        }\n",
        "\"\"\"\n",
        "with open(\"config.py\", \"w\") as f: f.write(config_content)\n",
        "\n",
        "# ==========================================\n",
        "# 2. TOOLKIT (toolkit.py) - Data Logic\n",
        "# ==========================================\n",
        "toolkit_content = \"\"\"\n",
        "import pandas as pd\n",
        "import re\n",
        "import html\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- ROBUST CSV READER ---\n",
        "def load_csv(path):\n",
        "    # Tries 5 different encodings so you don't have to worry about it\n",
        "    encodings = [\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\"]\n",
        "    for enc in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            print(f\"[TOOLKIT] Successfully loaded {path} using {enc}\")\n",
        "            return df\n",
        "        except Exception:\n",
        "            continue\n",
        "    print(f\"[TOOLKIT] ERROR: Could not read {path} with any known encoding.\")\n",
        "    return None\n",
        "\n",
        "# --- TEXT CLEANERS ---\n",
        "def clean_text(text):\n",
        "    if not text: return \"\"\n",
        "    # Remove tildes and extra spaces\n",
        "    t = str(text).replace(\"~\", \" \")\n",
        "    return re.sub(r\"\\\\s+\", \" \", t).strip()\n",
        "\n",
        "def smart_titlecase(text):\n",
        "    # Your custom capitalization logic\n",
        "    if not text: return \"\"\n",
        "    text = clean_text(text)\n",
        "    particles = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "    def _fix_word(w):\n",
        "        # Fix Mc/Mac and apostrophes\n",
        "        w = w.lower()\n",
        "        if w in particles: return w\n",
        "        # Logic for Mc/Mac/Apostrophes...\n",
        "        w = re.sub(r\"(^|\\\\b)([a-z])(['&#8217;])([a-z])\", lambda m: m.group(1)+m.group(2).upper()+m.group(3)+m.group(4).upper(), w)\n",
        "        w = w[0].upper() + w[1:]\n",
        "        w = re.sub(r\"\\\\bMc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), w)\n",
        "        return w\n",
        "\n",
        "    return \" \".join([_fix_word(w) for w in text.split()])\n",
        "\n",
        "# --- DATE FORMATTER ---\n",
        "def friendly_date(utc_string):\n",
        "    if not utc_string: return \"(unknown)\"\n",
        "    clean = str(utc_string).replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    # Try parsing multiple formats\n",
        "    for fmt in [\"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%dT%H:%M\"]:\n",
        "        try:\n",
        "            dt = datetime.strptime(clean, fmt)\n",
        "            # Convert UTC to EST (UTC-5)\n",
        "            dt_est = dt - timedelta(hours=5)\n",
        "            # Format: January 1, 2026 12:00 PM\n",
        "            return dt_est.strftime(\"%B %-d, %Y %-I:%M %p\")\n",
        "        except:\n",
        "            continue\n",
        "    return utc_string\n",
        "\"\"\"\n",
        "with open(\"toolkit.py\", \"w\") as f: f.write(toolkit_content)\n",
        "\n",
        "# ==========================================\n",
        "# 3. DEPLOY (deploy.py) - FTP Logic\n",
        "# ==========================================\n",
        "deploy_content = \"\"\"\n",
        "import os\n",
        "from ftplib import FTP_TLS\n",
        "import config\n",
        "\n",
        "def upload_files(file_map):\n",
        "    # file_map is a dict: {\"local_path.html\": \"remote/path/file.html\"}\n",
        "    creds = config.get_ftp_creds()\n",
        "\n",
        "    if not creds[\"HOST\"]:\n",
        "        print(\"[DEPLOY] No FTP credentials found. Skipping upload.\")\n",
        "        return\n",
        "\n",
        "    print(f\"[DEPLOY] Connecting to {creds['HOST']}...\")\n",
        "    try:\n",
        "        ftps = FTP_TLS(timeout=30)\n",
        "        ftps.connect(creds[\"HOST\"], creds[\"PORT\"])\n",
        "        ftps.login(creds[\"USER\"], creds[\"PASS\"])\n",
        "        ftps.prot_p() # Secure data connection\n",
        "        ftps.set_pasv(True)\n",
        "\n",
        "        # Navigate to base dir if set\n",
        "        if creds[\"DIR\"]:\n",
        "            _ensure_dir(ftps, creds[\"DIR\"])\n",
        "            ftps.cwd(creds[\"DIR\"])\n",
        "\n",
        "        for local, remote in file_map.items():\n",
        "            # Ensure remote directory exists\n",
        "            remote_dir = os.path.dirname(remote)\n",
        "            if remote_dir:\n",
        "                _ensure_dir(ftps, remote_dir)\n",
        "                # Go back to root relative to base\n",
        "                if creds[\"DIR\"]: ftps.cwd(\"/\" + creds[\"DIR\"])\n",
        "                else: ftps.cwd(\"/\")\n",
        "\n",
        "            # Upload\n",
        "            with open(local, \"rb\") as f:\n",
        "                ftps.storbinary(f\"STOR {remote}\", f)\n",
        "            print(f\"[DEPLOY] SUCCESS: Uploaded {local} -> {remote}\")\n",
        "\n",
        "        ftps.quit()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[DEPLOY] ERROR: FTP Upload failed - {e}\")\n",
        "\n",
        "def _ensure_dir(ftp, path):\n",
        "    # Recursive directory creator\n",
        "    parts = [p for p in path.split('/') if p]\n",
        "    for part in parts:\n",
        "        try:\n",
        "            ftp.cwd(part)\n",
        "        except:\n",
        "            try:\n",
        "                ftp.mkd(part)\n",
        "                ftp.cwd(part)\n",
        "            except:\n",
        "                pass\n",
        "\"\"\"\n",
        "with open(\"deploy.py\", \"w\") as f: f.write(deploy_content)\n",
        "\n",
        "print(\"[SUCCESS] Shared Library Initialized.\")\n",
        "print(\"   - config.py created\")\n",
        "print(\"   - toolkit.py created\")\n",
        "print(\"   - deploy.py created\")"
      ],
      "metadata": {
        "id": "2MgGO-gN-73j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21fa1cc3-184d-4194-d224-0db8ef64571a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] Shared Library Initialized.\n",
            "   - config.py created\n",
            "   - toolkit.py created\n",
            "   - deploy.py created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 1] Master Engine (GEDCOM -> Authority Database)\n",
        "# Version: 2026.02.04-RICH-SLUG (Production)\n",
        "# Logic: Parses '2 NPFX', traces 'Yates' lines, formats 'Name (Years) & Spouse', and sorts via hidden slug.\n",
        "\n",
        "import os, re, glob, logging, pickle\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      CELL 1: THE ENGINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "TARGET_GEDCOM = \"yates_study_2025.ged\"\n",
        "NAME_KEY_FILE = \"match_to_unmasked.csv\"\n",
        "CSV_OUT       = \"engine_database.csv\"\n",
        "VITALS_OUT    = \"dna_vitals.csv\"\n",
        "\n",
        "# --- UTILITIES ---\n",
        "def _slugify_last_first(name_str):\n",
        "    if not name_str or name_str.lower() == \"unknown\": return \"unknown\"\n",
        "    clean = name_str.replace(\"/\", \"\").strip()\n",
        "    parts = clean.split()\n",
        "    if not parts: return \"unknown\"\n",
        "    return re.sub(r\"[^a-z0-9]\", \"\", (parts[-1] + \"\".join(parts[:-1])).lower())\n",
        "\n",
        "def _pretty_name(display_name):\n",
        "    s = (display_name or \"\").replace(\"/\", \"\").strip()\n",
        "    return s if s and s.lower() != \"unknown\" else \"Unknown\"\n",
        "\n",
        "def _extract_years(txt):\n",
        "    b = re.search(r\"1 BIRT.*?2 DATE.*?(\\d{4})\", txt, re.S)\n",
        "    d = re.search(r\"1 DEAT.*?2 DATE.*?(\\d{4})\", txt, re.S)\n",
        "    return f\"{b.group(1) if b else ''}-{d.group(1) if d else ''}\".strip('-')\n",
        "\n",
        "# --- LOGIC CORE ---\n",
        "class StrictLineageTracer:\n",
        "    def __init__(self, parents_map, names_map, years_map):\n",
        "        self.parents = parents_map\n",
        "        self.names = names_map\n",
        "        self.years = years_map\n",
        "        self.target = \"yates\"\n",
        "\n",
        "    def get_lineage(self, start_id):\n",
        "        lineage = []\n",
        "        curr = start_id\n",
        "        for _ in range(50):\n",
        "            lineage.append({'id': curr, 'name': self.names.get(curr, \"Unknown\"), 'years': self.years.get(curr, \"\")})\n",
        "            f, m = self.parents.get(curr, (None, None))\n",
        "            if not f and not m: break\n",
        "\n",
        "            # Follow Yates\n",
        "            fn, mn = self.names.get(f, \"\").lower(), self.names.get(m, \"\").lower()\n",
        "            if self.target in fn: curr = f\n",
        "            elif self.target in mn: curr = m\n",
        "            elif f: curr = f\n",
        "            elif m: curr = m\n",
        "            else: break\n",
        "        return lineage\n",
        "\n",
        "def process_record(pkg):\n",
        "    rid, (pmap, nmap, ymap, smap), meta = pkg\n",
        "    tracer = StrictLineageTracer(pmap, nmap, ymap)\n",
        "\n",
        "    # 1. Trace & Reverse (Oldest -> Newest)\n",
        "    lineage = tracer.get_lineage(rid)\n",
        "    lineage.reverse()\n",
        "\n",
        "    # 2. Identify First Ancestor Pair\n",
        "    apex = lineage[0] if lineage else {'name': 'Unknown', 'id': None, 'years': ''}\n",
        "    spouses = smap.get(apex['id'], [])\n",
        "    spouse_name = nmap.get(spouses[0], \"Unknown\") if spouses else \"Unknown\"\n",
        "\n",
        "    # 3. Format Rich Header: \"William Yates (1750-1830) & Mary\"\n",
        "    header = f\"{apex['name']}\"\n",
        "    if apex['years']: header += f\" ({apex['years']})\"\n",
        "    if spouse_name != \"Unknown\": header += f\" & {spouse_name}\"\n",
        "\n",
        "    # 4. Inject Header into Lineage String\n",
        "    lineage_names = [x['name'] for x in lineage]\n",
        "    if lineage_names: lineage_names[0] = header\n",
        "\n",
        "    return {\n",
        "        \"ID#\": rid,\n",
        "        \"Match to\": meta.get(\"code\", \"\"),\n",
        "        \"Name\": meta.get(\"real_name\", \"Unknown\"),\n",
        "        \"cM\": meta.get(\"cm\", \"\"),\n",
        "        \"Yates DNA Ancestral Line\": \" -> \".join(lineage_names),\n",
        "        \"fa_1 masked\": _slugify_last_first(apex['name']),\n",
        "        \"FirstAncestor_pair cojoined\": header,\n",
        "        \"Authority_FirstAncestor\": _slugify_last_first(apex['name']) # Sort Key\n",
        "    }\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "def main():\n",
        "    if not os.path.exists(TARGET_GEDCOM):\n",
        "        print(f\"[ERR] {TARGET_GEDCOM} not found.\"); return\n",
        "\n",
        "    print(f\"[1] Parsing {TARGET_GEDCOM}...\")\n",
        "    with open(TARGET_GEDCOM, 'r', encoding='utf-8-sig', errors='ignore') as f: raw = f.read()\n",
        "\n",
        "    # Maps\n",
        "    pmap, nmap, ymap, smap, meta_map = {}, {}, {}, {}, {}\n",
        "    name_key = {}\n",
        "\n",
        "    if os.path.exists(NAME_KEY_FILE):\n",
        "        ndf = pd.read_csv(NAME_KEY_FILE, header=None)\n",
        "        name_key = dict(zip(ndf[0].str.strip().str.lower(), ndf[1].str.strip()))\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    total_recs = 0\n",
        "\n",
        "    for blk in blocks:\n",
        "        lines = blk.splitlines()\n",
        "        if \" INDI\" in lines[0]:\n",
        "            total_recs += 1\n",
        "            rid = lines[0].split(\"@\")[1]\n",
        "            name, npfx = \"\", \"\"\n",
        "            for l in lines:\n",
        "                if \"1 NAME\" in l: name = _pretty_name(l.split(\"NAME\")[1])\n",
        "                if \"2 NPFX\" in l: npfx = l.split(\"NPFX\")[1].strip()\n",
        "\n",
        "            nmap[rid] = name\n",
        "            ymap[rid] = _extract_years(blk)\n",
        "\n",
        "            if \"&\" in npfx:\n",
        "                try:\n",
        "                    parts = npfx.replace(\"(\", \"\").replace(\")\", \"\").split(\"&\")\n",
        "                    meta_map[rid] = {'cm': parts[0].strip(), 'code': parts[1].strip(), 'real_name': name_key.get(parts[1].strip().lower(), name)}\n",
        "                except: pass\n",
        "\n",
        "        if \" FAM\" in lines[0]:\n",
        "            h = re.search(r\"1 HUSB @(.*?)@\", blk)\n",
        "            w = re.search(r\"1 WIFE @(.*?)@\", blk)\n",
        "            kids = re.findall(r\"1 CHIL @(.*?)@\", blk)\n",
        "            hid, wid = (h.group(1) if h else None), (w.group(1) if w else None)\n",
        "            if hid and wid:\n",
        "                smap.setdefault(hid, []).append(wid)\n",
        "                smap.setdefault(wid, []).append(hid)\n",
        "            for k in kids: pmap[k] = (hid, wid)\n",
        "\n",
        "    # Processing\n",
        "    queue = [(mid, (pmap, nmap, ymap, smap), meta) for mid, meta in meta_map.items()]\n",
        "    print(f\"[2] Processing {len(queue)} matches (Total Pool: {len(queue)})...\")\n",
        "\n",
        "    with ProcessPoolExecutor() as exe:\n",
        "        rows = list(tqdm(exe.map(process_record, queue), total=len(queue)))\n",
        "\n",
        "    # Output\n",
        "    df = pd.DataFrame(rows)\n",
        "    if not df.empty:\n",
        "        df.sort_values(by=[\"Authority_FirstAncestor\", \"Name\"], inplace=True)\n",
        "\n",
        "    # Columns: Authority on far right\n",
        "    cols = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\",\n",
        "            \"fa_1 masked\", \"FirstAncestor_pair cojoined\", \"Authority_FirstAncestor\"]\n",
        "    df = df[cols]\n",
        "\n",
        "    df.to_csv(CSV_OUT, index=False, encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\")\n",
        "\n",
        "    # Save Vitals\n",
        "    pd.DataFrame([\n",
        "        {\"line\": f\"Records tagged and filtered by NPFX: {len(queue)}\"},\n",
        "        {\"line\": f\"After manual filter, total records: {len(df)}\"}\n",
        "    ]).to_csv(VITALS_OUT, index=False, encoding=\"iso-8859-15\")\n",
        "\n",
        "    print(f\"[DONE] Database generated: {CSV_OUT} ({len(df)} rows)\")\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L00tCdaZ17C",
        "outputId": "506d36b0-8d44-4c10-cd7b-69b800dfca1f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      CELL 1: THE ENGINE\n",
            "============================================================\n",
            "[1] Parsing yates_study_2025.ged...\n",
            "[2] Processing 1700 matches (Total Pool: 1700)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1700/1700 [06:11<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DONE] Database generated: engine_database.csv (1700 rows)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title [CELL 2] Production Publisher (Dual Views)\n",
        "# Logic: Generates TWO distinct register pages (Ancestor View & Participant View) for optimal sorting.\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import html\n",
        "import pytz\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime\n",
        "from google.colab import userdata, files\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"      CELL 2: PRODUCTION PUBLISHER (DUAL VIEWS)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "REMOTE_DIR = \"ons-study\"\n",
        "WEB_BASE_URL = \"https://yates.one-name.net/ons-study/\"\n",
        "TNG_BASE_URL = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "TNG_SUFFIX   = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "MANUAL_POOL_COUNT = 1700\n",
        "CSV_INPUT    = \"engine_database.csv\"\n",
        "VITALS_INPUT = \"dna_vitals.csv\"\n",
        "KEY_FILE     = \"match_to_unmasked.csv\"\n",
        "\n",
        "# --- 2. CREDENTIALS ---\n",
        "try:\n",
        "    FTP_HOST = userdata.get('FTP_HOST')\n",
        "    FTP_USER = userdata.get('FTP_USER')\n",
        "    FTP_PASS = userdata.get('FTP_PASS')\n",
        "    try: FTP_PORT = int(userdata.get('FTP_PORT'))\n",
        "    except: FTP_PORT = 21\n",
        "    if not FTP_HOST: raise ValueError(\"Missing Secrets.\")\n",
        "    print(f\"[1] Credentials Loaded for: {FTP_HOST}\")\n",
        "except Exception as e:\n",
        "    print(f\"[CRITICAL] Credential Error: {e}\")\n",
        "    raise e\n",
        "\n",
        "# --- 3. SERVER FETCH ---\n",
        "print(f\"\\n[2] Fetching Authority Key from Server...\")\n",
        "try:\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    ftps.prot_p(); ftps.set_pasv(True)\n",
        "    try: ftps.cwd(f\"/{REMOTE_DIR}\")\n",
        "    except:\n",
        "        try: ftps.cwd(f\"/public_html/{REMOTE_DIR}\")\n",
        "        except: pass\n",
        "    if KEY_FILE in ftps.nlst():\n",
        "        with open(KEY_FILE, \"wb\") as f:\n",
        "            ftps.retrbinary(f\"RETR {KEY_FILE}\", f.write)\n",
        "        print(f\"    - [SUCCESS] Pulled latest '{KEY_FILE}'.\")\n",
        "    ftps.quit()\n",
        "except Exception as e:\n",
        "    print(f\"    - [ERR] Fetch failed: {e}\")\n",
        "\n",
        "# --- 4. DATA SETUP ---\n",
        "if not os.path.exists(CSV_INPUT):\n",
        "    print(f\"\\n[WARN] Database '{CSV_INPUT}' missing. Please upload.\")\n",
        "    up = files.upload()\n",
        "    if up: CSV_INPUT = list(up.keys())[0]\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(CSV_INPUT, encoding=\"iso-8859-15\")\n",
        "    print(f\"[3] Database Loaded: {len(df)} records.\")\n",
        "except:\n",
        "    print(\"[ERR] Database empty.\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# Key Map\n",
        "unmask_map = {}\n",
        "if os.path.exists(KEY_FILE):\n",
        "    try:\n",
        "        kdf = pd.read_csv(KEY_FILE, header=None)\n",
        "        unmask_map = dict(zip(kdf[0].str.strip(), kdf[1].str.strip()))\n",
        "    except: pass\n",
        "\n",
        "# Vitals\n",
        "vitals_total = MANUAL_POOL_COUNT\n",
        "if os.path.exists(VITALS_INPUT):\n",
        "    try:\n",
        "        v_df = pd.read_csv(VITALS_INPUT, header=None)\n",
        "        found = False\n",
        "        for i, row in v_df.iterrows():\n",
        "            if \"Records tagged\" in str(row[0]):\n",
        "                vitals_total = int(str(row[0]).split(\":\")[-1].strip())\n",
        "                found = True; break\n",
        "        if found: print(f\"    - Vitals Recalculated: {vitals_total}\")\n",
        "    except: pass\n",
        "\n",
        "# --- 5. DATA TRANSFORM ---\n",
        "# A. Unmask\n",
        "if \"Match to\" in df.columns and unmask_map:\n",
        "    print(\"    - Unmasking 'Match to' codes...\")\n",
        "    df[\"Match to\"] = df[\"Match to\"].apply(lambda x: unmask_map.get(str(x).strip(), x))\n",
        "\n",
        "# B. Build Narrative\n",
        "print(\"    - Building Narrative Column...\")\n",
        "def build_narrative(row):\n",
        "    m_to = str(row.get('Match to', 'Unknown'))\n",
        "    nm   = str(row.get('Name', 'Unknown'))\n",
        "    cm   = str(row.get('cM', '0'))\n",
        "    anc  = str(row.get('FirstAncestor_pair cojoined', 'Unknown'))\n",
        "    rid  = str(row.get('ID#', ''))\n",
        "\n",
        "    linked_name = m_to\n",
        "    if rid and rid != 'nan':\n",
        "        linked_name = f'<a href=\"{TNG_BASE_URL}{rid}{TNG_SUFFIX}\" target=\"_blank\"><b>{m_to}</b></a>'\n",
        "\n",
        "    return f\"{linked_name} matches {nm} as a {cm} cM relative; they share a Yates ancestral line descending from {anc}.\"\n",
        "\n",
        "long_header = \"Participants who tested-Who they matched-Oldest known Yates ancestor\"\n",
        "df[long_header] = df.apply(build_narrative, axis=1)\n",
        "\n",
        "# Timezone\n",
        "est = pytz.timezone('US/Eastern')\n",
        "timestamp_str = datetime.now(est).strftime(\"%B %d, %Y %-I:%M %p EST\")\n",
        "\n",
        "# --- 6. HTML FACTORY ---\n",
        "def make_page(title, content, count, active_view=\"ancestor\"):\n",
        "    # Toggle Links\n",
        "    style_anc = 'font-weight:bold; color:#006064;' if active_view == 'ancestor' else 'color:#00acc1; text-decoration:none;'\n",
        "    style_par = 'font-weight:bold; color:#006064;' if active_view == 'participant' else 'color:#00acc1; text-decoration:none;'\n",
        "\n",
        "    toggle_html = f\"\"\"\n",
        "    <div style=\"text-align:center; padding: 10px; margin-bottom: 10px; font-family: sans-serif; font-size: 14px; background: #e0f7fa; border: 1px solid #b2ebf2;\">\n",
        "        <strong>Sort View:</strong> &nbsp;\n",
        "        <a href=\"ons_yates_dna_register.shtml\" style=\"{style_anc}\">By Ancestral Line</a> &nbsp;|&nbsp;\n",
        "        <a href=\"ons_yates_dna_register_participants.shtml\" style=\"{style_par}\">By Participant Name</a>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "\n",
        "    head = f\"\"\"<div style=\"background:#f4f4f4; border-top:1px solid #ddd; border-bottom:1px solid #ddd; font-family:sans-serif; font-size:12px; color:#555; padding:8px 15px; text-align:center; margin-bottom:0;\"><strong>Last updated:</strong> {timestamp_str} &nbsp;|&nbsp; <strong>Autosomal matches:</strong> {vitals_total:,} &nbsp;|&nbsp; <strong>Showing:</strong> {count:,}</div>\"\"\"\n",
        "    nav = r\"\"\"<style>nav.oldnav ul { background-color: #006064 !important; border-bottom: 2px solid #00acc1 !important; margin:0; } nav.oldnav a { color: #e0f7fa !important; } nav.oldnav a:hover { background-color: #00838f !important; }</style><nav class=\"oldnav\"><ul><li><a href=\"contents.shtml\">Contents</a></li><li><a href=\"yates_ancestor_register.shtml\">DNA Register</a></li><li><a href=\"match_count.shtml\">Match Count</a></li><li><a href=\"lineage_count.shtml\">Lineage Count</a></li><li><a href=\"dna_network.shtml\">DNA Network</a></li><li><a href=\"just-trees.shtml\">Trees</a></li><li><a href=\"subscribe_updates.shtml\">Subscribe Updates</a></li><li><a href=\"share_matches.shtml\">Share Your Matches</a></li><li><a href=\"gedmatchkits.htm\">Gedmatch Kits</a></li><li><a href=\"cousin_list_print.htm\">Print Cousin List</a></li><li><a href=\"yates_ancestor_register.csv\">Download CSV</a></li><li><a href=\"yates_ancestor_register.xlsx\">Download Excel</a></li><li><a href=\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\">Study Details</a></li><li><a href=\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\">Theory in Action</a></li></ul></nav>\"\"\"\n",
        "\n",
        "    style_overrides = \"\"\"\n",
        "    <style>\n",
        "        .table-scroll-wrapper { text-align: center; }\n",
        "        #reg-table { margin: 0 auto; width: 90%; }\n",
        "        #reg-table th { text-align: center !important; position:sticky; top:0; z-index:6; background:#fff; }\n",
        "        #reg-table td { text-align: left; padding: 8px 15px; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    return f\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"iso-8859-15\"><title>{title}</title><link rel=\"stylesheet\" href=\"partials_unified.css\"><link rel=\"stylesheet\" href=\"dna_tree_styles.css\">{style_overrides}</head><body id=\"top\"><div class=\"wrap\"><h1 class=\"centerline\">{title}</h1><div id=\"nav-slot\">{head}{nav}</div>{toggle_html}{content}</div></body></html>\"\"\"\n",
        "\n",
        "print(\"\\n[4] Rendering HTML...\")\n",
        "\n",
        "# VIEW 1: ANCESTOR SORT (Default)\n",
        "print(\"    - Generating Ancestor View...\")\n",
        "if \"fa_1 masked\" in df.columns:\n",
        "    # Sort Z-A by Ancestor Slug\n",
        "    df.sort_values(by=['fa_1 masked', 'Match to'], ascending=[False, True], inplace=True)\n",
        "tbl_anc = df.to_html(columns=[long_header], index=False, border=1, classes=\"dataframe sortable\", escape=False, table_id=\"reg-table\")\n",
        "html_anc = make_page(\"ONS Yates Study DNA Register (By Ancestor)\", f'<div class=\"table-scroll-wrapper\">{tbl_anc}</div>', len(df), \"ancestor\")\n",
        "\n",
        "# VIEW 2: PARTICIPANT SORT (Surname)\n",
        "print(\"    - Generating Participant View...\")\n",
        "# Create temp sort key for Surname\n",
        "df['_sort_key'] = df[\"Match to\"].astype(str).apply(lambda x: x.strip().split()[-1] if x.strip() else \"\")\n",
        "df.sort_values(by=['_sort_key', 'Match to'], ascending=[True, True], inplace=True)\n",
        "tbl_par = df.to_html(columns=[long_header], index=False, border=1, classes=\"dataframe sortable\", escape=False, table_id=\"reg-table\")\n",
        "html_par = make_page(\"ONS Yates Study DNA Register (By Participant)\", f'<div class=\"table-scroll-wrapper\">{tbl_par}</div>', len(df), \"participant\")\n",
        "\n",
        "\n",
        "# Other Pages\n",
        "tree_html = make_page(\"Ancestor Register (Trees View)\", f'<div class=\"table-scroll-wrapper\">{tbl_anc.replace(\"reg-table\", \"refactor-table\")}</div>', len(df), \"ancestor\")\n",
        "net_rows = \"\".join([f\"<tr><td>{r.get('Match to','')}</td><td>{html.escape(str(r.get('FirstAncestor_pair cojoined', r.get('Authority_FirstAncestor',''))))}</td><td>Yes</td><td>{html.escape(str(r.get('Name','')))} via ...</td><td>{html.escape(str(r.get('Yates DNA Ancestral Line','')))}</td></tr>\" for _, r in df.iterrows()])\n",
        "net_html = make_page(\"DNA Network\", f'<table id=\"reg-list\" class=\"sortable\" border=\"1\"><thead><tr><th>Match to</th><th>First Ancestor</th><th>Include</th><th>Summary</th><th>Lineage</th></tr></thead><tbody>{net_rows}</tbody></table>', len(df), \"\")\n",
        "\n",
        "# Stats\n",
        "grp = 'FirstAncestor_pair cojoined' if 'FirstAncestor_pair cojoined' in df.columns else 'Authority_FirstAncestor'\n",
        "lin = df[grp].value_counts().reset_index(); lin.columns = ['First Ancestor', 'Count']\n",
        "lin_html = make_page(\"Lineage Count Report\", f'<div class=\"centerline\"><p>Total Lines: {len(lin)}</p></div>{lin.to_html(index=False, border=1, classes=\"dataframe sortable\")}', len(lin), \"\")\n",
        "mat = df['Name'].value_counts().reset_index(); mat.columns = ['Participant', 'Entries']\n",
        "mat_html = make_page(\"Match Count Report\", f'<div class=\"centerline\"><p>Participants: {len(mat)}</p></div>{mat.to_html(index=False, border=1, classes=\"dataframe sortable\")}', len(mat), \"\")\n",
        "\n",
        "# --- 7. UPLOAD & VERIFY ---\n",
        "print(f\"\\n[5] Uploading to {FTP_HOST}...\")\n",
        "ftps = FTP_TLS(timeout=30); ftps.connect(FTP_HOST, FTP_PORT); ftps.login(FTP_USER, FTP_PASS); ftps.prot_p(); ftps.set_pasv(True)\n",
        "\n",
        "try: ftps.cwd(f\"/{REMOTE_DIR}\")\n",
        "except:\n",
        "    try: ftps.cwd(f\"/public_html/{REMOTE_DIR}\")\n",
        "    except: pass\n",
        "print(f\"    - Target Directory: {ftps.pwd()}\")\n",
        "\n",
        "uploads = {\n",
        "    \"yates_ancestor_register.shtml\": html_anc,              # Main File (Ancestor Sort)\n",
        "    \"ons_yates_dna_register.shtml\": html_anc,               # Alias (Ancestor Sort)\n",
        "    \"ons_yates_dna_register_participants.shtml\": html_par,  # NEW: Participant Sort\n",
        "    \"just-trees.shtml\": tree_html,\n",
        "    \"dna_network.shtml\": net_html,\n",
        "    \"lineage_count.shtml\": lin_html,\n",
        "    \"match_count.shtml\": mat_html,\n",
        "    CSV_INPUT: None\n",
        "}\n",
        "\n",
        "for fn, content in uploads.items():\n",
        "    if content:\n",
        "        with open(fn, \"w\", encoding=\"iso-8859-15\") as f: f.write(content)\n",
        "    src = fn if content else CSV_INPUT\n",
        "    if os.path.exists(src):\n",
        "        with open(src, \"rb\") as fh: ftps.storbinary(f\"STOR {fn}\", fh)\n",
        "        print(f\"    - Uploaded {fn}\")\n",
        "\n",
        "print(f\"\\n[6] Server Verification ({ftps.pwd()}):\")\n",
        "try:\n",
        "    file_list = ftps.nlst()\n",
        "    relevant_files = [f for f in file_list if f in uploads.keys()]\n",
        "    for f in relevant_files:\n",
        "        print(f\"    [OK] {f}\")\n",
        "        print(f\"         {WEB_BASE_URL}{f}\")\n",
        "    print(f\"    (Total files in directory: {len(file_list)})\")\n",
        "except Exception as e:\n",
        "    print(f\"    [WARN] Could not list directory: {e}\")\n",
        "\n",
        "ftps.quit()\n",
        "print(\"\\n[DONE] Dual Views Published.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmD-rdq_pbt9",
        "outputId": "25ac52da-0a64-47cf-ee70-bfd681577d6e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "      CELL 2: PRODUCTION PUBLISHER (DUAL VIEWS)\n",
            "============================================================\n",
            "[1] Credentials Loaded for: ftp.one-name.net\n",
            "\n",
            "[2] Fetching Authority Key from Server...\n",
            "    - [SUCCESS] Pulled latest 'match_to_unmasked.csv'.\n",
            "[3] Database Loaded: 1700 records.\n",
            "    - Vitals Recalculated: 1700\n",
            "    - Unmasking 'Match to' codes...\n",
            "    - Building Narrative Column...\n",
            "\n",
            "[4] Rendering HTML...\n",
            "    - Generating Ancestor View...\n",
            "    - Generating Participant View...\n",
            "\n",
            "[5] Uploading to ftp.one-name.net...\n",
            "    - Target Directory: /ons-study\n",
            "    - Uploaded yates_ancestor_register.shtml\n",
            "    - Uploaded ons_yates_dna_register.shtml\n",
            "    - Uploaded ons_yates_dna_register_participants.shtml\n",
            "    - Uploaded just-trees.shtml\n",
            "    - Uploaded dna_network.shtml\n",
            "    - Uploaded lineage_count.shtml\n",
            "    - Uploaded match_count.shtml\n",
            "    - Uploaded engine_database.csv\n",
            "\n",
            "[6] Server Verification (/ons-study):\n",
            "    [OK] ons_yates_dna_register.shtml\n",
            "         https://yates.one-name.net/ons-study/ons_yates_dna_register.shtml\n",
            "    [OK] engine_database.csv\n",
            "         https://yates.one-name.net/ons-study/engine_database.csv\n",
            "    [OK] yates_ancestor_register.shtml\n",
            "         https://yates.one-name.net/ons-study/yates_ancestor_register.shtml\n",
            "    [OK] match_count.shtml\n",
            "         https://yates.one-name.net/ons-study/match_count.shtml\n",
            "    [OK] just-trees.shtml\n",
            "         https://yates.one-name.net/ons-study/just-trees.shtml\n",
            "    [OK] ons_yates_dna_register_participants.shtml\n",
            "         https://yates.one-name.net/ons-study/ons_yates_dna_register_participants.shtml\n",
            "    [OK] dna_network.shtml\n",
            "         https://yates.one-name.net/ons-study/dna_network.shtml\n",
            "    [OK] lineage_count.shtml\n",
            "         https://yates.one-name.net/ons-study/lineage_count.shtml\n",
            "    (Total files in directory: 37)\n",
            "\n",
            "[DONE] Dual Views Published.\n"
          ]
        }
      ]
    }
  ]
}