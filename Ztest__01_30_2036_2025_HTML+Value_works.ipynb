{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMInQgdQrxExHHLVzA1IvV3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/Ztest__01_30_2036_2025_HTML%2BValue_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "13978130-8e09-4352-a274-ad134a415558",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ztest_01_28_2033(works!)_2025_stable\n",
        "\n",
        "# Standard Libraries\n",
        "import csv\n",
        "import glob\n",
        "from datetime import datetime\n",
        "\n",
        "# GEDCOM Parsing\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "\n",
        "# Data Processing\n",
        "import pandas as pd\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.styles import Alignment\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts first and last name from a GEDCOM record.\n",
        "    Handles missing or malformed names gracefully.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "\n",
        "    if name_start == 5 or name_end == -1:  # Meaning '1 NAME ' was not found\n",
        "        return \"UnknownName\"\n",
        "\n",
        "    name = record[name_start:name_end].strip()\n",
        "\n",
        "    # Handle cases where no '/' is present in the name\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")  # Take first 10 characters as default name\n",
        "\n",
        "    # Extract first and last name\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]  # first 10 chars\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}  # Global dictionary to hold name->ID mapping\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # weâ€™ll modify name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0  # Count YDNA occurrences\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1  # YDNA found\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "        print(f'Records with YDNA information: {ydna_count}')\n",
        "        print(f'Autosomal matches: {autosomal_count}')\n",
        "\n",
        "        # First-level filter: only those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Optional second-level filter from an Excel file\n",
        "        manual_filter_activated = True  # or False\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [\n",
        "                    dataset for dataset in self.filter_pool\n",
        "                    if dataset.get_gen_person() in manual_filtered_ids\n",
        "                ]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    # Just automatically return the first GEDCOM file found\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    # If you want a manual selection, uncomment the below while loop:\n",
        "    #\n",
        "    # while True:\n",
        "    #     for i, file in enumerate(gedcom_files, start=1):\n",
        "    #         print(f\"{i}. {file}\")\n",
        "    #     try:\n",
        "    #         selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "    #         if 1 <= selected_num <= len(gedcom_files):\n",
        "    #             return gedcom_files[selected_num - 1]\n",
        "    #         else:\n",
        "    #             print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "    #     except ValueError:\n",
        "    #         print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "################################################################################\n",
        "#          Execute GEDCOM Parsing & Build Our Filter Pool                      #\n",
        "################################################################################\n",
        "gedcom_file_path = select_gedcom_file()\n",
        "if gedcom_file_path:\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    # Gather individuals (last_name, individual_id) from the filter pool\n",
        "    individuals = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    ################################################################################\n",
        "    # Function: Extract ID from GEDCOM Record\n",
        "    ################################################################################\n",
        "    def extract_id(record):\n",
        "        \"\"\"\n",
        "        Extracts the ID from a GEDCOM record.\n",
        "        A valid ID is enclosed within '@' symbols.\n",
        "        \"\"\"\n",
        "        id_start = record.find('@') + 1\n",
        "        id_end = record.find('@', id_start)\n",
        "\n",
        "        if id_start == 0 or id_end == -1:  # If '@' is missing\n",
        "            return \"UnknownID\"\n",
        "\n",
        "        return record[id_start:id_end].strip()\n",
        "\n",
        "    # Read the GEDCOM file as raw text, parse out records\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "        data = file.read()\n",
        "\n",
        "    data = data.split('\\n0 ')  # Split records based on GEDCOM structure\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "else:\n",
        "    print(\"No GEDCOM file selected; exiting.\")\n",
        "    raise SystemExit\n",
        "\n",
        "################################################################################\n",
        "#        Functions to Traverse & Score Ancestors, Build Data for DataFrame     #\n",
        "################################################################################\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # We'll update anchor_gen1 if found\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    # Build generation_table, visited_pairs\n",
        "    find_parents(individual_id, 1, records)\n",
        "\n",
        "    # All possible ancestor paths for that ID\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "\n",
        "    filtered_ancestral_line_names = []\n",
        "\n",
        "    # Gather more info from the dataset\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "\n",
        "    # Build ancestral line (exclude anchor_gen1 itself)\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    # Reverse order\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "\n",
        "    # Check we did not accidentally include anchor_gen1\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(\n",
        "            f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\"\n",
        "        )\n",
        "\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "#         Build Rows for DataFrame from the Filter Pool                        #\n",
        "################################################################################\n",
        "combined_df_rows = []\n",
        "for dataset in gedcom_instance.filter_pool:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(\n",
        "        individual_id, gedcom_instance, records\n",
        "    )\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    individual_name = extract_name(records.get(individual_id, \"\"))\n",
        "\n",
        "    combined_df_rows.append(\n",
        "        [individual_id, sort, individual_name, cm, filtered_ancestral_line_str]\n",
        "    )\n",
        "\n",
        "################################################################################\n",
        "#       NO NEED TO MODIFY ABOVE THIS SECTION 28-1-2025                         #\n",
        "################################################################################\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "################################################################################\n",
        "#       Create and Populate Main DataFrame (combined_df)\n",
        "################################################################################\n",
        "columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "# Initialize the value_store dictionary\n",
        "value_store = {}\n",
        "\n",
        "# Populate value_store with data from the DataFrame, including a placeholder for 'Value'\n",
        "for _, row in combined_df.iterrows():\n",
        "    value_store[row[\"ID#\"]] = {\n",
        "        \"Match to\": row[\"Match to\"],\n",
        "        \"Name\": row[\"Name\"],\n",
        "        \"cM\": row[\"cM\"],\n",
        "        \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "        \"Value\": None  # Placeholder for 'Value'\n",
        "    }\n",
        "\n",
        "################################################################################\n",
        "#       Remove miscellaneous Distant ancestors (combined_df)\n",
        "################################################################################\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "    prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "\n",
        "# Order and clean up columns\n",
        "ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "combined_df = combined_df[ordered_columns]\n",
        "combined_df.index += 1\n",
        "combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "################################################################################\n",
        "# Ensure Required Columns Exist in Memory (Fix Missing Columns)\n",
        "################################################################################\n",
        "expected_columns = [\"ID#\", \"Surname\", \"Number\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "\n",
        "# Add missing columns dynamically with default values\n",
        "for col in expected_columns:\n",
        "    if col not in combined_df.columns:\n",
        "        if col in [\"Value\", \"Number\"]:\n",
        "            combined_df[col] = 0  # Default numeric columns to 0\n",
        "        else:\n",
        "            combined_df[col] = \"Unknown\"  # Default text columns to \"Unknown\"\n",
        "\n",
        "print(\"âœ… All required columns are now present in combined_df.\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "from datetime import datetime\n",
        "\n",
        "################################################################################\n",
        "#       Ensure Required Columns Exist in Memory\n",
        "################################################################################\n",
        "expected_columns = [\"ID#\", \"Surname\", \"Number\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "missing_columns = set(expected_columns) - set(combined_df.columns)\n",
        "\n",
        "# Add missing columns dynamically with default values\n",
        "for col in missing_columns:\n",
        "    if col in [\"Value\", \"Number\"]:\n",
        "        combined_df[col] = 0  # Default numeric columns to 0\n",
        "    else:\n",
        "        combined_df[col] = \"Unknown\"  # Default text columns to \"Unknown\"\n",
        "\n",
        "print(\"âœ… All required columns are now present in combined_df.\")\n",
        "\n",
        "################################################################################\n",
        "#       Segmentation and Frequency Analysis\n",
        "################################################################################\n",
        "def parse_line_to_pairs(line, delimiter=\"~~~\"):\n",
        "    \"\"\"Splits a given ancestral line into named pairs based on the delimiter.\"\"\"\n",
        "    return line.strip().split(delimiter) if pd.notna(line) else []\n",
        "\n",
        "def identify_all_shared_segments(df, ancestral_col, min_shared=2, min_size=2):\n",
        "    \"\"\"\n",
        "    Identify all possible shared segments (groups of min_size ancestral lines) across the dataset.\n",
        "    Returns a dictionary of shared segments with their frequencies.\n",
        "    \"\"\"\n",
        "    segment_counts = defaultdict(int)\n",
        "\n",
        "    # Iterate over each record\n",
        "    for _, row in df.iterrows():\n",
        "        if pd.isna(row[ancestral_col]):\n",
        "            continue  # Skip if the column is missing or NaN\n",
        "\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "\n",
        "        # Generate all possible segment combinations\n",
        "        for size in range(min_size, len(lines) + 1):\n",
        "            for subset in combinations(sorted(lines), size):\n",
        "                segment_counts[subset] += 1\n",
        "\n",
        "    # Keep only segments that meet the minimum shared count\n",
        "    shared_segments = {segment: count for segment, count in segment_counts.items() if count >= min_shared}\n",
        "\n",
        "    # Sort segments by size descending to prioritize larger segments\n",
        "    return dict(sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "def calculate_value(df, ancestral_col, shared_segments):\n",
        "    \"\"\"\n",
        "    Calculate 'Value' for each record based on multiple shared segments.\n",
        "    \"\"\"\n",
        "    sorted_segments = sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True)\n",
        "    df[\"Value\"] = 0  # Initialize value column\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if pd.isna(row[ancestral_col]):\n",
        "            df.at[idx, \"Value\"] = 0\n",
        "            continue\n",
        "\n",
        "        lines = [line.strip() for line in row[ancestral_col].split('~~~') if line.strip()]\n",
        "        value = 0\n",
        "        lines_copy = lines.copy()\n",
        "\n",
        "        for segment, freq in sorted_segments:\n",
        "            segment_list = list(segment)\n",
        "            if all(line in lines_copy for line in segment_list):\n",
        "                value += freq\n",
        "                for line in segment_list:\n",
        "                    lines_copy.remove(line)\n",
        "\n",
        "        value += len(lines_copy)\n",
        "        df.at[idx, \"Value\"] = value\n",
        "\n",
        "    return df\n",
        "\n",
        "################################################################################\n",
        "#       Main Processing\n",
        "################################################################################\n",
        "ancestral_col = \"Yates DNA Ancestral Line\"\n",
        "\n",
        "# Ensure correct column order\n",
        "ordered_columns = [\"ID#\", \"Surname\", \"Number\", \"Value\", \"Yates DNA Ancestral Line\"]\n",
        "missing_columns = set(ordered_columns) - set(combined_df.columns)\n",
        "\n",
        "if missing_columns:\n",
        "    print(f\"Error: The following columns are missing in the DataFrame: {missing_columns}\")\n",
        "else:\n",
        "    combined_df = combined_df[ordered_columns]\n",
        "    print(\"âœ… Columns reordered successfully.\")\n",
        "\n",
        "# Identify shared segments\n",
        "shared_segments_found = identify_all_shared_segments(combined_df, ancestral_col, min_shared=2, min_size=2)\n",
        "\n",
        "# Segment frequency analysis\n",
        "seg_df = pd.DataFrame(\n",
        "    [(\"~~~\".join(seg), freq) for seg, freq in shared_segments_found.items()],\n",
        "    columns=[\"Segment\", \"Frequency\"]\n",
        ").sort_values(by=\"Frequency\", ascending=False)\n",
        "\n",
        "# Display segment analysis\n",
        "print(\"\\nâœ… All Shared Segments and Frequencies:\")\n",
        "print(seg_df)\n",
        "\n",
        "# Calculate 'Value' based on shared segments\n",
        "df_final = calculate_value(combined_df, ancestral_col, shared_segments_found)\n",
        "\n",
        "# Display final DataFrame\n",
        "print(\"\\nâœ… Final DataFrame with Value:\")\n",
        "print(df_final[[\"ID#\", \"Surname\", \"Number\", \"Value\"]])\n",
        "\n",
        "# Create mapping of ID# to Value\n",
        "id_to_value_map = df_final.set_index(\"ID#\")[\"Value\"].to_dict()\n",
        "\n",
        "print(\"\\nâœ… Mapping of ID# to Value:\")\n",
        "print(id_to_value_map)\n",
        "\n",
        "################################################################################\n",
        "#       Ensure combined_df is updated before HTML output\n",
        "################################################################################\n",
        "combined_df[\"Value\"] = df_final[\"Value\"]\n",
        "\n",
        "# Sort dataset by \"Yates DNA Ancestral Line\" in descending order\n",
        "combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "\n",
        "print(\"\\nâœ… Processing Complete! ðŸš€\")\n",
        "\n",
        "################################################################################\n",
        "#       Generate HTML Output (Optional)\n",
        "################################################################################\n",
        "GENERATE_HTML = False  # Set to True if HTML output is needed\n",
        "\n",
        "if GENERATE_HTML:\n",
        "    current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "    output_html_path = f\"html_output_{current_datetime}.html\"\n",
        "\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table, th, td {\n",
        "      border: 1px solid black;\n",
        "      border-collapse: collapse;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "      text-align: center;\n",
        "    }\n",
        "    td {\n",
        "      text-align: center;\n",
        "    }\n",
        "    td:nth-child(6) {\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "\n",
        "    html_content = css_style + combined_df.to_html(\n",
        "        index=False,\n",
        "        escape=False,\n",
        "        border=0\n",
        "    )\n",
        "\n",
        "    with open(output_html_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"âœ… HTML output generated: {output_html_path}\")\n",
        "\n",
        "################################################################################\n",
        "# Export Final Data (Optional)\n",
        "################################################################################\n",
        "EXPORT_CSV = False  # Change to True if needed\n",
        "\n",
        "if EXPORT_CSV:\n",
        "    combined_df.to_csv(\"final_combined_df.csv\", index=False)\n",
        "    seg_df.to_csv(\"segments_discovered.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Data processing complete!\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95b2a4f-9060-4a1b-e498-411f09b91f55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 78\n",
            "After manual filter, total records: 79\n",
            "Records tagged and filtered by NPFX: 79\n",
            "âœ… All required columns are now present in combined_df.\n",
            "âœ… All required columns are now present in combined_df.\n",
            "âœ… Columns reordered successfully.\n",
            "\n",
            "âœ… All Shared Segments and Frequencies:\n",
            "                                                 Segment  Frequency\n",
            "38454  YatesFrancis&TichborneJane~~~YatesThomas&Steph...         69\n",
            "36807  YatesFrancis&TichborneJane~~~YatesJohn&Tetters...         69\n",
            "38452  YatesFrancis&TichborneJane~~~YatesJohn&Tetters...         69\n",
            "38463  YatesJohn&TettershalMaryEliza~~~YatesThomas&St...         69\n",
            "38450  YatesFrancis&TichborneJane~~~YatesGeorge1&Well...         68\n",
            "...                                                  ...        ...\n",
            "13162  LooneyChesterAr&TaylorGraceFlor~~~LooneyDaniel...          2\n",
            "13163  LooneyChesterAr&TaylorGraceFlor~~~LooneyDaniel...          2\n",
            "13164  LooneyChesterAr&TaylorGraceFlor~~~LooneyDaniel...          2\n",
            "13165  LooneyChesterAr&TaylorGraceFlor~~~LooneyDaniel...          2\n",
            "38866  YatesThomas&StephensDorothy~~~YatesWilliam&Cha...          2\n",
            "\n",
            "[38867 rows x 2 columns]\n",
            "\n",
            "âœ… Final DataFrame with Value:\n",
            "       ID#  Surname  Number  Value\n",
            "11  I54530  Unknown       0      6\n",
            "12  I54602  Unknown       0      6\n",
            "5   I48502  Unknown       0     17\n",
            "2   I46680  Unknown       0      7\n",
            "13  I55100  Unknown       0      8\n",
            "..     ...      ...     ...    ...\n",
            "72  I56965  Unknown       0      7\n",
            "58  I56174  Unknown       0     11\n",
            "4   I48150  Unknown       0      8\n",
            "14  I55292  Unknown       0     11\n",
            "8   I52566  Unknown       0     18\n",
            "\n",
            "[79 rows x 4 columns]\n",
            "\n",
            "âœ… Mapping of ID# to Value:\n",
            "{'I54530': 6, 'I54602': 6, 'I48502': 17, 'I46680': 7, 'I55100': 8, 'I47957': 16, 'I52228': 7, 'I52051': 8, 'I56904': 76, 'I56895': 8, 'I56858': 6, 'I56818': 19, 'I56840': 19, 'I56853': 4, 'I56848': 66, 'I56881': 10, 'I56834': 78, 'I56867': 7, 'I57208': 7, 'I56954': 2, 'I57637': 4, 'I56154': 7, 'I55749': 4, 'I57027': 13, 'I53819': 9, 'I54336': 12, 'I56188': 10, 'I56159': 67, 'I56197': 51, 'I56007': 5, 'I55942': 5, 'I55953': 49, 'I55930': 5, 'I55999': 5, 'I56018': 7, 'I55937': 13, 'I56037': 12, 'I55835': 13, 'I55796': 2, 'I55919': 2, 'I55892': 13, 'I55787': 13, 'I46501': 2, 'I55872': 3, 'I55824': 14, 'I55829': 13, 'I55803': 3, 'I55806': 3, 'I55782': 14, 'I56033': 21, 'I55852': 20, 'I55972': 21, 'I55866': 49, 'I56040': 6, 'I56045': 6, 'I56049': 3, 'I55907': 5, 'I55985': 5, 'I55960': 4, 'I55992': 5, 'I55748': 6, 'I55918': 3, 'I56023': 4, 'I55887': 3, 'I55963': 3, 'I56140': 5, 'I55815': 6, 'I55912': 4, 'I55899': 4, 'I57877': 12, 'I57086': 6, 'I55858': 5, 'I57656': 5, 'I57046': 9, 'I56965': 7, 'I56174': 11, 'I48150': 8, 'I55292': 11, 'I52566': 18}\n",
            "\n",
            "âœ… Processing Complete! ðŸš€\n",
            "âœ… Data processing complete!\n"
          ]
        }
      ]
    }
  ]
}