{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RROHlBgtXAYWOqDWQkIX8NCciZBqPcV3",
      "authorship_tag": "ABX9TyOcpzVgNflQbx8CYp2dhYr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/New_Gold__Orchestrator_20251207_1630.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIP"
      ],
      "metadata": {
        "id": "XtvXRl-lcavJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "ff0b548d-d3ca-46af-db12-80e92cb71fd3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST Cell"
      ],
      "metadata": {
        "id": "ydh7RNi7elx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 3 - Ancestor Register (Trees View; .shtml + SSI nav repair) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.02-CELL3-COL1=FIRST-ANCESTOR + LOCKIN + ENRICHED-EXCLUDE1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# - Authority source for \"First Ancestor\" is LOCKED-IN via server module:\n",
        "#     /partials/yates_authority_first_ancestor_map.py  -> AUTH_COUPLE_KEY_MAP\n",
        "# - Exclusion:\n",
        "#     Drop all couples older than: Francis Yates (1541-1588) & Jane Tichborne (1548-1580)\n",
        "#     using the SAME logic as your working Cell 3 (explicit prefix match + anchor trim).\n",
        "#\n",
        "# Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-LOCKIN+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-LOCKIN+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback, importlib.util\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "from string import Template as _T\n",
        "\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "# ---------- Display Policy ----------\n",
        "SUPPRESS_ID_COLUMN = True\n",
        "SUPPRESS_EMBEDDED_IDS_IN_TEXT = True\n",
        "\n",
        "LINEAGE_SPOUSE_SEP = \" & \"\n",
        "LINEAGE_COUPLE_SEP = \" ~ \"\n",
        "\n",
        "ALPHA_BY_FIRST_ANCESTOR_FATHER = True\n",
        "ALPHA_TIEBREAK_MOTHER_SURNAME  = True\n",
        "\n",
        "# ---------- Enriched exclusion prefix (formatted lineage) ----------\n",
        "# This is the exact prefix to strip from the formatted lineage display.\n",
        "# It intentionally ends with a trailing \"~\" to remove the separator too.\n",
        "ENRICHED_EXCLUDE_PREFIX = (\n",
        "    \"John Yates (1430-) & Still Searching ~ \"\n",
        "    \"William Yates (1389-1440) & Still Searching ~ \"\n",
        "    \"William Yates (1420-) & Still Searching ~ \"\n",
        "    \"Edmund Yates (1445-1472) & Margaret Cornell ~ \"\n",
        "    \"Richard Yates (1440-1498) & Joan Ashendon (1445-1499) ~ \"\n",
        "    \"John Yates (1471-1544) & Alice Hyde (1498-1523) ~ \"\n",
        "    \"Thomas Yates (1509-1565) & Elizabeth Fauconer (-1562) ~\"\n",
        ")\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip().strip(\"/\")\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "OUTPUT_NAME = \"just-trees.shtml\"\n",
        "REMOTE_HTML = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "DNA_CSS_HREF     = \"/partials/dna_tree_styles.css\"\n",
        "DNA_CSS_VERSION  = \"v2025-11-23-g3\"\n",
        "UNIFIED_CSS_HREF = \"/partials/partials_unified.css\"\n",
        "UNIFIED_CSS_VER  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "\n",
        "HEAD_LINK = (\n",
        "    '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />\\n'\n",
        "    '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />'\n",
        ") % (DNA_CSS_HREF, DNA_CSS_VERSION, UNIFIED_CSS_HREF, UNIFIED_CSS_VER)\n",
        "\n",
        "TABLE_WIDTH_PX = 5550\n",
        "\n",
        "# ---------- Authority LOCK-IN module (server) ----------\n",
        "AUTH_MODULE_REMOTE_DIR = \"partials\"\n",
        "AUTH_MODULE_BASENAME   = \"yates_authority_first_ancestor_map.py\"\n",
        "AUTH_MODULE_REMOTE     = posixpath.join(AUTH_MODULE_REMOTE_DIR, AUTH_MODULE_BASENAME)\n",
        "AUTH_MODULE_LOCAL      = \"yates_authority_first_ancestor_map.server.py\"\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV: %s rows=%d, cols=%d\" % (INPUT_CSV, len(df), len(df.columns)))\n",
        "\n",
        "if \"haplogroup\" not in df.columns:\n",
        "    df[\"haplogroup\"] = \"\"\n",
        "else:\n",
        "    df[\"haplogroup\"] = df[\"haplogroup\"].fillna(\"\")\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX = 0\n",
        "B_IDX = 1\n",
        "C_IDX = 2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\", \" \")\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
        "    return t.lower()\n",
        "\n",
        "LOCAL_RESOLVER = \"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER = \"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_file_from_server(remote_dir, basename, local_out):\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", \"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try:\n",
        "                        ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            ftps.mkd(p)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        ftps.cwd(p)\n",
        "            try:\n",
        "                ftps.cwd(remote_dir)\n",
        "            except Exception:\n",
        "                pass\n",
        "            with open(local_out, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % basename, f.write)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Server pull failed for %s/%s: %s\" % (remote_dir, basename, e))\n",
        "        return False\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path))\n",
        "        return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    ok = _pull_file_from_server(\"partials\", \"match_to_unmasked.csv\", \"match_to_unmasked.csv\")\n",
        "    if ok:\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    return local_path\n",
        "\n",
        "LOCAL_RESOLVER = _pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _read_csv_anyenc(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    dfx = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            dfx = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            dfx = None\n",
        "    if dfx is None:\n",
        "        raise RuntimeError(\"Unable to read CSV %s: %s\" % (path, last))\n",
        "    return dfx\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "    last = None\n",
        "    m = None\n",
        "    for enc in (\"utf-8-sig\", \"iso-8859-15\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            m = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            m = None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last)\n",
        "        return {}\n",
        "    cols = {c.lower(): c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\")\n",
        "        return {}\n",
        "    m = m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"] = m[cols[\"code\"]].map(_norm_code)\n",
        "    m[\"__val__\"] = m[cols[\"unmasked\"]].astype(str)\n",
        "    m = m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map = _load_resolver_to_map(LOCAL_RESOLVER)\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw = df.iloc[:, B_IDX].astype(str)\n",
        "masked_key = masked_raw.map(_norm_code)\n",
        "resolved   = masked_key.map(resolver_map)\n",
        "df.iloc[:, C_IDX] = resolved.fillna(\"\")\n",
        "\n",
        "print(\n",
        "    \"[OK] Column B -> C mapping: %d / %d  unmatched: %d\"\n",
        "    % (int(resolved.notna().sum()), len(df), len(df) - int(resolved.notna().sum()))\n",
        ")\n",
        "\n",
        "# ---------- Lineage formatting helpers ----------\n",
        "ID_TOKEN_RE = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "COUPLE_SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|/{2,}|\\|{2,}|~{2,})\\s*\", re.I)\n",
        "SPOUSE_SPLIT_RE = re.compile(r\"\\s*(?:&| and | AND |\\+)\\s*\", re.I)\n",
        "\n",
        "def _scrub_side_keep_name_years(side_text):\n",
        "    s = str(side_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)~([^~]+?)~(\\d{4}\\s*-\\s*(?:\\d{4})?)$\", s, flags=re.I)\n",
        "    if m:\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", (m.group(2) or \"\").strip())\n",
        "        yrs = re.sub(r\"\\s+\", \"\", (m.group(3) or \"\").strip())\n",
        "        return (\"%s (%s)\" % (name, yrs)) if name and yrs else (name or \"\")\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)~([^~]+?)(?:~([^~]+?))?$\", s, flags=re.I)\n",
        "    if m:\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", (m.group(2) or \"\").strip())\n",
        "        tail = re.sub(r\"\\s{2,}\", \" \", (m.group(3) or \"\").strip())\n",
        "        if tail and re.search(r\"\\d{4}\", tail):\n",
        "            tail = re.sub(r\"\\s+\", \"\", tail)\n",
        "            return (\"%s (%s)\" % (name, tail)) if name else \"\"\n",
        "        if tail and name:\n",
        "            return (\"%s %s\" % (name, tail)).strip()\n",
        "        return (name or tail or \"\").strip()\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", s, flags=re.I)\n",
        "    if m:\n",
        "        rest = (m.group(2) or \"\").strip()\n",
        "        yrs = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs = re.sub(r\"\\s+\", \"\", m2.group(1))\n",
        "            rest = rest[:m2.start()].strip()\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", rest).strip()\n",
        "        return (\"%s (%s)\" % (name, yrs)) if name and yrs else (name or \"\")\n",
        "\n",
        "    s = ID_TOKEN_RE.sub(\"\", s).replace(\"~\", \" \")\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# ---------- Enriched exclusion (drop very old lead-in couples) ----------\n",
        "def _norm_couple_for_match(s: str) -> str:\n",
        "    return re.sub(r\"\\s{2,}\", \" \", str(s or \"\")).strip().lower()\n",
        "\n",
        "_EXCLUDE_COUPLES = [\n",
        "    c.strip()\n",
        "    for c in re.split(r\"\\s*~\\s*\", (ENRICHED_EXCLUDE_PREFIX or \"\").strip().strip(\"~\"))\n",
        "    if c and c.strip()\n",
        "]\n",
        "\n",
        "def _strip_paren_years_anywhere(s: str) -> str:\n",
        "    return re.sub(r\"\\([^)]*\\)\", \"\", str(s or \"\")).strip()\n",
        "\n",
        "def _is_anchor_couple(couple_text: str) -> bool:\n",
        "    t = _strip_paren_years_anywhere(couple_text).lower()\n",
        "    return (\"francis yates\" in t) and (\"jane tichborne\" in t)\n",
        "\n",
        "def _apply_enriched_exclusion(joined: str) -> str:\n",
        "    s = str(joined or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "\n",
        "    couples = [c.strip() for c in s.split(LINEAGE_COUPLE_SEP) if c and c.strip()]\n",
        "    if not couples:\n",
        "        return s\n",
        "\n",
        "    # (A) Exact prefix removal by couple list match (robust to spacing)\n",
        "    if _EXCLUDE_COUPLES and len(couples) >= len(_EXCLUDE_COUPLES):\n",
        "        ok = True\n",
        "        for i in range(len(_EXCLUDE_COUPLES)):\n",
        "            if _norm_couple_for_match(couples[i]) != _norm_couple_for_match(_EXCLUDE_COUPLES[i]):\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            couples = couples[len(_EXCLUDE_COUPLES):]\n",
        "\n",
        "    # (B) Anchor trim if prefix did not match (or if data varies)\n",
        "    if couples:\n",
        "        for i, c in enumerate(couples):\n",
        "            if _is_anchor_couple(c):\n",
        "                couples = couples[i:]\n",
        "                break\n",
        "\n",
        "    return LINEAGE_COUPLE_SEP.join([c for c in couples if c]).strip()\n",
        "\n",
        "def _format_lineage_cell(text):\n",
        "    s = str(text or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "    couples = [t.strip() for t in COUPLE_SEP_RE.split(s) if t and t.strip()]\n",
        "    if not couples:\n",
        "        couples = [s]\n",
        "    out_couples = []\n",
        "    for c in couples:\n",
        "        parts = [p.strip() for p in SPOUSE_SPLIT_RE.split(c, maxsplit=1) if p and p.strip()]\n",
        "        if len(parts) == 2:\n",
        "            father = _scrub_side_keep_name_years(parts[0]).strip()\n",
        "            mother = _scrub_side_keep_name_years(parts[1]).strip()\n",
        "            couple = (father + LINEAGE_SPOUSE_SEP + mother).strip()\n",
        "        else:\n",
        "            couple = _scrub_side_keep_name_years(c).strip()\n",
        "        couple = re.sub(r\"\\s{2,}\", \" \", couple).strip()\n",
        "        out_couples.append(couple)\n",
        "    joined = (LINEAGE_COUPLE_SEP.join([c for c in out_couples if c])).strip()\n",
        "    joined = re.sub(r\"\\s{2,}\", \" \", joined).strip()\n",
        "\n",
        "    # Apply enriched early-ancestor truncation (Francis Yates & Jane Tichborne focus)\n",
        "    joined = _apply_enriched_exclusion(joined)\n",
        "\n",
        "    return joined\n",
        "\n",
        "def _maybe_format_lineage_columns(df_in):\n",
        "    if not SUPPRESS_EMBEDDED_IDS_IN_TEXT:\n",
        "        return df_in\n",
        "    df_out = df_in.copy()\n",
        "    pat = re.compile(r\"(ancestral|lineage|tree|path|ancestor|line)\", re.I)\n",
        "    cols = [c for c in df_out.columns if pat.search(str(c or \"\")) and str(c).strip().lower() != 'first ancestor']\n",
        "    if not cols:\n",
        "        for c in df_out.columns:\n",
        "            try:\n",
        "                ser = df_out[c].astype(str)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if ser.str.contains(r\"\\bI\\d+~\", regex=True, na=False).any() or ser.str.contains(r\"\\bI\\d+\\b\", regex=True, na=False).any():\n",
        "                if str(c).strip().lower() != 'first ancestor':\n",
        "                    cols.append(c)\n",
        "    cols = list(dict.fromkeys(cols))\n",
        "    if not cols:\n",
        "        return df_out\n",
        "    for c in cols:\n",
        "        try:\n",
        "            df_out[c] = df_out[c].astype(str).map(_format_lineage_cell)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"[OK] Lineage formatting applied to columns:\", \", \".join([str(c) for c in cols]))\n",
        "    return df_out\n",
        "\n",
        "def _strip_years(name_text):\n",
        "    s = str(name_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s*\\([^)]*\\)\\s*$\", \"\", s).strip()\n",
        "\n",
        "def _first_last_tokens(person_text):\n",
        "    s = _strip_years(person_text)\n",
        "    parts = s.split()\n",
        "    if not parts:\n",
        "        return (\"\", \"\")\n",
        "    return (parts[0], parts[-1])\n",
        "\n",
        "def _first_ancestor_sort_key(lineage_text):\n",
        "    s = str(lineage_text or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\")\n",
        "    first_couple = s.split(LINEAGE_COUPLE_SEP, 1)[0].strip()\n",
        "    father = \"\"\n",
        "    mother = \"\"\n",
        "    if LINEAGE_SPOUSE_SEP in first_couple:\n",
        "        father, mother = [p.strip() for p in first_couple.split(LINEAGE_SPOUSE_SEP, 1)]\n",
        "    else:\n",
        "        father = first_couple.strip()\n",
        "    f_given, f_surname = _first_last_tokens(father)\n",
        "    m_surname = \"\"\n",
        "    if ALPHA_TIEBREAK_MOTHER_SURNAME:\n",
        "        _m_given, m_surname = _first_last_tokens(mother)\n",
        "    return (f_surname.lower(), f_given.lower(), m_surname.lower())\n",
        "\n",
        "# ---------- Authority LOCK-IN load ----------\n",
        "def _download_authority_module_if_needed() -> str:\n",
        "    for p in (\"yates_authority_first_ancestor_map.py\", AUTH_MODULE_LOCAL):\n",
        "        if os.path.exists(p):\n",
        "            print(\"[OK] Using local authority module:\", os.path.abspath(p))\n",
        "            return p\n",
        "\n",
        "    if not all(os.environ.get(k) for k in (\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\")):\n",
        "        raise RuntimeError(\"Missing FTP creds; cannot download authority module %s\" % AUTH_MODULE_REMOTE)\n",
        "\n",
        "    ok = _pull_file_from_server(AUTH_MODULE_REMOTE_DIR, AUTH_MODULE_BASENAME, AUTH_MODULE_LOCAL)\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Authority module not found on server: /%s\" % AUTH_MODULE_REMOTE)\n",
        "    print(\"[OK] Pulled authority module from server -> %s\" % AUTH_MODULE_LOCAL)\n",
        "    return AUTH_MODULE_LOCAL\n",
        "\n",
        "def _import_authority_map(module_path: str) -> dict:\n",
        "    spec = importlib.util.spec_from_file_location(\"yates_authority_first_ancestor_map\", module_path)\n",
        "    if spec is None or spec.loader is None:\n",
        "        raise RuntimeError(\"Unable to import authority module from %s\" % module_path)\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)  # type: ignore\n",
        "    m = getattr(mod, \"AUTH_COUPLE_KEY_MAP\", None)\n",
        "    if not isinstance(m, dict) or not m:\n",
        "        raise RuntimeError(\"AUTH_COUPLE_KEY_MAP missing/empty in %s\" % module_path)\n",
        "    print(\"[OK] Authority map loaded from module: %d pairs\" % (len(m)//2))\n",
        "    return m\n",
        "\n",
        "AUTH_COUPLE_KEY_MAP = _import_authority_map(_download_authority_module_if_needed())\n",
        "\n",
        "# ---------- Authority key helpers ----------\n",
        "def _is_unknown_like(name_text):\n",
        "    s = str(name_text or \"\").strip().lower()\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", s).strip()\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    if not s:\n",
        "        return True\n",
        "    if s in (\"unknown\", \"none\", \"noneunknownname\"):\n",
        "        return True\n",
        "    if \"unknown name\" in s:\n",
        "        return True\n",
        "    if s.replace(\" \", \"\") in (\"noneunknownname\", \"unknownname\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _canon_lastfirst(name_text):\n",
        "    # Canonicalize to LastFirst token (no punctuation), matching authority convention.\n",
        "    if _is_unknown_like(name_text):\n",
        "        return \"unknown\"\n",
        "    s = str(name_text or \"\").strip()\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", s).strip()\n",
        "    toks = re.findall(r\"[A-Za-z0-9]+\", s.lower())\n",
        "    if not toks:\n",
        "        return \"\"\n",
        "    if len(toks) == 1:\n",
        "        return re.sub(r\"[^a-z0-9]+\", \"\", toks[0])\n",
        "    last = toks[-1]\n",
        "    first = \"\".join(toks[:-1])\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"\", last + first)\n",
        "\n",
        "def _couple_display_and_ids_from_raw_token(raw_token: str):\n",
        "    # raw_token is ONE couple from the raw lineage column (still has IDs).\n",
        "    raw = str(raw_token or \"\").strip()\n",
        "    if not raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "    parts = [p.strip() for p in SPOUSE_SPLIT_RE.split(raw, maxsplit=1) if p and p.strip()]\n",
        "    father_raw = parts[0] if len(parts) >= 1 else \"\"\n",
        "    mother_raw = parts[1] if len(parts) >= 2 else \"\"\n",
        "\n",
        "    f_id = \"\"\n",
        "    m_id = \"\"\n",
        "    mf = re.search(r\"\\b(I\\d+)\\b\", father_raw, flags=re.I)\n",
        "    if mf:\n",
        "        f_id = mf.group(1).upper()\n",
        "    mm = re.search(r\"\\b(I\\d+)\\b\", mother_raw, flags=re.I)\n",
        "    if mm:\n",
        "        m_id = mm.group(1).upper()\n",
        "\n",
        "    f_disp = _scrub_side_keep_name_years(father_raw).strip()\n",
        "    m_disp = _scrub_side_keep_name_years(mother_raw).strip()\n",
        "\n",
        "    if f_disp and m_disp:\n",
        "        disp = (f_disp + LINEAGE_SPOUSE_SEP + m_disp).strip()\n",
        "    else:\n",
        "        disp = (f_disp or m_disp or _scrub_side_keep_name_years(raw)).strip()\n",
        "\n",
        "    disp = re.sub(r\"\\s{2,}\", \" \", disp).strip()\n",
        "    return (disp, f_id, m_id)\n",
        "\n",
        "def _apply_enriched_exclusion_to_couples(couple_disps):\n",
        "    # Returns the trimmed display-couple list, using the SAME exclusion logic as the formatted lineage.\n",
        "    joined = LINEAGE_COUPLE_SEP.join([c for c in (couple_disps or []) if c and str(c).strip()])\n",
        "    joined2 = _apply_enriched_exclusion(joined)\n",
        "    if not joined2:\n",
        "        return []\n",
        "    return [c.strip() for c in joined2.split(LINEAGE_COUPLE_SEP) if c and c.strip()]\n",
        "\n",
        "def _first_ancestor_authority_value_from_raw_lineage(raw_lineage_text: str) -> str:\n",
        "    # CRITICAL: choose the FIRST COUPLE AFTER exclusion, not the oldest.\n",
        "    s = str(raw_lineage_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "\n",
        "    raw_tokens = [t.strip() for t in COUPLE_SEP_RE.split(s) if t and t.strip()]\n",
        "    if not raw_tokens:\n",
        "        raw_tokens = [s]\n",
        "\n",
        "    # Build untrimmed display list and ids list aligned to raw_tokens\n",
        "    disp_untrim = []\n",
        "    ids_untrim  = []\n",
        "    for tok in raw_tokens:\n",
        "        disp, f_id, m_id = _couple_display_and_ids_from_raw_token(tok)\n",
        "        disp_untrim.append(disp)\n",
        "        ids_untrim.append((f_id, m_id))\n",
        "\n",
        "    # Trim by display using the proven Cell 3 logic (prefix+anchor)\n",
        "    disp_trim = _apply_enriched_exclusion_to_couples(disp_untrim)\n",
        "\n",
        "    # Determine which raw token corresponds to the first trimmed couple\n",
        "    f_id = \"\"\n",
        "    m_id = \"\"\n",
        "    f_disp = \"\"\n",
        "    m_disp = \"\"\n",
        "    if disp_trim:\n",
        "        target = _norm_couple_for_match(disp_trim[0])\n",
        "        start_at = None\n",
        "        for i, d in enumerate(disp_untrim):\n",
        "            if _norm_couple_for_match(d) == target:\n",
        "                start_at = i\n",
        "                break\n",
        "        if start_at is None:\n",
        "            start_at = 0\n",
        "        # Use the ids for that couple (best possible)\n",
        "        if 0 <= start_at < len(ids_untrim):\n",
        "            f_id, m_id = ids_untrim[start_at]\n",
        "        # Also capture the display couple for fallback tokenization\n",
        "        if 0 <= start_at < len(disp_untrim):\n",
        "            first_couple_disp = disp_untrim[start_at]\n",
        "        else:\n",
        "            first_couple_disp = disp_trim[0]\n",
        "    else:\n",
        "        # nothing remained after trimming; fallback to original first couple\n",
        "        first_couple_disp = disp_untrim[0] if disp_untrim else \"\"\n",
        "\n",
        "    # Primary: authority lookup by IDs\n",
        "    if f_id and m_id:\n",
        "        k = AUTH_COUPLE_KEY_MAP.get((f_id.upper(), m_id.upper()), \"\")\n",
        "        if k:\n",
        "            return re.sub(r\"\\s+\", \"\", str(k))\n",
        "\n",
        "    # Fallback: synchronize to authority convention using LastFirst\n",
        "    if first_couple_disp and (LINEAGE_SPOUSE_SEP in first_couple_disp):\n",
        "        a, b = [p.strip() for p in first_couple_disp.split(LINEAGE_SPOUSE_SEP, 1)]\n",
        "        return _canon_lastfirst(a) + \"&\" + _canon_lastfirst(b)\n",
        "    if first_couple_disp:\n",
        "        return _canon_lastfirst(first_couple_disp)\n",
        "    return \"\"\n",
        "\n",
        "# ---------- Vitals ----------\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "LAST_UPDATED_TEXT  = \"\"\n",
        "AUTOSOMAL_MATCHES  = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    s = str(raw or \"\").strip()\n",
        "    if not s:\n",
        "        return \"(unknown)\"\n",
        "    s = s.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M\", \"%Y-%m-%dT%H:%M:%S\"]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(s, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24  = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12  = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, dt_est.day, dt_est.year, h12, dt_est.minute, ampm)\n",
        "\n",
        "def _format_num_with_commas(raw_val):\n",
        "    s_digits = re.sub(r\"[^0-9\\-]\", \"\", str(raw_val or \"\"))\n",
        "    if not s_digits:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return \"{:,}\".format(int(s_digits))\n",
        "    except Exception:\n",
        "        return s_digits\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global LAST_UPDATED_TEXT, AUTOSOMAL_MATCHES\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will be blank for vitals.\")\n",
        "        return\n",
        "    vdf = None\n",
        "    for enc in (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "            break\n",
        "        except Exception:\n",
        "            vdf = None\n",
        "    if vdf is None:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv\")\n",
        "        return\n",
        "    flat = [str(cell) for row in vdf.astype(str).values.tolist() for cell in row]\n",
        "    autosomal_raw = None\n",
        "    last_text = None\n",
        "    for cell in flat:\n",
        "        if autosomal_raw is None and \"Records tagged and filtered by NPFX\" in cell:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", cell)\n",
        "            if m:\n",
        "                autosomal_raw = m.group(1)\n",
        "        if last_text is None and \"LAST_UPDATED_TEXT\" in cell:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", cell)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "    if last_text is not None:\n",
        "        LAST_UPDATED_TEXT = _friendly_ts_from_utc(last_text)\n",
        "    AUTOSOMAL_MATCHES = _format_num_with_commas(autosomal_raw)\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "updated_str = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT or \"\")\n",
        "_updated_parts = [updated_str]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "UPDATED_BLOCK = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join(_updated_parts) + '</div>'\n",
        "\n",
        "NAV_BLOCK = '<div id=\"nav-slot\"><!--#include virtual=\"/partials/nav_block.shtml\" --></div>'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ---------- Display DF ----------\n",
        "display_df = df.copy()\n",
        "\n",
        "# drop ID#\n",
        "if SUPPRESS_ID_COLUMN and display_df.shape[1] >= 1:\n",
        "    display_df = display_df.drop(columns=[display_df.columns[A_IDX]], errors=\"ignore\")\n",
        "\n",
        "# Identify lineage/path column BEFORE formatting (so IDs still exist for authority lookup)\n",
        "lineage_cols_raw = [c for c in display_df.columns if re.search(r\"(ancestral|lineage|tree|path|ancestor|line)\", str(c or \"\"), re.I)]\n",
        "AUTH_LINEAGE_COL = lineage_cols_raw[0] if lineage_cols_raw else None\n",
        "\n",
        "# Compute authority first-ancestor values from the RAW lineage column,\n",
        "# but pick the FIRST COUPLE AFTER EXCLUSION (not the oldest).\n",
        "if AUTH_LINEAGE_COL:\n",
        "    fa_values = display_df[AUTH_LINEAGE_COL].astype(str).map(_first_ancestor_authority_value_from_raw_lineage)\n",
        "else:\n",
        "    fa_values = pd.Series([\"\"] * len(display_df))\n",
        "\n",
        "# REPLACE COLUMN 1 with First Ancestor values and rename header\n",
        "if display_df.shape[1] >= 1:\n",
        "    first_col_name = display_df.columns[0]\n",
        "    display_df[first_col_name] = fa_values.astype(str).map(lambda x: re.sub(r\"\\s+\", \"\", x).replace('&', '&#38;'))\n",
        "    display_df = display_df.rename(columns={first_col_name: \"First Ancestor\"})\n",
        "    print(\"[OK] Column 1 replaced with First Ancestor (authority, after exclusion).\")\n",
        "else:\n",
        "    print(\"[WARN] display_df has no columns to replace.\")\n",
        "\n",
        "# Remove any other \"First Ancestor\" columns to avoid duplication (keep the first one)\n",
        "fa_cols = [c for c in display_df.columns if str(c) == \"First Ancestor\"]\n",
        "if len(fa_cols) > 1:\n",
        "    keep_first = fa_cols[0]\n",
        "    drop_rest = fa_cols[1:]\n",
        "    display_df = display_df.drop(columns=drop_rest, errors=\"ignore\")\n",
        "    print(\"[OK] Dropped duplicate First Ancestor columns:\", \", \".join(drop_rest))\n",
        "\n",
        "# Now apply lineage formatting (removes embedded IDs AND applies enriched exclusion)\n",
        "display_df = _maybe_format_lineage_columns(display_df)\n",
        "\n",
        "# Alpha sort by first couple (kept)\n",
        "if ALPHA_BY_FIRST_ANCESTOR_FATHER:\n",
        "    lineage_cols = [c for c in display_df.columns if re.search(r\"(ancestral|lineage|tree|path|ancestor|line)\", str(c or \"\"), re.I)]\n",
        "    sort_col = lineage_cols[0] if lineage_cols else None\n",
        "    if sort_col:\n",
        "        sort_keys = display_df[sort_col].astype(str).map(_first_ancestor_sort_key)\n",
        "        display_df[\"__sort_surname__\"]    = [k[0] for k in sort_keys]\n",
        "        display_df[\"__sort_given__\"]      = [k[1] for k in sort_keys]\n",
        "        display_df[\"__sort_momsurname__\"] = [k[2] for k in sort_keys]\n",
        "\n",
        "        by_cols = [\"__sort_surname__\", \"__sort_given__\"]\n",
        "        if ALPHA_TIEBREAK_MOTHER_SURNAME:\n",
        "            by_cols.append(\"__sort_momsurname__\")\n",
        "\n",
        "        display_df = display_df.sort_values(by=by_cols, ascending=[True]*len(by_cols), kind=\"mergesort\").reset_index(drop=True)\n",
        "        display_df = display_df.drop(columns=[\"__sort_surname__\", \"__sort_given__\", \"__sort_momsurname__\"], errors=\"ignore\")\n",
        "        print(\"[OK] Alpha sort applied by first couple:\", sort_col, \"| keys=\", \",\".join(by_cols))\n",
        "\n",
        "# ---------- HTML table ----------\n",
        "visible_cols = [c for c in display_df.columns if c]\n",
        "\n",
        "table_html = display_df.to_html(\n",
        "    index=False,\n",
        "    columns=visible_cols,\n",
        "    escape=False,\n",
        "    border=1,\n",
        "    classes=\"dataframe sortable\"\n",
        ")\n",
        "\n",
        "if 'id=\"refactor-table\"' not in table_html:\n",
        "    table_html = re.sub(r\"<table([^>]*)>\", r'<table\\1 id=\"refactor-table\">', table_html, count=1)\n",
        "\n",
        "if 'class=\"dataframe sortable\"' not in table_html and \"sortable\" not in table_html:\n",
        "    table_html = table_html.replace('class=\"dataframe\"', 'class=\"dataframe sortable\"', 1)\n",
        "\n",
        "table_html = table_html.replace(\"<tbody>\\n<tr>\", \"<tbody>\\n<tr id=\\\"first-row\\\">\", 1)\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div id=\"top-scroll\" class=\"scroll-sync-top\">'\n",
        "    '<div class=\"scroll-sync-top-inner\" style=\"width:%dpx;\"></div>'\n",
        "    '</div>'\n",
        "    '<div id=\"bottom-scroll\" class=\"table-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (TABLE_WIDTH_PX, table_html)\n",
        "\n",
        "LATE_STYLE = r\"\"\"\n",
        "<style type=\"text/css\">\n",
        "#nav-slot, #nav-slot nav, #nav-slot .oldnav, #nav-slot .navbar{\n",
        "  display:block !important;\n",
        "  visibility:visible !important;\n",
        "  opacity:1 !important;\n",
        "}\n",
        "table.sortable thead{ display:table-header-group !important; visibility:visible !important; }\n",
        "table.sortable thead th{ display:table-cell !important; visibility:visible !important; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "JS_NAV_REPAIR = r\"\"\"\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function hasNavContainer(el){\n",
        "    if(!el) return false;\n",
        "    var n = el.querySelector('nav.oldnav, nav.navbar, .oldnav, .navbar');\n",
        "    return !!n;\n",
        "  }\n",
        "  function wrapFirstUL(el){\n",
        "    if(!el) return false;\n",
        "    var ul = el.querySelector('ul');\n",
        "    if(!ul) return false;\n",
        "    var nav = document.createElement('nav');\n",
        "    nav.className = 'oldnav';\n",
        "    nav.appendChild(ul);\n",
        "    while(el.firstChild){ el.removeChild(el.firstChild); }\n",
        "    el.appendChild(nav);\n",
        "    return true;\n",
        "  }\n",
        "  function looksLikeSSICommentOnly(el){\n",
        "    if(!el) return true;\n",
        "    var txt = (el.textContent || '').replace(/\\s+/g,'').toLowerCase();\n",
        "    if(!txt) return true;\n",
        "    if(txt.indexOf('<!--#include') >= 0) return true;\n",
        "    return false;\n",
        "  }\n",
        "  function injectRemoteNav(el){\n",
        "    try{\n",
        "      var xhr = new XMLHttpRequest();\n",
        "      xhr.open('GET', '/partials/nav_block.shtml', true);\n",
        "      xhr.onreadystatechange = function(){\n",
        "        if(xhr.readyState === 4){\n",
        "          if(xhr.status >= 200 && xhr.status < 300){\n",
        "            el.innerHTML = xhr.responseText;\n",
        "            if(!hasNavContainer(el)){\n",
        "              wrapFirstUL(el);\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      };\n",
        "      xhr.send(null);\n",
        "    }catch(e){}\n",
        "  }\n",
        "\n",
        "  function repairNav(){\n",
        "    var slot = document.getElementById('nav-slot');\n",
        "    if(!slot) return;\n",
        "\n",
        "    if(looksLikeSSICommentOnly(slot)){\n",
        "      injectRemoteNav(slot);\n",
        "      return;\n",
        "    }\n",
        "    if(!hasNavContainer(slot)){\n",
        "      wrapFirstUL(slot);\n",
        "      return;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if(document.readyState === 'loading'){\n",
        "    document.addEventListener('DOMContentLoaded', repairNav, false);\n",
        "  } else {\n",
        "    repairNav();\n",
        "  }\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "page_tpl = _T(r\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "$LATE_STYLE\n",
        "<style type=\"text/css\">\n",
        "/* Sticky second column (index 2) for Trees table */\n",
        "#refactor-table th:nth-child(2),\n",
        "#refactor-table td:nth-child(2){\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "#refactor-table th:nth-child(2){\n",
        "  z-index:7;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '').replace(/\\s+/g,' ').trim().toLowerCase();\n",
        "  }\n",
        "  function sortTable(tbl, colIndex, dir, keyColIndex){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "    var kIdx = (typeof keyColIndex === 'number') ? keyColIndex : colIndex;\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[kIdx]), B = textOf(b.cells[kIdx]);\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\-]/g,''));\n",
        "      if(!isNaN(nA) && !isNaN(nB)){ return asc ? (nA-nB) : (nB-nA); }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dir = 'asc';\n",
        "      th.addEventListener('click', function(){\n",
        "        dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "        var hdr = (th.textContent || th.innerText || '');\n",
        "        hdr = hdr.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'').replace(/\\s+/g,' ').trim().toLowerCase();\n",
        "        // Golden rule sync: sorting the lineage column uses First Ancestor (authority) as the key.\n",
        "        // Column 1 is 'First Ancestor' (index 0) in this build.\n",
        "        var keyColIndex = null;\n",
        "        if(hdr === 'yates dna ancestral line'){\n",
        "          keyColIndex = 0;\n",
        "        }\n",
        "\n",
        "        for (var j = 0; j < ths.length; j++){\n",
        "          ths[j].innerHTML = ths[j].innerHTML.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'');\n",
        "        }\n",
        "        th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl, idx, dir, (keyColIndex === null ? undefined : keyColIndex));\n",
        "      }, false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\-]/g,''),10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){ return String(n||''); }\n",
        "  }\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\+/g,' ')) : '';\n",
        "  }\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb = tbl.tBodies[0];\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200 ? 'block' : 'none'); }\n",
        "    toggle();\n",
        "    window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "  function bindSyncedScrollbars(){\n",
        "    var topScroll    = document.getElementById('top-scroll');\n",
        "    var bottomScroll = document.getElementById('bottom-scroll');\n",
        "    if(!(topScroll && bottomScroll)) return;\n",
        "    var syncing = false;\n",
        "    topScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      bottomScroll.scrollLeft = topScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "    bottomScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      topScroll.scrollLeft = bottomScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindBackToTop();\n",
        "    bindSearch();\n",
        "    bindSyncedScrollbars();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "$JS_NAV_REPAIR\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    LATE_STYLE=LATE_STYLE,\n",
        "    JS_NAV_REPAIR=JS_NAV_REPAIR,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER=SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df = display_df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w:\n",
        "        export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for seg in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host = os.environ.get(\"FTP_HOST\")\n",
        "ftp_user = os.environ.get(\"FTP_USER\")\n",
        "ftp_pass = os.environ.get(\"FTP_PASS\")\n",
        "ftp_port = int(os.environ.get(\"FTP_PORT\", \"21\") or \"21\")\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            with open(OUTPUT_NAME, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            with open(LOCAL_CSV, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/yates_authority_first_ancestor_map.py; enriched prefix exclusion applied BEFORE choosing first couple) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9c8-4rPUG21",
        "outputId": "262c6182-3517-4117-e182-920a272da01d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-LOCKIN+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=93, cols=6\n",
            "Using resolver: /content/match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 93 / 93  unmatched: 0\n",
            "[OK] Using local authority module: /content/yates_authority_first_ancestor_map.py\n",
            "[OK] Authority map loaded from module: 25 pairs\n",
            "[OK] Column 1 replaced with First Ancestor (authority, after exclusion).\n",
            "[OK] Lineage formatting applied to columns: Yates DNA Ancestral Line\n",
            "[OK] Alpha sort applied by first couple: First Ancestor | keys= __sort_surname__,__sort_given__,__sort_momsurname__\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/yates_authority_first_ancestor_map.py; enriched prefix exclusion applied BEFORE choosing first couple) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 0"
      ],
      "metadata": {
        "id": "pV0T5zIN0qvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.01.31-UNIFIED)\n",
        "# - Complete & runnable Colab cell: one contiguous block, no fragments.\n",
        "# - Source ASCII-only; any file writes must use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Authority:\n",
        "#     * Colab work dir: /content\n",
        "#     * Scripts pulled from: /partials/colab_cells/*.py  (server)\n",
        "#     * Latest GEDCOM pulled from: /tng/gedcom/*.ged     (server) if no local GEDCOM exists\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2026.01.31-UNIFIED | Encoding=ISO-8859-15\n",
        "# - Execution order (after pulls): cell1.py, cell2.py, cell2b.py, cell2c.py, cell2k.py, cell3.py\n",
        "# - IMPORTANT: Scripts are executed via \"python3 -u script.py\" (NOT exec/compile) to avoid\n",
        "#   multiprocessing pickling failures under ProcessPoolExecutor.\n",
        "# ====================================================================\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2026.01.31-UNIFIED | Encoding=ISO-8859-15\")\n",
        "\n",
        "DECLARED_LINES = 999  # audit-only; not enforced\n",
        "print(\"[AUDIT] DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "import os\n",
        "import socket\n",
        "import traceback\n",
        "import hashlib\n",
        "import glob\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "\n",
        "# ---------- 0) Env / secrets ----------\n",
        "\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\", \"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\", \"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\", \"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\", \"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\", \"\") or \"\").strip().strip(\"/\")\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\n",
        "    \"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\"\n",
        "    % (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\")\n",
        ")\n",
        "\n",
        "if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "    raise SystemExit(\"[FATAL] Missing FTP_HOST/FTP_USER/FTP_PASS; cannot run orchestrator.\")\n",
        "\n",
        "# ---------- 1) FTPS helpers ----------\n",
        "\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _ftps_connect():\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()  # Explicit FTPS\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for seg in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.cwd(seg)\n",
        "            except all_errors:\n",
        "                try:\n",
        "                    ftps.mkd(seg)\n",
        "                except all_errors:\n",
        "                    pass\n",
        "                ftps.cwd(seg)\n",
        "    return ftps\n",
        "\n",
        "def _sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "def _safe_nlst(ftps):\n",
        "    try:\n",
        "        return ftps.nlst()\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "# ---------- 2) Pull authority scripts from /partials/colab_cells ----------\n",
        "\n",
        "SCRIPT_REMOTE_DIR = \"/partials/colab_cells\"\n",
        "SCRIPT_NAMES = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell2k.py\", \"cell3.py\"]\n",
        "\n",
        "def pull_authority_scripts():\n",
        "    print(\"[STEP] Pulling authority scripts from server ...\")\n",
        "    pulled = 0\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        try:\n",
        "            pwd0 = ftps.pwd()\n",
        "        except Exception:\n",
        "            pwd0 = \"(unknown)\"\n",
        "        print(\"[OK] Connected via FTPS (explicit AUTH TLS).\")\n",
        "        print(\"[INFO] Initial PWD on server: %s\" % pwd0)\n",
        "\n",
        "        # Navigate to script dir\n",
        "        try:\n",
        "            try:\n",
        "                ftps.cwd(\"/\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            for seg in [p for p in SCRIPT_REMOTE_DIR.split(\"/\") if p]:\n",
        "                ftps.cwd(seg)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Unable to cwd to %s (%s)\" % (SCRIPT_REMOTE_DIR, e))\n",
        "\n",
        "        print(\"[INFO] Using remote dir for scripts: %s\" % SCRIPT_REMOTE_DIR)\n",
        "\n",
        "        listing = _safe_nlst(ftps)\n",
        "        if listing:\n",
        "            print(\"[INFO] Remote listing sample (first 25): %s\" % \", \".join(listing[:25]))\n",
        "        else:\n",
        "            print(\"[WARN] Remote listing is empty/unavailable for %s\" % SCRIPT_REMOTE_DIR)\n",
        "\n",
        "        os.chdir(\"/content\")\n",
        "        for name in SCRIPT_NAMES:\n",
        "            try:\n",
        "                local_path = os.path.join(\"/content\", name)\n",
        "                with open(local_path, \"wb\") as f:\n",
        "                    ftps.retrbinary(\"RETR \" + name, f.write)\n",
        "                sz = os.path.getsize(local_path)\n",
        "                sh = _sha256_of_file(local_path)\n",
        "                print(\"[PULL] %s -> %s  size=%d  sha256=%s\" % (name, local_path, sz, sh))\n",
        "                pulled += 1\n",
        "            except Exception as e:\n",
        "                print(\"[MISS] Could not pull %s: %s\" % (name, e))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Pulled %d script(s) from authority shelf.\" % pulled)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Script pull failed:\", e)\n",
        "        traceback.print_exc()\n",
        "    return pulled\n",
        "\n",
        "# ---------- 3) GEDCOM selection: prefer local, else pull latest ----------\n",
        "\n",
        "GEDCOM_REMOTE_DIR = \"/tng/gedcom\"\n",
        "LOCAL_GED_DIR = \"/content\"\n",
        "\n",
        "def _pick_local_gedcom_if_present():\n",
        "    \"\"\"\n",
        "    Prefer any *.ged already in /content.\n",
        "    Deterministic: newest mtime, tie-breaker lexicographic.\n",
        "    \"\"\"\n",
        "    os.chdir(LOCAL_GED_DIR)\n",
        "    geds = glob.glob(\"*.ged\")\n",
        "    if not geds:\n",
        "        return None\n",
        "    def _key(p):\n",
        "        try:\n",
        "            return (float(os.path.getmtime(p)), str(p))\n",
        "        except Exception:\n",
        "            return (0.0, str(p))\n",
        "    geds.sort(key=_key, reverse=True)\n",
        "    chosen = geds[0]\n",
        "    try:\n",
        "        ts = datetime.fromtimestamp(os.path.getmtime(chosen)).isoformat(sep=\" \", timespec=\"seconds\")\n",
        "    except Exception:\n",
        "        ts = \"unknown\"\n",
        "    print(\"[INFO] Local GEDCOM present in /content. Using: %s (mtime=%s)\" % (chosen, ts))\n",
        "    return os.path.join(LOCAL_GED_DIR, chosen)\n",
        "\n",
        "def _choose_latest_gedcom(ftps, names):\n",
        "    \"\"\"\n",
        "    Pick the newest *.ged using MDTM if available; fallback = last alphabetically.\n",
        "    \"\"\"\n",
        "    ged_files = [n for n in names if n.lower().endswith(\".ged\")]\n",
        "    if not ged_files:\n",
        "        return None\n",
        "\n",
        "    latest_name = None\n",
        "    latest_ts = None\n",
        "\n",
        "    for nm in ged_files:\n",
        "        ts = None\n",
        "        try:\n",
        "            resp = ftps.sendcmd(\"MDTM \" + nm)  # '213 YYYYMMDDhhmmss'\n",
        "            parts = resp.strip().split()\n",
        "            if len(parts) == 2 and parts[0] == \"213\":\n",
        "                ts = parts[1]\n",
        "        except Exception:\n",
        "            ts = None\n",
        "        if ts is None:\n",
        "            ts = \"00000000000000\" + nm\n",
        "        if latest_ts is None or ts > latest_ts:\n",
        "            latest_ts = ts\n",
        "            latest_name = nm\n",
        "    return latest_name\n",
        "\n",
        "def pull_latest_gedcom_if_needed():\n",
        "    \"\"\"\n",
        "    If a GEDCOM already exists locally, do NOT pull from server.\n",
        "    Otherwise pull latest from /tng/gedcom and ensure exactly one *.ged in /content.\n",
        "    \"\"\"\n",
        "    local = _pick_local_gedcom_if_present()\n",
        "    if local:\n",
        "        print(\"[STEP] Skipping server GEDCOM pull (local GEDCOM already present).\")\n",
        "        print(\"[INFO] Cell 1 will see local GEDCOM: %s\" % local)\n",
        "        return\n",
        "\n",
        "    print(\"\\n[STEP] Pulling latest GEDCOM from %s ...\" % GEDCOM_REMOTE_DIR)\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        try:\n",
        "            ftps.cwd(\"/\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        for seg in [p for p in GEDCOM_REMOTE_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "        names = _safe_nlst(ftps)\n",
        "        if not names:\n",
        "            print(\"[WARN] No files listed in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        latest = _choose_latest_gedcom(ftps, names)\n",
        "        if not latest:\n",
        "            print(\"[WARN] No .ged files found in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        print(\"[INFO] Latest GEDCOM on server: %s\" % latest)\n",
        "\n",
        "        # Clean any old local GEDCOMs so Cell 1 cannot accidentally pick the wrong one\n",
        "        try:\n",
        "            for fname in os.listdir(LOCAL_GED_DIR):\n",
        "                if fname.lower().endswith(\".ged\"):\n",
        "                    try:\n",
        "                        os.remove(os.path.join(LOCAL_GED_DIR, fname))\n",
        "                        print(\"[CLEAN] Removed old local GEDCOM:\", fname)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] Could not remove %s: %s\" % (fname, e))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Could not scan local GED dir:\", e)\n",
        "\n",
        "        local_path = os.path.join(LOCAL_GED_DIR, latest)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR \" + latest, f.write)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        sz = os.path.getsize(local_path)\n",
        "        print(\"[OK] Pulled GEDCOM -> %s  size=%d bytes\" % (local_path, sz))\n",
        "        print(\"[INFO] Cell 1 will now see exactly one *.ged in /content.\")\n",
        "    except Exception:\n",
        "        print(\"[WARN] GEDCOM pull failed; Cell 1 will use any existing local *.ged instead.\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 4) Run scripts in order (subprocess; fixes ProcessPool pickling) ----------\n",
        "\n",
        "def run_script(path):\n",
        "    print(\"\\n[RUN] %s\" % path)\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[SKIP] %s not found in /content.\" % path)\n",
        "        return\n",
        "\n",
        "    # Use the same Python interpreter, unbuffered output.\n",
        "    cmd = [sys.executable, \"-u\", path]\n",
        "\n",
        "    try:\n",
        "        # Stream stdout+stderr together so logs appear in-order in Colab.\n",
        "        p = subprocess.Popen(\n",
        "            cmd,\n",
        "            cwd=\"/content\",\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "            encoding=\"utf-8\",\n",
        "            errors=\"replace\",\n",
        "            env=os.environ.copy(),\n",
        "        )\n",
        "\n",
        "        assert p.stdout is not None\n",
        "        for line in p.stdout:\n",
        "            # Print already has newline; avoid double spacing.\n",
        "            print(line.rstrip(\"\\n\"))\n",
        "\n",
        "        rc = p.wait()\n",
        "        if rc == 0:\n",
        "            print(\"[DONE] %s\" % path)\n",
        "        else:\n",
        "            print(\"[ERROR] Script failed (exit=%d): %s\" % (rc, path))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Exception while running %s: %s\" % (path, e))\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    os.chdir(\"/content\")\n",
        "\n",
        "    pulled = pull_authority_scripts()\n",
        "    if pulled == 0:\n",
        "        print(\"[FATAL] No authority scripts pulled; aborting.\")\n",
        "        return\n",
        "\n",
        "    pull_latest_gedcom_if_needed()\n",
        "\n",
        "    script_order = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell2k.py\", \"cell3.py\"]\n",
        "    print(\"\\n[STEP] Running scripts in order: %s\" % \", \".join(script_order))\n",
        "    for s in script_order:\n",
        "        run_script(os.path.join(\"/content\", s))\n",
        "\n",
        "main()\n",
        "\n",
        "print(\"\\n--- Cell 0 That's all folks, Orchestrator complete (authority scripts pulled, GEDCOM local-preferred, then Cell1/2/2b/2c/2k/3 executed) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkMmob1Y7uWv",
        "outputId": "907ec706-3426-4c89-fb8e-71f1fb64e074"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2026.01.31-UNIFIED | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=999\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[STEP] Pulling authority scripts from server ...\n",
            "[OK] Connected via FTPS (explicit AUTH TLS).\n",
            "[INFO] Initial PWD on server: /\n",
            "[INFO] Using remote dir for scripts: /partials/colab_cells\n",
            "[INFO] Remote listing sample (first 25): cell2.py, cell2b.py, cell2b_NetworkAuthority.py, cell2k.py, cell2d.py, ., cell2c.py, .., cell0_netupdate.py, cell3.py, cell1.py, cell0.py\n",
            "[PULL] cell1.py -> /content/cell1.py  size=24143  sha256=2a92b754edb79cad\n",
            "[PULL] cell2.py -> /content/cell2.py  size=34275  sha256=8df41f3e925278c7\n",
            "[PULL] cell2b.py -> /content/cell2b.py  size=51109  sha256=990c463e7a0fe0c0\n",
            "[PULL] cell2c.py -> /content/cell2c.py  size=33026  sha256=22fd337bd39e7abf\n",
            "[PULL] cell2k.py -> /content/cell2k.py  size=45269  sha256=77a8738001871c6b\n",
            "[PULL] cell3.py -> /content/cell3.py  size=40678  sha256=8e80a0aa139618f2\n",
            "[OK] Pulled 6 script(s) from authority shelf.\n",
            "[INFO] Local GEDCOM present in /content. Using: yates_study_2025.ged (mtime=2026-02-03 01:35:03)\n",
            "[STEP] Skipping server GEDCOM pull (local GEDCOM already present).\n",
            "[INFO] Cell 1 will see local GEDCOM: /content/yates_study_2025.ged\n",
            "\n",
            "[STEP] Running scripts in order: cell1.py, cell2.py, cell2b.py, cell2c.py, cell2k.py, cell3.py\n",
            "\n",
            "[RUN] /content/cell1.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2026.01.31-UNIFIED-BASELINE | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[INFO] Local GEDCOM present in content. Selected newest: yates_study_2025.ged (mtime=2026-02-03 01:35:03)\n",
            "[INFO] Using GEDCOM: yates_study_2025.ged\n",
            "GEDCOM contained 63510 total records\n",
            "Records tagged and filtered by NPFX: 1700\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1700\n",
            "After manual filter, total records: 93\n",
            "[OK] Wrote autosomal_count.txt = 1700\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: February 2, 2026 9:08 PM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 93 individuals with chunk-based parallel...\n",
            "\n",
            "Building Yates Lines (Stage 1):   0%|          | 0/93 [00:00<?, ?it/s]\n",
            "Building Yates Lines (Stage 1):  54%|█████▍    | 50/93 [00:17<00:14,  2.92it/s]\n",
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:30<00:00,  3.02it/s]\n",
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:30<00:00,  3.01it/s]\n",
            "2026-02-03 02:08:54,702 - INFO - Exported FIRST ANCESTOR PAIRS CSV -> first_ancestor_pairs.csv\n",
            "[OK] Wrote first_ancestor_pairs.csv (93 rows)\n",
            "2026-02-03 02:08:54,706 - INFO - Exported CSV -> final_combined_df_with_value_labels.csv\n",
            "2026-02-03 02:08:54,715 - INFO - Exported HTML -> cell1_work_table.htm\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploaded: first_ancestor_pairs.csv -> /partials/first_ancestor_pairs.csv\n",
            "[OK] Uploads complete to /partials/\n",
            "[OK] First ancestor pairs CSV at: /partials/first_ancestor_pairs.csv\n",
            "\n",
            "--- Cell 1 Complete: lineage tokens include ID + display name + birth-death years. ---\n",
            "--- first_ancestor_pairs.csv: FirstPair_LastFirst is a slug (no spaces/commas): yatesjohn&gaterjoane. ---\n",
            "[DONE] /content/cell1.py\n",
            "\n",
            "[RUN] /content/cell2.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2_3Col_Swap23 | Version=2026.02.02-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=1880\n",
            "[LAYOUT] Column widths (px): 1=220 2=1240 3=420\n",
            "[PULL] first_ancestor_pairs.csv -> /content/first_ancestor_pairs.server.csv\n",
            "[OK] Authority first-ancestor map: 25 pairs\n",
            "[OK] Loaded CSV: 93 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Saved render: /content/yates_ancestor_register.shtml\n",
            "[DEBUG] SSI nav include present: True\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 52046\n",
            "partials/ons_yates_dna_register.shtml : 52046\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "CSS:       https://yates.one-name.net/partials/partials_unified.css\n",
            "\n",
            "--- Cell 2 complete (DISPLAY SWAP 2<->3; Match Summary sorts by First Ancestor) ---\n",
            "[DONE] /content/cell2.py\n",
            "\n",
            "[RUN] /content/cell2b.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts_UnifiedHeaders | Version=2026.02.01-CELL2HDR-CELL2B1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 2, 2026 9:08 PM\n",
            "[OK] Loaded CSV for counts: 93 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 74956\n",
            "partials/lineage_count.shtml : 94428\n",
            "partials/cousin_list_print.htm : 34197\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "[DONE] /content/cell2b.py\n",
            "\n",
            "[RUN] /content/cell2c.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 2, 2026 9:08 PM\n",
            "[OK] Loaded CSV for DNA Network: 93 rows, 6 cols\n",
            "[PULL] dna/network_first_ancestors.txt -> /content/dna_network_first_ancestors.txt\n",
            "[INFO] Authority file synced from server.\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[INFO] Loaded 56 authority First Ancestors from dna_network_first_ancestors.txt\n",
            "[INFO] First 10 authority entries:\n",
            "   1. I47639 John Yates 1430-&I55584 Still Searching\n",
            "   2. I24496 John Yates 1581-1648&I24497 Joane Gater 1587-1666\n",
            "   3. I12324 William Yates 1720-&I12323 Anne Betty Thornborough 1725-\n",
            "   4. I19162 William Yates 1795-1866&I19163 Sally Parker 1806-\n",
            "   5. I12442 Thomas Yates 1670-&I5717 Monica Drury 1670-\n",
            "   6. I12192 Samuel Bedford Yates 1757-1844&I12193 Jane Millsaps 1753-1845\n",
            "   7. I37203 William Yates 1786-1862&I37260 Elizabeth Edwards 1793-1866\n",
            "   8. I25516 Thomas Yates 1700-1754&I55675 Still Searching\n",
            "   9. I53956 Phillip Yates &I55707 Still Searching\n",
            "  10. I10576 Joseph Yates 1754-&I10168 Mary Leigh 1753-1811\n",
            "[INFO] Filtered DNA network rows by authority list: 93 -> 93\n",
            "[OK] Wrote DNA Network partial: /content/partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 114782\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network: https://yates.one-name.net/partials/dna_network.shtml\n",
            "[DONE] /content/cell2c.py\n",
            "\n",
            "[RUN] /content/cell2k.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): February 2, 2026 9:08 PM\n",
            "[OK] Loaded CSV for unified DNA Network: 93 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote unified DNA Network partial: /content/partials/dna_network.shtml\n",
            "[OK] Wrote register CSV: /content/dna_network_register.csv\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[PUT] dna_network_register.csv -> dna/dna_network_register.csv\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 138301\n",
            "dna/dna_network_register.csv : 79487\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\n",
            "Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\n",
            "[DONE] /content/cell2k.py\n",
            "\n",
            "[RUN] /content/cell3.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=93, cols=6\n",
            "Resolver not found locally; attempting server pull ...\n",
            "[OK] Pulled resolver from server -> match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 93 / 93  unmatched: 0\n",
            "Using authority: /content/first_ancestor_pairs.server.csv\n",
            "[OK] Authority map ready: 25 pairs\n",
            "[OK] Column 1 replaced with First Ancestor (authority).\n",
            "[OK] Lineage formatting applied to columns: Yates DNA Ancestral Line\n",
            "[OK] Alpha sort applied by first couple: First Ancestor | keys= __sort_surname__,__sort_given__,__sort_momsurname__\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/first_ancestor_pairs.csv; enriched prefix exclusion applied) ---\n",
            "[DONE] /content/cell3.py\n",
            "\n",
            "--- Cell 0 That's all folks, Orchestrator complete (authority scripts pulled, GEDCOM local-preferred, then Cell1/2/2b/2c/2k/3 executed) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "JvOlmbj91AGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload + FIRST ANCESTOR PAIRS CSV (Sortable) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.01.31-UNIFIED-BASELINE)\n",
        "# - Complete and runnable Colab cell, one contiguous block (no fragments).\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout/colors via /partials/partials_unified.css (single baseline).\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2026.01.31-UNIFIED-BASELINE | Encoding=ISO-8859-15\n",
        "# =========================================================================================================\n",
        "\n",
        "import os, re, glob, logging, socket, traceback\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "from string import Template\n",
        "\n",
        "CELL_NAME = \"Cell1_FTPS_Explicit\"\n",
        "VERSION   = \"2026.01.31-UNIFIED-BASELINE\"\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=%s | Version=%s | Encoding=ISO-8859-15\" % (CELL_NAME, VERSION))\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(CELL_NAME)\n",
        "\n",
        "def _now_est_string():\n",
        "    try:\n",
        "        from zoneinfo import ZoneInfo\n",
        "        tz = ZoneInfo(\"America/New_York\")\n",
        "        now = datetime.now(tz)\n",
        "    except Exception:\n",
        "        now = datetime.now()\n",
        "    month_name = now.strftime(\"%B\")\n",
        "    day = now.day\n",
        "    year = now.year\n",
        "    hour_24 = now.hour\n",
        "    minute = now.minute\n",
        "    ampm = \"AM\" if hour_24 < 12 else \"PM\"\n",
        "    hour_12 = hour_24 % 12\n",
        "    if hour_12 == 0:\n",
        "        hour_12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, day, year, hour_12, minute, ampm)\n",
        "\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\",\"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\",\"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\",\"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\",\"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\",\"\") or \"\").strip().strip(\"/\")\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\" %\n",
        "      (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\"))\n",
        "\n",
        "def _ftps_connect():\n",
        "    if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "        raise RuntimeError(\"Missing FTP_HOST/FTP_USER/FTP_PASS.\")\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for p in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(p)\n",
        "        except all_errors:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except all_errors:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "\n",
        "def _ftps_upload(ftps, local_path, remote_name):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR \" + remote_name, fh)\n",
        "    print(\"[OK] Uploaded: %s -> %s/%s\" % (local_path, ftps.pwd().rstrip(\"/\"), remote_name))\n",
        "\n",
        "REMOTE_DIR        = \"partials\"\n",
        "CSV_OUT_LOCAL     = \"final_combined_df_with_value_labels.csv\"\n",
        "HTML_OUT_LOCAL    = \"cell1_work_table.htm\"\n",
        "ABS_CSV_URL       = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(CSV_OUT_LOCAL))\n",
        "ABS_HOME_URL      = \"/index.htm\"\n",
        "VITALS_CSV_PATH        = \"dna_vitals.csv\"\n",
        "AUTOSOMAL_COUNT_TXT    = \"autosomal_count.txt\"\n",
        "\n",
        "# First ancestor pairs CSV (sortable)\n",
        "FIRST_ANCESTOR_PAIRS_LOCAL = \"first_ancestor_pairs.csv\"\n",
        "ABS_FIRST_ANCESTOR_PAIRS_URL = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(FIRST_ANCESTOR_PAIRS_LOCAL))\n",
        "\n",
        "def _pick_local_gedcom_if_present():\n",
        "    geds = glob.glob(\"*.ged\")\n",
        "    if not geds:\n",
        "        return \"\"\n",
        "    def _key(p):\n",
        "        try:\n",
        "            return (float(os.path.getmtime(p)), str(p))\n",
        "        except Exception:\n",
        "            return (0.0, str(p))\n",
        "    geds.sort(key=_key, reverse=True)\n",
        "    chosen = geds[0]\n",
        "    try:\n",
        "        ts = datetime.fromtimestamp(os.path.getmtime(chosen)).isoformat(sep=\" \", timespec=\"seconds\")\n",
        "    except Exception:\n",
        "        ts = \"unknown\"\n",
        "    print(\"[INFO] Local GEDCOM present in content. Selected newest: %s (mtime=%s)\" % (chosen, ts))\n",
        "    return chosen\n",
        "\n",
        "def _ensure_gedcom_available():\n",
        "    chosen = _pick_local_gedcom_if_present()\n",
        "    if chosen:\n",
        "        return chosen\n",
        "    if \"download_latest_gedcom_from_tng\" in globals() and callable(globals()[\"download_latest_gedcom_from_tng\"]):\n",
        "        print(\"[INFO] No local GEDCOM found. Calling existing download_latest_gedcom_from_tng() ...\")\n",
        "        try:\n",
        "            globals()[\"download_latest_gedcom_from_tng\"]()\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] download_latest_gedcom_from_tng() failed:\", e)\n",
        "    return _pick_local_gedcom_if_present()\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        return self.gen_person.strip(\"@\")\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            cm = v.split(\"&\")[0].strip()\n",
        "        elif \"**\" in v:\n",
        "            cm = v.split(\"**\")[0].strip()\n",
        "        else:\n",
        "            cm = v.strip()\n",
        "        try:\n",
        "            int(cm)\n",
        "            return cm\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            s = v.split(\"&\")[1]\n",
        "            return (s.split(\"**\")[0] if \"**\" in s else s).strip()\n",
        "        return \"\"\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        return v.split(\"**\")[1].strip() if \"**\" in v else \"\"\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "        self.total_records = 0\n",
        "        self.npfx_count = 0\n",
        "        self.ydna_count = 0\n",
        "        self.autosomal_count = 0\n",
        "        self.after_manual_filter_total = 0\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(\" \", 2)\n",
        "            if not parts or not parts[0].isdigit():\n",
        "                continue\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith(\"@\") and tag.endswith(\"@\") and value == \"INDI\":\n",
        "                total += 1\n",
        "                current = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current)\n",
        "            elif current is not None:\n",
        "                if level == 2 and tag == \"NPFX\":\n",
        "                    npfx_count += 1\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                    if value and \"**\" in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal = npfx_count - ydna_count\n",
        "\n",
        "        self.total_records = total\n",
        "        self.npfx_count = npfx_count\n",
        "        self.ydna_count = ydna_count\n",
        "        self.autosomal_count = autosomal\n",
        "\n",
        "        print(\"GEDCOM contained %d total records\" % total)\n",
        "        print(\"Records tagged and filtered by NPFX: %d\" % npfx_count)\n",
        "        print(\"Records with YDNA information: %d\" % ydna_count)\n",
        "        print(\"Autosomal matches (NPFX minus YDNA): %d\" % autosomal)\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        try:\n",
        "            df_filter = pd.read_excel(\"filtered_ids.xlsx\")\n",
        "            manual_ids = set(str(x) for x in df_filter[\"ID\"])\n",
        "            self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_ids]\n",
        "            print(\"After manual filter, total records: %d\" % len(self.filter_pool))\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "\n",
        "        self.after_manual_filter_total = len(self.filter_pool)\n",
        "        return autosomal\n",
        "\n",
        "def _chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "def _extract_display_name_from_indi_block(txt):\n",
        "    if not txt:\n",
        "        return \"Unknown Name\"\n",
        "    m = re.search(r\"(^|\\n)1 NAME ([^\\n\\r]+)\", \"\\n\" + txt)\n",
        "    if not m:\n",
        "        return \"Unknown Name\"\n",
        "    name_line = (m.group(2) or \"\").strip()\n",
        "    if \"/\" not in name_line:\n",
        "        return name_line.strip() or \"Unknown Name\"\n",
        "    parts = name_line.split(\"/\", 2)\n",
        "    given = (parts[0] or \"\").strip()\n",
        "    sur = (parts[1] or \"\").strip()\n",
        "    out = (given + \" \" + sur).strip()\n",
        "    out = re.sub(r\"\\s+\", \" \", out)\n",
        "    return out if out else \"Unknown Name\"\n",
        "\n",
        "def _extract_year_from_date_line(date_line):\n",
        "    years = re.findall(r\"(\\d{4})\", str(date_line or \"\"))\n",
        "    return years[-1] if years else \"\"\n",
        "\n",
        "def _extract_birth_death_years_from_indi_block(txt):\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    b_year = \"\"\n",
        "    d_year = \"\"\n",
        "    m = re.search(r\"(^|\\n)1 BIRT\\b.*?(?:\\n2 DATE ([^\\n\\r]+))?\", \"\\n\" + txt, flags=re.S)\n",
        "    if m:\n",
        "        b_year = _extract_year_from_date_line(m.group(2) or \"\")\n",
        "    m2 = re.search(r\"(^|\\n)1 DEAT\\b.*?(?:\\n2 DATE ([^\\n\\r]+))?\", \"\\n\" + txt, flags=re.S)\n",
        "    if m2:\n",
        "        d_year = _extract_year_from_date_line(m2.group(2) or \"\")\n",
        "    if not b_year and not d_year:\n",
        "        return \"\"\n",
        "    return \"%s-%s\" % (b_year, d_year)\n",
        "\n",
        "def _find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        _find_parents(father_id, generation + 1, parents_map)\n",
        "    if mother_id:\n",
        "        _find_parents(mother_id, generation + 1, parents_map)\n",
        "\n",
        "def _find_distant(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(_find_distant(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(_find_distant(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "def _filter_lineage(winning_ids, gen_table, display_name_map, years_map):\n",
        "    matching = []\n",
        "    for generation, pair in gen_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_ids or id2 in winning_ids:\n",
        "            matching.append((generation, pair))\n",
        "    matching.sort(key=lambda x: x[0])\n",
        "\n",
        "    lines = []\n",
        "    for _, pair in matching:\n",
        "        pid1, pid2 = pair\n",
        "        n1 = (display_name_map.get(pid1, \"Unknown Name\") or \"Unknown Name\").replace(\"~\", \" \").strip()\n",
        "        n2 = (display_name_map.get(pid2, \"Unknown Name\") or \"Unknown Name\").replace(\"~\", \" \").strip()\n",
        "        y1 = (years_map.get(pid1, \"\") or \"\").replace(\"~\", \" \").strip()\n",
        "        y2 = (years_map.get(pid2, \"\") or \"\").replace(\"~\", \" \").strip()\n",
        "        t1 = \"%s~%s~%s\" % (pid1, n1, y1)\n",
        "        t2 = \"%s~%s~%s\" % (pid2, n2, y2)\n",
        "        lines.append(\"%s&%s\" % (t1, t2))\n",
        "\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "def _process_record(individual_id, ged, parents_map, display_name_map, years_map):\n",
        "    global generation_table, visited_pairs\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    _find_parents(individual_id, 1, parents_map)\n",
        "    paths = _find_distant(individual_id, parents_map)\n",
        "    best_score, best_path = None, None\n",
        "    for path in paths:\n",
        "        score = 0\n",
        "        for idx, pid in enumerate(path):\n",
        "            nm = (display_name_map.get(pid, \"\") or \"\")\n",
        "            if \"Yates\" in nm:\n",
        "                score += (idx + 1)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score, best_path = score, path\n",
        "    best_path = best_path or []\n",
        "    best_ids  = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str  = _filter_lineage(set(best_ids), generation_table, display_name_map, years_map)\n",
        "\n",
        "    cm_value = \"\"\n",
        "    sort_value = \"\"\n",
        "    ydna_value = \"\"\n",
        "    for ds in ged.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value   = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "\n",
        "    short_name = display_name_map.get(individual_id, \"Unknown Name\")\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "# ---------- First ancestor pair CSV ----------\n",
        "\n",
        "# ---------- Authority lock-in module (First Ancestor couple map) ----------\n",
        "# Purpose:\n",
        "# - Write an importable python module containing AUTH_COUPLE_KEY_MAP built from df_pairs.\n",
        "# - This eliminates cross-cell ambiguity: Cell 2 / Cell 3 can import the exact map.\n",
        "#\n",
        "# Usage in Cell 2 / Cell 3:\n",
        "#   from yates_authority_first_ancestor_map import AUTH_COUPLE_KEY_MAP\n",
        "\n",
        "LOCKIN_MODULE_LOCAL = \"yates_authority_first_ancestor_map.py\"\n",
        "\n",
        "def _write_authority_lockin_module_from_df_pairs(df_pairs, out_py=LOCKIN_MODULE_LOCAL):\n",
        "    \"\"\"\n",
        "    Build mapping (Ancestor1_ID, Ancestor2_ID) -> FirstPair_LastFirst (and reverse order)\n",
        "    from the in-memory df_pairs and write a standalone module.\n",
        "\n",
        "    Notes:\n",
        "    - Values are written EXACTLY as in df_pairs['FirstPair_LastFirst'] (no HTML escaping).\n",
        "    - Keys are uppercased IDs (I####).\n",
        "    - File is written as iso-8859-15 with xmlcharrefreplace (site-safe).\n",
        "    \"\"\"\n",
        "    if df_pairs is None or df_pairs.empty:\n",
        "        raise RuntimeError(\"df_pairs is empty; cannot lock in authority map.\")\n",
        "\n",
        "    required = [\"FirstPair_Ancestor1_ID\", \"FirstPair_Ancestor2_ID\", \"FirstPair_LastFirst\"]\n",
        "    for c in required:\n",
        "        if c not in df_pairs.columns:\n",
        "            raise RuntimeError(\"df_pairs missing required column: %s\" % c)\n",
        "\n",
        "    m = {}\n",
        "    for _, r in df_pairs.iterrows():\n",
        "        a1 = str(r.get(\"FirstPair_Ancestor1_ID\", \"\") or \"\").strip().upper()\n",
        "        a2 = str(r.get(\"FirstPair_Ancestor2_ID\", \"\") or \"\").strip().upper()\n",
        "        k  = str(r.get(\"FirstPair_LastFirst\", \"\") or \"\").strip()\n",
        "        k  = re.sub(r\"\\s+\", \"\", k)\n",
        "        if not (a1 and a2 and k):\n",
        "            continue\n",
        "        m[(a1, a2)] = k\n",
        "        m[(a2, a1)] = k\n",
        "\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Authority map built empty from df_pairs\")\n",
        "\n",
        "    header = [\n",
        "        \"# -*- coding: iso-8859-15 -*-\",\n",
        "        '\"\"\"',\n",
        "        \"AUTO-GENERATED FILE - DO NOT HAND EDIT\",\n",
        "        \"Generated by: Cell 1 (GEDCOM -> CSV build)\",\n",
        "        \"Contains: AUTH_COUPLE_KEY_MAP dict for (Ancestor1_ID, Ancestor2_ID) -> FirstPair_LastFirst\",\n",
        "        '\"\"\"',\n",
        "        \"\",\n",
        "        \"AUTH_COUPLE_KEY_MAP = {\",\n",
        "    ]\n",
        "\n",
        "    # Deterministic order for stable diffs\n",
        "    lines = []\n",
        "    for (a1, a2) in sorted(m.keys()):\n",
        "        lines.append(\"    (%r, %r): %r,\" % (a1, a2, m[(a1, a2)]))\n",
        "\n",
        "    footer = [\"}\", \"\"]\n",
        "    with open(out_py, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(\"\\n\".join(header + lines + footer))\n",
        "\n",
        "    print(\"[OK] Wrote authority lock-in module:\", os.path.abspath(out_py), \"| pairs:\", (len(m)//2))\n",
        "    return out_py\n",
        "\n",
        "\n",
        "\n",
        "def _slug_no_space_no_comma(s):\n",
        "    s = (s or \"\").lower()\n",
        "    s = s.replace(\",\", \"\").replace(\" \", \"\")\n",
        "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def _name_to_lastfirst_pretty_and_slug(display_name):\n",
        "    s = (display_name or \"\").strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    if not s or s.lower() == \"unknown name\":\n",
        "        pretty = \"Unknown\"\n",
        "        return pretty, _slug_no_space_no_comma(pretty)\n",
        "    parts = s.split(\" \")\n",
        "    if len(parts) == 1:\n",
        "        pretty = parts[0]\n",
        "        return pretty, _slug_no_space_no_comma(pretty)\n",
        "    last = parts[-1]\n",
        "    given = \" \".join(parts[:-1]).strip()\n",
        "    pretty = (\"%s, %s\" % (last, given)) if given else last\n",
        "    return pretty, _slug_no_space_no_comma(pretty)\n",
        "\n",
        "def _parse_first_pair_tokens(lineage_str):\n",
        "    s = (lineage_str or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
        "    first_seg = s.split(\"~~~\", 1)[0].strip()\n",
        "    if \"&\" not in first_seg:\n",
        "        return (\"\", \"\", \"\", \"\", \"\", \"\", \"\")\n",
        "    left, right = first_seg.split(\"&\", 1)\n",
        "\n",
        "    def _split_token(tok):\n",
        "        tok = (tok or \"\").strip()\n",
        "        parts = tok.split(\"~\")\n",
        "        pid = (parts[0] if len(parts) > 0 else \"\").strip()\n",
        "        nm  = (parts[1] if len(parts) > 1 else \"\").strip()\n",
        "        yrs = (parts[2] if len(parts) > 2 else \"\").strip()\n",
        "        return pid, nm, yrs\n",
        "\n",
        "    pid1, nm1, yrs1 = _split_token(left)\n",
        "    pid2, nm2, yrs2 = _split_token(right)\n",
        "\n",
        "    lf1_pretty, lf1_slug = _name_to_lastfirst_pretty_and_slug(nm1)\n",
        "    lf2_pretty, lf2_slug = _name_to_lastfirst_pretty_and_slug(nm2)\n",
        "\n",
        "    pair_lastfirst = \"%s&%s\" % (lf1_slug, lf2_slug)\n",
        "    return (pid1, lf1_pretty, yrs1, pid2, lf2_pretty, yrs2, pair_lastfirst)\n",
        "\n",
        "def main():\n",
        "    gedcom_path = _ensure_gedcom_available()\n",
        "    if not gedcom_path:\n",
        "        print(\"No GEDCOM files found in content, and no download produced one.\")\n",
        "        return False\n",
        "\n",
        "    print(\"[INFO] Using GEDCOM: %s\" % gedcom_path)\n",
        "\n",
        "    ged = Gedcom(gedcom_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "\n",
        "    with open(AUTOSOMAL_COUNT_TXT, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "    print(\"[OK] Wrote %s = %d\" % (AUTOSOMAL_COUNT_TXT, autosomal_count))\n",
        "\n",
        "    last_updated_text = _now_est_string()\n",
        "    print(\"[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: %s\" % last_updated_text)\n",
        "\n",
        "    npfx_count = int(ged.npfx_count)\n",
        "    after_manual_filter_total = int(ged.after_manual_filter_total or len(ged.filter_pool))\n",
        "\n",
        "    vitals_lines = [\n",
        "        \"Records tagged and filtered by NPFX: %d\" % npfx_count,\n",
        "        \"After manual filter, total records: %d\" % after_manual_filter_total,\n",
        "        \"LAST_UPDATED_TEXT: %s\" % last_updated_text,\n",
        "    ]\n",
        "    pd.DataFrame({\"line\": vitals_lines}).to_csv(\n",
        "        VITALS_CSV_PATH,\n",
        "        index=False,\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    )\n",
        "    print(\"[OK] Wrote dna_vitals.csv -> %s\" % os.path.abspath(VITALS_CSV_PATH))\n",
        "\n",
        "    with open(gedcom_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find(\"\\n\")\n",
        "        flend = len(blk) if flend == -1 else flend\n",
        "        first_line = blk[:flend]\n",
        "        if \"@\" in first_line:\n",
        "            s = first_line.find(\"@\") + 1\n",
        "            e = first_line.find(\"@\", s)\n",
        "            rec_id = first_line[s:e].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    families = {}\n",
        "    display_name_map = {}\n",
        "    years_map = {}\n",
        "\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if \"FAM\" in txt[:50]:\n",
        "            father_idx = txt.find(\"1 HUSB @\")\n",
        "            husb_id = txt[father_idx + len(\"1 HUSB @\"):txt.find(\"@\", father_idx + len(\"1 HUSB @\"))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find(\"1 WIFE @\")\n",
        "            wife_id = txt[wife_idx + len(\"1 WIFE @\"):txt.find(\"@\", wife_idx + len(\"1 WIFE @\"))] if wife_idx != -1 else None\n",
        "            kids = [ln.split(\"@\")[1] for ln in txt.split(\"\\n\") if ln.strip().startswith(\"1 CHIL @\")]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "        if \"INDI\" in txt[:50]:\n",
        "            display_name_map[rec_id] = _extract_display_name_from_indi_block(txt)\n",
        "            years_map[rec_id] = _extract_birth_death_years_from_indi_block(txt)\n",
        "\n",
        "    for _, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(\"Processing %d individuals with chunk-based parallel...\" % len(individual_ids))\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "\n",
        "    from functools import partial as _partial\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as ex, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in _chunks(individual_ids, chunk_size):\n",
        "            func = _partial(_process_record, ged=ged, parents_map=parents_map, display_name_map=display_name_map, years_map=years_map)\n",
        "            results = list(ex.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "\n",
        "    # ----- First ancestor pair CSV -----\n",
        "    pairs_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        match_id = str(row.get(\"ID#\", \"\") or \"\").strip()\n",
        "        lineage  = str(row.get(\"Yates DNA Ancestral Line\", \"\") or \"\")\n",
        "        pid1, lf1, yrs1, pid2, lf2, yrs2, pair_lastfirst = _parse_first_pair_tokens(lineage)\n",
        "        pairs_rows.append({\n",
        "            \"MatchID\": match_id,\n",
        "            \"FirstPair_Ancestor1_ID\": pid1,\n",
        "            \"FirstPair_Ancestor1_LastFirst\": lf1,\n",
        "            \"FirstPair_Ancestor1_Years\": yrs1,\n",
        "            \"FirstPair_Ancestor2_ID\": pid2,\n",
        "            \"FirstPair_Ancestor2_LastFirst\": lf2,\n",
        "            \"FirstPair_Ancestor2_Years\": yrs2,\n",
        "            \"FirstPair_LastFirst\": pair_lastfirst,\n",
        "        })\n",
        "\n",
        "    df_pairs = pd.DataFrame(pairs_rows)\n",
        "    if not df_pairs.empty:\n",
        "        df_pairs.sort_values(by=[\"FirstPair_LastFirst\", \"MatchID\"], inplace=True)\n",
        "\n",
        "    with open(FIRST_ANCESTOR_PAIRS_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df_pairs.to_csv(index=False))\n",
        "    logger.info(\"Exported FIRST ANCESTOR PAIRS CSV -> %s\", FIRST_ANCESTOR_PAIRS_LOCAL)\n",
        "    print(\"[OK] Wrote %s (%d rows)\" % (FIRST_ANCESTOR_PAIRS_LOCAL, len(df_pairs)))\n",
        "\n",
        "    # ----- Authority lock-in module (python import) -----\n",
        "    try:\n",
        "        _write_authority_lockin_module_from_df_pairs(df_pairs, LOCKIN_MODULE_LOCAL)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Authority lock-in module not written:\", e)\n",
        "\n",
        "\n",
        "    # ----- Existing main CSV export -----\n",
        "    with open(CSV_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df.to_csv(index=False))\n",
        "    logger.info(\"Exported CSV -> %s\", CSV_OUT_LOCAL)\n",
        "\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Yates DNA Ancestral Line\"]\n",
        "\n",
        "    # Ensure table is styled/recognized as sortable by your site conventions\n",
        "    table_html = df.to_html(index=False, columns=final_cols, escape=False, border=1, classes=[\"sortable\"])\n",
        "\n",
        "    page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Cell 1 Working Table</title>\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/partials_unified.css\" />\n",
        "</head>\n",
        "<body>\n",
        "<div class=\"wrap\">\n",
        "  <h1>Cell 1 Working Table</h1>\n",
        "\n",
        "  <div class=\"updated\">\n",
        "    <a href=\"$HOME\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "    &nbsp;|&nbsp; Last updated: $LAST_UPDATED_TEXT\n",
        "    &nbsp;|&nbsp; Download: <a href=\"$CSV\">$CSV</a>\n",
        "    &nbsp;|&nbsp; First ancestor pairs: <a href=\"$PAIRS\">$PAIRS</a>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"downloads\">\n",
        "    <a href=\"$CSV\">/partials/$CSV_NAME</a>\n",
        "    &nbsp;|&nbsp;\n",
        "    <a href=\"$PAIRS\">/partials/$PAIRS_NAME</a>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"table-scroll-wrapper\">\n",
        "    <div class=\"table-scroll\">\n",
        "      $TABLE\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "</body>\n",
        "</html>\"\"\")\n",
        "\n",
        "    page = page_tpl.safe_substitute(\n",
        "        HOME=ABS_HOME_URL,\n",
        "        CSV=ABS_CSV_URL,\n",
        "        CSV_NAME=os.path.basename(ABS_CSV_URL),\n",
        "        PAIRS=ABS_FIRST_ANCESTOR_PAIRS_URL,\n",
        "        PAIRS_NAME=os.path.basename(ABS_FIRST_ANCESTOR_PAIRS_URL),\n",
        "        TABLE=table_html,\n",
        "        LAST_UPDATED_TEXT=last_updated_text,\n",
        "    )\n",
        "\n",
        "    with open(HTML_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(page)\n",
        "    logger.info(\"Exported HTML -> %s\", HTML_OUT_LOCAL)\n",
        "\n",
        "    return True\n",
        "\n",
        "ok = main()\n",
        "\n",
        "if ok and FTP_HOST and FTP_USER and FTP_PASS:\n",
        "    print(\"[INFO] Uploading artifacts to /partials/ ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        _ftps_ensure_dir(ftps, \"partials\")\n",
        "        for p in [CSV_OUT_LOCAL, HTML_OUT_LOCAL, VITALS_CSV_PATH, AUTOSOMAL_COUNT_TXT, FIRST_ANCESTOR_PAIRS_LOCAL, LOCKIN_MODULE_LOCAL]:\n",
        "            try:\n",
        "                _ftps_upload(ftps, p, os.path.basename(p))\n",
        "            except Exception as e:\n",
        "                print(\"[ERROR] Upload failed for %s: %s\" % (p, e))\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Uploads complete to /partials/\")\n",
        "        print(\"[OK] First ancestor pairs CSV at: %s\" % ABS_FIRST_ANCESTOR_PAIRS_URL)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing creds or build failed).\")\n",
        "\n",
        "print(\"\\n--- Cell 1 Complete: lineage tokens include ID + display name + birth-death years. ---\")\n",
        "print(\"--- first_ancestor_pairs.csv: FirstPair_LastFirst is a slug (no spaces/commas): yatesjohn&gaterjoane. ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload + FIRST ANCESTOR PAIRS CSV (Sortable) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZWINLBfKALj",
        "outputId": "ffb70673-279d-4510-9473-a1c4f137838f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2026.01.31-UNIFIED-BASELINE | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[INFO] Local GEDCOM present in content. Selected newest: yates_study_2025.ged (mtime=2026-02-03 01:35:03)\n",
            "[INFO] Using GEDCOM: yates_study_2025.ged\n",
            "GEDCOM contained 63510 total records\n",
            "Records tagged and filtered by NPFX: 1700\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1700\n",
            "After manual filter, total records: 93\n",
            "[OK] Wrote autosomal_count.txt = 1700\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: February 2, 2026 8:40 PM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 93 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:29<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Wrote first_ancestor_pairs.csv (93 rows)\n",
            "[OK] Wrote authority lock-in module: /content/yates_authority_first_ancestor_map.py | pairs: 25\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploaded: first_ancestor_pairs.csv -> /partials/first_ancestor_pairs.csv\n",
            "[OK] Uploaded: yates_authority_first_ancestor_map.py -> /partials/yates_authority_first_ancestor_map.py\n",
            "[OK] Uploads complete to /partials/\n",
            "[OK] First ancestor pairs CSV at: /partials/first_ancestor_pairs.csv\n",
            "\n",
            "--- Cell 1 Complete: lineage tokens include ID + display name + birth-death years. ---\n",
            "--- first_ancestor_pairs.csv: FirstPair_LastFirst is a slug (no spaces/commas): yatesjohn&gaterjoane. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "TKAmqiIIDaxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: iso-8859-15 -*-\n",
        "# Cell2_3Col_AuthorityFirstAncestor - SWAP COL 2 & 3 (DISPLAY) + GOLDEN SORT RULE\n",
        "# Version=2026.02.02-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR-LOCKIN+ENRICHED-EXCLUDE1\n",
        "#\n",
        "# Intent:\n",
        "# - Display columns 1, 2, 3 with columns 2 and 3 swapped:\n",
        "#     1) Match to\n",
        "#     2) Match Summary\n",
        "#     3) First Ancestor\n",
        "# - Golden rule preserved:\n",
        "#     Clicking \"Match Summary\" sorts using the data in \"First Ancestor\".\n",
        "# - Authority is LOCKED-IN via python module hosted on server:\n",
        "#     /partials/yates_authority_first_ancestor_map.py  -> AUTH_COUPLE_KEY_MAP\n",
        "# - Enriched exclusion:\n",
        "#     Drop all couples older than: Francis Yates (1541-1588) & Jane Tichborne (1548-1580)\n",
        "#     (same logic as your working Cell 3: explicit prefix match + anchor trim).\n",
        "#\n",
        "# Notes:\n",
        "# - Paste into a Colab cell OR run as a script in the same working directory.\n",
        "# - Output written as iso-8859-15 with xmlcharrefreplace.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_3Col_Swap23 | Version=2026.02.02-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR-LOCKIN+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "import importlib.util\n",
        "\n",
        "# ---------- A) LAYOUT CONTROL BLOCK ----------\n",
        "# Swapped widths to match swapped display:\n",
        "#   Col 2 = Match Summary (wide)\n",
        "#   Col 3 = First Ancestor (narrow)\n",
        "COL_1_PX = 220\n",
        "COL_2_PX = 1240\n",
        "COL_3_PX = 420\n",
        "\n",
        "COL_WIDTHS = [COL_1_PX, COL_2_PX, COL_3_PX]\n",
        "TABLE_TOTAL_WIDTH_PX = sum(COL_WIDTHS)\n",
        "\n",
        "print(\"[LAYOUT] TABLE_TOTAL_WIDTH_PX=%d\" % TABLE_TOTAL_WIDTH_PX)\n",
        "print(\"[LAYOUT] Column widths (px): 1=%d 2=%d 3=%d\" % (COL_1_PX, COL_2_PX, COL_3_PX))\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Authority LOCK-IN module path on server\n",
        "AUTH_MODULE_REMOTE_DIR = \"partials\"\n",
        "AUTH_MODULE_BASENAME   = \"yates_authority_first_ancestor_map.py\"\n",
        "AUTH_MODULE_REMOTE     = posixpath.join(AUTH_MODULE_REMOTE_DIR, AUTH_MODULE_BASENAME)\n",
        "AUTH_MODULE_LOCAL      = \"yates_authority_first_ancestor_map.server.py\"\n",
        "\n",
        "LOCAL_HTML        = \"yates_ancestor_register.shtml\"\n",
        "REMOTE_HTML_CANON = posixpath.join(\"partials\", \"yates_ancestor_register.shtml\")\n",
        "REMOTE_HTML_LEG   = posixpath.join(\"partials\", \"ons_yates_dna_register.shtml\")\n",
        "\n",
        "FTP_DIR  = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "NAV_FALLBACK_HTML = \"\"\n",
        "\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR        = \"partials\"\n",
        "SERVER_MAPPING_BASENAME    = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE      = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# ---------- Enriched exclusion prefix (formatted lineage) ----------\n",
        "# Same as Cell 3 (explicit couple list up to Thomas Yates & Elizabeth Fauconer)\n",
        "LINEAGE_SPOUSE_SEP = \" & \"\n",
        "LINEAGE_COUPLE_SEP = \" ~ \"\n",
        "\n",
        "ENRICHED_EXCLUDE_PREFIX = (\n",
        "    \"John Yates (1430-) & Still Searching ~ \"\n",
        "    \"William Yates (1389-1440) & Still Searching ~ \"\n",
        "    \"William Yates (1420-) & Still Searching ~ \"\n",
        "    \"Edmund Yates (1445-1472) & Margaret Cornell ~ \"\n",
        "    \"Richard Yates (1440-1498) & Joan Ashendon (1445-1499) ~ \"\n",
        "    \"John Yates (1471-1544) & Alice Hyde (1498-1523) ~ \"\n",
        "    \"Thomas Yates (1509-1565) & Elizabeth Fauconer (-1562) ~\"\n",
        ")\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) CSV helpers ----------\n",
        "def _read_csv_anyenc(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    dfx = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            dfx = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            dfx = None\n",
        "    if dfx is None:\n",
        "        raise RuntimeError(\"Unable to read CSV %s: %s\" % (path, last))\n",
        "    return dfx\n",
        "\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    dfm = _read_csv_anyenc(path)\n",
        "    if dfm.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    dfm = dfm.iloc[:, :2].copy()\n",
        "    dfm.columns = [\"code\", \"unmasked\"]\n",
        "    dfm[\"code\"]     = dfm[\"code\"].astype(str).str.strip().str.lower()\n",
        "    dfm[\"unmasked\"] = dfm[\"unmasked\"].astype(str).str.strip()\n",
        "    dfm = dfm[dfm[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if dfm.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return dfm\n",
        "\n",
        "# ---------- 3.1) Resolver ----------\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Text utils ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"  + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, couple_text_html):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        couple_text_html,\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 4.1) Parse enriched first ancestor token ----------\n",
        "def _scrub_side_keep_name_years(side_raw: str):\n",
        "    side_raw = _clean_piece(side_raw or \"\")\n",
        "    if not side_raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "\n",
        "    if \"~\" in side_raw:\n",
        "        bits = [b.strip() for b in side_raw.split(\"~\")]\n",
        "        bits = [b for b in bits if b != \"\"]\n",
        "        if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "            pid = bits[0].upper()\n",
        "            nm  = normalize_person_name(bits[1]) if len(bits) >= 2 else \"\"\n",
        "            yrs = _clean_piece(bits[2]) if len(bits) >= 3 else \"\"\n",
        "            return (pid, nm, yrs)\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", side_raw, flags=re.I)\n",
        "    if m:\n",
        "        pid  = m.group(1).upper()\n",
        "        rest = _clean_piece(m.group(2))\n",
        "        yrs  = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs  = _clean_piece(m2.group(1).replace(\" \", \"\"))\n",
        "            rest = _clean_piece(rest[:m2.start()])\n",
        "        nm = normalize_person_name(rest) if rest else \"\"\n",
        "        return (pid, nm, yrs)\n",
        "\n",
        "    nm2 = smart_titlecase(side_raw) if \" \" in side_raw else smart_titlecase(surname_given_from_token(side_raw)[0])\n",
        "    return (\"\", nm2, \"\")\n",
        "\n",
        "def _couple_display_and_ids_from_token(raw_token: str):\n",
        "    raw = _clean_piece(raw_token or \"\")\n",
        "    if not raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", raw, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        pid, nm, yrs = _scrub_side_keep_name_years(raw)\n",
        "        disp = nm or raw\n",
        "        if yrs:\n",
        "            disp = disp + \" (%s)\" % yrs\n",
        "        return (disp, pid, \"\")\n",
        "\n",
        "    f_id, f_nm, f_yrs = _scrub_side_keep_name_years(parts[0])\n",
        "    m_id, m_nm, m_yrs = _scrub_side_keep_name_years(parts[1])\n",
        "\n",
        "    f_disp = f_nm or normalize_person_name(parts[0])\n",
        "    m_disp = m_nm or normalize_person_name(parts[1])\n",
        "\n",
        "    if f_yrs:\n",
        "        f_disp = f_disp + \" (%s)\" % f_yrs\n",
        "    if m_yrs:\n",
        "        m_disp = m_disp + \" (%s)\" % m_yrs\n",
        "\n",
        "    disp = \"%s%s%s\" % (f_disp, LINEAGE_SPOUSE_SEP, m_disp)\n",
        "    return (disp, f_id, m_id)\n",
        "\n",
        "# ---------- Enriched exclusion (same logic as Cell 3) ----------\n",
        "def _norm_couple_for_match(s: str) -> str:\n",
        "    return re.sub(r\"\\s{2,}\", \" \", str(s or \"\")).strip().lower()\n",
        "\n",
        "_EXCLUDE_COUPLES = [\n",
        "    c.strip()\n",
        "    for c in re.split(r\"\\s*~\\s*\", (ENRICHED_EXCLUDE_PREFIX or \"\").strip().strip(\"~\"))\n",
        "    if c and c.strip()\n",
        "]\n",
        "\n",
        "def _strip_paren_years_anywhere(s: str) -> str:\n",
        "    return re.sub(r\"\\([^)]*\\)\", \"\", str(s or \"\")).strip()\n",
        "\n",
        "def _is_anchor_couple(couple_text: str) -> bool:\n",
        "    t = _strip_paren_years_anywhere(couple_text).lower()\n",
        "    return (\"francis yates\" in t) and (\"jane tichborne\" in t)\n",
        "\n",
        "def _apply_enriched_exclusion_to_couples(couples):\n",
        "    couples = [c for c in (couples or []) if c and str(c).strip()]\n",
        "    if not couples:\n",
        "        return couples\n",
        "\n",
        "    # (A) Exact prefix removal by couple list match\n",
        "    if _EXCLUDE_COUPLES and len(couples) >= len(_EXCLUDE_COUPLES):\n",
        "        ok = True\n",
        "        for i in range(len(_EXCLUDE_COUPLES)):\n",
        "            if _norm_couple_for_match(couples[i]) != _norm_couple_for_match(_EXCLUDE_COUPLES[i]):\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            couples = couples[len(_EXCLUDE_COUPLES):]\n",
        "\n",
        "    # (B) Anchor trim: drop everything before Francis+Jane if present anywhere\n",
        "    if couples:\n",
        "        for i, c in enumerate(couples):\n",
        "            if _is_anchor_couple(c):\n",
        "                couples = couples[i:]\n",
        "                break\n",
        "\n",
        "    return couples\n",
        "\n",
        "# ---------- 4.2) Load authority LOCK-IN module ----------\n",
        "def _download_authority_module_if_needed() -> str:\n",
        "    # Prefer a local module if already present\n",
        "    for p in (\"yates_authority_first_ancestor_map.py\", AUTH_MODULE_LOCAL):\n",
        "        if os.path.exists(p):\n",
        "            print(\"[OK] Using local authority module:\", os.path.abspath(p))\n",
        "            return p\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        raise RuntimeError(\"Missing FTP creds; cannot download authority module %s\" % AUTH_MODULE_REMOTE)\n",
        "\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(AUTH_MODULE_REMOTE_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, AUTH_MODULE_BASENAME, AUTH_MODULE_LOCAL)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Authority module not found on server: /%s\" % _remote_path(AUTH_MODULE_REMOTE))\n",
        "\n",
        "    return AUTH_MODULE_LOCAL\n",
        "\n",
        "def _import_authority_map(module_path: str) -> dict:\n",
        "    spec = importlib.util.spec_from_file_location(\"yates_authority_first_ancestor_map\", module_path)\n",
        "    if spec is None or spec.loader is None:\n",
        "        raise RuntimeError(\"Unable to import authority module from %s\" % module_path)\n",
        "    mod = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(mod)  # type: ignore\n",
        "    m = getattr(mod, \"AUTH_COUPLE_KEY_MAP\", None)\n",
        "    if not isinstance(m, dict) or not m:\n",
        "        raise RuntimeError(\"AUTH_COUPLE_KEY_MAP missing/empty in %s\" % module_path)\n",
        "    print(\"[OK] Authority map loaded from module: %d pairs\" % (len(m)//2))\n",
        "    return m\n",
        "\n",
        "AUTH_COUPLE_KEY_MAP = _import_authority_map(_download_authority_module_if_needed())\n",
        "\n",
        "# ---------- 5) Read main CSV ----------\n",
        "def find_col(df0, patterns, prefer_exact=None):\n",
        "    cols = list(df0.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df0.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "df = _read_csv_anyenc(CSV_IN)\n",
        "print(\"[OK] Loaded CSV: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "id_col    = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col  = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "cm_col    = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "path_col  = find_col(df, [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "                     [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"])\n",
        "\n",
        "for req, nm in [(id_col,\"ID#/PersonID\"), (match_col,\"Match to\"), (name_col,\"Name\"), (cm_col,\"cM\"), (path_col,\"Lineage\")]:\n",
        "    if not req:\n",
        "        raise ValueError(\"CSV missing required column: %s\" % nm)\n",
        "\n",
        "# ---------- 5.1) Vitals (optional) ----------\n",
        "AUTOSOMAL_MATCHES = \"\"\n",
        "LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    raw = str(raw or \"\").replace(\"UTC\",\"\").strip()\n",
        "    m = re.match(r\"^(\\d{4})-(\\d{2})-(\\d{2})[ T](\\d{2}):(\\d{2})(?::(\\d{2}))?$\", raw)\n",
        "    if not m:\n",
        "        return raw\n",
        "    Y, Mo, D, h, mi, s = [int(x or \"0\") for x in m.groups()]\n",
        "    import datetime as _dt\n",
        "    dt = _dt.datetime(Y, Mo, D, h, mi, s) - _dt.timedelta(hours=5)\n",
        "    months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "    hh = dt.hour\n",
        "    ampm = \"AM\" if hh < 12 else \"PM\"\n",
        "    h12 = hh % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (months[dt.month-1], dt.day, dt.year, h12, dt.minute, ampm)\n",
        "\n",
        "def _format_int_with_commas(s):\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"[^0-9\\-]\", \"\", str(s))\n",
        "    if not t:\n",
        "        return str(s)\n",
        "    try:\n",
        "        return \"{:,}\".format(int(t))\n",
        "    except Exception:\n",
        "        return str(s)\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global AUTOSOMAL_MATCHES, LAST_UPDATED_TEXT\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will omit counts and last-updated text.\")\n",
        "        return\n",
        "    vdf = _read_csv_anyenc(path)\n",
        "    flat = [str(cell) for row in vdf.astype(str).values.tolist() for cell in row]\n",
        "    autosomal = last_text = None\n",
        "    for cell in flat:\n",
        "        if autosomal is None and \"Records tagged and filtered by NPFX\" in cell:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", cell)\n",
        "            if m:\n",
        "                autosomal = m.group(1)\n",
        "        if last_text is None and \"LAST_UPDATED_TEXT\" in cell:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", cell)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "    if last_text is not None:\n",
        "        last_text = _friendly_ts_from_utc(last_text)\n",
        "    AUTOSOMAL_MATCHES = _format_int_with_commas(autosomal) if autosomal else \"\"\n",
        "    LAST_UPDATED_TEXT = last_text or \"\"\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "# ---------- 6) Transform + compute authority sort key ----------\n",
        "_setup_resolver()\n",
        "\n",
        "def _authority_token_from_display_name(name_text: str) -> str:\n",
        "    # Only used for LAST-resort; the preferred key is the authority module lookup.\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", str(name_text or \"\"))\n",
        "    s = s.replace(\"\\u00a0\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    if \"unknown\" in s:\n",
        "        return \"unknown\"\n",
        "    s = re.sub(r\"[^a-z0-9 ]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    parts = s.split(\" \")\n",
        "    if len(parts) == 1:\n",
        "        return re.sub(r\"[^a-z0-9]+\", \"\", parts[0])\n",
        "    surname = parts[-1]\n",
        "    givens  = parts[:-1]\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"\", surname + \"\".join(givens))\n",
        "\n",
        "def _authority_couple_token_from_display(couple_disp: str) -> str:\n",
        "    s = str(couple_disp or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    parts = [p.strip() for p in re.split(r\"\\s*(?:&| and )\\s*\", s, maxsplit=1, flags=re.I) if p.strip()]\n",
        "    if len(parts) == 2:\n",
        "        a = _authority_token_from_display_name(parts[0])\n",
        "        b = _authority_token_from_display_name(parts[1])\n",
        "        if a and b:\n",
        "            return a + \"&\" + b\n",
        "        return (a or \"\") + (\"&\" if (a or b) else \"\") + (b or \"\")\n",
        "    return _authority_token_from_display_name(s)\n",
        "\n",
        "out_match_to = []\n",
        "out_first_ancestor = []\n",
        "out_summary  = []\n",
        "out_sort_key = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw  = row.get(match_col, \"\")\n",
        "    subject_name = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_html = \"<strong>%s</strong>\" % _html.escape(subject_name or \"\", quote=False)\n",
        "\n",
        "    pid          = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_raw  = row.get(name_col, \"\")\n",
        "    matchee_name = norm_matchee_name(matchee_raw) or subject_name\n",
        "\n",
        "    if pid:\n",
        "        matchee_url = (\n",
        "            \"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\"\n",
        "            % (TNG_BASE, pid, TNG_TREE)\n",
        "        )\n",
        "        matchee_name_html = '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (\n",
        "            _html.escape(matchee_url, quote=True),\n",
        "            _html.escape(matchee_name or \"\", quote=False),\n",
        "        )\n",
        "    else:\n",
        "        matchee_name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val       = row.get(cm_col, \"0\")\n",
        "    raw_tokens   = split_tokens(row.get(path_col, \"\"))\n",
        "\n",
        "    # Convert raw tokens to formatted couple displays (+ ids), then apply enriched exclusion\n",
        "    couple_disps = []\n",
        "    couple_ids   = []\n",
        "    for t in raw_tokens:\n",
        "        disp, f_id, m_id = _couple_display_and_ids_from_token(t)\n",
        "        couple_disps.append(disp)\n",
        "        couple_ids.append((f_id, m_id))\n",
        "\n",
        "    couple_disps = _apply_enriched_exclusion_to_couples(couple_disps)\n",
        "\n",
        "    # Keep tokens aligned: if we trimmed couples, recompute the first couple IDs by re-parsing\n",
        "    # from the corresponding raw token range when possible; otherwise fallback parse display.\n",
        "    # Best: find the first displayed couple inside the raw list and use that index.\n",
        "    first_disp = couple_disps[0] if couple_disps else \"\"\n",
        "    first_idx  = 0\n",
        "    if first_disp:\n",
        "        for i, d0 in enumerate([_clean_piece(x) for x in couple_disps]):\n",
        "            pass\n",
        "        for i, d0 in enumerate([_clean_piece(x) for x in couple_disps]):\n",
        "            pass\n",
        "        for i, d in enumerate([_clean_piece(x) for x in couple_disps]):\n",
        "            # only used for comparison normalization\n",
        "            break\n",
        "        # map displayed couples back to original displayed list (not raw tokens)\n",
        "        # locate first_disp within the original untrimmed couple_disps_untrim by norm\n",
        "    # Rebuild trimmed ids by matching against original sequence\n",
        "    couple_disps_untrim = []\n",
        "    couple_ids_untrim   = []\n",
        "    for t in raw_tokens:\n",
        "        disp, f_id, m_id = _couple_display_and_ids_from_token(t)\n",
        "        couple_disps_untrim.append(disp)\n",
        "        couple_ids_untrim.append((f_id, m_id))\n",
        "\n",
        "    # If we trimmed, find where the trimmed list begins within the untrimmed list.\n",
        "    start_at = 0\n",
        "    if couple_disps and couple_disps_untrim:\n",
        "        target = _norm_couple_for_match(couple_disps[0])\n",
        "        found = None\n",
        "        for i, d in enumerate(couple_disps_untrim):\n",
        "            if _norm_couple_for_match(d) == target:\n",
        "                found = i\n",
        "                break\n",
        "        if found is not None:\n",
        "            start_at = found\n",
        "\n",
        "    trimmed_ids = couple_ids_untrim[start_at:start_at+len(couple_disps)] if couple_disps else []\n",
        "    gens_total  = len(couple_disps)\n",
        "\n",
        "    couple_disp = couple_disps[0] if couple_disps else \"\"\n",
        "    f_id = \"\"\n",
        "    m_id = \"\"\n",
        "    if trimmed_ids:\n",
        "        f_id, m_id = trimmed_ids[0]\n",
        "    else:\n",
        "        # fallback: try to pull IDs from the original first token\n",
        "        if couple_ids_untrim:\n",
        "            f_id, m_id = couple_ids_untrim[0]\n",
        "\n",
        "    # Authority key for the (possibly trimmed) first couple\n",
        "    auth_key = \"\"\n",
        "    if f_id and m_id:\n",
        "        auth_key = re.sub(r\"\\s+\", \"\", AUTH_COUPLE_KEY_MAP.get((f_id.upper(), m_id.upper()), \"\"))\n",
        "\n",
        "    if not auth_key:\n",
        "        auth_key = _authority_couple_token_from_display(couple_disp)\n",
        "\n",
        "    couple_html = _html.escape(couple_disp or \"\", quote=False) if couple_disp else \"\"\n",
        "    summary_html = build_header(subject_name_html, cm_val, matchee_name_html, gens_total, couple_html)\n",
        "\n",
        "    out_match_to.append(_html.escape(subject_name or \"\", quote=False))\n",
        "    out_first_ancestor.append(re.sub(r\"\\s+\", \"\", str(auth_key or \"\")).replace(\"&\", \"&#38;\"))\n",
        "    out_summary.append(summary_html)\n",
        "\n",
        "    auth_key = re.sub(r\"\\s+\", \"\", auth_key or \"\")\n",
        "    out_sort_key.append(auth_key or \"zzzzzzzzzzzzzzzzzzzzzzzz\")\n",
        "\n",
        "df_out = pd.DataFrame({\n",
        "    \"Match to\": out_match_to,\n",
        "    \"First Ancestor\": out_first_ancestor,\n",
        "    \"Match Summary\": out_summary,\n",
        "    \"__sort__\": out_sort_key,\n",
        "})\n",
        "df_out = df_out.sort_values(by=\"__sort__\", kind=\"mergesort\").drop(columns=[\"__sort__\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- 7) HTML ----------\n",
        "ROOT_VAR_STYLE = '<style type=\"text/css\">:root{--table-width-px:%dpx;}</style>' % int(TABLE_TOTAL_WIDTH_PX)\n",
        "\n",
        "updated_label = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT or \"\")\n",
        "_updated_parts = [updated_label]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "UPDATED_BLOCK = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join([p for p in _updated_parts if p.strip()]) + '</div>'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls controls-spaced centerline\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "LATE_OVERRIDE_BLOCK = \"\"\n",
        "\n",
        "# Display order: 1) Match to, 2) Match Summary, 3) First Ancestor\n",
        "col_headers = [\n",
        "    (\"Match to\", \"center\"),\n",
        "    (\"Match Summary\", \"left\"),\n",
        "    (\"First Ancestor\", \"center\"),\n",
        "]\n",
        "col_data = [\n",
        "    df_out[\"Match to\"].tolist(),\n",
        "    df_out[\"Match Summary\"].tolist(),\n",
        "    df_out[\"First Ancestor\"].tolist(),\n",
        "]\n",
        "\n",
        "thead_cells = []\n",
        "for idx, (hdr, align) in enumerate(col_headers):\n",
        "    wpx = COL_WIDTHS[idx]\n",
        "    style_attr = \"width:%dpx; display:table-cell !important;\" % wpx\n",
        "    if align == \"center\":\n",
        "        thead_cells.append('<th class=\"center-header\" style=\"%s\">%s</th>' % (style_attr, hdr))\n",
        "    else:\n",
        "        thead_cells.append('<th class=\"left-header\" style=\"%s\">%s</th>' % (style_attr, hdr))\n",
        "\n",
        "thead_html = (\n",
        "    '<thead style=\"display:table-header-group !important;\">\\n'\n",
        "    '  <tr style=\"display:table-row !important;\">'\n",
        "    + \"\".join(thead_cells)\n",
        "    + \"</tr>\\n</thead>\"\n",
        ")\n",
        "\n",
        "tbody_lines = [\"<tbody>\"]\n",
        "for r in range(len(df_out)):\n",
        "    cells = []\n",
        "    for c in range(len(col_headers)):\n",
        "        wpx = COL_WIDTHS[c]\n",
        "        val = col_data[c][r]\n",
        "        val_str = \"\" if val is None else str(val)\n",
        "        cells.append('<td style=\"width:%dpx;\">%s</td>' % (wpx, val_str))\n",
        "    tbody_lines.append(\"  <tr>\" + \"\".join(cells) + \"</tr>\")\n",
        "tbody_lines.append(\"</tbody>\")\n",
        "tbody_html = \"\\n\".join(tbody_lines)\n",
        "\n",
        "html_table = (\n",
        "    '<table border=\"1\" class=\"dataframe sortable dna-register-table\" id=\"refactor-table\">'\n",
        "    + thead_html + \"\\n\" + tbody_html + \"</table>\"\n",
        ")\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div class=\"table-scroll\" id=\"bottom-scroll\">%s</div>'\n",
        "    \"</div>\"\n",
        ") % (html_table,)\n",
        "\n",
        "JS_NAV_REPAIR = \"\"\n",
        "\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "$ROOT_VAR_STYLE\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $LATE_OVERRIDE_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "\n",
        "$JS_NAV_REPAIR\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '')\n",
        "      .replace(/\\\\s+/g,' ')\n",
        "      .trim()\n",
        "      .toLowerCase();\n",
        "  }\n",
        "\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''), 10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){\n",
        "      return String(n||'');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "\n",
        "  function sortTableByKey(tbl, keyColIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[keyColIndex]);\n",
        "      var B = textOf(b.cells[keyColIndex]);\n",
        "\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,''));\n",
        "      var nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "\n",
        "      if(!isNaN(nA) && !isNaN(nB)){\n",
        "        return asc ? (nA - nB) : (nB - nA);\n",
        "      }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      frag.appendChild(rows[i]);\n",
        "    }\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "\n",
        "    // Display indexes after swap:\n",
        "    //   0 Match to\n",
        "    //   1 Match Summary   <-- user clicks here\n",
        "    //   2 First Ancestor  <-- key column\n",
        "    function keyIndexForDisplayIndex(displayIdx){\n",
        "      return (displayIdx === 1) ? 2 : displayIdx;\n",
        "    }\n",
        "\n",
        "    for(var i=0;i<ths.length;i++){\n",
        "      (function(displayIdx){\n",
        "        var th  = ths[displayIdx];\n",
        "        var dir = 'asc';\n",
        "\n",
        "        th.addEventListener('click', function(){\n",
        "          dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "\n",
        "          for (var j = 0; j < ths.length; j++){\n",
        "            ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "          }\n",
        "\n",
        "          th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "\n",
        "          var keyIdx = keyIndexForDisplayIndex(displayIdx);\n",
        "          sortTableByKey(tbl, keyIdx, dir);\n",
        "        }, false);\n",
        "      })(i);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "\n",
        "    var tb   = tbl.tBodies[0];\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt  = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "\n",
        "    box.addEventListener('input',  onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindSearch();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK           = HEAD_LINK,\n",
        "    ROOT_VAR_STYLE      = ROOT_VAR_STYLE,\n",
        "    UPDATED_BLOCK       = UPDATED_BLOCK,\n",
        "    NAV_BLOCK           = NAV_BLOCK,\n",
        "    LATE_OVERRIDE_BLOCK = LATE_OVERRIDE_BLOCK,\n",
        "    CONTROLS_BLOCK      = CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER      = SCROLL_WRAPPER,\n",
        "    JS_NAV_REPAIR       = JS_NAV_REPAIR,\n",
        ")\n",
        "\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"[OK] Saved render: %s\" % os.path.abspath(LOCAL_HTML))\n",
        "print(\"[DEBUG] SSI nav include present:\", (\"<!--#include\" in final_html))\n",
        "\n",
        "# ---------- 8) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload main HTML failed: %s\" % e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [_remote_path(REMOTE_HTML_CANON), _remote_path(REMOTE_HTML_LEG)]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\")\n",
        "        print(\"Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\")\n",
        "        print(\"CSS:       https://yates.one-name.net/partials/partials_unified.css\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session: %s\" % e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "\n",
        "print(\"\\n--- Cell 2 complete (DISPLAY SWAP 2<->3; Match Summary sorts by First Ancestor; authority lock-in module; enriched prefix excluded) ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDqQchmvTfoS",
        "outputId": "3619a5c5-51d0-44f3-927e-99a9f461808a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_3Col_Swap23 | Version=2026.02.02-SWAP23-MATCHSUMMARY-SORTBY-ANCESTOR-LOCKIN+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=1880\n",
            "[LAYOUT] Column widths (px): 1=220 2=1240 3=420\n",
            "[OK] Using local authority module: /content/yates_authority_first_ancestor_map.py\n",
            "[OK] Authority map loaded from module: 25 pairs\n",
            "[OK] Loaded CSV: 93 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Saved render: /content/yates_ancestor_register.shtml\n",
            "[DEBUG] SSI nav include present: True\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 52351\n",
            "partials/ons_yates_dna_register.shtml : 52351\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "CSS:       https://yates.one-name.net/partials/partials_unified.css\n",
            "\n",
            "--- Cell 2 complete (DISPLAY SWAP 2<->3; Match Summary sorts by First Ancestor; authority lock-in module; enriched prefix excluded) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2b"
      ],
      "metadata": {
        "id": "jg9EmTNcTOi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 2b (Unified headers from Cell 2 + Cell 2b functionality) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.01-CELL2HDR-CELL2B1)\n",
        "# * Complete & runnable Colab cell - one contiguous block.\n",
        "# * Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# * XHTML 1.0 Transitional; typography/layout/colors via /partials/partials_unified.css (Cell 2 baseline).\n",
        "# * No nav fallback markup. SSI include only (Cell 2 behavior).\n",
        "# * Fix: single updated/header line with a single \"Showing: <span id='showing-count'></span>\" (JS-populated).\n",
        "# * Fix: updateShowing() counts visible rows in the register table when filtering via selection menu.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2b_Counts_UnifiedHeaders | Version=2026.02.01-CELL2HDR-CELL2B1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# NOTE: In Colab, the notebook cell text is not reliably introspectable for an exact line-count audit.\n",
        "DECLARED_LINES = -1\n",
        "print(\"[AUDIT] DECLARED_LINES={}\".format(DECLARED_LINES))\n",
        "\n",
        "import os, re, posixpath, socket, traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "# Cell 2 baseline stylesheet (single canonical CSS)\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "# Shared nav include (SSI) - no fallback\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "# vitals authority (built by Cell 1)\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "# TNG settings for cousin links\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "# Local partial paths\n",
        "MATCH_COUNT_LOCAL   = os.path.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_LOCAL = os.path.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_LOCAL  = os.path.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# Remote partial paths\n",
        "MATCH_COUNT_REMOTE   = posixpath.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_REMOTE = posixpath.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_REMOTE  = posixpath.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# ---------- 1a) Load vitals authority from dna_vitals.csv ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    last_updated_raw = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display, last_updated_raw\n",
        "\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display, last_updated_raw\n",
        "\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_raw = ts\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display, last_updated_raw\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY, LAST_UPDATED_RAW = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver (match_to_unmasked.csv on server) ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) CSV + name helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ASCII-only separators.\n",
        "SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(')([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def _split_first_last(display: str):\n",
        "    d = _clean_piece(display or \"\")\n",
        "    if not d:\n",
        "        return (\"\", \"\")\n",
        "    parts = d.split()\n",
        "    if len(parts) == 1:\n",
        "        return (\"\", parts[0])\n",
        "    return (\" \".join(parts[:-1]), parts[-1])\n",
        "\n",
        "def _name_bold_last_html(display: str) -> str:\n",
        "    first, last = _split_first_last(display)\n",
        "    if not last and not first:\n",
        "        return \"\"\n",
        "    if not first:\n",
        "        return '<span class=\"mc-last\">%s</span>' % _html.escape(last, quote=False)\n",
        "    return '%s <span class=\"mc-last\">%s</span>' % (_html.escape(first, quote=False), _html.escape(last, quote=False))\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "\n",
        "    def _side_to_name(s):\n",
        "        s = _clean_piece(s)\n",
        "        if \"~\" in s:\n",
        "            bits = [b.strip() for b in s.split(\"~\") if b.strip() != \"\"]\n",
        "            if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "                if len(bits) >= 2:\n",
        "                    return normalize_person_name(bits[1])\n",
        "                return \"\"\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "\n",
        "    left = _side_to_name(parts[0])\n",
        "    right = _side_to_name(parts[1])\n",
        "    if left and right:\n",
        "        return (left, right)\n",
        "    return (\"\", \"\")\n",
        "\n",
        "# ---------- 4b) Lineage label normalization ----------\n",
        "_AMP_SPLIT = re.compile(r\"\\s*(?:&|and|AND|\\+)\\s*\", re.I)\n",
        "\n",
        "def normalize_parents_label(raw: str) -> str:\n",
        "    raw = _clean_piece(raw or \"\")\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    parts = _AMP_SPLIT.split(raw, maxsplit=1)\n",
        "    if len(parts) == 2:\n",
        "        left = smart_titlecase(parts[0])\n",
        "        right = smart_titlecase(parts[1])\n",
        "        left = _clean_piece(left)\n",
        "        right = _clean_piece(right)\n",
        "        if left and right:\n",
        "            return \"%s & %s\" % (left, right)\n",
        "    return smart_titlecase(raw)\n",
        "\n",
        "def _parse_enriched_parent_side(side_raw: str):\n",
        "    side_raw = _clean_piece(side_raw or \"\")\n",
        "    if not side_raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "    if \"~\" in side_raw:\n",
        "        bits = [b.strip() for b in side_raw.split(\"~\")]\n",
        "        bits = [b for b in bits if b != \"\"]\n",
        "        if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "            pid = bits[0].upper()\n",
        "            nm = normalize_person_name(bits[1]) if len(bits) >= 2 else \"\"\n",
        "            yrs = _clean_piece(bits[2]) if len(bits) >= 3 else \"\"\n",
        "            return (pid, nm, yrs)\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", side_raw, flags=re.I)\n",
        "    if m:\n",
        "        pid = m.group(1).upper()\n",
        "        rest = _clean_piece(m.group(2))\n",
        "        yrs = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs = _clean_piece(m2.group(1).replace(\" \", \"\"))\n",
        "            rest = _clean_piece(rest[:m2.start()])\n",
        "        nm = normalize_person_name(rest) if rest else \"\"\n",
        "        return (pid, nm, yrs)\n",
        "\n",
        "    nm2 = smart_titlecase(side_raw) if \" \" in side_raw else smart_titlecase(surname_given_from_token(side_raw)[0])\n",
        "    return (\"\", nm2, \"\")\n",
        "\n",
        "def _lineage_label_with_links(first_raw: str) -> str:\n",
        "    raw = _clean_piece(first_raw or \"\")\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", raw, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return '<span class=\"lc-new\">%s</span>' % _html.escape(normalize_parents_label(raw) or raw, quote=False)\n",
        "\n",
        "    f_id, f_nm, f_yrs = _parse_enriched_parent_side(parts[0])\n",
        "    m_id, m_nm, m_yrs = _parse_enriched_parent_side(parts[1])\n",
        "\n",
        "    def _side_html(pid, name, yrs, legacy_side):\n",
        "        name = _clean_piece(name)\n",
        "        yrs = _clean_piece(yrs)\n",
        "        label = _html.escape(normalize_person_name(legacy_side), quote=False)\n",
        "        if name:\n",
        "            label = _html.escape(name, quote=False)\n",
        "        if yrs:\n",
        "            label = label + ' <span class=\"mc-code\">(%s)</span>' % _html.escape(yrs, quote=False)\n",
        "        if pid:\n",
        "            href = \"%s/familychart.php?personID=%s&tree=%s\" % (TNG_BASE.rstrip(\"/\"), pid.upper(), TNG_TREE)\n",
        "            return '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (_html.escape(href, quote=True), label)\n",
        "        return label\n",
        "\n",
        "    left = _side_html(f_id, f_nm, f_yrs, parts[0])\n",
        "    right = _side_html(m_id, m_nm, m_yrs, parts[1])\n",
        "    return '<span class=\"lc-new\">%s &amp; %s</span>' % (left, right)\n",
        "\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
        "    return t\n",
        "\n",
        "# ---------- CSS helpers (page scoped) ----------\n",
        "_MATCH_COUNT_ONECOL_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \"/* Match Count: ONE centered column, alpha order, scroll shows max 10 rows. */\\n\"\n",
        "    \"#ref-table{border-collapse:separate !important; border-spacing:0 !important;}\\n\"\n",
        "    \"#ref-table thead{position:absolute !important; left:-9999px !important; top:auto !important; width:1px !important; height:1px !important; overflow:hidden !important;}\\n\"\n",
        "    \"#ref-table, #ref-table tbody{display:block !important;}\\n\"\n",
        "    \"#ref-tb{padding:6px 4px 2px 4px !important;display:block !important;width: var(--mc-col-w) !important;max-width: 92vw !important;margin: 0 auto !important;box-sizing:border-box !important;--mc-row-h: 40px;--mc-col-w: 360px;max-height: calc(var(--mc-row-h) * 10) !important;overflow-y: auto !important;overflow-x: hidden !important;}\\n\"\n",
        "    \".tile-head{width: var(--mc-col-w) !important;max-width: 92vw !important;margin: 0 auto 6px auto !important;padding: 6px 10px !important;box-sizing:border-box !important;border: 1px solid #ddd !important;border-radius: 10px !important;background: #f7f7f7 !important;display:flex !important;align-items:center !important;justify-content:space-between !important;font-weight:bold !important;}\\n\"\n",
        "    \".tile-head .th-left{flex:1 1 auto !important; min-width:0 !important;}\\n\"\n",
        "    \".tile-head .th-right{flex:0 0 auto !important; margin-left:10px !important; white-space:nowrap !important;}\\n\"\n",
        "    \"#ref-tb tr{display:flex !important; align-items:center !important;box-sizing:border-box !important;height: var(--mc-row-h) !important;margin:0 0 6px 0 !important;padding:0 10px !important;border:1px solid #ddd !important;border-radius:10px !important;background:#fff !important;overflow:hidden !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row{border-color:#d1a500 !important; box-shadow:0 0 0 2px rgba(209,165,0,0.15) inset !important;}\\n\"\n",
        "    \"#ref-tb td{border:none !important; padding:0 !important;}\\n\"\n",
        "    \"#ref-tb td.mc-name{flex:1 1 auto !important; min-width:0 !important; white-space:nowrap !important; overflow:hidden !important; text-overflow:ellipsis !important;}\\n\"\n",
        "    \"#ref-tb td.mc-count{flex:0 0 auto !important; margin-left:10px !important; font-weight:bold !important;}\\n\"\n",
        "    \"#ref-tb a.count-pick{text-decoration:none !important; padding:2px 6px !important; border:1px solid #ccc !important; border-radius:8px !important; display:inline-block !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row a.count-pick{border-color:#d1a500 !important;}\\n\"\n",
        "    \".mc-last{font-weight:bold !important;}\\n\"\n",
        "    \".mc-code{font-size:90% !important; color:#666 !important;}\\n\"\n",
        "    \"/* Ensure register headers stay visible */\\n\"\n",
        "    \"#reg-list thead{display:table-header-group !important;}\\n\"\n",
        "    \"#reg-list thead tr{display:table-row !important;}\\n\"\n",
        "    \"#reg-list thead th{display:table-cell !important;}\\n\"\n",
        "    \"#reg-list th{position:sticky !important; top:0 !important; background:#ffffff !important; z-index:10 !important;}\\n\"\n",
        "    \"@media print{ #ref-tb{max-height:none !important; overflow:visible !important; width:auto !important; max-width:none !important; margin:0 !important;} #ref-table thead{position:static !important; left:auto !important; width:auto !important; height:auto !important; overflow:visible !important;}}\\n\"\n",
        "    \"</style>\\n\"\n",
        ")\n",
        "\n",
        "_LINEAGE_COUNT_ONECOL_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \":root{--lc-col-w:min(760px,calc(100vw - 28px));}\\n\"\n",
        "    \".table-scroll.lineage-scroll{max-height:520px;overflow-y:auto;overflow-x:hidden;}\\n\"\n",
        "    \".table-scroll.reg-scroll{max-height:none;overflow:visible;}\\n\"\n",
        "    \"#ref-table{border-collapse:separate !important;border-spacing:0 !important;}\\n\"\n",
        "    \"#ref-table thead{position:absolute !important; left:-9999px !important; top:auto !important; width:1px !important; height:1px !important; overflow:hidden !important;}\\n\"\n",
        "    \"#ref-table,#ref-table tbody{display:block !important;}\\n\"\n",
        "    \".tile-head{width:var(--lc-col-w) !important;margin:0 auto 6px auto !important;padding:6px 10px !important;box-sizing:border-box !important;border:1px solid #ddd !important;border-radius:12px !important;background:#f7f7f7 !important;display:flex !important;align-items:center !important;justify-content:space-between !important;font-weight:bold !important;}\\n\"\n",
        "    \".tile-head .th-left{flex:1 1 auto !important; min-width:0 !important;}\\n\"\n",
        "    \".tile-head .th-right{flex:0 0 auto !important; margin-left:10px !important; white-space:nowrap !important;}\\n\"\n",
        "    \"#ref-tb{padding:4px 4px 2px 4px !important;display:block !important;width:var(--lc-col-w) !important;margin:0 auto !important;}\\n\"\n",
        "    \"#ref-tb tr{display:flex !important;align-items:center !important;gap:6px !important;width:100% !important;margin:5px auto !important;padding:7px 9px !important;border:1px solid #ddd !important;border-radius:12px !important;background:#fff !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row{border-color:#d1a500 !important;box-shadow:0 0 0 2px rgba(209,165,0,0.15) inset !important;}\\n\"\n",
        "    \"#ref-tb td{border:none !important;padding:0 !important;vertical-align:top !important;}\\n\"\n",
        "    \"#ref-tb td.lc-name{flex:1 1 auto !important;white-space:normal !important;overflow:visible !important;text-overflow:clip !important;}\\n\"\n",
        "    \"#ref-tb td.lc-count{flex:0 0 auto !important;margin-left:0 !important;font-weight:bold !important;white-space:nowrap !important;}\\n\"\n",
        "    \"#ref-tb a.count-pick{text-decoration:none !important;padding:2px 7px !important;border:1px solid #ccc !important;border-radius:10px !important;display:inline-block !important;}\\n\"\n",
        "    \"#ref-tb tr.sel-row a.count-pick{border-color:#d1a500 !important;}\\n\"\n",
        "    \".lc-new{display:block;margin-top:0;}\\n\"\n",
        "    \".lc-new a{text-decoration:none;}\\n\"\n",
        "    \".lc-new a:hover{text-decoration:underline;}\\n\"\n",
        "    \"#reg-list thead{display:table-header-group !important;}\\n\"\n",
        "    \"#reg-list thead tr{display:table-row !important;}\\n\"\n",
        "    \"#reg-list thead th{display:table-cell !important;}\\n\"\n",
        "    \"#reg-list th{position:sticky !important; top:0 !important; background:#ffffff !important; z-index:10 !important;}\\n\"\n",
        "    \"@media print{ #ref-table thead{position:static !important; left:auto !important; width:auto !important; height:auto !important; overflow:visible !important;}}\\n\"\n",
        "    \"</style>\\n\"\n",
        ")\n",
        "\n",
        "# ---------- Cell 2 header builder (single updated line; single Showing span) ----------\n",
        "def _build_updated_block() -> str:\n",
        "    # Match Cell 2 structure: Last updated: <span id=last-updated>...</span> | Autosomal matches: N | Showing: <span id=showing-count></span>\n",
        "    parts = []\n",
        "    parts.append('Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_DISPLAY or \"(unknown)\", quote=False))\n",
        "    parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES_TEXT or \"(unknown)\", quote=False))\n",
        "    parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "    return '<div class=\"updated centerline\">%s</div>' % (' &nbsp;|&nbsp; '.join(parts))\n",
        "\n",
        "def _partial_head(title, helper_css=\"\"):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n\"\n",
        "        \"<head>\\n\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        \"<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\" />\\n\"\n",
        "        \"<title>%s</title>\\n\" % _html.escape(title, quote=False)\n",
        "        + HEAD_LINK + \"\\n\"\n",
        "        + (helper_css or \"\")\n",
        "        + \"</head>\\n<body id=\\\"top\\\">\\n\"\n",
        "        \"<div class=\\\"wrap\\\">\\n\"\n",
        "        \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title, quote=False)\n",
        "        + _build_updated_block() + \"\\n\"\n",
        "        + NAV_BLOCK + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "          \"Showing: \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "          \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    # updateShowing() mirrors Cell 2 idea: compute visible row count.\n",
        "    # Here we count visible rows in reg-list (the register) when it exists.\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\\n\"\n",
        "        \"function formatWithCommas(n){\\n\"\n",
        "        \" try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }\\n\"\n",
        "        \" catch(e){ return String(n||''); }\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function countVisibleRowsInTable(tableId){\\n\"\n",
        "        \" var t=document.getElementById(tableId);\\n\"\n",
        "        \" if(!(t && t.tBodies && t.tBodies.length)) return 0;\\n\"\n",
        "        \" var rows=t.tBodies[0].rows, n=0;\\n\"\n",
        "        \" for(var i=0;i<rows.length;i++){ if(rows[i].style.display !== 'none') n++; }\\n\"\n",
        "        \" return n;\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function updateShowing(){\\n\"\n",
        "        \" var el=document.getElementById('showing-count');\\n\"\n",
        "        \" if(!el) return;\\n\"\n",
        "        \" var n = 0;\\n\"\n",
        "        \" if(document.getElementById('reg-list')){ n = countVisibleRowsInTable('reg-list'); }\\n\"\n",
        "        \" else if(document.getElementById('ref-table')){ n = countVisibleRowsInTable('ref-table'); }\\n\"\n",
        "        \" el.textContent = formatWithCommas(n);\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function ySelEachRow(tb, cb){ if(!tb) return; var rows=tb.getElementsByTagName('tr'); for(var i=0;i<rows.length;i++){cb(rows[i]);} }\\n\"\n",
        "        \"function ySelClear(tr){ if(!tr) return; tr.removeAttribute('data-selected'); var cls=tr.className||''; cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,''); tr.className=cls; tr.style.backgroundColor=''; }\\n\"\n",
        "        \"function ySelToggle(a){ var tr=a; while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;} if(!tr) return false; var sel=tr.getAttribute('data-selected')==='1'; if(sel){ ySelClear(tr);}else{ tr.setAttribute('data-selected','1'); var cls=tr.className||''; if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';} tr.style.backgroundColor='#fff2cc'; } updateShowing(); return false; }\\n\"\n",
        "        \"function ySelGetTBody(tbodyId){ var tb=document.getElementById(tbodyId); if(tb) return tb; var t=document.getElementById('ref-table'); if(!t) return null; if(t.tBodies&&t.tBodies.length){return t.tBodies[0];} return t; }\\n\"\n",
        "        \"function ySelShowSelected(tbodyId){\\n\"\n",
        "        \" var tb=ySelGetTBody(tbodyId); if(!tb) return false;\\n\"\n",
        "        \" ySelEachRow(tb,function(tr){ var sel=tr.getAttribute('data-selected')==='1'; tr.style.display=sel?'':'none'; });\\n\"\n",
        "        \" var rl=document.getElementById('reg-list');\\n\"\n",
        "        \" if(rl){\\n\"\n",
        "        \"  var selVals=[];\\n\"\n",
        "        \"  ySelEachRow(tb,function(tr){ if(tr.getAttribute('data-selected')==='1'){ var v=tr.getAttribute('data-filter')||tr.getAttribute('data-lineage')||tr.getAttribute('data-code')||tr.getAttribute('data-q')||''; if(v){selVals.push(v);} } });\\n\"\n",
        "        \"  if(selVals.length===0){ updateShowing(); return false; }\\n\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\\n\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"    var r=rows[i]; var lv=r.getAttribute('data-filter')||r.getAttribute('data-lineage')||r.getAttribute('data-code')||'';\\n\"\n",
        "        \"    var show=false; for(var j=0;j<selVals.length;j++){ if(lv===selVals[j]){show=true; break;} }\\n\"\n",
        "        \"    r.style.display=show?'':'none';\\n\"\n",
        "        \"  }\\n\"\n",
        "        \" }\\n\"\n",
        "        \" updateShowing();\\n\"\n",
        "        \" return false;\\n\"\n",
        "        \"}\\n\"\n",
        "        \"function ySelShowAll(tbodyId){ var tb=ySelGetTBody(tbodyId); if(!tb) return false; ySelEachRow(tb,function(tr){tr.style.display='';}); var rl=document.getElementById('reg-list'); if(rl){ var rows=rl.getElementsByTagName('tr'); for(var i=0;i<rows.length;i++){rows[i].style.display='';} } updateShowing(); return false; }\\n\"\n",
        "        \"function ySelReset(tbodyId){ var tb=ySelGetTBody(tbodyId); if(!tb) return false; ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);}); var rl=document.getElementById('reg-list'); if(rl){ var rows=rl.getElementsByTagName('tr'); for(var i=0;i<rows.length;i++){rows[i].style.display='';} } updateShowing(); return false; }\\n\"\n",
        "        \"window.ySelToggle=ySelToggle; window.ySelShowSelected=ySelShowSelected; window.ySelShowAll=ySelShowAll; window.ySelReset=ySelReset;\\n\"\n",
        "        \"document.addEventListener('DOMContentLoaded', function(){ updateShowing(); }, false);\\n\"\n",
        "        \"})();\\n\"\n",
        "        \"//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(row, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        name_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        name_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "    return subject_name_html, name_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Match Count partial (ONE COL, ALPHA) ----------\n",
        "def build_match_count_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    codes_raw = main_df[match_col].astype(str).map(lambda x: x.strip())\n",
        "    keys_norm = codes_raw.map(_norm_code_for_count)\n",
        "\n",
        "    counts_series = keys_norm.value_counts(dropna=False)\n",
        "    counts = counts_series.reset_index()\n",
        "    if counts.shape[1] >= 2:\n",
        "        counts.columns = [\"norm_key\", \"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"] = counts.index.astype(str)\n",
        "        counts[\"Count\"] = counts_series.values\n",
        "        counts = counts[[\"norm_key\", \"Count\"]]\n",
        "\n",
        "    first_display = {}\n",
        "    raw_list = codes_raw.tolist()\n",
        "    norm_list = keys_norm.tolist()\n",
        "    for code_disp, k in zip(raw_list, norm_list):\n",
        "        if k not in first_display and str(k) != \"\":\n",
        "            first_display[k] = code_disp\n",
        "\n",
        "    counts[\"Code\"] = counts[\"norm_key\"].map(lambda k: first_display.get(k, k))\n",
        "    counts[\"Unmasked\"] = counts[\"norm_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "\n",
        "    disp_names = []\n",
        "    sort_alpha = []\n",
        "    for _, r in counts.iterrows():\n",
        "        code = str(r.get(\"Code\", \"\") or \"\").strip()\n",
        "        unm = str(r.get(\"Unmasked\", \"\") or \"\").strip()\n",
        "        label = (unm or code).strip()\n",
        "        disp = normalize_person_name(label)\n",
        "        disp_names.append(disp)\n",
        "        sort_alpha.append((disp or label).lower())\n",
        "\n",
        "    counts[\"Disp\"] = disp_names\n",
        "    counts[\"SortAlpha\"] = sort_alpha\n",
        "    counts = counts.sort_values(by=[\"SortAlpha\", \"Count\"], ascending=[True, False], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    total_participants = int(len(counts))\n",
        "    page_title = \"Network participants (matches): %d\" % total_participants\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(page_title, helper_css=_MATCH_COUNT_ONECOL_CSS))\n",
        "\n",
        "    html.append('<div class=\"tile-head\"><span class=\"th-left\">Match to</span><span class=\"th-right\">Count</span></div>')\n",
        "\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th>Match to</th><th>Count</th>')\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in counts.iterrows():\n",
        "        code = str(r.get(\"Code\", \"\") or \"\").strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        norm_key = _norm_code_for_count(code)\n",
        "\n",
        "        unm = (str(r.get(\"Unmasked\", \"\") or \"\")).strip()\n",
        "        label = (unm or code).strip()\n",
        "\n",
        "        disp = str(r.get(\"Disp\", \"\") or \"\").strip()\n",
        "        name_part = _name_bold_last_html(disp) if disp else _html.escape(label, quote=False)\n",
        "\n",
        "        code_part = \"\"\n",
        "        if code:\n",
        "            code_part = ' <span class=\"mc-code\">(%s)</span>' % _html.escape(code, quote=False)\n",
        "        disp_html = name_part + code_part\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td class=\\\"mc-name\\\">%s</td>\"\n",
        "            \"<td class=\\\"mc-count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td></tr>\"\n",
        "            % (\n",
        "                _html.escape(label, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                disp_html,\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected participant(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead style=\"display:table-header-group !important;\"><tr style=\"display:table-row !important;\">'\n",
        "        '<th style=\"display:table-cell !important;\">Match to</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Name</th>'\n",
        "        '<th style=\"display:table-cell !important;\">cM</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        code_raw = str(row.get(match_col, \"\")).strip()\n",
        "        if not code_raw:\n",
        "            continue\n",
        "        norm_key = _norm_code_for_count(code_raw)\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(row, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Lineage Count partial ----------\n",
        "def build_lineage_count_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    first_series = (\n",
        "        main_df.get(\"First Ancestor\", pd.Series(dtype=str))\n",
        "        .astype(str)\n",
        "        .map(lambda x: x.strip())\n",
        "    )\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    vis_labels = []\n",
        "    sort_alpha = []\n",
        "    for _, r in lin_df.iterrows():\n",
        "        raw = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        vis = normalize_parents_label(raw) or raw\n",
        "        vis_labels.append(vis)\n",
        "        sort_alpha.append(vis.lower())\n",
        "    lin_df[\"VisLabel\"] = vis_labels\n",
        "    lin_df[\"SortAlpha\"] = sort_alpha\n",
        "\n",
        "    lin_df = lin_df.sort_values([\"Count\", \"First Ancestor\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Lineage Count\", helper_css=_LINEAGE_COUNT_ONECOL_CSS))\n",
        "    html[-1] = html[-1].replace('<div class=\"table-scroll\">', '<div class=\"table-scroll lineage-scroll\">', 1)\n",
        "\n",
        "    html.append('<div class=\"tile-head\"><span class=\"th-left\">First Ancestor</span><span class=\"th-right\">Count</span></div>')\n",
        "\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append('<th>First Ancestor</th><th>Count</th>')\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first_raw = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        lineage_key = first_raw\n",
        "\n",
        "        parents_html = _lineage_label_with_links(first_raw)\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td class=\\\"lc-name\\\">%s</td>\"\n",
        "            \"<td class=\\\"lc-count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first_raw, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(lineage_key, quote=True),\n",
        "                _html.escape(lineage_key, quote=True),\n",
        "                parents_html,\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append(\"</div>\")\n",
        "    html.append('<div class=\"table-scroll reg-scroll\">\\n')\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected lineage(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead style=\"display:table-header-group !important;\"><tr style=\"display:table-row !important;\">'\n",
        "        '<th style=\"display:table-cell !important;\">Match to</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Name</th>'\n",
        "        '<th style=\"display:table-cell !important;\">cM</th>'\n",
        "        '<th style=\"display:table-cell !important;\">Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        first = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        if not first:\n",
        "            continue\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(row, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 9) Cousin printable partial ----------\n",
        "def build_cousin_print_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    rows = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        subject_raw = row.get(match_col, \"\")\n",
        "        subject_name = normalize_person_name(MATCH_TO_UNMASKED.get(str(subject_raw).strip().lower(), subject_raw))\n",
        "        subject_name_html = \"<strong>%s</strong>\" % subject_name if subject_name else \"\"\n",
        "\n",
        "        pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "        matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "        if pid:\n",
        "            matchee_name_html = (\n",
        "                '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "                'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "                % (TNG_BASE, pid, TNG_TREE, matchee_name)\n",
        "            )\n",
        "        else:\n",
        "            matchee_name_html = matchee_name\n",
        "\n",
        "        cm_val = row.get(cm_col, \"0\")\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        gens_total = len(tokens)\n",
        "\n",
        "        if \"common_husband\" in main_df.columns and \"common_wife\" in main_df.columns:\n",
        "            husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "            wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "            if not husband_raw and not wife_raw:\n",
        "                husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "        else:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "        header_html = build_header(subject_name_html or subject_name, cm_val, matchee_name_html, gens_total, husband_raw, wife_raw)\n",
        "        rows.append(header_html)\n",
        "\n",
        "    rows_sorted = sorted(rows)\n",
        "\n",
        "    html_rows = [\n",
        "        '<table border=\"1\" id=\"refactor-table\" class=\"sortable\"><thead><tr><th>Match Summary</th></tr></thead><tbody>'\n",
        "    ]\n",
        "    for v in rows_sorted:\n",
        "        html_rows.append(\"<tr><td>%s</td></tr>\" % v)\n",
        "    html_rows.append(\"</tbody></table>\")\n",
        "\n",
        "    cousin_html = (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\" \"\n",
        "        \"\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\"><head>\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\"\n",
        "        \"<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\" />\"\n",
        "        \"<title>Cousin List (Printable)</title>\"\n",
        "        + HEAD_LINK +\n",
        "        \"</head><body onload=\\\"window.print();\\\">\"\n",
        "        \"<div class=\\\"wrap\\\">\"\n",
        "        \"<h1 class=\\\"centerline\\\">Cousin List (Printable)</h1>\"\n",
        "        \"<div class=\\\"table-scroll\\\">%s</div>\"\n",
        "        \"</div></body></html>\"\n",
        "        % \"\".join(html_rows)\n",
        "    )\n",
        "    return cousin_html\n",
        "\n",
        "# ---------- 10) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for counts: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(df, [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"], [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"])\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column for First Ancestor.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    first_ancestors = []\n",
        "    for _, row in df.iterrows():\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    mc_html = build_match_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(MATCH_COUNT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(mc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(MATCH_COUNT_LOCAL))\n",
        "\n",
        "    lc_html = build_lineage_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(LINEAGE_COUNT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(lc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(LINEAGE_COUNT_LOCAL))\n",
        "\n",
        "    cousin_html = build_cousin_print_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(COUSIN_PRINT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(COUSIN_PRINT_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, MATCH_COUNT_LOCAL, _remote_path(MATCH_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, LINEAGE_COUNT_LOCAL, _remote_path(LINEAGE_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, COUSIN_PRINT_LOCAL, _remote_path(COUSIN_PRINT_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload partials failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [_remote_path(MATCH_COUNT_REMOTE), _remote_path(LINEAGE_COUNT_REMOTE), _remote_path(COUSIN_PRINT_REMOTE)]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.shtml\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2b (Unified headers from Cell 2 + Cell 2b functionality) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAZRkyHlz_tR",
        "outputId": "f2c6f7ab-7dfd-41f9-93f9-c8f5478c0d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts_UnifiedHeaders | Version=2026.02.01-CELL2HDR-CELL2B1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 226\n",
            "[VITALS] last updated (display): February 1, 2026 8:30 AM\n",
            "[OK] Loaded CSV for counts: 226 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 160850\n",
            "partials/lineage_count.shtml : 210518\n",
            "partials/cousin_list_print.htm : 81504\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2b_NetworkAuthority"
      ],
      "metadata": {
        "id": "MUDyjlbJbQoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2b_NetworkAuthority - Write dna_network_first_ancestors.txt ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Reads the same master CSV used by Cell 2b and derives a de-duplicated\n",
        "#   list of \"first ancestors\" from the lineage/ancestral-line column.\n",
        "# - Writes that list as /content/dna_network_first_ancestors.txt, one per line.\n",
        "# - This file is then consumed by Cell 2d (SaveNetworkAuthority) and Cell 2c\n",
        "#   (Match Specific Produced DNA Network).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2b_NetworkAuthority | Version=2025.12.06 | Encoding=ISO-8859-15 | DECLARED_LINES=160\n",
        "\n",
        "DECLARED_LINES = 160\n",
        "print(\n",
        "    \"[CONFIRM] Golden Rules active | \"\n",
        "    \"Cell=Cell2b_NetworkAuthority | \"\n",
        "    \"Version=2025.12.06 | \"\n",
        "    \"Encoding=ISO-8859-15 | \"\n",
        "    \"DECLARED_LINES=%d\" % DECLARED_LINES\n",
        ")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "\n",
        "# This should match the master CSV name used by Cell 2b.\n",
        "MASTER_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Authority file that Cell 2d expects and that Cell 2c already uses.\n",
        "AUTHORITY_PATH = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# ---------- 2) Helpers ----------\n",
        "\n",
        "def _clean_piece(text):\n",
        "    \\\"\\\"\\\"Normalize whitespace and tildes inside a lineage token.\\\"\\\"\\\"\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "# Same separator logic we used in Cell 2c: split a lineage path into tokens.\n",
        "SEP_RE = re.compile(\n",
        "    r\"\\s*(?:\"\n",
        "    r\"\\u2192\"          # unicode right arrow\n",
        "    r\"|&rarr;\"         # HTML right arrow\n",
        "    r\"|;|>|,\"\n",
        "    r\"|~{2,}\"          # multiple tildes\n",
        "    r\"|/{2,}\"          # double slash\n",
        "    r\"|\\|{2,}\"         # double pipe\n",
        "    r\")\\s*\"\n",
        ")\n",
        "\n",
        "def split_tokens(s):\n",
        "    \\\"\\\"\\\"Split a lineage path string into tokens using SEP_RE.\\\"\\\"\\\"\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    \\\"\\\"\\\"\n",
        "    Find a column in df using regex patterns and optional preferred names.\n",
        "    Returns the column name or None.\n",
        "    \\\"\\\"\\\"\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    # Preferred exact names first\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    # Otherwise use regex patterns\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ---------- 3) Load master CSV ----------\n",
        "\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(MASTER_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "\n",
        "if df is None:\n",
        "    raise SystemExit(\n",
        "        \"[ERROR] Unable to read CSV: %s (%r)\" % (MASTER_CSV, _last_err)\n",
        "    )\n",
        "\n",
        "print(\n",
        "    \"[OK] Loaded master CSV for network authority: %d rows, %d cols\"\n",
        "    % (len(df), len(df.columns))\n",
        ")\n",
        "\n",
        "# ---------- 4) Identify lineage / ancestral-line column ----------\n",
        "\n",
        "line_col = find_col(\n",
        "    df,\n",
        "    patterns=[\n",
        "        r\"(yates\\s*dna\\s*ancestral\\s*line)\",\n",
        "        r\"(ancestral\\s*line)\",\n",
        "        r\"(lineage)\"\n",
        "    ],\n",
        "    prefer_exact=[\n",
        "        \"Yates DNA Ancestral Line\",\n",
        "        \"Ancestral Line\",\n",
        "        \"Lineage\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "if not line_col:\n",
        "    raise SystemExit(\n",
        "        \"[ERROR] Cannot find lineage/ancestral-line column in master CSV.\"\n",
        "    )\n",
        "\n",
        "print(\"[OK] Using lineage column:\", repr(line_col))\n",
        "\n",
        "# ---------- 5) Derive first ancestors and de-duplicate ----------\n",
        "\n",
        "first_ancestors = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    tokens = split_tokens(row.get(line_col, \"\"))\n",
        "    first = _clean_piece(tokens[0]) if tokens else \"\"\n",
        "    if first:\n",
        "        first_ancestors.append(first)\n",
        "\n",
        "total_raw = len(first_ancestors)\n",
        "unique_list = []\n",
        "seen = set()\n",
        "\n",
        "# Preserve original encounter order, but discard duplicates and empties.\n",
        "for anc in first_ancestors:\n",
        "    key = anc.strip()\n",
        "    if not key:\n",
        "        continue\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    unique_list.append(key)\n",
        "\n",
        "print(\"[INFO] Total first-ancestor tokens collected: %d\" % total_raw)\n",
        "print(\"[INFO] Unique non-empty first ancestors:    %d\" % len(unique_list))\n",
        "\n",
        "# Optional: show a short preview in the notebook\n",
        "for idx, anc in enumerate(unique_list[:25], start=1):\n",
        "    print(\"  %2d. %s\" % (idx, anc))\n",
        "\n",
        "if not unique_list:\n",
        "    print(\"[WARN] No non-empty first ancestors found. Authority file will NOT be written.\")\n",
        "else:\n",
        "    # ---------- 6) Write dna_network_first_ancestors.txt ----------\n",
        "    try:\n",
        "        # Ensure directory exists (AUTHORITY_PATH is just a filename in /content).\n",
        "        os.makedirs(os.path.dirname(AUTHORITY_PATH) or \".\", exist_ok=True)\n",
        "        with open(\n",
        "            AUTHORITY_PATH,\n",
        "            \"w\",\n",
        "            encoding=\"iso-8859-15\",\n",
        "            errors=\"xmlcharrefreplace\",\n",
        "        ) as f:\n",
        "            for anc in unique_list:\n",
        "                f.write(anc.strip() + \"\\n\")\n",
        "        print(\"[OK] Wrote authority file:\", os.path.abspath(AUTHORITY_PATH))\n",
        "        print(\"[OK] Lines written:\", len(unique_list))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Failed to write authority file:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Cell2b_NetworkAuthority complete (authority file ready for Cell 2d) ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2b_NetworkAuthority - Write dna_network_first_ancestors.txt ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kpgsxWD3WTx",
        "outputId": "3fd942ab-dc56-45ca-9011-a6251d5335fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2b_NetworkAuthority | Version=2025.12.06 | Encoding=ISO-8859-15 | DECLARED_LINES=160\n",
            "[OK] Loaded master CSV for network authority: 93 rows, 6 cols\n",
            "[OK] Using lineage column: 'Yates DNA Ancestral Line'\n",
            "[INFO] Total first-ancestor tokens collected: 93\n",
            "[INFO] Unique non-empty first ancestors:    26\n",
            "   1. RobinsonWallaceAl&JonesSallieMcL\n",
            "   2. UnknownName&FordVeatriceM\n",
            "   3. YatesFrancis&TichborneJane\n",
            "   4. YatesJohn&BarfieldElizabeth\n",
            "   5. YatesJohn&GaterJoane\n",
            "   6. YatesThomas&SearchingStill\n",
            "   7. YatesThomas&SmithMary\n",
            "   8. YatesUriah&OakesSarah\n",
            "   9. YatesWilliam&BoothAnamariaE\n",
            "  10. YatesWilliam&EdwardsElizabeth\n",
            "  11. YatesWilliam&HouseHannah\n",
            "  12. YatesWilliam&NeedhamMary\n",
            "  13. YatesWilliam&ParkerSally\n",
            "  14. YatesWilliam&PikeEsther\n",
            "  15. YatesWilliam&RidyardAnn\n",
            "  16. YatesWilliam&SaltPhoebe\n",
            "  17. YatesWilliam&ThornburyAnne\n",
            "  18. YatesWilliamBa&BullockMartha\n",
            "  19. YatesWilliamCh&McManusEmilyMill\n",
            "  20. YatesWilliamJo&HolsteadSarahJane\n",
            "  21. YatesWilliamLe&LampingEllenIsab\n",
            "  22. YatesWilliamM&OsborneNancyAnn\n",
            "  23. YatesWilliamNi&HaysElizabeth\n",
            "  24. YatesWilliamPr&McKinneyElizabeth\n",
            "  25. YatesWilliamRo&DavisMalissa\n",
            "[OK] Wrote authority file: /content/dna_network_first_ancestors.txt\n",
            "[OK] Lines written: 26\n",
            "\n",
            "--- Cell2b_NetworkAuthority complete (authority file ready for Cell 2d) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2c"
      ],
      "metadata": {
        "id": "ah9XcKRcqLU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2c - Match Specific Produced DNA Network ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G4)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout via /partials/dna_tree_styles.css (linked only).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "DNA_NETWORK_LOCAL = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "DNA_NETWORK_REMOTE = posixpath.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "# Authority list is maintained on the server at /dna/network_first_ancestors.txt\n",
        "DNA_NETWORK_AUTH_REMOTE = \"dna/network_first_ancestors.txt\"\n",
        "DNA_NETWORK_AUTH_LOCAL = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# ---------- 1a) Load vitals ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def sync_authority_from_server():\n",
        "    \"\"\"\n",
        "    Pull /dna/network_first_ancestors.txt from the server (if it exists)\n",
        "    into DNA_NETWORK_AUTH_LOCAL.\n",
        "    \"\"\"\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[INFO] FTP creds missing; authority sync skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        remote = _remote_path(DNA_NETWORK_AUTH_REMOTE)\n",
        "        ok = ftp_download_if_exists(ftps, remote, DNA_NETWORK_AUTH_LOCAL)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        if ok:\n",
        "            print(\"[INFO] Authority file synced from server.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Authority sync failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = False\n",
        "        try:\n",
        "            local_name = SERVER_MAPPING_LOCAL_CACHE\n",
        "            with open(local_name, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % SERVER_MAPPING_BASENAME, f.write)\n",
        "            print(\"[PULL] %s -> %s\" % (SERVER_MAPPING_BASENAME, os.path.abspath(local_name)))\n",
        "            ok = True\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                if os.path.exists(SERVER_MAPPING_LOCAL_CACHE):\n",
        "                    os.remove(SERVER_MAPPING_LOCAL_CACHE)\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"[MISS] %s (%s)\" % (SERVER_MAPPING_BASENAME, e))\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) Helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ASCII-only separators.\n",
        "SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\",\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    if \"-\" in token:\n",
        "        token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) HTML shell ----------\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        + \"%s\\n\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        + \"<title>%s</title>\\n\" % _html.escape(title)\n",
        "        + \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        + \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title)\n",
        "        + \"<div class=\\\"updated centerline\\\">\"\n",
        "        \"Last updated: %s &nbsp;|&nbsp; \"\n",
        "        \"Showing: %s\"\n",
        "        \"</div>\\n\" % (\n",
        "            _html.escape(LAST_UPDATED_DISPLAY),\n",
        "            _html.escape(AUTOSOMAL_MATCHES_TEXT),\n",
        "        )\n",
        "        + NAV_BLOCK\n",
        "        + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "        \"Showing: \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "        \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function ySelEachRow(tb, cb){\"\n",
        "        \" if(!tb) return;\"\n",
        "        \" var rows=tb.getElementsByTagName('tr');\"\n",
        "        \" for(var i=0;i<rows.length;i++){cb(rows[i]);}\"\n",
        "        \"}\"\n",
        "        \"function ySelClear(tr){\"\n",
        "        \" if(!tr) return;\"\n",
        "        \" tr.removeAttribute('data-selected');\"\n",
        "        \" var cls=tr.className||'';\"\n",
        "        \" cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,'');\"\n",
        "        \" tr.className=cls;\"\n",
        "        \" tr.style.backgroundColor='';\"\n",
        "        \"}\"\n",
        "        \"function ySelToggle(a){\"\n",
        "        \" var tr=a;\"\n",
        "        \" while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;}\"\n",
        "        \" if(!tr) return false;\"\n",
        "        \" var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \" if(sel){\"\n",
        "        \"  ySelClear(tr);\"\n",
        "        \" }else{\"\n",
        "        \"  tr.setAttribute('data-selected','1');\"\n",
        "        \"  var cls=tr.className||'';\"\n",
        "        \"  if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';}\"\n",
        "        \"  tr.style.backgroundColor='#fff2cc';\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelGetTBody(tbodyId){\"\n",
        "        \" var tb=document.getElementById(tbodyId);\"\n",
        "        \" return tb || null;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowSelected(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){\"\n",
        "        \"  var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \"  tr.style.display=sel?'':'none';\"\n",
        "        \" });\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowAll(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display='';});\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelReset(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);});\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"window.ySelToggle=ySelToggle;\"\n",
        "        \"window.ySelShowSelected=ySelShowSelected;\"\n",
        "        \"window.ySelShowAll=ySelShowAll;\"\n",
        "        \"window.ySelReset=ySelReset;\"\n",
        "        \"})();\"\n",
        "        \"(function(){\"\n",
        "        \" function collectFirstAncestors(){\"\n",
        "        \"  var rows=document.querySelectorAll('#ref-tb tr');\"\n",
        "        \"  var seen=Object.create(null);\"\n",
        "        \"  var out=[];\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    var cells=rows[i].getElementsByTagName('td');\"\n",
        "        \"    if(!cells.length) continue;\"\n",
        "        \"    var txt=(cells[0].textContent||cells[0].innerText||'').replace(/\\\\s+/g,' ').trim();\"\n",
        "        \"    if(!txt) continue;\"\n",
        "        \"    if(!seen[txt]){seen[txt]=true; out.push(txt);}\"\n",
        "        \"  }\"\n",
        "        \"  return out;\"\n",
        "        \" }\"\n",
        "        \" function setStatus(msg,isError){\"\n",
        "        \"  var span=document.getElementById('save-network-status');\"\n",
        "        \"  if(!span) return;\"\n",
        "        \"  span.textContent=msg;\"\n",
        "        \"  span.style.color=isError?'#990000':'#006600';\"\n",
        "        \" }\"\n",
        "        \" function onClickSaveNetwork(){\"\n",
        "        \"  var ancestors=collectFirstAncestors();\"\n",
        "        \"  if(!ancestors.length){\"\n",
        "        \"    setStatus('No ancestors found to save.',true);\"\n",
        "        \"    return;\"\n",
        "        \"  }\"\n",
        "        \"  setStatus('Saving '+ancestors.length+' ancestors...',false);\"\n",
        "        \"  fetch('/dna/save_network.php',{\"\n",
        "        \"    method:'POST',\"\n",
        "        \"    headers:{'Content-Type':'application/json'},\"\n",
        "        \"    body:JSON.stringify({ancestors:ancestors})\"\n",
        "        \"  }).then(function(resp){\"\n",
        "        \"    if(!resp.ok){throw new Error('HTTP '+resp.status);}\"\n",
        "        \"    return resp.json();\"\n",
        "        \"  }).then(function(data){\"\n",
        "        \"    if(data&&data.status==='ok'){\"\n",
        "        \"      var n=(typeof data.saved==='number')?data.saved:ancestors.length;\"\n",
        "        \"      setStatus('Saved '+n+' ancestors to network authority.',false);\"\n",
        "        \"    }else{\"\n",
        "        \"      setStatus('Unexpected response while saving network.',true);\"\n",
        "        \"    }\"\n",
        "        \"  }).catch(function(err){\"\n",
        "        \"    setStatus('Save failed: '+err,true);\"\n",
        "        \"  });\"\n",
        "        \" }\"\n",
        "        \" function init(){\"\n",
        "        \"  var btn=document.getElementById('save-network-btn');\"\n",
        "        \"  if(!btn) return;\"\n",
        "        \"  btn.addEventListener('click',onClickSaveNetwork,false);\"\n",
        "        \" }\"\n",
        "        \" if(document.readyState==='loading'){\"\n",
        "        \"  document.addEventListener('DOMContentLoaded',init,false);\"\n",
        "        \" }else{\"\n",
        "        \"  init();\"\n",
        "        \" }\"\n",
        "        \"})();\"\n",
        "        \"\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(\n",
        "    row,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return subject_name_html, matchee_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Network authority + page builder ----------\n",
        "def _load_network_authority(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_network_first_ancestors.txt not found; using all First Ancestors.\")\n",
        "        return []\n",
        "    vals = []\n",
        "    with open(path, \"r\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as fh:\n",
        "        for line in fh:\n",
        "            t = line.strip()\n",
        "            if t:\n",
        "                vals.append(t)\n",
        "    if not vals:\n",
        "        print(\"[INFO] dna_network_first_ancestors.txt is empty; using all First Ancestors.\")\n",
        "        return []\n",
        "    print(\"[INFO] Loaded %d authority First Ancestors from %s\" % (len(vals), path))\n",
        "    if vals:\n",
        "        preview = vals[:10]\n",
        "        print(\"[INFO] First 10 authority entries:\")\n",
        "        for i, fa in enumerate(preview, 1):\n",
        "            print(\"  %2d. %s\" % (i, fa))\n",
        "    return vals\n",
        "\n",
        "def build_network_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    # Build First Ancestor (raw, not normalized) and full lineage\n",
        "    first_ancestors = []\n",
        "    full_lineages = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        path_raw = str(row.get(path_col, \"\") or \"\")\n",
        "        tokens = split_tokens(path_raw)\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "        full_lineages.append(_clean_piece(path_raw))\n",
        "    df = main_df.copy()\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "    df[\"Full Lineage\"] = full_lineages\n",
        "\n",
        "    # Apply authority filter if present\n",
        "    auth_vals = _load_network_authority(DNA_NETWORK_AUTH_LOCAL)\n",
        "    if auth_vals:\n",
        "        auth_set = set(auth_vals)\n",
        "        before = len(df)\n",
        "        df = df[df[\"First Ancestor\"].isin(auth_set)].copy()\n",
        "        after = len(df)\n",
        "        print(\"[INFO] Filtered DNA network rows by authority list: %d -> %d\" % (before, after))\n",
        "        if after == 0:\n",
        "            print(\"[WARN] Authority filter eliminated all rows; falling back to full dataset.\")\n",
        "            df = main_df.copy()\n",
        "            df[\"First Ancestor\"] = first_ancestors\n",
        "            df[\"Full Lineage\"] = full_lineages\n",
        "    else:\n",
        "        print(\"[INFO] No authority filter applied; DNA network uses all rows.\")\n",
        "\n",
        "    # Summary counts for top table (deduplicated First Ancestors)\n",
        "    first_series = df[\"First Ancestor\"].astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values(\n",
        "        [\"Count\", \"First Ancestor\"],\n",
        "        ascending=[False, True],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    title = \"Match Specific Produced DNA Network\"\n",
        "    html.append(_partial_head(title))\n",
        "\n",
        "    # 7a) First Ancestor summary table (top, deduplicated)\n",
        "    html.append('<h2 class=\"centerline\">Match Specific Produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"ref-table\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th style=\"width:80%\">First Ancestor</th>'\n",
        "        '<th style=\"width:20%\">Showing</th>'\n",
        "        '</tr></thead><tbody id=\"ref-tb\">'\n",
        "    )\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td class=\\\"count\\\">%d</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    # 7a.1) Button + status line to push current First Ancestors to /dna/save_network.php\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin:10px 0 15px 0;\">'\n",
        "        '<button id=\"save-network-btn\" type=\"button\">Update DNA Network Authority</button>'\n",
        "        '<span id=\"save-network-status\" style=\"margin-left:8px; font-size:90%;\"></span>'\n",
        "        '</div>'\n",
        "    )\n",
        "\n",
        "    # 7b) DNA Register rows table (below)\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for produced DNA network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>First Ancestor</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Full Lineage</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "    for _, row in df.iterrows():\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "        first = _html.escape(str(row.get(\"First Ancestor\", \"\")).strip())\n",
        "        lineage = _html.escape(str(row.get(\"Full Lineage\", \"\")).strip())\n",
        "\n",
        "        tr = (\n",
        "            \"<tr>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (match_to_html, first, cm_html, lineage, header_html)\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for DNA Network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    # Sync the authority list from the server (dna/network_first_ancestors.txt)\n",
        "    sync_authority_from_server()\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    network_html = build_network_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        DNA_NETWORK_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(network_html)\n",
        "    print(\"[OK] Wrote DNA Network partial:\", os.path.abspath(DNA_NETWORK_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; upload of dna_network.shtml skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, DNA_NETWORK_LOCAL, _remote_path(DNA_NETWORK_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network.shtml failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p = _remote_path(DNA_NETWORK_REMOTE)\n",
        "        sz = ftp_size(ftps, p)\n",
        "        print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URL ---\")\n",
        "        print(\"DNA Network: https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session for dna_network.shtml:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2c - Match Specific Produced DNA Network ======\n"
      ],
      "metadata": {
        "id": "L0AlOeKGBwo2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7723e43b-e4af-4031-bfcf-cb3f9e2737c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 226\n",
            "[VITALS] last updated (display): February 1, 2026 8:30 AM\n",
            "[OK] Loaded CSV for DNA Network: 226 rows, 6 cols\n",
            "[PULL] dna/network_first_ancestors.txt -> /content/dna_network_first_ancestors.txt\n",
            "[INFO] Authority file synced from server.\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[INFO] Loaded 1 authority First Ancestors from dna_network_first_ancestors.txt\n",
            "[INFO] First 10 authority entries:\n",
            "   1. YatesWilliam&ThornburyAnne\n",
            "[INFO] Filtered DNA network rows by authority list: 226 -> 0\n",
            "[WARN] Authority filter eliminated all rows; falling back to full dataset.\n",
            "[OK] Wrote DNA Network partial: /content/partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 287673\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network: https://yates.one-name.net/partials/dna_network.shtml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2d"
      ],
      "metadata": {
        "id": "XM9k3wCyx4Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2d - Check Network Authority (Server Reader) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G2)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - ASCII-only source; any writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Deterministic audit banner + DECLARED_LINES.\n",
        "# - Purpose:\n",
        "#     * Read the current authority list from:\n",
        "#         https://yates.one-name.net/dna/network_first_ancestors.txt\n",
        "#     * De-duplicate it and print a clean, ordered list.\n",
        "# - This cell does NOT write or POST anything; the browser button does that.\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "import requests\n",
        "\n",
        "DECLARED_LINES = 80\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2d_CheckNetworkAuthority \"\n",
        "      \"| Version=2025.12.06-G2 | Encoding=ISO-8859-15 | DECLARED_LINES=%d\"\n",
        "      % DECLARED_LINES)\n",
        "\n",
        "AUTH_URL = \"https://yates.one-name.net/dna/network_first_ancestors.txt\"\n",
        "\n",
        "print(\"[INFO] Fetching authority from:\", AUTH_URL)\n",
        "\n",
        "try:\n",
        "    resp = requests.get(AUTH_URL, timeout=20)\n",
        "    print(\"[INFO] HTTP status:\", resp.status_code)\n",
        "    if resp.status_code != 200:\n",
        "        raise SystemExit(\"[ERROR] Could not fetch authority file.\")\n",
        "    raw_text = resp.text\n",
        "except Exception as e:\n",
        "    raise SystemExit(\"[ERROR] Request failed: %s\" % e)\n",
        "\n",
        "lines = []\n",
        "for ln in raw_text.splitlines():\n",
        "    s = ln.strip()\n",
        "    if not s:\n",
        "        continue\n",
        "    if s.startswith(\"#\"):\n",
        "        continue\n",
        "    lines.append(s)\n",
        "\n",
        "seen = {}\n",
        "unique = []\n",
        "for s in lines:\n",
        "    if s not in seen:\n",
        "        seen[s] = True\n",
        "        unique.append(s)\n",
        "\n",
        "print(\"[INFO] Total lines (including comments/blank):\", len(raw_text.splitlines()))\n",
        "print(\"[INFO] Unique First Ancestors:\", len(unique))\n",
        "\n",
        "for idx, val in enumerate(unique, 1):\n",
        "    print(\" %2d. %s\" % (idx, val))\n",
        "\n",
        "print(\"\\n--- Cell2d_CheckNetworkAuthority complete ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2d - Check Network Authority ======================\n"
      ],
      "metadata": {
        "id": "Y572NoTjDHBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 2k"
      ],
      "metadata": {
        "id": "H2k15Th8cWfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 2k - Unified DNA Network View (FLOW tiles; header-safe; nav-safe; dynamic Showing) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.01-CELL2K-HDRSAFE1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional.\n",
        "# - Uses canonical /partials/partials_unified.css for baseline; adds tiny scoped helper CSS + header-safety CSS.\n",
        "# - SSI nav include inserted once (no duplication). Optional JS nav repair is NOT used here.\n",
        "# - Header placement matches Cell 3 pattern: H1 -> UPDATED block -> NAV -> controls -> table.\n",
        "# - Header \"Showing:\" is dynamic (selection/filter aware) and reflects visible register rows excluding \"No\"/excluded.\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# NOTE: In Colab, the notebook cell text is not reliably introspectable for an exact line-count audit.\n",
        "DECLARED_LINES = -1\n",
        "print(\"[AUDIT] DECLARED_LINES={}\".format(DECLARED_LINES))\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip().strip(\"/\")\n",
        "\n",
        "# Baseline stylesheet (same as Cell 3 / the fixed Cell 2c)\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "# SSI navigation include (inserted once)\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "DNA_NETWORK_LOCAL  = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "DNA_NETWORK_REMOTE = posixpath.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "REGISTER_CSV_LOCAL  = \"dna_network_register.csv\"\n",
        "REGISTER_CSV_REMOTE = \"dna/dna_network_register.csv\"\n",
        "\n",
        "# ---------- 1a) Vitals ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    # Site convention: EST = UTC-5 (no DST)\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) Name + token helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = token.lower()\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token,\n",
        "    )\n",
        "    if \"-\" in token:\n",
        "        token = \"-\".join([w[:1].upper() + w[1:] for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token[0].upper() + token[1:]\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def format_name_bold_last(display_name: str) -> str:\n",
        "    s = _clean_piece(display_name or \"\")\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    parts = [p for p in s.split(\" \") if p]\n",
        "    if len(parts) == 1:\n",
        "        return '<span class=\"dn-last\">%s</span>' % _html.escape(parts[0])\n",
        "    first = \" \".join(parts[:-1])\n",
        "    last = parts[-1]\n",
        "    return (\n",
        "        '<span class=\"dn-first\">%s</span> <span class=\"dn-last\">%s</span>'\n",
        "        % (_html.escape(first), _html.escape(last))\n",
        "    )\n",
        "\n",
        "def _last_first_keys(display_name: str):\n",
        "    s = _clean_piece(display_name or \"\")\n",
        "    if not s:\n",
        "        return (\"\", \"\")\n",
        "    parts = [p for p in s.split(\" \") if p]\n",
        "    if len(parts) == 1:\n",
        "        return (parts[0].lower(), \"\")\n",
        "    last = parts[-1].lower()\n",
        "    first = parts[0].lower()\n",
        "    return (last, first)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(x):\n",
        "        return smart_titlecase(x) if \" \" in x else smart_titlecase(surname_given_from_token(x)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_display_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_display_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) HTML shell (header placement + header safety) ----------\n",
        "def _partial_head(title: str) -> str:\n",
        "    # Header safety learned from Cell 3: force THEAD/TH visible even if upstream CSS hides them.\n",
        "    late_style = (\n",
        "        '<style type=\"text/css\">\\n'\n",
        "        '/* Header/Nav safety. */\\n'\n",
        "        '#nav-slot, #nav-slot nav, #nav-slot .oldnav, #nav-slot .navbar{display:block !important; visibility:visible !important; opacity:1 !important;}\\n'\n",
        "        'table.sortable thead{display:table-header-group !important; visibility:visible !important;}\\n'\n",
        "        'table.sortable thead th{display:table-cell !important; visibility:visible !important;}\\n'\n",
        "        '/* Participants: flow tiles; show Name (bold last) + row-count only. */\\n'\n",
        "        'table.dn-participants thead{display:none !important;}\\n'\n",
        "        'table.dn-participants tbody tr{display:inline-block;vertical-align:top;margin:4px 10px 4px 0;padding:4px 8px;border:1px solid #ddd;border-radius:4px;background:#fff;white-space:nowrap;width:auto;max-width:360px;}\\n'\n",
        "        'table.dn-participants tbody td{display:inline-block;padding:0 6px 0 0;vertical-align:baseline;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(1){display:none !important;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(2){min-width:0;max-width:280px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(3){display:none !important;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(4){min-width:18px;text-align:right;padding-right:0;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(2) .dn-last{font-weight:700;}\\n'\n",
        "        'table.dn-participants tbody td:nth-child(2) .dn-first{font-weight:400;}\\n'\n",
        "        '/* Sticky left column for register table. */\\n'\n",
        "        '#reg-list{border-collapse:collapse;}\\n'\n",
        "        '#reg-list th, #reg-list td{white-space:nowrap;}\\n'\n",
        "        '#reg-list thead th{position:sticky; top:0; z-index:3;}\\n'\n",
        "        '#reg-list th:first-child, #reg-list td:first-child{position:sticky; left:0; z-index:2; background:#ffffff;}\\n'\n",
        "        '</style>\\n'\n",
        "    )\n",
        "\n",
        "    # Updated block (match Cell 3 / Cell 2c conventions)\n",
        "    parts = []\n",
        "    parts.append('Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_DISPLAY))\n",
        "    if AUTOSOMAL_MATCHES_TEXT and AUTOSOMAL_MATCHES_TEXT != \"(unknown)\":\n",
        "        parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES_TEXT))\n",
        "    parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "    updated_block = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join(parts) + '</div>'\n",
        "\n",
        "    # SSI include inside a slot (matches Cell 3 pattern; no JS repair here)\n",
        "    nav_slot = '<div id=\"nav-slot\">%s</div>' % NAV_BLOCK\n",
        "\n",
        "    menu = (\n",
        "        '<div class=\"selection-menu centerline\">'\n",
        "        'View: '\n",
        "        '<a href=\"#\" onclick=\"return dnShowSelected();\">Selected</a> &nbsp;|&nbsp; '\n",
        "        '<a href=\"#\" onclick=\"return dnShowAll();\">All</a> &nbsp;|&nbsp; '\n",
        "        '<a href=\"#\" onclick=\"return dnReset();\">Reset</a>'\n",
        "        '</div>'\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        '<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\\n'\n",
        "        ' \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\\n'\n",
        "        '<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\\n'\n",
        "        '<head>\\n'\n",
        "        '<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\\n'\n",
        "        '<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n'\n",
        "        '<title>%s</title>\\n'\n",
        "        '%s\\n'\n",
        "        '%s'\n",
        "        '</head>\\n'\n",
        "        '<body>\\n'\n",
        "        '<div class=\"wrap\">\\n'\n",
        "        '<h1 class=\"centerline\">%s</h1>\\n'\n",
        "        '%s\\n'\n",
        "        '%s\\n'\n",
        "        '%s\\n'\n",
        "        '<div class=\"table-scroll\">\\n'\n",
        "        % (_html.escape(title), HEAD_LINK, late_style, _html.escape(title), updated_block, nav_slot, menu)\n",
        "    )\n",
        "\n",
        "def _partial_tail() -> str:\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\\n\"\n",
        "        \"  var selectedMatches={};\\n\"\n",
        "        \"  var viewMode='all';\\n\"\n",
        "        \"  function hasSelection(){for(var k in selectedMatches){if(selectedMatches.hasOwnProperty(k)){return true;}}return false;}\\n\"\n",
        "        \"  function setRowSelected(tr, sel){if(!tr) return; tr.setAttribute('data-selected', sel ? '1' : '0'); tr.style.backgroundColor = sel ? '#fff2cc' : '';}\\n\"\n",
        "        \"  function setParticipantsVisible(show){var wrap=document.getElementById('participants-wrapper'); if(!wrap) return; wrap.style.display = show ? '' : 'none';}\\n\"\n",
        "        \"  function setFirstAncVisible(show){var wrap=document.getElementById('first-anc-wrapper'); if(!wrap) return; wrap.style.display = show ? '' : 'none';}\\n\"\n",
        "        \"  function formatWithCommas(n){try{var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US');}catch(e){return String(n||'');}}\\n\"\n",
        "        \"  function updateShowingCount(n){var el=document.getElementById('showing-count'); if(!el) return; el.textContent=formatWithCommas(n);}\\n\"\n",
        "        \"  function updateSummaryFromVisible(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    var participants={}; var firstAncestors={}; var visibleLines=0;\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      var r=rows[i];\\n\"\n",
        "        \"      if(r.style.display==='none') continue;\\n\"\n",
        "        \"      if(r.getAttribute('data-excluded')==='1') continue;\\n\"\n",
        "        \"      var incCell=r.querySelector('.dn-include-cell');\\n\"\n",
        "        \"      if(incCell){var inc=(incCell.textContent||''); var incNorm=inc.toLowerCase().replace(/\\\\s+/g,''); if(incNorm!=='yes') continue;}\\n\"\n",
        "        \"      visibleLines++;\\n\"\n",
        "        \"      var mk=r.getAttribute('data-match')||'';\\n\"\n",
        "        \"      var fa=r.getAttribute('data-first')||'';\\n\"\n",
        "        \"      if(mk){participants[mk]=true;} if(fa){firstAncestors[fa]=true;}\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    var pCount=0, faCount=0, k;\\n\"\n",
        "        \"    for(k in participants){if(participants.hasOwnProperty(k)){pCount++;}}\\n\"\n",
        "        \"    for(k in firstAncestors){if(firstAncestors.hasOwnProperty(k)){faCount++;}}\\n\"\n",
        "        \"    var selCount=0; for(k in selectedMatches){if(selectedMatches.hasOwnProperty(k)){selCount++;}}\\n\"\n",
        "        \"    var elSel=document.getElementById('dn-sum-selected');\\n\"\n",
        "        \"    var elP=document.getElementById('dn-sum-part');\\n\"\n",
        "        \"    var elL=document.getElementById('dn-sum-lines');\\n\"\n",
        "        \"    var elFA=document.getElementById('dn-sum-fa');\\n\"\n",
        "        \"    if(elSel){elSel.textContent='Selected Participant: '+selCount;}\\n\"\n",
        "        \"    if(elP){elP.textContent='Network Participants: '+pCount;}\\n\"\n",
        "        \"    if(elL){elL.textContent='Network Ancestral lines: '+visibleLines;}\\n\"\n",
        "        \"    if(elFA){elFA.textContent='Network First Ancestors: '+faCount;}\\n\"\n",
        "        \"    updateShowingCount(visibleLines);\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function updateRegister(){\\n\"\n",
        "        \"    var regRows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    if(viewMode!=='selected' || !hasSelection()){\\n\"\n",
        "        \"      for(var i=0;i<regRows.length;i++){regRows[i].style.display='';}\\n\"\n",
        "        \"    }else{\\n\"\n",
        "        \"      var focusFirst={};\\n\"\n",
        "        \"      for(var i2=0;i2<regRows.length;i2++){\\n\"\n",
        "        \"        var r2=regRows[i2];\\n\"\n",
        "        \"        var mk=r2.getAttribute('data-match')||'';\\n\"\n",
        "        \"        var fa=r2.getAttribute('data-first')||'';\\n\"\n",
        "        \"        if(mk && fa && selectedMatches[mk]){focusFirst[fa]=true;}\\n\"\n",
        "        \"      }\\n\"\n",
        "        \"      for(var i3=0;i3<regRows.length;i3++){\\n\"\n",
        "        \"        var r3=regRows[i3];\\n\"\n",
        "        \"        var fa2=r3.getAttribute('data-first')||'';\\n\"\n",
        "        \"        var show=!!focusFirst[fa2];\\n\"\n",
        "        \"        r3.style.display = show ? '' : 'none';\\n\"\n",
        "        \"      }\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    updateSummaryFromVisible();\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function dnToggleMatchRow(tr){\\n\"\n",
        "        \"    if(!tr) return false;\\n\"\n",
        "        \"    var key=tr.getAttribute('data-match')||'';\\n\"\n",
        "        \"    if(!key) return false;\\n\"\n",
        "        \"    if(selectedMatches[key]){delete selectedMatches[key]; setRowSelected(tr,false);}else{selectedMatches[key]=true; setRowSelected(tr,true);}\\n\"\n",
        "        \"    updateRegister(); return false;\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function wireMatchRows(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#match-tb tr');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      (function(r){r.onclick=function(e){dnToggleMatchRow(r); if(e && e.preventDefault){e.preventDefault();}};})(rows[i]);\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function dnToggleFirstAnc(){var wrap=document.getElementById('first-anc-wrapper'); if(!wrap) return false; var hidden=(wrap.style.display==='none'); setFirstAncVisible(hidden); return false;}\\n\"\n",
        "        \"  function dnShowSelected(){viewMode='selected'; updateRegister(); setParticipantsVisible(false); setFirstAncVisible(false); return false;}\\n\"\n",
        "        \"  function dnShowAll(){viewMode='all'; updateRegister(); setParticipantsVisible(true); setFirstAncVisible(true); return false;}\\n\"\n",
        "        \"  function dnReset(){\\n\"\n",
        "        \"    selectedMatches={}; viewMode='all';\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#match-tb tr');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){setRowSelected(rows[i],false);}\\n\"\n",
        "        \"    var rrows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    for(var j=0;j<rrows.length;j++){\\n\"\n",
        "        \"      rrows[j].style.display=''; rrows[j].setAttribute('data-excluded','0'); rrows[j].style.opacity='';\\n\"\n",
        "        \"      var c=rrows[j].querySelector('.dn-include-cell'); if(c){c.textContent='Yes';}\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    updateSummaryFromVisible(); setParticipantsVisible(true); setFirstAncVisible(true); return false;\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function toggleExcludeRow(tr){\\n\"\n",
        "        \"    if(!tr) return;\\n\"\n",
        "        \"    var cur=(tr.getAttribute('data-excluded')==='1');\\n\"\n",
        "        \"    var next=!cur;\\n\"\n",
        "        \"    tr.setAttribute('data-excluded', next ? '1' : '0');\\n\"\n",
        "        \"    tr.style.opacity = next ? '0.45' : '';\\n\"\n",
        "        \"    var cell=tr.querySelector('.dn-include-cell'); if(cell){cell.textContent = next ? 'No' : 'Yes';}\\n\"\n",
        "        \"    updateSummaryFromVisible();\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function wireRegisterRows(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      (function(r){\\n\"\n",
        "        \"        var cell=r.querySelector('.dn-include-cell'); if(!cell) return;\\n\"\n",
        "        \"        cell.onclick=function(e){toggleExcludeRow(r); if(e && e.preventDefault){e.preventDefault();} return false;};\\n\"\n",
        "        \"      })(rows[i]);\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  function dnDownloadCurrentCSV(){\\n\"\n",
        "        \"    var rows=document.querySelectorAll('#reg-tb tr');\\n\"\n",
        "        \"    var out=[];\\n\"\n",
        "        \"    function esc(v){\\n\"\n",
        "        \"      if(v==null){v='';}\\n\"\n",
        "        \"      v=String(v);\\n\"\n",
        "        \"      if(v.indexOf('\\\"')>=0||v.indexOf(',')>=0||v.indexOf('\\\\n')>=0||v.indexOf('\\\\r')>=0){v='\\\"'+v.replace(/\\\"/g,'\\\"\\\"')+'\\\"';}\\n\"\n",
        "        \"      return v;\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    out.push('Match to,First Ancestor,Include in proof,Match Summary,Full Lineage');\\n\"\n",
        "        \"    for(var i=0;i<rows.length;i++){\\n\"\n",
        "        \"      var r=rows[i];\\n\"\n",
        "        \"      if(r.style.display==='none') continue;\\n\"\n",
        "        \"      if(r.getAttribute('data-excluded')==='1') continue;\\n\"\n",
        "        \"      var tds=r.getElementsByTagName('td');\\n\"\n",
        "        \"      if(tds.length<5) continue;\\n\"\n",
        "        \"      var inc=(tds[2].textContent||'');\\n\"\n",
        "        \"      var incNorm=inc.toLowerCase().replace(/\\\\s+/g,'');\\n\"\n",
        "        \"      if(incNorm!=='yes') continue;\\n\"\n",
        "        \"      var match_to=(tds[0].textContent||'').trim();\\n\"\n",
        "        \"      var first=(tds[1].textContent||'').trim();\\n\"\n",
        "        \"      var summary=(tds[3].textContent||'').trim();\\n\"\n",
        "        \"      var lineage=(tds[4].textContent||'').trim();\\n\"\n",
        "        \"      out.push([esc(match_to),esc(first),esc(inc),esc(summary),esc(lineage)].join(','));\\n\"\n",
        "        \"    }\\n\"\n",
        "        \"    var csv=out.join('\\\\r\\\\n');\\n\"\n",
        "        \"    var blob=new Blob([csv],{type:'text/csv'});\\n\"\n",
        "        \"    var url=URL.createObjectURL(blob);\\n\"\n",
        "        \"    var a=document.createElement('a'); a.href=url; a.download='dna_network_register_live.csv';\\n\"\n",
        "        \"    document.body.appendChild(a); a.click(); document.body.removeChild(a); URL.revokeObjectURL(url);\\n\"\n",
        "        \"    return false;\\n\"\n",
        "        \"  }\\n\"\n",
        "        \"  window.dnShowSelected=dnShowSelected;\\n\"\n",
        "        \"  window.dnShowAll=dnShowAll;\\n\"\n",
        "        \"  window.dnReset=dnReset;\\n\"\n",
        "        \"  window.dnToggleFirstAnc=dnToggleFirstAnc;\\n\"\n",
        "        \"  window.dnDownloadCurrentCSV=dnDownloadCurrentCSV;\\n\"\n",
        "        \"  function init(){wireMatchRows(); wireRegisterRows(); setParticipantsVisible(true); setFirstAncVisible(true); updateRegister();}\\n\"\n",
        "        \"  if(document.readyState==='loading'){document.addEventListener('DOMContentLoaded',init,false);}else{init();}\\n\"\n",
        "        \"})();\\n\"\n",
        "        \"//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) Row builder ----------\n",
        "def build_register_row(row, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str):\n",
        "    subject_raw = str(row.get(match_col, \"\") or \"\")\n",
        "    subject_code = subject_raw.strip()\n",
        "    key = subject_code.lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_code)\n",
        "\n",
        "    subject_display = normalize_person_name(subject_unmasked or subject_code)\n",
        "    subject_display_html = _html.escape(subject_display or \"\")\n",
        "\n",
        "    match_to_html = subject_display_html\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    base_matchee = norm_matchee_name(row.get(name_col, \"\")) or subject_display\n",
        "    matchee_name = normalize_person_name(base_matchee)\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_display_html or subject_display,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return match_to_html, header_html, key\n",
        "\n",
        "# ---------- 7) Unified page + CSV builder ----------\n",
        "def build_network_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str):\n",
        "    first_ancestors = []\n",
        "    full_lineages = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        path_raw = str(row.get(path_col, \"\") or \"\")\n",
        "        tokens = split_tokens(path_raw)\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "        full_lineages.append(_clean_piece(path_raw))\n",
        "    df = main_df.copy()\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "    df[\"Full Lineage\"] = full_lineages\n",
        "\n",
        "    first_series = df[\"First Ancestor\"].astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values([\"Count\", \"First Ancestor\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    # Participants: derive + normalize + sort by last name; keep code in data (hidden in UI)\n",
        "    part_df = df[[match_col]].copy()\n",
        "    part_df[\"match_key\"] = part_df[match_col].astype(str).str.strip().str.lower()\n",
        "    part_df[\"match_code_raw\"] = part_df[match_col].astype(str).str.strip()\n",
        "\n",
        "    unmasked_series = part_df[\"match_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "    part_df[\"match_label\"] = unmasked_series\n",
        "    mask_empty = part_df[\"match_label\"] == \"\"\n",
        "    part_df.loc[mask_empty, \"match_label\"] = part_df.loc[mask_empty, \"match_code_raw\"]\n",
        "    part_df[\"match_label\"] = part_df[\"match_label\"].map(lambda x: normalize_person_name(x) if str(x).strip() else \"\")\n",
        "    part_df = part_df[part_df[\"match_key\"] != \"\"]\n",
        "\n",
        "    if part_df.empty:\n",
        "        p_counts = pd.DataFrame(columns=[\"match_key\", \"match_code_raw\", \"match_label\", \"rows\"])\n",
        "    else:\n",
        "        grp = part_df.groupby(\"match_key\")\n",
        "        rows_series = grp.size().rename(\"rows\")\n",
        "        code_series = grp[\"match_code_raw\"].first()\n",
        "        label_series = grp[\"match_label\"].first()\n",
        "        p_counts = pd.concat([code_series, label_series, rows_series], axis=1).reset_index()\n",
        "        p_counts.columns = [\"match_key\", \"match_code_raw\", \"match_label\", \"rows\"]\n",
        "\n",
        "        p_counts[\"sort_last\"] = p_counts[\"match_label\"].map(lambda x: _last_first_keys(x)[0])\n",
        "        p_counts[\"sort_first\"] = p_counts[\"match_label\"].map(lambda x: _last_first_keys(x)[1])\n",
        "\n",
        "        p_counts = p_counts.sort_values(\n",
        "            [\"sort_last\", \"sort_first\", \"match_label\", \"match_key\"],\n",
        "            ascending=[True, True, True, True],\n",
        "            kind=\"mergesort\",\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "    full_participants = int(p_counts.shape[0])\n",
        "    full_lines = int(first_series[first_series != \"\"].shape[0])\n",
        "    full_first_anc = int(len(vc.index))\n",
        "\n",
        "    html = []\n",
        "    title = \"Match Specific Produced DNA Network\"\n",
        "    html.append(_partial_head(title))\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">Match Specific Produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<p class=\"centerline\">This unified view shows participants (matches), First Ancestors '\n",
        "        'produced by the current DNA Network, and the detailed DNA Register rows below. '\n",
        "        'Click tiles in the participants section to focus on a subset; use the Selected / All / Reset '\n",
        "        'links above to change the view. In the register, you can mark individual lines as '\n",
        "        'excluded from the proof.</p>'\n",
        "    )\n",
        "\n",
        "    # Participants: header includes total count; tiles show Name (bold last) + row-count number only\n",
        "    html.append('<div id=\"participants-wrapper\">')\n",
        "    html.append('<h3>Network participants (matches): %d</h3>' % full_participants)\n",
        "    if p_counts.empty:\n",
        "        html.append('<p><em>No participants could be derived from the produced DNA network.</em></p>')\n",
        "    else:\n",
        "        html.append('<table id=\"participants-table\" class=\"dn-participants sortable\" border=\"1\">')\n",
        "        html.append('<tbody id=\"match-tb\">')\n",
        "        for _, r in p_counts.iterrows():\n",
        "            mkey = str(r.get(\"match_key\", \"\")).strip().lower()\n",
        "            code_raw = str(r.get(\"match_code_raw\", \"\")).strip()\n",
        "            label = str(r.get(\"match_label\", \"\")).strip()\n",
        "            rows_count = int(str(r.get(\"rows\", \"0\")).strip() or \"0\")\n",
        "\n",
        "            label_html = format_name_bold_last(label)\n",
        "\n",
        "            tr = (\n",
        "                '<tr data-match=\"%s\">'\n",
        "                '<td></td>'\n",
        "                '<td>%s</td>'\n",
        "                '<td>%s</td>'\n",
        "                '<td class=\"count\">%d</td>'\n",
        "                '</tr>'\n",
        "                % (\n",
        "                    _html.escape(mkey, quote=True),\n",
        "                    label_html,\n",
        "                    _html.escape(code_raw),\n",
        "                    rows_count,\n",
        "                )\n",
        "            )\n",
        "            html.append(tr)\n",
        "        html.append(\"</tbody></table>\")\n",
        "    html.append(\"</div>\")  # participants-wrapper\n",
        "\n",
        "    # First Ancestor table + toggle\n",
        "    html.append('<h3>First Ancestors produced by this DNA Network</h3>')\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin-bottom:4px;\">'\n",
        "        '<a href=\"#\" onclick=\"return dnToggleFirstAnc();\">Hide / show First Ancestors table</a>'\n",
        "        '</div>'\n",
        "    )\n",
        "    html.append('<div id=\"first-anc-wrapper\">')\n",
        "    if lin_df.empty:\n",
        "        html.append('<p><em>No First Ancestors could be derived from the produced DNA network.</em></p>')\n",
        "    else:\n",
        "        html.append(\n",
        "            '<table id=\"first-anc-table\" class=\"sortable\" border=\"1\">'\n",
        "            '<thead><tr>'\n",
        "            '<th style=\"width:80%\">First Ancestor</th>'\n",
        "            '<th style=\"width:20%\">Showing</th>'\n",
        "            '</tr></thead><tbody id=\"anc-tb\">'\n",
        "        )\n",
        "        for _, r in lin_df.iterrows():\n",
        "            first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "            cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "            tr = (\n",
        "                '<tr data-first=\"%s\">'\n",
        "                '<td>%s</td>'\n",
        "                '<td class=\"count\">%d</td>'\n",
        "                '</tr>'\n",
        "                % (\n",
        "                    _html.escape(first, quote=True),\n",
        "                    _html.escape(first),\n",
        "                    cnt,\n",
        "                )\n",
        "            )\n",
        "            html.append(tr)\n",
        "        html.append(\"</tbody></table>\")\n",
        "    html.append(\"</div>\")  # first-anc-wrapper\n",
        "\n",
        "    # Summary block + CSV link\n",
        "    html.append(\n",
        "        '<table id=\"dn-summary\" class=\"summary-block\" border=\"0\" style=\"margin:10px auto 6px auto;\">'\n",
        "        '<tr>'\n",
        "        '<td><strong>Current proof scope:</strong></td>'\n",
        "        '<td>'\n",
        "        '<span id=\"dn-sum-selected\">Selected Participant: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-part\">Network Participants: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-lines\">Network Ancestral lines: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-fa\">Network First Ancestors: 0</span> &nbsp;|&nbsp; '\n",
        "        '<a href=\"#\" onclick=\"return dnDownloadCurrentCSV();\">Download register CSV</a>'\n",
        "        '</td>'\n",
        "        '</tr>'\n",
        "        '</table>'\n",
        "    )\n",
        "\n",
        "    # IMPORTANT: '%%' needed because this string is used with % formatting below\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin:2px 0 10px 0; font-size:90%%;\">'\n",
        "        'Full study scope (all network lines): '\n",
        "        'Participants: %d  |  Ancestral lines: %d  |  First Ancestors: %d'\n",
        "        '</div>' % (full_participants, full_lines, full_first_anc)\n",
        "    )\n",
        "\n",
        "    # Register rows + CSV rows\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>First Ancestor</th>'\n",
        "        '<th>Include in proof</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '<th>Full Lineage</th>'\n",
        "        '</tr></thead><tbody id=\"reg-tb\">'\n",
        "    )\n",
        "\n",
        "    csv_rows = []\n",
        "    tag_re = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        subject_html, header_html, mkey = build_register_row(row, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "        first_raw = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        first_esc = _html.escape(first_raw)\n",
        "        lineage_raw = str(row.get(\"Full Lineage\", \"\")).strip()\n",
        "        lineage_esc = _html.escape(lineage_raw)\n",
        "\n",
        "        tr = (\n",
        "            '<tr data-match=\"%s\" data-first=\"%s\" data-excluded=\"0\">'\n",
        "            '<td>%s</td>'\n",
        "            '<td>%s</td>'\n",
        "            '<td class=\"dn-include-cell\">Yes</td>'\n",
        "            '<td>%s</td>'\n",
        "            '<td>%s</td>'\n",
        "            '</tr>'\n",
        "            % (\n",
        "                _html.escape(mkey, quote=True),\n",
        "                _html.escape(first_raw, quote=True),\n",
        "                subject_html,\n",
        "                first_esc,\n",
        "                header_html,\n",
        "                lineage_esc,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "        summary_plain = tag_re.sub(\"\", header_html)\n",
        "        csv_rows.append({\n",
        "            \"Match code\": mkey,\n",
        "            \"First Ancestor\": first_raw,\n",
        "            \"Include in proof\": \"Yes\",\n",
        "            \"Match Summary\": summary_plain,\n",
        "            \"Full Lineage\": lineage_raw,\n",
        "        })\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html), csv_rows\n",
        "\n",
        "# ---------- 8) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for unified DNA Network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    network_html, csv_rows = build_network_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "    with open(DNA_NETWORK_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(network_html)\n",
        "    print(\"[OK] Wrote unified DNA Network partial:\", os.path.abspath(DNA_NETWORK_LOCAL))\n",
        "\n",
        "    if csv_rows:\n",
        "        reg_df = pd.DataFrame(csv_rows, columns=[\n",
        "            \"Match code\",\n",
        "            \"First Ancestor\",\n",
        "            \"Include in proof\",\n",
        "            \"Match Summary\",\n",
        "            \"Full Lineage\",\n",
        "        ])\n",
        "        reg_df.to_csv(\n",
        "            REGISTER_CSV_LOCAL,\n",
        "            index=False,\n",
        "            encoding=\"iso-8859-15\",\n",
        "            errors=\"xmlcharrefreplace\",\n",
        "        )\n",
        "        print(\"[OK] Wrote register CSV:\", os.path.abspath(REGISTER_CSV_LOCAL))\n",
        "    else:\n",
        "        print(\"[WARN] No register rows; CSV not written.\")\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; upload of dna_network.shtml and CSV skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, DNA_NETWORK_LOCAL, _remote_path(DNA_NETWORK_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network.shtml failed:\", e)\n",
        "\n",
        "        if csv_rows:\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, REGISTER_CSV_LOCAL, _remote_path(REGISTER_CSV_REMOTE))\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] Upload register CSV failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p_html = _remote_path(DNA_NETWORK_REMOTE)\n",
        "        sz_html = ftp_size(ftps, p_html)\n",
        "        print(\"%s : %s\" % (p_html, sz_html if sz_html is not None else \"(SIZE unsupported)\"))\n",
        "        if csv_rows:\n",
        "            p_csv = _remote_path(REGISTER_CSV_REMOTE)\n",
        "            sz_csv = ftp_size(ftps, p_csv)\n",
        "            print(\"%s : %s\" % (p_csv, sz_csv if sz_csv is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URL ---\")\n",
        "        print(\"DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "        if csv_rows:\n",
        "            print(\"Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session for dna_network.shtml / register CSV:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2k - Unified DNA Network View (FLOW tiles; header-safe; nav-safe; dynamic Showing) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmrAfgftE8Q4",
        "outputId": "19b8e764-3361-4ca7-fc56-57ae86774401"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2026.02.01-CELL2K-HDRSAFE1 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=-1\n",
            "[VITALS] autosomal (after manual filter): 226\n",
            "[VITALS] last updated (display): February 1, 2026 8:32 PM\n",
            "[OK] Loaded CSV for unified DNA Network: 226 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Wrote unified DNA Network partial: /content/partials/dna_network.shtml\n",
            "[OK] Wrote register CSV: /content/dna_network_register.csv\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[PUT] dna_network_register.csv -> dna/dna_network_register.csv\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 329932\n",
            "dna/dna_network_register.csv : 209699\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\n",
            "Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "INiJljOS1kRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: iso-8859-15 -*-\n",
        "# ====== CUT START [1/1] CELL 3 - Ancestor Register (Trees View; .shtml + SSI nav repair) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.02-CELL3-COL1=FIRST-ANCESTOR + ENRICHED-EXCLUDE1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# - Authority source for \"First Ancestor\" (same as Cell 2):\n",
        "#     /partials/first_ancestor_pairs.csv (downloaded via FTP) provides mapping:\n",
        "#       (FirstPair_Ancestor1_ID, FirstPair_Ancestor2_ID) -> FirstPair_LastFirst\n",
        "# - Change in this build:\n",
        "#     (1) Column 1 is REPLACED with \"First Ancestor\" values (authority key).\n",
        "#     (2) The extra trailing \"First Ancestor\" column is removed to avoid duplication.\n",
        "#     (3) UPDATED EXCLUSION: drop the enriched early-ancestor prefix up to Frances Yates & Jane Tichborne.\n",
        "#\n",
        "# Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "from string import Template as _T\n",
        "\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "# ---------- Display Policy ----------\n",
        "SUPPRESS_ID_COLUMN = True\n",
        "SUPPRESS_EMBEDDED_IDS_IN_TEXT = True\n",
        "\n",
        "LINEAGE_SPOUSE_SEP = \" & \"\n",
        "LINEAGE_COUPLE_SEP = \" ~ \"\n",
        "\n",
        "ALPHA_BY_FIRST_ANCESTOR_FATHER = True\n",
        "ALPHA_TIEBREAK_MOTHER_SURNAME  = True\n",
        "\n",
        "# ---------- Enriched exclusion prefix (formatted lineage) ----------\n",
        "# This is the exact prefix to strip from the formatted lineage display.\n",
        "# It intentionally ends with a trailing \"~\" to remove the separator too.\n",
        "ENRICHED_EXCLUDE_PREFIX = (\n",
        "    \"John Yates (1430-) & Still Searching ~ \"\n",
        "    \"William Yates (1389-1440) & Still Searching ~ \"\n",
        "    \"William Yates (1420-) & Still Searching ~ \"\n",
        "    \"Edmund Yates (1445-1472) & Margaret Cornell ~ \"\n",
        "    \"Richard Yates (1440-1498) & Joan Ashendon (1445-1499) ~ \"\n",
        "    \"John Yates (1471-1544) & Alice Hyde (1498-1523) ~ \"\n",
        "    \"Thomas Yates (1509-1565) & Elizabeth Fauconer (-1562) ~\"\n",
        ")\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip().strip(\"/\")\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "OUTPUT_NAME = \"just-trees.shtml\"\n",
        "REMOTE_HTML = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "DNA_CSS_HREF     = \"/partials/dna_tree_styles.css\"\n",
        "DNA_CSS_VERSION  = \"v2025-11-23-g3\"\n",
        "UNIFIED_CSS_HREF = \"/partials/partials_unified.css\"\n",
        "UNIFIED_CSS_VER  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "\n",
        "HEAD_LINK = (\n",
        "    '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />\\n'\n",
        "    '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />'\n",
        ") % (DNA_CSS_HREF, DNA_CSS_VERSION, UNIFIED_CSS_HREF, UNIFIED_CSS_VER)\n",
        "\n",
        "TABLE_WIDTH_PX = 5550\n",
        "\n",
        "# ---------- Authority file (same as Cell 2) ----------\n",
        "AUTH_REMOTE_DIR   = \"partials\"\n",
        "AUTH_BASENAME     = \"first_ancestor_pairs.csv\"\n",
        "AUTH_LOCAL_CACHE  = \"first_ancestor_pairs.server.csv\"\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV: %s rows=%d, cols=%d\" % (INPUT_CSV, len(df), len(df.columns)))\n",
        "\n",
        "if \"haplogroup\" not in df.columns:\n",
        "    df[\"haplogroup\"] = \"\"\n",
        "else:\n",
        "    df[\"haplogroup\"] = df[\"haplogroup\"].fillna(\"\")\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX = 0\n",
        "B_IDX = 1\n",
        "C_IDX = 2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\", \" \")\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
        "    return t.lower()\n",
        "\n",
        "LOCAL_RESOLVER = \"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER = \"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_file_from_server(remote_dir, basename, local_out):\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", \"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try:\n",
        "                        ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            ftps.mkd(p)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        ftps.cwd(p)\n",
        "            try:\n",
        "                ftps.cwd(remote_dir)\n",
        "            except Exception:\n",
        "                pass\n",
        "            with open(local_out, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % basename, f.write)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Server pull failed for %s/%s: %s\" % (remote_dir, basename, e))\n",
        "        return False\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path))\n",
        "        return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    ok = _pull_file_from_server(\"partials\", \"match_to_unmasked.csv\", \"match_to_unmasked.csv\")\n",
        "    if ok:\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    return local_path\n",
        "\n",
        "LOCAL_RESOLVER = _pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _read_csv_anyenc(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    dfx = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            dfx = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            dfx = None\n",
        "    if dfx is None:\n",
        "        raise RuntimeError(\"Unable to read CSV %s: %s\" % (path, last))\n",
        "    return dfx\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    if not os.path.exists(path):\n",
        "        return {}\n",
        "    last = None\n",
        "    m = None\n",
        "    for enc in (\"utf-8-sig\", \"iso-8859-15\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            m = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            m = None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last)\n",
        "        return {}\n",
        "    cols = {c.lower(): c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\")\n",
        "        return {}\n",
        "    m = m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"] = m[cols[\"code\"]].map(_norm_code)\n",
        "    m[\"__val__\"] = m[cols[\"unmasked\"]].astype(str)\n",
        "    m = m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map = _load_resolver_to_map(LOCAL_RESOLVER)\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw = df.iloc[:, B_IDX].astype(str)\n",
        "masked_key = masked_raw.map(_norm_code)\n",
        "resolved   = masked_key.map(resolver_map)\n",
        "df.iloc[:, C_IDX] = resolved.fillna(\"\")\n",
        "\n",
        "print(\n",
        "    \"[OK] Column B -> C mapping: %d / %d  unmatched: %d\"\n",
        "    % (int(resolved.notna().sum()), len(df), len(df) - int(resolved.notna().sum()))\n",
        ")\n",
        "\n",
        "# ---------- Lineage formatting helpers ----------\n",
        "ID_TOKEN_RE = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "COUPLE_SEP_RE = re.compile(r\"\\s*(?:->|&rarr;|;|>|/{2,}|\\|{2,}|~{2,})\\s*\", re.I)\n",
        "SPOUSE_SPLIT_RE = re.compile(r\"\\s*(?:&| and | AND |\\+)\\s*\", re.I)\n",
        "\n",
        "def _scrub_side_keep_name_years(side_text):\n",
        "    s = str(side_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)~([^~]+?)~(\\d{4}\\s*-\\s*(?:\\d{4})?)$\", s, flags=re.I)\n",
        "    if m:\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", (m.group(2) or \"\").strip())\n",
        "        yrs = re.sub(r\"\\s+\", \"\", (m.group(3) or \"\").strip())\n",
        "        return (\"%s (%s)\" % (name, yrs)) if name and yrs else (name or \"\")\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)~([^~]+?)(?:~([^~]+?))?$\", s, flags=re.I)\n",
        "    if m:\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", (m.group(2) or \"\").strip())\n",
        "        tail = re.sub(r\"\\s{2,}\", \" \", (m.group(3) or \"\").strip())\n",
        "        if tail and re.search(r\"\\d{4}\", tail):\n",
        "            tail = re.sub(r\"\\s+\", \"\", tail)\n",
        "            return (\"%s (%s)\" % (name, tail)) if name else \"\"\n",
        "        if tail and name:\n",
        "            return (\"%s %s\" % (name, tail)).strip()\n",
        "        return (name or tail or \"\").strip()\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", s, flags=re.I)\n",
        "    if m:\n",
        "        rest = (m.group(2) or \"\").strip()\n",
        "        yrs = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs = re.sub(r\"\\s+\", \"\", m2.group(1))\n",
        "            rest = rest[:m2.start()].strip()\n",
        "        name = re.sub(r\"\\s{2,}\", \" \", rest).strip()\n",
        "        return (\"%s (%s)\" % (name, yrs)) if name and yrs else (name or \"\")\n",
        "\n",
        "    s = ID_TOKEN_RE.sub(\"\", s).replace(\"~\", \" \")\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------- Enriched exclusion (drop very old lead-in couples) ----------\n",
        "# Goal: keep the displayed lineage focused starting at:\n",
        "#   Francis Yates (1541-1588) & Jane Tichborne (1548-1580)\n",
        "#\n",
        "# We support two compatible mechanisms:\n",
        "#   (A) Exact prefix removal using the explicit ENRICHED_EXCLUDE_PREFIX couples.\n",
        "#   (B) Anchor-based trimming: if the anchor couple is present, drop everything before it.\n",
        "\n",
        "def _norm_couple_for_match(s: str) -> str:\n",
        "    # lower + collapse whitespace; keep punctuation so we can match the authority text precisely\n",
        "    return re.sub(r\"\\s{2,}\", \" \", str(s or \"\")).strip().lower()\n",
        "\n",
        "# Build couple list from the explicit prefix string (safe if user updates prefix later)\n",
        "_EXCLUDE_COUPLES = [\n",
        "    c.strip()\n",
        "    for c in re.split(r\"\\s*~\\s*\", (ENRICHED_EXCLUDE_PREFIX or \"\").strip().strip(\"~\"))\n",
        "    if c and c.strip()\n",
        "]\n",
        "\n",
        "def _strip_paren_years_anywhere(s: str) -> str:\n",
        "    return re.sub(r\"\\([^)]*\\)\", \"\", str(s or \"\")).strip()\n",
        "\n",
        "def _is_anchor_couple(couple_text: str) -> bool:\n",
        "    t = _strip_paren_years_anywhere(couple_text).lower()\n",
        "    return (\"francis yates\" in t) and (\"jane tichborne\" in t)\n",
        "\n",
        "def _apply_enriched_exclusion(joined: str) -> str:\n",
        "    s = str(joined or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "\n",
        "    couples = [c.strip() for c in s.split(LINEAGE_COUPLE_SEP) if c and c.strip()]\n",
        "    if not couples:\n",
        "        return s\n",
        "\n",
        "    # (A) Exact prefix removal by couple list match (robust to spacing)\n",
        "    if _EXCLUDE_COUPLES and len(couples) >= len(_EXCLUDE_COUPLES):\n",
        "        ok = True\n",
        "        for i in range(len(_EXCLUDE_COUPLES)):\n",
        "            if _norm_couple_for_match(couples[i]) != _norm_couple_for_match(_EXCLUDE_COUPLES[i]):\n",
        "                ok = False\n",
        "                break\n",
        "        if ok:\n",
        "            couples = couples[len(_EXCLUDE_COUPLES):]\n",
        "\n",
        "    # (B) Anchor trim if prefix did not match (or if data varies)\n",
        "    if couples:\n",
        "        for i, c in enumerate(couples):\n",
        "            if _is_anchor_couple(c):\n",
        "                couples = couples[i:]\n",
        "                break\n",
        "\n",
        "    return LINEAGE_COUPLE_SEP.join([c for c in couples if c]).strip()\n",
        "\n",
        "def _format_lineage_cell(text):\n",
        "    s = str(text or \"\").strip()\n",
        "    if not s:\n",
        "        return s\n",
        "    couples = [t.strip() for t in COUPLE_SEP_RE.split(s) if t and t.strip()]\n",
        "    if not couples:\n",
        "        couples = [s]\n",
        "    out_couples = []\n",
        "    for c in couples:\n",
        "        parts = [p.strip() for p in SPOUSE_SPLIT_RE.split(c, maxsplit=1) if p and p.strip()]\n",
        "        if len(parts) == 2:\n",
        "            father = _scrub_side_keep_name_years(parts[0]).strip()\n",
        "            mother = _scrub_side_keep_name_years(parts[1]).strip()\n",
        "            couple = (father + LINEAGE_SPOUSE_SEP + mother).strip()\n",
        "        else:\n",
        "            couple = _scrub_side_keep_name_years(c).strip()\n",
        "        couple = re.sub(r\"\\s{2,}\", \" \", couple).strip()\n",
        "        out_couples.append(couple)\n",
        "    joined = (LINEAGE_COUPLE_SEP.join([c for c in out_couples if c])).strip()\n",
        "    joined = re.sub(r\"\\s{2,}\", \" \", joined).strip()\n",
        "\n",
        "    # Apply enriched early-ancestor truncation (Frances Yates & Jane Tichborne focus)\n",
        "    joined = _apply_enriched_exclusion(joined)\n",
        "\n",
        "    return joined\n",
        "\n",
        "\n",
        "def _maybe_format_lineage_columns(df_in):\n",
        "    if not SUPPRESS_EMBEDDED_IDS_IN_TEXT:\n",
        "        return df_in\n",
        "    df_out = df_in.copy()\n",
        "    pat = re.compile(r\"(ancestral|lineage|tree|path|ancestor|line)\", re.I)\n",
        "    cols = [c for c in df_out.columns if pat.search(str(c or \"\")) and str(c).strip().lower() != 'first ancestor']\n",
        "    if not cols:\n",
        "        for c in df_out.columns:\n",
        "            try:\n",
        "                ser = df_out[c].astype(str)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if ser.str.contains(r\"\\bI\\d+~\", regex=True, na=False).any() or ser.str.contains(r\"\\bI\\d+\\b\", regex=True, na=False).any():\n",
        "                if str(c).strip().lower() != 'first ancestor':\n",
        "                    cols.append(c)\n",
        "    cols = list(dict.fromkeys(cols))\n",
        "    if not cols:\n",
        "        return df_out\n",
        "    for c in cols:\n",
        "        try:\n",
        "            df_out[c] = df_out[c].astype(str).map(_format_lineage_cell)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(\"[OK] Lineage formatting applied to columns:\", \", \".join([str(c) for c in cols]))\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def _strip_years(name_text):\n",
        "    s = str(name_text or \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s*\\([^)]*\\)\\s*$\", \"\", s).strip()\n",
        "\n",
        "def _first_last_tokens(person_text):\n",
        "    s = _strip_years(person_text)\n",
        "    parts = s.split()\n",
        "    if not parts:\n",
        "        return (\"\", \"\")\n",
        "    return (parts[0], parts[-1])\n",
        "\n",
        "def _first_ancestor_sort_key(lineage_text):\n",
        "    s = str(lineage_text or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\")\n",
        "    first_couple = s.split(LINEAGE_COUPLE_SEP, 1)[0].strip()\n",
        "    father = \"\"\n",
        "    mother = \"\"\n",
        "    if LINEAGE_SPOUSE_SEP in first_couple:\n",
        "        father, mother = [p.strip() for p in first_couple.split(LINEAGE_SPOUSE_SEP, 1)]\n",
        "    else:\n",
        "        father = first_couple.strip()\n",
        "    f_given, f_surname = _first_last_tokens(father)\n",
        "    m_surname = \"\"\n",
        "    if ALPHA_TIEBREAK_MOTHER_SURNAME:\n",
        "        _m_given, m_surname = _first_last_tokens(mother)\n",
        "    return (f_surname.lower(), f_given.lower(), m_surname.lower())\n",
        "\n",
        "# ---------- Authority load + first-ancestor value ----------\n",
        "def _find_col(df0, patterns, prefer_exact=None):\n",
        "    cols = list(df0.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df0.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def load_authority_map() -> dict:\n",
        "    local_path = AUTH_LOCAL_CACHE if os.path.exists(AUTH_LOCAL_CACHE) else AUTH_BASENAME\n",
        "    if not os.path.exists(local_path):\n",
        "        print(\"Authority not found locally; attempting server pull ...\")\n",
        "        ok = _pull_file_from_server(\"partials\", AUTH_BASENAME, AUTH_LOCAL_CACHE)\n",
        "        if not ok:\n",
        "            raise RuntimeError(\"Authority file missing: expected /partials/%s\" % AUTH_BASENAME)\n",
        "        local_path = AUTH_LOCAL_CACHE\n",
        "        print(\"[OK] Pulled authority from server -> %s\" % local_path)\n",
        "    else:\n",
        "        print(\"Using authority:\", os.path.abspath(local_path))\n",
        "\n",
        "    adf = _read_csv_anyenc(local_path)\n",
        "    a1_col  = _find_col(adf, [r\"ancestor1.*_id$\", r\"firstpair_ancestor1_id$\"], [\"FirstPair_Ancestor1_ID\"])\n",
        "    a2_col  = _find_col(adf, [r\"ancestor2.*_id$\", r\"firstpair_ancestor2_id$\"], [\"FirstPair_Ancestor2_ID\"])\n",
        "    key_col = _find_col(adf, [r\"firstpair_lastfirst$\"], [\"FirstPair_LastFirst\"])\n",
        "    if not (a1_col and a2_col and key_col):\n",
        "        raise RuntimeError(\"Authority CSV missing required columns. Need Ancestor1_ID, Ancestor2_ID, FirstPair_LastFirst.\")\n",
        "    m = {}\n",
        "    for _, r in adf.iterrows():\n",
        "        a1 = str(r.get(a1_col, \"\")).strip()\n",
        "        a2 = str(r.get(a2_col, \"\")).strip()\n",
        "        k  = re.sub(r\"\\s+\", \"\", str(r.get(key_col, \"\")))\n",
        "        if not (a1 and a2 and k):\n",
        "            continue\n",
        "        m[(a1.upper(), a2.upper())] = k\n",
        "        m[(a2.upper(), a1.upper())] = k\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Authority map built empty from %s\" % local_path)\n",
        "    print(\"[OK] Authority map ready: %d pairs\" % (len(m)//2))\n",
        "    return m\n",
        "\n",
        "AUTH_COUPLE_KEY_MAP = load_authority_map()\n",
        "\n",
        "def _canon_side(txt):\n",
        "    t = re.sub(r\"\\([^)]*\\)\", \"\", str(txt or \"\"))\n",
        "    t = re.sub(r\"[^A-Za-z0-9]+\", \"\", t).lower()\n",
        "    return t\n",
        "\n",
        "\n",
        "def _is_unknown_like(name_text):\n",
        "    s = str(name_text or \"\").strip().lower()\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", s).strip()\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    if not s:\n",
        "        return True\n",
        "    if s in (\"unknown\", \"none\", \"noneunknownname\"):\n",
        "        return True\n",
        "    if \"unknown name\" in s:\n",
        "        return True\n",
        "    if s.replace(\" \", \"\") in (\"noneunknownname\", \"unknownname\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _canon_lastfirst(name_text):\n",
        "    # Canonicalize a person name into a LastFirst key (no punctuation),\n",
        "    # matching the authority file convention (FirstPair_LastFirst).\n",
        "    if _is_unknown_like(name_text):\n",
        "        return \"unknown\"\n",
        "    s = str(name_text or \"\").strip()\n",
        "    s = re.sub(r\"\\([^)]*\\)\", \"\", s).strip()\n",
        "    toks = re.findall(r\"[A-Za-z0-9]+\", s.lower())\n",
        "    if not toks:\n",
        "        return \"\"\n",
        "    if len(toks) == 1:\n",
        "        return re.sub(r\"[^a-z0-9]+\", \"\", toks[0])\n",
        "    last = toks[-1]\n",
        "    first = \"\".join(toks[:-1])\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"\", last + first)\n",
        "\n",
        "def _extract_first_couple_ids(raw_lineage_text: str):\n",
        "    s = str(raw_lineage_text or \"\").strip()\n",
        "    if not s:\n",
        "        return (\"\", \"\", \"\", \"\")\n",
        "    couples = [t.strip() for t in COUPLE_SEP_RE.split(s) if t and t.strip()]\n",
        "    first = couples[0] if couples else s\n",
        "    parts = [p.strip() for p in SPOUSE_SPLIT_RE.split(first, maxsplit=1) if p and p.strip()]\n",
        "    father_raw = parts[0] if len(parts) >= 1 else \"\"\n",
        "    mother_raw = parts[1] if len(parts) >= 2 else \"\"\n",
        "    f_id = \"\"\n",
        "    m_id = \"\"\n",
        "    mf = re.search(r\"\\b(I\\d+)\\b\", father_raw, flags=re.I)\n",
        "    if mf:\n",
        "        f_id = mf.group(1).upper()\n",
        "    mm = re.search(r\"\\b(I\\d+)\\b\", mother_raw, flags=re.I)\n",
        "    if mm:\n",
        "        m_id = mm.group(1).upper()\n",
        "    f_disp = _scrub_side_keep_name_years(father_raw).strip()\n",
        "    m_disp = _scrub_side_keep_name_years(mother_raw).strip()\n",
        "    return (f_id, m_id, f_disp, m_disp)\n",
        "\n",
        "def _first_ancestor_authority_value(raw_lineage_text: str) -> str:\n",
        "    f_id, m_id, f_disp, m_disp = _extract_first_couple_ids(raw_lineage_text)\n",
        "\n",
        "    # Primary: authority lookup by IDs\n",
        "    if f_id and m_id:\n",
        "        k = AUTH_COUPLE_KEY_MAP.get((f_id, m_id), \"\")\n",
        "        if k:\n",
        "            return re.sub(r\"\\s+\", \"\", k)\n",
        "\n",
        "    # Fallback: LastFirst canonical key, synchronized with authority convention\n",
        "    if f_disp and m_disp:\n",
        "        return _canon_lastfirst(f_disp) + \"&\" + _canon_lastfirst(m_disp)\n",
        "    if f_disp:\n",
        "        return _canon_lastfirst(f_disp)\n",
        "    return \"\"\n",
        "\n",
        "# ---------- Vitals ----------\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "LAST_UPDATED_TEXT  = \"\"\n",
        "AUTOSOMAL_MATCHES  = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    s = str(raw or \"\").strip()\n",
        "    if not s:\n",
        "        return \"(unknown)\"\n",
        "    s = s.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\", \"%Y-%m-%dT%H:%M\", \"%Y-%m-%dT%H:%M:%S\"]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(s, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24  = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12  = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, dt_est.day, dt_est.year, h12, dt_est.minute, ampm)\n",
        "\n",
        "def _format_num_with_commas(raw_val):\n",
        "    s_digits = re.sub(r\"[^0-9\\-]\", \"\", str(raw_val or \"\"))\n",
        "    if not s_digits:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return \"{:,}\".format(int(s_digits))\n",
        "    except Exception:\n",
        "        return s_digits\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global LAST_UPDATED_TEXT, AUTOSOMAL_MATCHES\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will be blank for vitals.\")\n",
        "        return\n",
        "    vdf = None\n",
        "    for enc in (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "            break\n",
        "        except Exception:\n",
        "            vdf = None\n",
        "    if vdf is None:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv\")\n",
        "        return\n",
        "    flat = [str(cell) for row in vdf.astype(str).values.tolist() for cell in row]\n",
        "    autosomal_raw = None\n",
        "    last_text = None\n",
        "    for cell in flat:\n",
        "        if autosomal_raw is None and \"Records tagged and filtered by NPFX\" in cell:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", cell)\n",
        "            if m:\n",
        "                autosomal_raw = m.group(1)\n",
        "        if last_text is None and \"LAST_UPDATED_TEXT\" in cell:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", cell)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "    if last_text is not None:\n",
        "        LAST_UPDATED_TEXT = _friendly_ts_from_utc(last_text)\n",
        "    AUTOSOMAL_MATCHES = _format_num_with_commas(autosomal_raw)\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "updated_str = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT or \"\")\n",
        "_updated_parts = [updated_str]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "UPDATED_BLOCK = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join(_updated_parts) + '</div>'\n",
        "\n",
        "NAV_BLOCK = '<div id=\"nav-slot\"><!--#include virtual=\"/partials/nav_block.shtml\" --></div>'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ---------- Display DF ----------\n",
        "display_df = df.copy()\n",
        "\n",
        "# drop ID#\n",
        "if SUPPRESS_ID_COLUMN and display_df.shape[1] >= 1:\n",
        "    display_df = display_df.drop(columns=[display_df.columns[A_IDX]], errors=\"ignore\")\n",
        "\n",
        "# Identify lineage/path column BEFORE formatting (so IDs still exist for authority lookup)\n",
        "lineage_cols_raw = [c for c in display_df.columns if re.search(r\"(ancestral|lineage|tree|path|ancestor|line)\", str(c or \"\"), re.I)]\n",
        "AUTH_LINEAGE_COL = lineage_cols_raw[0] if lineage_cols_raw else None\n",
        "\n",
        "# Compute authority first-ancestor values from the raw lineage column\n",
        "if AUTH_LINEAGE_COL:\n",
        "    fa_values = display_df[AUTH_LINEAGE_COL].astype(str).map(_first_ancestor_authority_value)\n",
        "else:\n",
        "    fa_values = pd.Series([\"\"] * len(display_df))\n",
        "\n",
        "# REPLACE COLUMN 1 with First Ancestor values and rename header\n",
        "if display_df.shape[1] >= 1:\n",
        "    first_col_name = display_df.columns[0]\n",
        "    display_df[first_col_name] = fa_values.astype(str).map(lambda x: re.sub(r\"\\s+\", \"\", x).replace('&', '&#38;'))\n",
        "    display_df = display_df.rename(columns={first_col_name: \"First Ancestor\"})\n",
        "    print(\"[OK] Column 1 replaced with First Ancestor (authority).\")\n",
        "else:\n",
        "    print(\"[WARN] display_df has no columns to replace.\")\n",
        "\n",
        "# Remove any other \"First Ancestor\" columns to avoid duplication (keep the first one)\n",
        "fa_cols = [c for c in display_df.columns if str(c) == \"First Ancestor\"]\n",
        "if len(fa_cols) > 1:\n",
        "    keep_first = fa_cols[0]\n",
        "    drop_rest = fa_cols[1:]\n",
        "    display_df = display_df.drop(columns=drop_rest, errors=\"ignore\")\n",
        "    print(\"[OK] Dropped duplicate First Ancestor columns:\", \", \".join(drop_rest))\n",
        "\n",
        "# Now apply lineage formatting (removes embedded IDs AND applies enriched exclusion)\n",
        "display_df = _maybe_format_lineage_columns(display_df)\n",
        "\n",
        "# Alpha sort by first couple (kept)\n",
        "if ALPHA_BY_FIRST_ANCESTOR_FATHER:\n",
        "    lineage_cols = [c for c in display_df.columns if re.search(r\"(ancestral|lineage|tree|path|ancestor|line)\", str(c or \"\"), re.I)]\n",
        "    sort_col = lineage_cols[0] if lineage_cols else None\n",
        "    if sort_col:\n",
        "        sort_keys = display_df[sort_col].astype(str).map(_first_ancestor_sort_key)\n",
        "        display_df[\"__sort_surname__\"]    = [k[0] for k in sort_keys]\n",
        "        display_df[\"__sort_given__\"]      = [k[1] for k in sort_keys]\n",
        "        display_df[\"__sort_momsurname__\"] = [k[2] for k in sort_keys]\n",
        "\n",
        "        by_cols = [\"__sort_surname__\", \"__sort_given__\"]\n",
        "        if ALPHA_TIEBREAK_MOTHER_SURNAME:\n",
        "            by_cols.append(\"__sort_momsurname__\")\n",
        "\n",
        "        display_df = display_df.sort_values(by=by_cols, ascending=[True]*len(by_cols), kind=\"mergesort\").reset_index(drop=True)\n",
        "        display_df = display_df.drop(columns=[\"__sort_surname__\", \"__sort_given__\", \"__sort_momsurname__\"], errors=\"ignore\")\n",
        "        print(\"[OK] Alpha sort applied by first couple:\", sort_col, \"| keys=\", \",\".join(by_cols))\n",
        "\n",
        "# ---------- HTML table ----------\n",
        "visible_cols = [c for c in display_df.columns if c]\n",
        "\n",
        "table_html = display_df.to_html(\n",
        "    index=False,\n",
        "    columns=visible_cols,\n",
        "    escape=False,\n",
        "    border=1,\n",
        "    classes=\"dataframe sortable\"\n",
        ")\n",
        "\n",
        "if 'id=\"refactor-table\"' not in table_html:\n",
        "    table_html = re.sub(r\"<table([^>]*)>\", r'<table\\1 id=\"refactor-table\">', table_html, count=1)\n",
        "\n",
        "if 'class=\"dataframe sortable\"' not in table_html and \"sortable\" not in table_html:\n",
        "    table_html = table_html.replace('class=\"dataframe\"', 'class=\"dataframe sortable\"', 1)\n",
        "\n",
        "table_html = table_html.replace(\"<tbody>\\n<tr>\", \"<tbody>\\n<tr id=\\\"first-row\\\">\", 1)\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div id=\"top-scroll\" class=\"scroll-sync-top\">'\n",
        "    '<div class=\"scroll-sync-top-inner\" style=\"width:%dpx;\"></div>'\n",
        "    '</div>'\n",
        "    '<div id=\"bottom-scroll\" class=\"table-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (TABLE_WIDTH_PX, table_html)\n",
        "\n",
        "LATE_STYLE = r\"\"\"\n",
        "<style type=\"text/css\">\n",
        "#nav-slot, #nav-slot nav, #nav-slot .oldnav, #nav-slot .navbar{\n",
        "  display:block !important;\n",
        "  visibility:visible !important;\n",
        "  opacity:1 !important;\n",
        "}\n",
        "table.sortable thead{ display:table-header-group !important; visibility:visible !important; }\n",
        "table.sortable thead th{ display:table-cell !important; visibility:visible !important; }\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "JS_NAV_REPAIR = r\"\"\"\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function hasNavContainer(el){\n",
        "    if(!el) return false;\n",
        "    var n = el.querySelector('nav.oldnav, nav.navbar, .oldnav, .navbar');\n",
        "    return !!n;\n",
        "  }\n",
        "  function wrapFirstUL(el){\n",
        "    if(!el) return false;\n",
        "    var ul = el.querySelector('ul');\n",
        "    if(!ul) return false;\n",
        "    var nav = document.createElement('nav');\n",
        "    nav.className = 'oldnav';\n",
        "    nav.appendChild(ul);\n",
        "    while(el.firstChild){ el.removeChild(el.firstChild); }\n",
        "    el.appendChild(nav);\n",
        "    return true;\n",
        "  }\n",
        "  function looksLikeSSICommentOnly(el){\n",
        "    if(!el) return true;\n",
        "    var txt = (el.textContent || '').replace(/\\s+/g,'').toLowerCase();\n",
        "    if(!txt) return true;\n",
        "    if(txt.indexOf('<!--#include') >= 0) return true;\n",
        "    return false;\n",
        "  }\n",
        "  function injectRemoteNav(el){\n",
        "    try{\n",
        "      var xhr = new XMLHttpRequest();\n",
        "      xhr.open('GET', '/partials/nav_block.shtml', true);\n",
        "      xhr.onreadystatechange = function(){\n",
        "        if(xhr.readyState === 4){\n",
        "          if(xhr.status >= 200 && xhr.status < 300){\n",
        "            el.innerHTML = xhr.responseText;\n",
        "            if(!hasNavContainer(el)){\n",
        "              wrapFirstUL(el);\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      };\n",
        "      xhr.send(null);\n",
        "    }catch(e){}\n",
        "  }\n",
        "\n",
        "  function repairNav(){\n",
        "    var slot = document.getElementById('nav-slot');\n",
        "    if(!slot) return;\n",
        "\n",
        "    if(looksLikeSSICommentOnly(slot)){\n",
        "      injectRemoteNav(slot);\n",
        "      return;\n",
        "    }\n",
        "    if(!hasNavContainer(slot)){\n",
        "      wrapFirstUL(slot);\n",
        "      return;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if(document.readyState === 'loading'){\n",
        "    document.addEventListener('DOMContentLoaded', repairNav, false);\n",
        "  } else {\n",
        "    repairNav();\n",
        "  }\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "page_tpl = _T(r\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "$LATE_STYLE\n",
        "<style type=\"text/css\">\n",
        "/* Sticky second column (index 2) for Trees table */\n",
        "#refactor-table th:nth-child(2),\n",
        "#refactor-table td:nth-child(2){\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "#refactor-table th:nth-child(2){\n",
        "  z-index:7;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '').replace(/\\s+/g,' ').trim().toLowerCase();\n",
        "  }\n",
        "  function sortTable(tbl, colIndex, dir, keyColIndex){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "    var kIdx = (typeof keyColIndex === 'number') ? keyColIndex : colIndex;\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[kIdx]), B = textOf(b.cells[kIdx]);\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\-]/g,''));\n",
        "      if(!isNaN(nA) && !isNaN(nB)){ return asc ? (nA-nB) : (nB-nA); }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dir = 'asc';\n",
        "      th.addEventListener('click', function(){\n",
        "        dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "        var hdr = (th.textContent || th.innerText || '');\n",
        "        hdr = hdr.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'').replace(/\\s+/g,' ').trim().toLowerCase();\n",
        "        // Golden rule sync: sorting the lineage column uses First Ancestor (authority) as the key.\n",
        "        // Column 1 is 'First Ancestor' (index 0) in this build.\n",
        "        var keyColIndex = null;\n",
        "        if(hdr === 'yates dna ancestral line'){\n",
        "          keyColIndex = 0;\n",
        "        }\n",
        "\n",
        "        for (var j = 0; j < ths.length; j++){\n",
        "          ths[j].innerHTML = ths[j].innerHTML.replace(/\\s+\\(asc\\)|\\s+\\(desc\\)/,'');\n",
        "        }\n",
        "        th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl, idx, dir, (keyColIndex === null ? undefined : keyColIndex));\n",
        "      }, false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\-]/g,''),10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){ return String(n||''); }\n",
        "  }\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\+/g,' ')) : '';\n",
        "  }\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb = tbl.tBodies[0];\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200 ? 'block' : 'none'); }\n",
        "    toggle();\n",
        "    window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "  function bindSyncedScrollbars(){\n",
        "    var topScroll    = document.getElementById('top-scroll');\n",
        "    var bottomScroll = document.getElementById('bottom-scroll');\n",
        "    if(!(topScroll && bottomScroll)) return;\n",
        "    var syncing = false;\n",
        "    topScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      bottomScroll.scrollLeft = topScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "    bottomScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      topScroll.scrollLeft = bottomScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindBackToTop();\n",
        "    bindSearch();\n",
        "    bindSyncedScrollbars();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "$JS_NAV_REPAIR\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    LATE_STYLE=LATE_STYLE,\n",
        "    JS_NAV_REPAIR=JS_NAV_REPAIR,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER=SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df = display_df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w:\n",
        "        export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for seg in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host = os.environ.get(\"FTP_HOST\")\n",
        "ftp_user = os.environ.get(\"FTP_USER\")\n",
        "ftp_pass = os.environ.get(\"FTP_PASS\")\n",
        "ftp_port = int(os.environ.get(\"FTP_PORT\", \"21\") or \"21\")\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            with open(OUTPUT_NAME, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            with open(LOCAL_CSV, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/first_ancestor_pairs.csv; enriched prefix exclusion applied) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRPsS6gPHsAb",
        "outputId": "76700f21-9c87-49d2-aba4-a96eaf64d73a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell3_Trees_SSI_NavRepair | Version=2026.02.02-CELL3-COL1=FIRST-ANCESTOR-EXCLFIX2+ENRICHED-EXCLUDE1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=1700, cols=6\n",
            "Using resolver: /content/match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 1700 / 1700  unmatched: 0\n",
            "Using authority: /content/first_ancestor_pairs.server.csv\n",
            "[OK] Authority map ready: 255 pairs\n",
            "[OK] Column 1 replaced with First Ancestor (authority).\n",
            "[OK] Lineage formatting applied to columns: Yates DNA Ancestral Line\n",
            "[OK] Alpha sort applied by first couple: First Ancestor | keys= __sort_surname__,__sort_given__,__sort_momsurname__\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (Column 1 replaced with First Ancestor from /partials/first_ancestor_pairs.csv; enriched prefix exclusion applied) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# debug"
      ],
      "metadata": {
        "id": "9G7Y0HwjtZIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2 (3-column register) - NAV+HEADER REPAIR + SSI-safe headers ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2026.02.01-UNIFIED-BASELINE-NAVREPAIR1)\n",
        "# - Complete and runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout/colors via /partials/partials_unified.css (single baseline).\n",
        "# - Fixes in this build:\n",
        "#     (1) SSI nav include kept, PLUS JS fallback if SSI not parsed / markup missing.\n",
        "#     (2) Header \"Showing:\" count remains dynamic (search-filter aware).\n",
        "#     (3) SSI-safe header survivability: inline THEAD/TH display styles + late overrides after nav.\n",
        "# - VITALS FIX (aligned to repaired Cell2k/Cell2c behavior):\n",
        "#     (A) Parse dna_vitals.csv via 'line' column when present (preferred).\n",
        "#     (B) Detect last_updated_text case-insensitively (supports LAST_UPDATED_TEXT and last_updated_text).\n",
        "#     (C) Keep autosomal matches parsing (Records tagged and filtered by NPFX) but also accept\n",
        "#         After manual filter, total records as fallback.\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2_3Col_AuthorityFirstAncestor | Version=2026.02.01-UNIFIED-BASELINE-NAVREPAIR1+VITALSFIX1 | Encoding=ISO-8859-15\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_3Col_AuthorityFirstAncestor | Version=2026.02.01-UNIFIED-BASELINE-NAVREPAIR1+VITALSFIX1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# ---------- A) LAYOUT CONTROL BLOCK ----------\n",
        "COL_1_PX = 220\n",
        "COL_2_PX = 420\n",
        "COL_3_PX = 1240\n",
        "\n",
        "COL_WIDTHS = [COL_1_PX, COL_2_PX, COL_3_PX]\n",
        "TABLE_TOTAL_WIDTH_PX = sum(COL_WIDTHS)\n",
        "\n",
        "print(\"[LAYOUT] TABLE_TOTAL_WIDTH_PX=%d\" % TABLE_TOTAL_WIDTH_PX)\n",
        "print(\"[LAYOUT] Column widths (px): 1=%d 2=%d 3=%d\" % (COL_1_PX, COL_2_PX, COL_3_PX))\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Authority file created by Cell 1 (already on server)\n",
        "AUTH_REMOTE_DIR   = \"partials\"\n",
        "AUTH_BASENAME     = \"first_ancestor_pairs.csv\"\n",
        "AUTH_REMOTE_NAME  = posixpath.join(AUTH_REMOTE_DIR, AUTH_BASENAME)\n",
        "AUTH_LOCAL_CACHE  = \"first_ancestor_pairs.server.csv\"\n",
        "\n",
        "# NOTE: main register pages now .shtml (SSI pages only)\n",
        "LOCAL_HTML        = \"yates_ancestor_register.shtml\"\n",
        "REMOTE_HTML_CANON = posixpath.join(\"partials\", \"yates_ancestor_register.shtml\")\n",
        "REMOTE_HTML_LEG   = posixpath.join(\"partials\", \"ons_yates_dna_register.shtml\")\n",
        "\n",
        "FTP_DIR  = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "# Baseline stylesheet (single canonical CSS)\n",
        "UNIFIED_CSS_BASENAME = \"partials_unified.css\"\n",
        "# Cache buster helps Cloudflare / browser refresh.\n",
        "UNIFIED_CSS_VERSION  = \"v2026-02-01-unified-blue-refactor1\"\n",
        "UNIFIED_CSS_HREF     = \"/partials/%s?%s\" % (UNIFIED_CSS_BASENAME, UNIFIED_CSS_VERSION)\n",
        "HEAD_LINK            = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % UNIFIED_CSS_HREF\n",
        "\n",
        "# SSI navigation include (kept)\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "# Fallback nav markup (used only if SSI is not parsed / not present)\n",
        "NAV_FALLBACK_HTML = ''\n",
        "\n",
        "# Path for vitals from Cell 1\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "\n",
        "# Resolver for Match to (masked -> unmasked)\n",
        "SERVER_PARTIALS_DIR        = \"partials\"\n",
        "SERVER_MAPPING_BASENAME    = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE      = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) CSV helpers ----------\n",
        "def _read_csv_anyenc(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    dfx = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            dfx = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            dfx = None\n",
        "    if dfx is None:\n",
        "        raise RuntimeError(\"Unable to read CSV %s: %s\" % (path, last))\n",
        "    return dfx\n",
        "\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    dfm = _read_csv_anyenc(path)\n",
        "    if dfm.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    dfm = dfm.iloc[:, :2].copy()\n",
        "    dfm.columns = [\"code\", \"unmasked\"]\n",
        "    dfm[\"code\"]     = dfm[\"code\"].astype(str).str.strip().str.lower()\n",
        "    dfm[\"unmasked\"] = dfm[\"unmasked\"].astype(str).str.strip()\n",
        "    dfm = dfm[dfm[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if dfm.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return dfm\n",
        "\n",
        "# ---------- 3.1) Resolver ----------\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Text utils ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['&#8217;])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"  + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, couple_text_html):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        couple_text_html,\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 4.1) Parse enriched first ancestor token ----------\n",
        "LINEAGE_SPOUSE_SEP = \" & \"\n",
        "\n",
        "def _scrub_side_keep_name_years(side_raw: str):\n",
        "    side_raw = _clean_piece(side_raw or \"\")\n",
        "    if not side_raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "\n",
        "    if \"~\" in side_raw:\n",
        "        bits = [b.strip() for b in side_raw.split(\"~\")]\n",
        "        bits = [b for b in bits if b != \"\"]\n",
        "        if bits and re.match(r\"^I\\d+$\", bits[0], re.I):\n",
        "            pid = bits[0].upper()\n",
        "            nm  = normalize_person_name(bits[1]) if len(bits) >= 2 else \"\"\n",
        "            yrs = _clean_piece(bits[2]) if len(bits) >= 3 else \"\"\n",
        "            return (pid, nm, yrs)\n",
        "\n",
        "    m = re.match(r\"^(I\\d+)\\s+(.*)$\", side_raw, flags=re.I)\n",
        "    if m:\n",
        "        pid  = m.group(1).upper()\n",
        "        rest = _clean_piece(m.group(2))\n",
        "        yrs  = \"\"\n",
        "        m2 = re.search(r\"(\\b\\d{4}\\s*-\\s*(?:\\d{4})?\\b)\\s*$\", rest)\n",
        "        if m2:\n",
        "            yrs  = _clean_piece(m2.group(1).replace(\" \", \"\"))\n",
        "            rest = _clean_piece(rest[:m2.start()])\n",
        "        nm = normalize_person_name(rest) if rest else \"\"\n",
        "        return (pid, nm, yrs)\n",
        "\n",
        "    nm2 = smart_titlecase(side_raw) if \" \" in side_raw else smart_titlecase(surname_given_from_token(side_raw)[0])\n",
        "    return (\"\", nm2, \"\")\n",
        "\n",
        "def _first_ancestor_display_and_ids(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\", \"\")\n",
        "    raw = _clean_piece(tokens[0])\n",
        "    if not raw:\n",
        "        return (\"\", \"\", \"\")\n",
        "\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", raw, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        pid, nm, yrs = _scrub_side_keep_name_years(raw)\n",
        "        disp = nm or raw\n",
        "        if yrs:\n",
        "            disp = disp + \" (%s)\" % yrs\n",
        "        return (disp, pid, \"\")\n",
        "\n",
        "    f_id, f_nm, f_yrs = _scrub_side_keep_name_years(parts[0])\n",
        "    m_id, m_nm, m_yrs = _scrub_side_keep_name_years(parts[1])\n",
        "\n",
        "    f_disp = f_nm or normalize_person_name(parts[0])\n",
        "    m_disp = m_nm or normalize_person_name(parts[1])\n",
        "\n",
        "    if f_yrs:\n",
        "        f_disp = f_disp + \" (%s)\" % f_yrs\n",
        "    if m_yrs:\n",
        "        m_disp = m_disp + \" (%s)\" % m_yrs\n",
        "\n",
        "    disp = \"%s%s%s\" % (f_disp, LINEAGE_SPOUSE_SEP, m_disp)\n",
        "    return (disp, f_id, m_id)\n",
        "\n",
        "# ---------- 4.2) Load authority first-ancestor pairs ----------\n",
        "def load_authority_first_ancestor_map() -> dict:\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        raise RuntimeError(\"Missing FTP creds; cannot download authority file %s\" % AUTH_REMOTE_NAME)\n",
        "\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(AUTH_REMOTE_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, AUTH_BASENAME, AUTH_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if not ok:\n",
        "        raise RuntimeError(\"Authority file not found on server: /%s\" % _remote_path(AUTH_REMOTE_NAME))\n",
        "\n",
        "    adf = _read_csv_anyenc(AUTH_LOCAL_CACHE)\n",
        "\n",
        "    def find_col(df0, patterns, prefer_exact=None):\n",
        "        cols = list(df0.columns)\n",
        "        lowmap = {c.lower(): c for c in cols}\n",
        "        if prefer_exact:\n",
        "            for name in prefer_exact:\n",
        "                if name in df0.columns:\n",
        "                    return name\n",
        "                if name and name.lower() in lowmap:\n",
        "                    return lowmap[name.lower()]\n",
        "        for pat in patterns:\n",
        "            rx = re.compile(pat, re.I)\n",
        "            for c in cols:\n",
        "                if rx.search(c):\n",
        "                    return c\n",
        "        return None\n",
        "\n",
        "    a1_col  = find_col(adf, [r\"ancestor1.*_id$\", r\"firstpair_ancestor1_id$\"], [\"FirstPair_Ancestor1_ID\"])\n",
        "    a2_col  = find_col(adf, [r\"ancestor2.*_id$\", r\"firstpair_ancestor2_id$\"], [\"FirstPair_Ancestor2_ID\"])\n",
        "    key_col = find_col(adf, [r\"firstpair_lastfirst$\"], [\"FirstPair_LastFirst\"])\n",
        "\n",
        "    if not (a1_col and a2_col and key_col):\n",
        "        raise RuntimeError(\"Authority CSV missing required columns. Need Ancestor1_ID, Ancestor2_ID, FirstPair_LastFirst.\")\n",
        "\n",
        "    m = {}\n",
        "    for _, r in adf.iterrows():\n",
        "        a1 = str(r.get(a1_col, \"\")).strip()\n",
        "        a2 = str(r.get(a2_col, \"\")).strip()\n",
        "        k  = str(r.get(key_col, \"\")).strip()\n",
        "        if not (a1 and a2 and k):\n",
        "            continue\n",
        "        m[(a1.upper(), a2.upper())] = k\n",
        "        m[(a2.upper(), a1.upper())] = k\n",
        "\n",
        "    if not m:\n",
        "        raise RuntimeError(\"Authority mapping built empty from %s\" % AUTH_LOCAL_CACHE)\n",
        "\n",
        "    print(\"[OK] Authority first-ancestor map: %d pairs\" % (len(m)//2))\n",
        "    return m\n",
        "\n",
        "AUTH_COUPLE_KEY_MAP = load_authority_first_ancestor_map()\n",
        "\n",
        "# ---------- 5) Read main CSV ----------\n",
        "def find_col(df0, patterns, prefer_exact=None):\n",
        "    cols = list(df0.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df0.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "df = _read_csv_anyenc(CSV_IN)\n",
        "print(\"[OK] Loaded CSV: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "id_col    = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col  = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "cm_col    = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "path_col  = find_col(df, [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "                     [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"])\n",
        "\n",
        "for req, nm in [(id_col,\"ID#/PersonID\"), (match_col,\"Match to\"), (name_col,\"Name\"), (cm_col,\"cM\"), (path_col,\"Lineage\")]:\n",
        "    if not req:\n",
        "        raise ValueError(\"CSV missing required column: %s\" % nm)\n",
        "\n",
        "# ---------- 5.1) Vitals (FIXED to match repaired Cell2k/Cell2c style) ----------\n",
        "AUTOSOMAL_MATCHES = \"\"\n",
        "LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    s = str(raw or \"\").strip()\n",
        "    if not s:\n",
        "        return \"(unknown)\"\n",
        "    s = s.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(s, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)  # site convention: UTC-5, no DST\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24  = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12  = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _format_int_with_commas(s):\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"[^0-9\\-]\", \"\", str(s))\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return \"{:,}\".format(int(t))\n",
        "    except Exception:\n",
        "        return t\n",
        "\n",
        "def _scan_vitals_text_iter(text_iter):\n",
        "    \"\"\"\n",
        "    Shared parser used by Cell2k/Cell2c pattern:\n",
        "    - last_updated_text: <utc timestamp>  (case-insensitive)\n",
        "    - Records tagged and filtered by NPFX ... <number>\n",
        "    - After manual filter, total records: <number> (fallback for autosomal if needed)\n",
        "    \"\"\"\n",
        "    autosomal_raw = None\n",
        "    last_text_raw = None\n",
        "\n",
        "    rx_last = re.compile(r\"^\\s*last_updated_text\\s*:\\s*(.+)\\s*$\", re.I)\n",
        "    rx_npf  = re.compile(r\"records\\s+tagged\\s+and\\s+filtered\\s+by\\s+npfx\", re.I)\n",
        "    rx_after = re.compile(r\"^\\s*after\\s+manual\\s+filter,\\s*total\\s+records\\s*:\\s*(.+)\\s*$\", re.I)\n",
        "\n",
        "    for raw in text_iter:\n",
        "        line = str(raw or \"\").strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # last_updated_text\n",
        "        if last_text_raw is None:\n",
        "            m = rx_last.match(line)\n",
        "            if m:\n",
        "                last_text_raw = m.group(1).strip()\n",
        "\n",
        "        # autosomal preferred: NPFX line\n",
        "        if autosomal_raw is None and rx_npf.search(line):\n",
        "            m2 = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m2:\n",
        "                autosomal_raw = m2.group(1).strip()\n",
        "\n",
        "        # autosomal fallback: \"After manual filter...\" line\n",
        "        if autosomal_raw is None:\n",
        "            m3 = rx_after.match(line)\n",
        "            if m3:\n",
        "                mnum = re.search(r\"(\\d[\\d,]*)\", m3.group(1))\n",
        "                if mnum:\n",
        "                    autosomal_raw = mnum.group(1).strip()\n",
        "\n",
        "        if autosomal_raw is not None and last_text_raw is not None:\n",
        "            break\n",
        "\n",
        "    return autosomal_raw, last_text_raw\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global AUTOSOMAL_MATCHES, LAST_UPDATED_TEXT\n",
        "    AUTOSOMAL_MATCHES = \"\"\n",
        "    LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will omit counts and last-updated text.\")\n",
        "        return\n",
        "\n",
        "    vdf = _read_csv_anyenc(path)\n",
        "\n",
        "    # Preferred: parse from 'line' column (matches repaired Cell2k/Cell2c)\n",
        "    autosomal_raw = None\n",
        "    last_text_raw = None\n",
        "    if \"line\" in [c.lower() for c in vdf.columns]:\n",
        "        # find actual column name preserving case\n",
        "        col_map = {c.lower(): c for c in vdf.columns}\n",
        "        line_col = col_map.get(\"line\")\n",
        "        autosomal_raw, last_text_raw = _scan_vitals_text_iter(vdf[line_col].astype(str).tolist())\n",
        "    else:\n",
        "        # Fallback: scan all cells flattened\n",
        "        flat = [str(cell) for row in vdf.astype(str).values.tolist() for cell in row]\n",
        "        autosomal_raw, last_text_raw = _scan_vitals_text_iter(flat)\n",
        "\n",
        "    if last_text_raw is not None:\n",
        "        LAST_UPDATED_TEXT = _friendly_ts_from_utc(last_text_raw)\n",
        "    else:\n",
        "        LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "    AUTOSOMAL_MATCHES = _format_int_with_commas(autosomal_raw) if autosomal_raw else \"\"\n",
        "\n",
        "    print(\"[VITALS] autosomal=%s  last_updated=%s\" % (\n",
        "        AUTOSOMAL_MATCHES or \"(blank)\",\n",
        "        LAST_UPDATED_TEXT or \"(blank)\",\n",
        "    ))\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "# ---------- 6) Transform + compute authority sort key ----------\n",
        "_setup_resolver()\n",
        "\n",
        "out_match_to = []\n",
        "out_auth_key = []\n",
        "out_summary  = []\n",
        "out_sort_key = []\n",
        "\n",
        "def _canon_side(txt):\n",
        "    t = re.sub(r\"\\([^)]*\\)\", \"\", str(txt or \"\"))\n",
        "    t = re.sub(r\"[^A-Za-z0-9]+\", \"\", t).lower()\n",
        "    return t\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw  = row.get(match_col, \"\")\n",
        "    subject_name = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_html = \"<strong>%s</strong>\" % _html.escape(subject_name or \"\", quote=False)\n",
        "\n",
        "    pid          = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_raw  = row.get(name_col, \"\")\n",
        "    matchee_name = norm_matchee_name(matchee_raw) or subject_name\n",
        "\n",
        "    if pid:\n",
        "        matchee_url = (\n",
        "            \"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\"\n",
        "            % (TNG_BASE, pid, TNG_TREE)\n",
        "        )\n",
        "        matchee_name_html = '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (\n",
        "            _html.escape(matchee_url, quote=True),\n",
        "            _html.escape(matchee_name or \"\", quote=False),\n",
        "        )\n",
        "    else:\n",
        "        matchee_name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val     = row.get(cm_col, \"0\")\n",
        "    tokens     = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    couple_disp, f_id, m_id = _first_ancestor_display_and_ids(tokens)\n",
        "\n",
        "    auth_key = \"\"\n",
        "    if f_id and m_id:\n",
        "        auth_key = AUTH_COUPLE_KEY_MAP.get((f_id.upper(), m_id.upper()), \"\")\n",
        "\n",
        "    if not auth_key:\n",
        "        parts = [p.strip() for p in re.split(r\"\\s*&\\s*\", couple_disp, maxsplit=1)]\n",
        "        if len(parts) == 2:\n",
        "            auth_key = _canon_side(parts[0]) + \"&\" + _canon_side(parts[1])\n",
        "        else:\n",
        "            auth_key = _canon_side(couple_disp)\n",
        "\n",
        "    couple_html = _html.escape(couple_disp or \"\", quote=False) if couple_disp else \"\"\n",
        "    summary_html = build_header(subject_name_html, cm_val, matchee_name_html, gens_total, couple_html)\n",
        "\n",
        "    out_match_to.append(_html.escape(subject_name or \"\", quote=False))\n",
        "    out_auth_key.append(_html.escape(auth_key or \"\", quote=False))\n",
        "    out_summary.append(summary_html)\n",
        "    out_sort_key.append(auth_key or \"zzzzzzzzzzzzzzzzzzzzzzzz\")\n",
        "\n",
        "df_out = pd.DataFrame({\n",
        "    \"Match to\": out_match_to,\n",
        "    \"First Ancestor Key\": out_auth_key,\n",
        "    \"Match Summary\": out_summary,\n",
        "    \"__sort__\": out_sort_key,\n",
        "})\n",
        "df_out = df_out.sort_values(by=\"__sort__\", kind=\"mergesort\").drop(columns=[\"__sort__\"]).reset_index(drop=True)\n",
        "\n",
        "# ---------- 7) HTML ----------\n",
        "ROOT_VAR_STYLE = '<style type=\"text/css\">:root{--table-width-px:%dpx;}</style>' % int(TABLE_TOTAL_WIDTH_PX)\n",
        "\n",
        "updated_label = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT or \"\")\n",
        "_updated_parts = [updated_label]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "\n",
        "UPDATED_BLOCK = '<div class=\"updated centerline\">' + ' &nbsp;|&nbsp; '.join([p for p in _updated_parts if p.strip()]) + '</div>'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls controls-spaced centerline\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "LATE_OVERRIDE_BLOCK = \"\"\n",
        "\n",
        "# Build fixed-width table (3 cols)\n",
        "col_headers = [\n",
        "    (\"Match to\", \"center\"),\n",
        "    (\"First Ancestor\", \"center\"),\n",
        "    (\"Match Summary\", \"left\"),\n",
        "]\n",
        "col_data = [\n",
        "    df_out[\"Match to\"].tolist(),\n",
        "    df_out[\"First Ancestor Key\"].tolist(),\n",
        "    df_out[\"Match Summary\"].tolist(),\n",
        "]\n",
        "\n",
        "thead_cells = []\n",
        "for idx, (hdr, align) in enumerate(col_headers):\n",
        "    wpx = COL_WIDTHS[idx]\n",
        "    style_attr = \"width:%dpx; display:table-cell !important;\" % wpx\n",
        "    if align == \"center\":\n",
        "        thead_cells.append('<th class=\"center-header\" style=\"%s\">%s</th>' % (style_attr, hdr))\n",
        "    else:\n",
        "        thead_cells.append('<th class=\"left-header\" style=\"%s\">%s</th>' % (style_attr, hdr))\n",
        "\n",
        "thead_html = (\n",
        "    '<thead style=\"display:table-header-group !important;\">\\n'\n",
        "    '  <tr style=\"display:table-row !important;\">'\n",
        "    + \"\".join(thead_cells)\n",
        "    + \"</tr>\\n</thead>\"\n",
        ")\n",
        "\n",
        "tbody_lines = [\"<tbody>\"]\n",
        "for r in range(len(df_out)):\n",
        "    cells = []\n",
        "    for c in range(len(col_headers)):\n",
        "        wpx = COL_WIDTHS[c]\n",
        "        val = col_data[c][r]\n",
        "        val_str = \"\" if val is None else str(val)\n",
        "        cells.append('<td style=\"width:%dpx;\">%s</td>' % (wpx, val_str))\n",
        "    tbody_lines.append(\"  <tr>\" + \"\".join(cells) + \"</tr>\")\n",
        "tbody_lines.append(\"</tbody>\")\n",
        "tbody_html = \"\\n\".join(tbody_lines)\n",
        "\n",
        "html_table = (\n",
        "    '<table border=\"1\" class=\"dataframe sortable dna-register-table\" id=\"refactor-table\">'\n",
        "    + thead_html + \"\\n\" + tbody_html + \"</table>\"\n",
        ")\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div class=\"table-scroll\" id=\"bottom-scroll\">%s</div>'\n",
        "    \"</div>\"\n",
        ") % (html_table,)\n",
        "\n",
        "JS_NAV_REPAIR = \"\"\n",
        "\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "$ROOT_VAR_STYLE\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $LATE_OVERRIDE_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "\n",
        "$JS_NAV_REPAIR\n",
        "\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '')\n",
        "      .replace(/\\\\s+/g,' ')\n",
        "      .trim()\n",
        "      .toLowerCase();\n",
        "  }\n",
        "\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''), 10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){\n",
        "      return String(n||'');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[colIndex]),\n",
        "          B = textOf(b.cells[colIndex]);\n",
        "\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "\n",
        "      if(!isNaN(nA) && !isNaN(nB)){\n",
        "        return asc ? (nA - nB) : (nB - nA);\n",
        "      }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      frag.appendChild(rows[i]);\n",
        "    }\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "\n",
        "    for(var i=0;i<ths.length;i++){\n",
        "      (function(idx){\n",
        "        var th  = ths[idx];\n",
        "        var dir = 'asc';\n",
        "        th.addEventListener('click', function(){\n",
        "          dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "          for (var j = 0; j < ths.length; j++){\n",
        "            ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "          }\n",
        "          th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "          sortTable(tbl, idx, dir);\n",
        "        }, false);\n",
        "      })(i);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "\n",
        "    var tb   = tbl.tBodies[0];\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt  = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "\n",
        "    box.addEventListener('input',  onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindSearch();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK           = HEAD_LINK,\n",
        "    ROOT_VAR_STYLE      = ROOT_VAR_STYLE,\n",
        "    UPDATED_BLOCK       = UPDATED_BLOCK,\n",
        "    NAV_BLOCK           = NAV_BLOCK,\n",
        "    NAV_FALLBACK_HTML   = NAV_FALLBACK_HTML,\n",
        "    LATE_OVERRIDE_BLOCK = LATE_OVERRIDE_BLOCK,\n",
        "    CONTROLS_BLOCK      = CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER      = SCROLL_WRAPPER,\n",
        "    JS_NAV_REPAIR       = JS_NAV_REPAIR,\n",
        ")\n",
        "\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"[OK] Saved render: %s\" % os.path.abspath(LOCAL_HTML))\n",
        "\n",
        "print(\"[DEBUG] SSI nav include present:\", (\"<!--#include\" in final_html))\n",
        "\n",
        "# ---------- 8) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload main HTML failed: %s\" % e)\n",
        "\n",
        "        print(\"\\\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [_remote_path(REMOTE_HTML_CANON), _remote_path(REMOTE_HTML_LEG)]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\\\n--- Open URLs ---\")\n",
        "        print(\"Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\")\n",
        "        print(\"Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\")\n",
        "        print(\"CSS:       https://yates.one-name.net/partials/partials_unified.css\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session: %s\" % e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "\n",
        "print(\"\\\\n--- Cell 2 complete (NAVREPAIR1+VITALSFIX1: vitals parsing aligned to Cell2k/Cell2c) ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2 (3-column register) - NAV+HEADER REPAIR + SSI-safe headers ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCwqe1XLGuTI",
        "outputId": "ab96b328-91cc-4b6c-f675-8af1bfa85b91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_3Col_AuthorityFirstAncestor | Version=2026.02.01-UNIFIED-BASELINE-NAVREPAIR1+VITALSFIX1 | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=1880\n",
            "[LAYOUT] Column widths (px): 1=220 2=420 3=1240\n",
            "[PULL] first_ancestor_pairs.csv -> /content/first_ancestor_pairs.server.csv\n",
            "[OK] Authority first-ancestor map: 56 pairs\n",
            "[OK] Loaded CSV: 226 rows, 6 cols\n",
            "[VITALS] autosomal=1,700  last_updated=February 1, 2026 8:32 PM\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 94 codes\n",
            "[OK] Saved render: /content/yates_ancestor_register.shtml\n",
            "[DEBUG] SSI nav include present: True\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "\\n--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 116693\n",
            "partials/ons_yates_dna_register.shtml : 116693\n",
            "\\n--- Open URLs ---\n",
            "Canonical: https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy:    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "CSS:       https://yates.one-name.net/partials/partials_unified.css\n",
            "\\n--- Cell 2 complete (NAVREPAIR1+VITALSFIX1: vitals parsing aligned to Cell2k/Cell2c) ---\n"
          ]
        }
      ]
    }
  ]
}