{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1RROHlBgtXAYWOqDWQkIX8NCciZBqPcV3",
      "authorship_tag": "ABX9TyPfVTiM3gGiMP2XQFiMZ2eR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/New_Gold__Orchestrator_20251207_1630.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PIP"
      ],
      "metadata": {
        "id": "XtvXRl-lcavJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "bc106e32-e519-4cc1-af88-474fbd2cc8cb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "ERROR: unknown command \"caas_jupyter_tools\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n",
        "!pip caas_jupyter_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 0"
      ],
      "metadata": {
        "id": "ydh7RNi7elx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.07-G4)\n",
        "# - Complete & runnable Colab cell: one contiguous block, no fragments.\n",
        "# - Source ASCII-only; any file writes must use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Authority:\n",
        "#     * Colab work dir: /content\n",
        "#     * Scripts pulled from: /partials/colab_cells/*.py  (server)\n",
        "#     * Latest GEDCOM pulled from: /tng/gedcom/*.ged     (server)\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2025.12.07-G4 | Encoding=ISO-8859-15\n",
        "# - Execution order (after pulls): cell1.py, cell2.py, cell2b.py, cell2c.py, cell2k.py, cell3.py\n",
        "# ====================================================================\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2025.12.07-G4 | Encoding=ISO-8859-15\")\n",
        "\n",
        "DECLARED_LINES = 260\n",
        "print(\"[AUDIT] DECLARED_LINES=%d\" % DECLARED_LINES)\n",
        "\n",
        "import os, socket, traceback, hashlib\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "\n",
        "# ---------- 0) Env / secrets ----------\n",
        "\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\", \"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\", \"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\", \"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\", \"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\", \"\") or \"\").strip().strip(\"/\")\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\n",
        "    \"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\"\n",
        "    % (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\")\n",
        ")\n",
        "\n",
        "if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "    raise SystemExit(\"[FATAL] Missing FTP_HOST/FTP_USER/FTP_PASS; cannot run orchestrator.\")\n",
        "\n",
        "# ---------- 1) FTPS helpers ----------\n",
        "\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _ftps_connect():\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()  # Explicit FTPS\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for seg in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.cwd(seg)\n",
        "            except all_errors:\n",
        "                try:\n",
        "                    ftps.mkd(seg)\n",
        "                except all_errors:\n",
        "                    pass\n",
        "                ftps.cwd(seg)\n",
        "    return ftps\n",
        "\n",
        "def _sha256_of_file(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, \"rb\") as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b\"\"):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()[:16]\n",
        "\n",
        "# ---------- 2) Pull authority scripts from /partials/colab_cells ----------\n",
        "\n",
        "SCRIPT_REMOTE_DIR = \"/partials/colab_cells\"\n",
        "SCRIPT_NAMES = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell2k.py\", \"cell3.py\"]\n",
        "\n",
        "def pull_authority_scripts():\n",
        "    print(\"[STEP] Pulling authority scripts from server ...\")\n",
        "    pulled = 0\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        try:\n",
        "            pwd0 = ftps.pwd()\n",
        "        except Exception:\n",
        "            pwd0 = \"(unknown)\"\n",
        "        print(\"[OK] Connected via FTPS (explicit AUTH TLS).\")\n",
        "        print(\"[INFO] Initial PWD on server: %s\" % pwd0)\n",
        "\n",
        "        # Move to /partials/colab_cells, independent of FTP_DIR\n",
        "        try:\n",
        "            # go to root, then walk to /partials/colab_cells\n",
        "            try:\n",
        "                ftps.cwd(\"/\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            for seg in [p for p in SCRIPT_REMOTE_DIR.split(\"/\") if p]:\n",
        "                ftps.cwd(seg)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\"Unable to cwd to %s (%s)\" % (SCRIPT_REMOTE_DIR, e))\n",
        "\n",
        "        print(\"[INFO] Using remote dir for scripts: %s\" % SCRIPT_REMOTE_DIR)\n",
        "\n",
        "        os.chdir(\"/content\")\n",
        "        for name in SCRIPT_NAMES:\n",
        "            try:\n",
        "                local_path = os.path.join(\"/content\", name)\n",
        "                with open(local_path, \"wb\") as f:\n",
        "                    ftps.retrbinary(\"RETR \" + name, f.write)\n",
        "                sz = os.path.getsize(local_path)\n",
        "                sh = _sha256_of_file(local_path)\n",
        "                print(\"[PULL] %s -> %s  size=%d  sha256=%s\" % (name, local_path, sz, sh))\n",
        "                pulled += 1\n",
        "            except Exception as e:\n",
        "                print(\"[MISS] Could not pull %s: %s\" % (name, e))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Pulled %d script(s) from authority shelf.\" % pulled)\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Script pull failed:\", e)\n",
        "        traceback.print_exc()\n",
        "    return pulled\n",
        "\n",
        "# ---------- 3) Pull latest GEDCOM from /tng/gedcom ----------\n",
        "\n",
        "GEDCOM_REMOTE_DIR = \"/tng/gedcom\"\n",
        "LOCAL_GED_DIR = \"/content\"\n",
        "\n",
        "def _choose_latest_gedcom(ftps, names):\n",
        "    \"\"\"\n",
        "    Pick the newest *.ged using MDTM if available; fallback = last alphabetically.\n",
        "    \"\"\"\n",
        "    ged_files = [n for n in names if n.lower().endswith(\".ged\")]\n",
        "    if not ged_files:\n",
        "        return None\n",
        "\n",
        "    latest_name = None\n",
        "    latest_ts = None\n",
        "\n",
        "    for nm in ged_files:\n",
        "        ts = None\n",
        "        try:\n",
        "            # MDTM response: '213 YYYYMMDDhhmmss'\n",
        "            resp = ftps.sendcmd(\"MDTM \" + nm)\n",
        "            parts = resp.strip().split()\n",
        "            if len(parts) == 2 and parts[0] == \"213\":\n",
        "                ts = parts[1]\n",
        "        except Exception:\n",
        "            ts = None\n",
        "        if ts is None:\n",
        "            # Fallback: lexical key\n",
        "            ts = \"00000000000000\" + nm\n",
        "        if latest_ts is None or ts > latest_ts:\n",
        "            latest_ts = ts\n",
        "            latest_name = nm\n",
        "    return latest_name\n",
        "\n",
        "def pull_latest_gedcom():\n",
        "    print(\"\\n[STEP] Pulling latest GEDCOM from %s ...\" % GEDCOM_REMOTE_DIR)\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        # Go to absolute /tng/gedcom irrespective of FTP_DIR\n",
        "        try:\n",
        "            ftps.cwd(\"/\")\n",
        "        except Exception:\n",
        "            pass\n",
        "        for seg in [p for p in GEDCOM_REMOTE_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "        names = ftps.nlst()\n",
        "        if not names:\n",
        "            print(\"[WARN] No files listed in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        latest = _choose_latest_gedcom(ftps, names)\n",
        "        if not latest:\n",
        "            print(\"[WARN] No .ged files found in %s\" % GEDCOM_REMOTE_DIR)\n",
        "            try:\n",
        "                ftps.quit()\n",
        "            except Exception:\n",
        "                pass\n",
        "            return\n",
        "\n",
        "        print(\"[INFO] Latest GEDCOM on server: %s\" % latest)\n",
        "\n",
        "        # Clean any old local GEDCOMs so Cell 1 cannot accidentally pick the wrong one\n",
        "        try:\n",
        "            for fname in os.listdir(LOCAL_GED_DIR):\n",
        "                if fname.lower().endswith(\".ged\"):\n",
        "                    try:\n",
        "                        os.remove(os.path.join(LOCAL_GED_DIR, fname))\n",
        "                        print(\"[CLEAN] Removed old local GEDCOM:\", fname)\n",
        "                    except Exception as e:\n",
        "                        print(\"[WARN] Could not remove %s: %s\" % (fname, e))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Could not scan local GED dir:\", e)\n",
        "\n",
        "        # Download latest into /content with same filename\n",
        "        local_path = os.path.join(LOCAL_GED_DIR, latest)\n",
        "        with open(local_path, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR \" + latest, f.write)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        sz = os.path.getsize(local_path)\n",
        "        print(\"[OK] Pulled GEDCOM -> %s  size=%d bytes\" % (local_path, sz))\n",
        "        print(\"[INFO] Cell 1 will now see exactly one *.ged in /content.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] GEDCOM pull failed; Cell 1 will use any existing local *.ged instead.\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 4) Run scripts in order ----------\n",
        "\n",
        "def run_script(path):\n",
        "    print(\"\\n[RUN] %s\" % path)\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[SKIP] %s not found in /content.\" % path)\n",
        "        return\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"iso-8859-15\", errors=\"ignore\") as f:\n",
        "            code = f.read()\n",
        "        compiled = compile(code, path, \"exec\")\n",
        "        exec(compiled, globals(), globals())\n",
        "        print(\"[DONE] %s\" % path)\n",
        "    except SystemExit as e:\n",
        "        print(\"[EXIT] %s exited: %s\" % (path, e))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Exception while running %s: %s\" % (path, e))\n",
        "        traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    os.chdir(\"/content\")\n",
        "    pulled = pull_authority_scripts()\n",
        "    if pulled == 0:\n",
        "        print(\"[FATAL] No authority scripts pulled; aborting.\")\n",
        "        return\n",
        "\n",
        "    # Always pull the latest GEDCOM snapshot before running Cell 1\n",
        "    pull_latest_gedcom()\n",
        "\n",
        "    script_order = [\"cell1.py\", \"cell2.py\", \"cell2b.py\", \"cell2c.py\", \"cell2k.py\", \"cell3.py\"]\n",
        "    print(\"\\n[STEP] Running scripts in order:\", \", \".join(script_order))\n",
        "    for s in script_order:\n",
        "        run_script(os.path.join(\"/content\", s))\n",
        "\n",
        "main()\n",
        "\n",
        "print(\"\\n--- Cell 0 That's all folks, Orchestrator complete (authority scripts + latest GEDCOM pulled, then Cell1/2/2b/2c/2k/3 executed) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 0 - Orchestrator (Authority Scripts + Latest GEDCOM) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXBnI004qDtq",
        "outputId": "24fe774a-ef5d-4ff0-bfd5-4ea987cd109d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell0_Orchestrator | Version=2025.12.07-G4 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=260\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "[STEP] Pulling authority scripts from server ...\n",
            "[OK] Connected via FTPS (explicit AUTH TLS).\n",
            "[INFO] Initial PWD on server: /\n",
            "[INFO] Using remote dir for scripts: /partials/colab_cells\n",
            "[PULL] cell1.py -> /content/cell1.py  size=22254  sha256=8affb5e8a2ebba3e\n",
            "[PULL] cell2.py -> /content/cell2.py  size=39821  sha256=857ed34e45060d8d\n",
            "[PULL] cell2b.py -> /content/cell2b.py  size=37933  sha256=cf6ad854d980bc24\n",
            "[PULL] cell2c.py -> /content/cell2c.py  size=33006  sha256=1e055d943a746059\n",
            "[PULL] cell2k.py -> /content/cell2k.py  size=39671  sha256=3e4409cf09622b84\n",
            "[PULL] cell3.py -> /content/cell3.py  size=23320  sha256=e7091d2d79a4d827\n",
            "[OK] Pulled 6 script(s) from authority shelf.\n",
            "\n",
            "[STEP] Pulling latest GEDCOM from /tng/gedcom ...\n",
            "[INFO] Latest GEDCOM on server: yates_study_2025.ged\n",
            "[OK] Pulled GEDCOM -> /content/yates_study_2025.ged  size=38705063 bytes\n",
            "[INFO] Cell 1 will now see exactly one *.ged in /content.\n",
            "\n",
            "[STEP] Running scripts in order: cell1.py, cell2.py, cell2b.py, cell2c.py, cell2k.py, cell3.py\n",
            "\n",
            "[RUN] /content/cell1.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.27-G2 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "Automatically selecting the first GEDCOM file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:Cell1_FTPS_Explicit:filtered_ids.xlsx not found. Skipping second-level manual filter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GEDCOM contained 62567 total records\n",
            "Records tagged and filtered by NPFX: 1605\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1605\n",
            "[OK] Wrote autosomal_count.txt = 1605\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: December 7, 2025 7:29 PM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 1605 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 1605/1605 [11:44<00:00,  2.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUMMARY] GEDCOM total records: 62567\n",
            "[SUMMARY] NPFX-tagged records: 1605\n",
            "[SUMMARY] Autosomal matches (NPFX minus YDNA): 1605\n",
            "[SUMMARY] After manual filter, total records: 1605\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploads complete to /partials/\n",
            "\n",
            "--- Cell 1 Complete: CSV + HTML + dna_vitals.csv + autosomal_count.txt built with ISO-8859-15; explicit FTPS used. ---\n",
            "[DONE] /content/cell1.py\n",
            "\n",
            "[RUN] /content/cell2.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=3480\n",
            "[LAYOUT] Column widths (px): 1=80 2=220 3=60 4=1200 5=120 6=1800\n",
            "[OK] Loaded CSV: 1605 rows, 6 cols\n",
            "[OK] Loaded vitals from dna_vitals.csv -> autosomal=1,605, showing=1,605, last_updated_text=December 7, 2025 7:29 PM\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Wrote stylesheet: /content/partials/dna_tree_styles.css\n",
            "[OK] Saved canonical render: /content/yates_ancestor_register.shtml\n",
            "[PUT] partials/dna_tree_styles.css -> partials/dna_tree_styles.css\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "[PUT] yates_ancestor_register.csv -> partials/yates_ancestor_register.csv\n",
            "[PUT] yates_ancestor_register.xlsx -> partials/yates_ancestor_register.xlsx\n",
            "[OK] Uploaded CSV/XLSX -> /partials/\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 1786627\n",
            "partials/ons_yates_dna_register.shtml : 1786627\n",
            "partials/yates_ancestor_register.csv : 713921\n",
            "partials/yates_ancestor_register.xlsx : 156904\n",
            "partials/dna_tree_styles.css : 3855\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy (ons_):    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "Trees (Cell 3):   https://yates.one-name.net/partials/just-trees.shtml\n",
            "Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "Bust cache once if needed by appending ?v=v2025-12-01-g1 to the URL.\n",
            "[DONE] /content/cell2.py\n",
            "\n",
            "[RUN] /content/cell2b.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.12.06-G2 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 1,605\n",
            "[VITALS] last updated (display): December 7, 2025 7:29 PM\n",
            "[OK] Loaded CSV for counts: 1605 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 950814\n",
            "partials/lineage_count.shtml : 1070659\n",
            "partials/cousin_list_print.htm : 530043\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "[DONE] /content/cell2b.py\n",
            "\n",
            "[RUN] /content/cell2c.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 1,605\n",
            "[VITALS] last updated (display): December 7, 2025 7:29 PM\n",
            "[OK] Loaded CSV for DNA Network: 1605 rows, 6 cols\n",
            "[PULL] dna/network_first_ancestors.txt -> /content/dna_network_first_ancestors.txt\n",
            "[INFO] Authority file synced from server.\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[INFO] Loaded 1 authority First Ancestors from dna_network_first_ancestors.txt\n",
            "[INFO] First 10 authority entries:\n",
            "   1. YatesWilliam&ThornburyAnne\n",
            "[INFO] Filtered DNA network rows by authority list: 1605 -> 15\n",
            "[OK] Wrote DNA Network partial: /content/partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 15056\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network: https://yates.one-name.net/partials/dna_network.shtml\n",
            "[DONE] /content/cell2c.py\n",
            "\n",
            "[RUN] /content/cell2k.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2025.12.07-K12 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=520\n",
            "[VITALS] autosomal (after manual filter): 1,605\n",
            "[VITALS] last updated (display): December 7, 2025 7:29 PM\n",
            "[OK] Loaded CSV for unified DNA Network: 1605 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote unified DNA Network partial: /content/partials/dna_network.shtml\n",
            "[OK] Wrote register CSV: /content/dna_network_register.csv\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[PUT] dna_network_register.csv -> dna/dna_network_register.csv\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 1384555\n",
            "dna/dna_network_register.csv : 753663\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\n",
            "Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\n",
            "[DONE] /content/cell2k.py\n",
            "\n",
            "[RUN] /content/cell3.py\n",
            "[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=1605, cols=6\n",
            "Resolver not found locally; attempting server pull ...\n",
            "[OK] Pulled resolver from server -> match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 1605 / 1605  unmatched: 0\n",
            "[OK] Vitals from dna_vitals.csv -> autosomal=1,605, showing=1,605, last_updated_text=December 7, 2025 7:29 PM\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (.shtml + SSI nav; top+bottom scroll; sticky col 2; sortable/searchable with live 'Showing' count; LAST_UPDATED_TEXT + Autosomal matches from dna_vitals.csv, now comma-formatted; exports + upload ready) ---\n",
            "[DONE] /content/cell3.py\n",
            "\n",
            "--- Cell 0 That's all folks, Orchestrator complete (authority scripts + latest GEDCOM pulled, then Cell1/2/2b/2c/2k/3 executed) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "JvOlmbj91AGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.11.27-G2)\n",
        "# - Complete and runnable Colab cell, one contiguous block (no fragments).\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography via /partials/dna_tree_styles.css (HTML export only).\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.27-G2 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes; replace non-Latin with XML entities.\n",
        "# =================================================================================================\n",
        "\n",
        "import os, re, glob, logging, functools, socket, traceback, hashlib\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from ftplib import FTP_TLS, all_errors\n",
        "from string import Template\n",
        "\n",
        "CELL_NAME = \"Cell1_FTPS_Explicit\"\n",
        "VERSION   = \"2025.11.27-G2\"\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=%s | Version=%s | Encoding=ISO-8859-15\" % (CELL_NAME, VERSION))\n",
        "\n",
        "# ---------- Logging ----------\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(CELL_NAME)\n",
        "\n",
        "# ---------- Timezone helper (EST) ----------\n",
        "def _now_est_string():\n",
        "    \"\"\"\n",
        "    Return a human formatted EST timestamp like:\n",
        "    November 27, 2025 8:09 PM\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Python 3.9+ standard library\n",
        "        from zoneinfo import ZoneInfo\n",
        "        tz = ZoneInfo(\"America/New_York\")\n",
        "        now = datetime.now(tz)\n",
        "    except Exception:\n",
        "        # Fallback: naive local time if zoneinfo is unavailable\n",
        "        now = datetime.now()\n",
        "\n",
        "    month_name = now.strftime(\"%B\")\n",
        "    day = now.day\n",
        "    year = now.year\n",
        "    hour_24 = now.hour\n",
        "    minute = now.minute\n",
        "    ampm = \"AM\" if hour_24 < 12 else \"PM\"\n",
        "    hour_12 = hour_24 % 12\n",
        "    if hour_12 == 0:\n",
        "        hour_12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (month_name, day, year, hour_12, minute, ampm)\n",
        "\n",
        "# ---------- Secrets (env or userdata) ----------\n",
        "def _get_env(k, default=\"\"):\n",
        "    try:\n",
        "        from google.colab import userdata  # type: ignore\n",
        "        v = userdata.get(k)\n",
        "        return v if v is not None else os.environ.get(k, default)\n",
        "    except Exception:\n",
        "        return os.environ.get(k, default)\n",
        "\n",
        "FTP_HOST = (_get_env(\"FTP_HOST\",\"\") or \"\").strip()\n",
        "FTP_USER = (_get_env(\"FTP_USER\",\"\") or \"\").strip()\n",
        "FTP_PASS = _get_env(\"FTP_PASS\",\"\") or \"\"\n",
        "FTP_PORT = int(_get_env(\"FTP_PORT\",\"21\") or \"21\")\n",
        "FTP_DIR  = (_get_env(\"FTP_DIR\",\"\") or \"\").strip().strip(\"/\")\n",
        "PASSIVE_MODE = True\n",
        "\n",
        "def _mask(s, keep=3):\n",
        "    s = \"\" if s is None else str(s)\n",
        "    if not s:\n",
        "        return \"(empty)\"\n",
        "    return (s[:keep] + \"***\" + s[-keep:]) if len(s) > keep * 2 else s[0:1] + \"***\"\n",
        "\n",
        "print(\"[ENV] HOST=%s  USER=%s  PASS=%s  PORT=%d  DIR=%s\" %\n",
        "      (_mask(FTP_HOST), _mask(FTP_USER, 2), \"***\", FTP_PORT, (\"/\" + FTP_DIR) if FTP_DIR else \"(root)\"))\n",
        "\n",
        "# ---------- FTPS (Explicit AUTH TLS) ----------\n",
        "def _ftps_connect():\n",
        "    if not (FTP_HOST and FTP_USER and FTP_PASS):\n",
        "        raise RuntimeError(\"Missing FTP_HOST/FTP_USER/FTP_PASS.\")\n",
        "    socket.setdefaulttimeout(30)\n",
        "    ftps = FTP_TLS(timeout=30)\n",
        "    ftps.connect(FTP_HOST, FTP_PORT)\n",
        "    ftps.auth()                 # Explicit FTPS: AUTH TLS before login\n",
        "    ftps.login(FTP_USER, FTP_PASS)\n",
        "    try:\n",
        "        ftps.prot_p()           # Encrypt data channel\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(PASSIVE_MODE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for p in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(p)\n",
        "        except all_errors:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except all_errors:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "\n",
        "def _ftps_upload(ftps, local_path, remote_name):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR \" + remote_name, fh)\n",
        "    print(\"[OK] Uploaded: %s -> %s/%s\" % (local_path, ftps.pwd().rstrip(\"/\"), remote_name))\n",
        "\n",
        "# ---------- Outputs / Paths ----------\n",
        "REMOTE_DIR        = \"partials\"\n",
        "CSV_OUT_LOCAL     = \"final_combined_df_with_value_labels.csv\"\n",
        "HTML_OUT_LOCAL    = \"cell1_work_table.htm\"\n",
        "ABS_CSV_URL       = \"/%s/%s\" % (REMOTE_DIR, os.path.basename(CSV_OUT_LOCAL))\n",
        "ABS_HOME_URL      = \"/index.htm\"\n",
        "\n",
        "# vitals file used by Cell 2 / Cell 3\n",
        "VITALS_CSV_PATH        = \"dna_vitals.csv\"\n",
        "AUTOSOMAL_COUNT_TXT    = \"autosomal_count.txt\"  # legacy count for Cell 3 JS\n",
        "\n",
        "# ---------- Minimal GEDCOM parse helpers ----------\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get(\"NAME\", \"\") or \"\"\n",
        "        parts = name.split(\"/\", 1)\n",
        "        first_name = parts[0].split(\" \")[0] if parts else \"\"\n",
        "        last_name  = parts[1].rstrip(\"/\") if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip(\"@\")\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            cm = v.split(\"&\")[0].strip()\n",
        "        elif \"**\" in v:\n",
        "            cm = v.split(\"**\")[0].strip()\n",
        "        else:\n",
        "            cm = v.strip()\n",
        "        try:\n",
        "            int(cm)\n",
        "            return cm\n",
        "        except Exception:\n",
        "            return \"\"\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        if \"&\" in v:\n",
        "            s = v.split(\"&\")[1]\n",
        "            return (s.split(\"**\")[0] if \"**\" in s else s).strip()\n",
        "        return \"\"\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        v = self.extractable_detail.get(\"NPFX\", \"\") or \"\"\n",
        "        return v.split(\"**\")[1].strip() if \"**\" in v else \"\"\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return (self.extractable_detail.get(\"FAMC\", \"\") or \"\").strip(\"@\")\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "        # Counters / vitals\n",
        "        self.total_records = 0\n",
        "        self.npfx_count = 0\n",
        "        self.ydna_count = 0\n",
        "        self.autosomal_count = 0\n",
        "        self.after_manual_filter_total = 0\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(\" \", 2)\n",
        "            if not parts or not parts[0].isdigit():\n",
        "                continue\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith(\"@\") and tag.endswith(\"@\") and value == \"INDI\":\n",
        "                total += 1\n",
        "                current = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current)\n",
        "            elif current is not None:\n",
        "                if level == 1 and tag in [\"NAME\", \"FAMC\"]:\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == \"NPFX\":\n",
        "                    npfx_count += 1\n",
        "                    current.add_extractable_detail(tag, value)\n",
        "                    if value and \"**\" in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal = npfx_count - ydna_count\n",
        "\n",
        "        # Store vitals on the instance\n",
        "        self.total_records = total\n",
        "        self.npfx_count = npfx_count\n",
        "        self.ydna_count = ydna_count\n",
        "        self.autosomal_count = autosomal\n",
        "\n",
        "        print(\"GEDCOM contained %d total records\" % total)\n",
        "        print(\"Records tagged and filtered by NPFX: %d\" % npfx_count)\n",
        "        print(\"Records with YDNA information: %d\" % ydna_count)\n",
        "        print(\"Autosomal matches (NPFX minus YDNA): %d\" % autosomal)\n",
        "\n",
        "        # First-level filter: keep only records with NPFX\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        # Second-level manual filter via filtered_ids.xlsx (if present)\n",
        "        try:\n",
        "            df_filter = pd.read_excel(\"filtered_ids.xlsx\")\n",
        "            manual_ids = set(str(x) for x in df_filter[\"ID\"])\n",
        "            self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_ids]\n",
        "            print(\"After manual filter, total records: %d\" % len(self.filter_pool))\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "\n",
        "        # Final: record the post-filter count as a vital\n",
        "        self.after_manual_filter_total = len(self.filter_pool)\n",
        "\n",
        "        return autosomal\n",
        "\n",
        "def _chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "def _quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find(\"\\n\", start)\n",
        "    end = len(full_text) if end == -1 else end\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if \"/\" not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split(\"/\", 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "def _find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        _find_parents(father_id, generation + 1, parents_map)\n",
        "    if mother_id:\n",
        "        _find_parents(mother_id, generation + 1, parents_map)\n",
        "\n",
        "def _find_distant(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(_find_distant(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(_find_distant(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "def _filter_lineage(winning_ids, gen_table, names_map):\n",
        "    matching = []\n",
        "    for generation, pair in gen_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_ids or id2 in winning_ids:\n",
        "            matching.append((generation, pair))\n",
        "    matching.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for _, pair in matching:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(\"%s&%s\" % (name_pair[0], name_pair[1]))\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "def _process_record(individual_id, ged, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    _find_parents(individual_id, 1, parents_map)\n",
        "    paths = _find_distant(individual_id, parents_map)\n",
        "    best_score, best_path = None, None\n",
        "    for path in paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx + 1) for idx, nm in enumerate(name_path) if \"Yates\" in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score, best_path = score, path\n",
        "    best_path = best_path or []\n",
        "    best_ids  = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str  = _filter_lineage(set(best_ids), generation_table, names_map)\n",
        "    cm_value = \"\"\n",
        "    sort_value = \"\"\n",
        "    ydna_value = \"\"\n",
        "    for ds in ged.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value   = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "# ---------- Main build ----------\n",
        "def main():\n",
        "    files = glob.glob(\"*.ged\")\n",
        "    if not files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return False\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    gedcom_path = files[0]\n",
        "\n",
        "    # Parse GEDCOM, build datasets, and compute vitals\n",
        "    ged = Gedcom(gedcom_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "\n",
        "    # Legacy autosomal_count.txt (used by Cell 3 JS)\n",
        "    with open(AUTOSOMAL_COUNT_TXT, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "    print(\"[OK] Wrote %s = %d\" % (AUTOSOMAL_COUNT_TXT, autosomal_count))\n",
        "\n",
        "    # EST-based LAST_UPDATED_TEXT for dna_vitals and HTML\n",
        "    last_updated_text = _now_est_string()\n",
        "    print(\"[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: %s\" % last_updated_text)\n",
        "\n",
        "    npfx_count = int(ged.npfx_count)\n",
        "    after_manual_filter_total = int(ged.after_manual_filter_total or len(ged.filter_pool))\n",
        "\n",
        "    vitals_lines = [\n",
        "        \"Records tagged and filtered by NPFX: %d\" % npfx_count,\n",
        "        \"After manual filter, total records: %d\" % after_manual_filter_total,\n",
        "        \"LAST_UPDATED_TEXT: %s\" % last_updated_text,\n",
        "    ]\n",
        "    vitals_df = pd.DataFrame({\"line\": vitals_lines})\n",
        "    vitals_df.to_csv(\n",
        "        VITALS_CSV_PATH,\n",
        "        index=False,\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    )\n",
        "    print(\"[OK] Wrote dna_vitals.csv -> %s\" % os.path.abspath(VITALS_CSV_PATH))\n",
        "\n",
        "    # Re-read GEDCOM raw text for ancestor-building\n",
        "    with open(gedcom_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw = f.read()\n",
        "\n",
        "    blocks = raw.split(\"\\n0 \")\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find(\"\\n\")\n",
        "        flend = len(blk) if flend == -1 else flend\n",
        "        first_line = blk[:flend]\n",
        "        if \"@\" in first_line:\n",
        "            s = first_line.find(\"@\") + 1\n",
        "            e = first_line.find(\"@\", s)\n",
        "            rec_id = first_line[s:e].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map, names_map, families = {}, {}, {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if \"FAM\" in txt[:50]:\n",
        "            father_idx = txt.find(\"1 HUSB @\")\n",
        "            husb_id = txt[father_idx + len(\"1 HUSB @\"):txt.find(\"@\", father_idx + len(\"1 HUSB @\"))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find(\"1 WIFE @\")\n",
        "            wife_id = txt[wife_idx + len(\"1 WIFE @\"):txt.find(\"@\", wife_idx + len(\"1 WIFE @\"))] if wife_idx != -1 else None\n",
        "            kids = [ln.split(\"@\")[1] for ln in txt.split(\"\\n\") if ln.strip().startswith(\"1 CHIL @\")]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    for rec_id, txt in all_records.items():\n",
        "        names_map[rec_id] = _quick_extract_name(\"\\n\" + txt)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(\"Processing %d individuals with chunk-based parallel...\" % len(individual_ids))\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    from functools import partial as _partial\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as ex, tqdm(\n",
        "        total=len(individual_ids),\n",
        "        desc=\"Building Yates Lines (Stage 1)\"\n",
        "    ) as pbar:\n",
        "        for chunk in _chunks(individual_ids, chunk_size):\n",
        "            func = _partial(_process_record, ged=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(ex.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def _trim_prefix(row):\n",
        "        prefix = (\n",
        "            \"YatesJohn&SearchingStill~~~\"\n",
        "            \"YatesWilliam&SearchingStill~~~\"\n",
        "            \"YatesWilliam&SearchingStill~~~\"\n",
        "            \"YatesEdmund&CornellMargaret~~~\"\n",
        "            \"YatesRichard&AshendonJoan~~~\"\n",
        "            \"YatesJohn&HydeAlice~~~\"\n",
        "            \"YatesThomas&FauconerElizabeth~~~\"\n",
        "        )\n",
        "        s = str(row[\"Yates DNA Ancestral Line\"])\n",
        "        if s.startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = s[len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(_trim_prefix, axis=1)\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "\n",
        "    # CSV (ISO-8859-15 as required)\n",
        "    with open(CSV_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(df.to_csv(index=False))\n",
        "    logger.info(\"Exported CSV -> %s\", CSV_OUT_LOCAL)\n",
        "\n",
        "    # HTML (XHTML 1.0 Transitional; Times via external CSS is implied; inline minimal styles ok)\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Yates DNA Ancestral Line\"]\n",
        "    table_html = df.to_html(index=False, columns=final_cols, escape=False, border=1)\n",
        "\n",
        "    page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Cell 1 Working Table</title>\n",
        "<link rel=\"stylesheet\" type=\"text/css\" href=\"/partials/dna_tree_styles.css\" />\n",
        "<style type=\"text/css\">\n",
        "  html { scroll-behavior: smooth; }\n",
        "  body { background:#ffffff; color:#222222; margin:0; padding:20px; }\n",
        "  h1 { margin:0 0 8px 0; font-size:24px; line-height:1.2; text-align:center; }\n",
        "  .meta { text-align:center; font-size:12px; color:#555555; margin:2px 0 12px 0; }\n",
        "  .downloads { text-align:center; margin:4px 0 12px 0; font-size:13px; }\n",
        "  a { color:#154b8b; text-decoration:none; }\n",
        "  a:hover { text-decoration:underline; }\n",
        "  table { width:100%%; border-collapse:collapse; }\n",
        "  th, td { border:1px solid #333333; padding:6px 8px; vertical-align:top; }\n",
        "  th { background:#e3eaf8; text-align:left; }\n",
        "  td:nth-child(5) { text-align:left; white-space:normal; }\n",
        "</style>\n",
        "</head>\n",
        "<body>\n",
        "  <h1>Cell 1 Working Table</h1>\n",
        "  <div class=\"meta\">\n",
        "    <a href=\"$HOME\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "    &nbsp;|&nbsp; Last updated: $LAST_UPDATED_TEXT\n",
        "    &nbsp;|&nbsp; Download: <a href=\"$CSV\">$CSV</a>\n",
        "  </div>\n",
        "  <div class=\"downloads\"><a href=\"$CSV\">/partials/$CSV_NAME</a></div>\n",
        "  $TABLE\n",
        "</body>\n",
        "</html>\"\"\")\n",
        "\n",
        "    page = page_tpl.safe_substitute(\n",
        "        HOME=ABS_HOME_URL,\n",
        "        CSV=ABS_CSV_URL,\n",
        "        CSV_NAME=os.path.basename(ABS_CSV_URL),\n",
        "        TABLE=table_html,\n",
        "        LAST_UPDATED_TEXT=last_updated_text,\n",
        "    )\n",
        "\n",
        "    with open(HTML_OUT_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(page)\n",
        "    logger.info(\"Exported HTML -> %s\", HTML_OUT_LOCAL)\n",
        "\n",
        "    print(\"[SUMMARY] GEDCOM total records: %d\" % ged.total_records)\n",
        "    print(\"[SUMMARY] NPFX-tagged records: %d\" % ged.npfx_count)\n",
        "    print(\"[SUMMARY] Autosomal matches (NPFX minus YDNA): %d\" % ged.autosomal_count)\n",
        "    print(\"[SUMMARY] After manual filter, total records: %d\" % ged.after_manual_filter_total)\n",
        "\n",
        "    return True\n",
        "\n",
        "ok = main()\n",
        "\n",
        "# ---------- Upload to /partials (Explicit FTPS AUTH TLS) ----------\n",
        "if ok and FTP_HOST and FTP_USER and FTP_PASS:\n",
        "    print(\"[INFO] Uploading artifacts to /partials/ ...\")\n",
        "    try:\n",
        "        ftps = _ftps_connect()\n",
        "        _ftps_ensure_dir(ftps, \"partials\")\n",
        "        try:\n",
        "            _ftps_upload(ftps, CSV_OUT_LOCAL, os.path.basename(CSV_OUT_LOCAL))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] CSV upload failed:\", e)\n",
        "        try:\n",
        "            _ftps_upload(ftps, HTML_OUT_LOCAL, os.path.basename(HTML_OUT_LOCAL))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] HTML upload failed:\", e)\n",
        "\n",
        "        # Also upload dna_vitals.csv and autosomal_count.txt so they are in the same authority path\n",
        "        try:\n",
        "            _ftps_upload(ftps, VITALS_CSV_PATH, os.path.basename(VITALS_CSV_PATH))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] dna_vitals.csv upload failed:\", e)\n",
        "        try:\n",
        "            _ftps_upload(ftps, AUTOSOMAL_COUNT_TXT, os.path.basename(AUTOSOMAL_COUNT_TXT))\n",
        "        except Exception as e:\n",
        "            print(\"[ERROR] autosomal_count.txt upload failed:\", e)\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[OK] Uploads complete to /partials/\")\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing creds or build failed).\")\n",
        "\n",
        "print(\"\\n--- Cell 1 Complete: CSV + HTML + dna_vitals.csv + autosomal_count.txt built with ISO-8859-15; explicit FTPS used. ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 1 - GEDCOM -> CSV + HTML + Upload (Explicit FTPS, ISO-8859-15) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGfU-fkHHJ3d",
        "outputId": "f97125d3-1341-4649-e95e-fbdc35e8d1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell1_FTPS_Explicit | Version=2025.11.27-G2 | Encoding=ISO-8859-15\n",
            "[ENV] HOST=ftp***net  USER=ad***et  PASS=***  PORT=21  DIR=(root)\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 62567 total records\n",
            "Records tagged and filtered by NPFX: 1605\n",
            "Records with YDNA information: 0\n",
            "Autosomal matches (NPFX minus YDNA): 1605\n",
            "After manual filter, total records: 93\n",
            "[OK] Wrote autosomal_count.txt = 1605\n",
            "[OK] LAST_UPDATED_TEXT (EST) for dna_vitals.csv: December 6, 2025 11:30 AM\n",
            "[OK] Wrote dna_vitals.csv -> /content/dna_vitals.csv\n",
            "Processing 93 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 93/93 [00:56<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUMMARY] GEDCOM total records: 62567\n",
            "[SUMMARY] NPFX-tagged records: 1605\n",
            "[SUMMARY] Autosomal matches (NPFX minus YDNA): 1605\n",
            "[SUMMARY] After manual filter, total records: 93\n",
            "[INFO] Uploading artifacts to /partials/ ...\n",
            "[OK] Uploaded: final_combined_df_with_value_labels.csv -> /partials/final_combined_df_with_value_labels.csv\n",
            "[OK] Uploaded: cell1_work_table.htm -> /partials/cell1_work_table.htm\n",
            "[OK] Uploaded: dna_vitals.csv -> /partials/dna_vitals.csv\n",
            "[OK] Uploaded: autosomal_count.txt -> /partials/autosomal_count.txt\n",
            "[OK] Uploads complete to /partials/\n",
            "\n",
            "--- Cell 1 Complete: CSV + HTML + dna_vitals.csv + autosomal_count.txt built with ISO-8859-15; explicit FTPS used. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "TKAmqiIIDaxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2 - Build + Publish DNA Register (All styling via stylesheet) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.01-G1)\n",
        "# - Complete and runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout via /partials/dna_tree_styles.css (this cell writes it).\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os, re, posixpath, socket, traceback, urllib.parse as _u\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# ---------- A) LAYOUT CONTROL BLOCK (ONE PLACE TO TUNE WIDTHS) ----------\n",
        "# Col 1 = Match to\n",
        "# Col 2 = Name\n",
        "# Col 3 = cM\n",
        "# Col 4 = Match Summary\n",
        "# Col 5 = Website\n",
        "# Col 6 = Yates DNA Ancestral Lines\n",
        "\n",
        "COL_1_PX = 80\n",
        "COL_2_PX = 220\n",
        "COL_3_PX = 60\n",
        "COL_4_PX = 1200\n",
        "COL_5_PX = 120\n",
        "COL_6_PX = 1800\n",
        "\n",
        "COL_WIDTHS = [COL_1_PX, COL_2_PX, COL_3_PX, COL_4_PX, COL_5_PX, COL_6_PX]\n",
        "TABLE_TOTAL_WIDTH_PX = sum(COL_WIDTHS)\n",
        "\n",
        "# Hide these column indices (zero-based) in the rendered table: 2,3,5\n",
        "HIDE_COLS_ZERO_BASED = {1, 2, 4}\n",
        "\n",
        "print(\"[LAYOUT] TABLE_TOTAL_WIDTH_PX=%d\" % TABLE_TOTAL_WIDTH_PX)\n",
        "print(\"[LAYOUT] Column widths (px): 1=%d 2=%d 3=%d 4=%d 5=%d 6=%d\" %\n",
        "      (COL_1_PX, COL_2_PX, COL_3_PX, COL_4_PX, COL_5_PX, COL_6_PX))\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# NOTE: main register pages now .shtml (SSI pages only)\n",
        "LOCAL_HTML        = \"yates_ancestor_register.shtml\"\n",
        "REMOTE_HTML_CANON = posixpath.join(\"partials\", \"yates_ancestor_register.shtml\")\n",
        "REMOTE_HTML_LEG   = posixpath.join(\"partials\", \"ons_yates_dna_register.shtml\")\n",
        "\n",
        "DNA_REGISTER_ABS = \"https://yates.one-name.net/partials/ons_yates_dna_register.shtml\"\n",
        "TREES_ABS        = \"https://yates.one-name.net/partials/just-trees.htm\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV  = \"%s.csv\"  % EXPORT_BASENAME\n",
        "LOCAL_XLSX = \"%s.xlsx\" % EXPORT_BASENAME\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", os.path.basename(LOCAL_CSV))\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", os.path.basename(LOCAL_XLSX))\n",
        "\n",
        "FTP_DIR  = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "HOME_URL        = \"https://yates.one-name.net/partials/yates_ancestor_register.shtml\"\n",
        "REMOTE_NAME_ABS = HOME_URL\n",
        "\n",
        "ARROW_ENTITY         = \"&rarr;\"\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "\n",
        "SERVER_PARTIALS_DIR        = \"partials\"\n",
        "SERVER_MAPPING_BASENAME    = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE      = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# Stylesheet\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "STYLESHEET_LOCAL    = os.path.join(\"partials\", STYLESHEET_BASENAME)\n",
        "STYLESHEET_REMOTE   = posixpath.join(\"partials\", STYLESHEET_BASENAME)\n",
        "CSS_VERSION         = \"v2025-12-01-g1\"\n",
        "STYLESHEET_HREF     = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK           = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "# Path for vitals from Cell 1\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "\n",
        "# ---------- 2) FTP ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    for seg in [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"]     = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str):\n",
        "        return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Name/text utils ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\", \"del\", \"della\", \"der\", \"van\", \"von\", \"da\", \"dos\", \"das\", \"di\", \"la\", \"le\", \"du\", \"of\"\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"  + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        if i > 0 and w.lower() in _PARTICLES:\n",
        "            out.append(w.lower())\n",
        "        else:\n",
        "            out.append(_smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname      = token[:idx]\n",
        "    given        = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm    = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname          = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        if \" \" in s:\n",
        "            return smart_titlecase(s)\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END:\n",
        "        s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "# ---------- 5) Read CSV ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols   = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "_encs     = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "_last_err = None\n",
        "df        = None\n",
        "for _e in _encs:\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_IN, encoding=_e, dtype=str, keep_default_na=False)\n",
        "        break\n",
        "    except Exception as _ex:\n",
        "        _last_err = _ex\n",
        "        df        = None\n",
        "if df is None:\n",
        "    raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, _last_err))\n",
        "print(\"[OK] Loaded CSV: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "id_col    = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "name_col  = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "cm_col    = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "path_col  = find_col(\n",
        "    df,\n",
        "    [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "    [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        ")\n",
        "\n",
        "if not id_col:\n",
        "    raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "if not match_col:\n",
        "    raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "if not name_col:\n",
        "    raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col:\n",
        "    raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not path_col:\n",
        "    raise ValueError(\"CSV missing lineage/path column.\")\n",
        "\n",
        "# ---------- 5.1) Read vitals from dna_vitals.csv ----------\n",
        "AUTOSOMAL_MATCHES = \"\"\n",
        "SHOWING_STATIC    = \"\"\n",
        "LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a UTC timestamp like\n",
        "      'YYYY-MM-DD HH:MM UTC'\n",
        "      'YYYY-MM-DD HH:MM:SS UTC'\n",
        "      'YYYY-MM-DDTHH:MM UTC'\n",
        "      'YYYY-MM-DDTHH:MM:SS UTC'\n",
        "    to 'Month D, YYYY H:MM AM/PM' in approx. US Eastern (UTC-5).\n",
        "    If parsing fails, return the original string.\n",
        "    \"\"\"\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _format_int_with_commas(s):\n",
        "    \"\"\"\n",
        "    Take a numeric-like string, strip non-digits, and return with thousands\n",
        "    separators (e.g., '1604' -> '1,604'). On failure, return the input.\n",
        "    \"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    t = re.sub(r\"[^0-9\\-]\", \"\", str(s))\n",
        "    if not t:\n",
        "        return str(s)\n",
        "    try:\n",
        "        n = int(t)\n",
        "        return \"{:,}\".format(n)\n",
        "    except Exception:\n",
        "        return str(s)\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global AUTOSOMAL_MATCHES, SHOWING_STATIC, LAST_UPDATED_TEXT\n",
        "    AUTOSOMAL_MATCHES = \"\"\n",
        "    SHOWING_STATIC    = \"\"\n",
        "    LAST_UPDATED_TEXT = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; header will omit counts and last-updated text.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        vdf = pd.read_csv(path, dtype=str, encoding=\"iso-8859-15\", keep_default_na=False)\n",
        "    except Exception:\n",
        "        encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "        last = None\n",
        "        vdf  = None\n",
        "        for enc in encs:\n",
        "            try:\n",
        "                vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "        if vdf is None:\n",
        "            print(\"[WARN] Unable to read dna_vitals.csv: %s\" % last)\n",
        "            return\n",
        "\n",
        "    flat = []\n",
        "    for row in vdf.astype(str).values.tolist():\n",
        "        for cell in row:\n",
        "            flat.append(str(cell))\n",
        "\n",
        "    autosomal = None\n",
        "    showing   = None\n",
        "    last_text = None\n",
        "\n",
        "    for cell in flat:\n",
        "        s = str(cell)\n",
        "        if \"Records tagged and filtered by NPFX\" in s and autosomal is None:\n",
        "            m = re.search(r\"(\\d+)\", s)\n",
        "            if m:\n",
        "                autosomal = m.group(1)\n",
        "        if \"After manual filter, total records\" in s and showing is None:\n",
        "            m = re.search(r\"(\\d+)\", s)\n",
        "            if m:\n",
        "                showing = m.group(1)\n",
        "        if \"LAST_UPDATED_TEXT\" in s and last_text is None:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", s)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "\n",
        "    if autosomal is None or showing is None or last_text is None:\n",
        "        all_text = \" \".join(flat)\n",
        "        nums = re.findall(r\"\\d+\", all_text)\n",
        "        if autosomal is None and len(nums) >= 1:\n",
        "            autosomal = nums[0]\n",
        "        if showing is None and len(nums) >= 2:\n",
        "            showing = nums[1]\n",
        "\n",
        "    if last_text is not None:\n",
        "        last_text = _friendly_ts_from_utc(last_text)\n",
        "\n",
        "    AUTOSOMAL_MATCHES = _format_int_with_commas(autosomal) if autosomal else \"\"\n",
        "    SHOWING_STATIC    = _format_int_with_commas(showing)   if showing   else \"\"\n",
        "    LAST_UPDATED_TEXT = last_text or \"\"\n",
        "\n",
        "    print(\"[OK] Loaded vitals from %s -> autosomal=%s, showing=%s, last_updated_text=%s\"\n",
        "          % (path,\n",
        "             AUTOSOMAL_MATCHES or \"?\",\n",
        "             SHOWING_STATIC or \"?\",\n",
        "             LAST_UPDATED_TEXT or \"(blank)\"))\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "if SHOWING_STATIC:\n",
        "    try:\n",
        "        show_raw = re.sub(r\"[^0-9\\-]\", \"\", SHOWING_STATIC)\n",
        "        csv_rows = len(df)\n",
        "        if show_raw:\n",
        "            if int(show_raw) != csv_rows:\n",
        "                print(\"[WARN] dna_vitals showing (%s) != CSV rows (%d)\" %\n",
        "                      (show_raw, csv_rows))\n",
        "    except Exception as _e:\n",
        "        print(\"[WARN] Unable to compare showing from dna_vitals.csv to CSV rows: %s\" % _e)\n",
        "\n",
        "# ---------- 6) Transform ----------\n",
        "_setup_resolver()\n",
        "\n",
        "headers          = []\n",
        "lineages         = []\n",
        "subjects         = []\n",
        "first_ancestors  = []\n",
        "display_match_to = []\n",
        "display_name     = []\n",
        "\n",
        "LINEAGE_HEADER_SAFE = \"Yates DNA Ancestral Lines\"\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw    = row.get(match_col, \"\")\n",
        "    subject_name   = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_b = \"<strong>%s</strong>\" % subject_name if subject_name else subject_name\n",
        "\n",
        "    pid          = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_raw  = row.get(name_col, \"\")\n",
        "    matchee_name = norm_matchee_name(matchee_raw) or subject_name\n",
        "\n",
        "    if pid:\n",
        "        matchee_url = (\n",
        "            \"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\"\n",
        "            % (TNG_BASE, pid, TNG_TREE)\n",
        "        )\n",
        "        matchee_name_html = '<a href=\"%s\" target=\"_blank\" rel=\"noopener\">%s</a>' % (matchee_url, matchee_name)\n",
        "    else:\n",
        "        matchee_name_html = matchee_name\n",
        "\n",
        "    cm_val      = row.get(cm_col, \"0\")\n",
        "    tokens      = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total  = len(tokens)\n",
        "    tokens_disp = tokens[:7]\n",
        "\n",
        "    if \"common_husband\" in df.columns and \"common_wife\" in df.columns:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw    = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(subject_name_b or subject_name, cm_val, matchee_name_html, gens_total, husband_raw, wife_raw)\n",
        "    if tokens_disp:\n",
        "        tokens_disp[0] = \"<strong>%s</strong>\" % tokens_disp[0]\n",
        "    sep          = \" %s \" % ARROW_ENTITY\n",
        "    lineage_text = sep.join(tokens_disp) if tokens_disp else \"\"\n",
        "\n",
        "    headers.append(header_html)\n",
        "    lineages.append(lineage_text)\n",
        "    subjects.append(subject_name)\n",
        "    first_ancestors.append(tokens[0] if tokens else \"\")\n",
        "\n",
        "    display_match_to.append(subject_name)\n",
        "    display_name.append(matchee_name_html)\n",
        "\n",
        "df[\"Match Summary\"]      = headers\n",
        "df[LINEAGE_HEADER_SAFE]  = lineages\n",
        "df[\"Subject\"]            = subjects\n",
        "df[\"First Ancestor\"]     = [_clean_piece(x) for x in first_ancestors]\n",
        "\n",
        "# ---------- 6.1) Clean exports ----------\n",
        "TAG_RE = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "def _html_to_text(s: str) -> str:\n",
        "    t = TAG_RE.sub(\"\", str(s or \"\"))\n",
        "    t = _html.unescape(t)\n",
        "    t = t.replace(\"\\u2192\", \"->\")\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "def _extract_find_url(subject_name: str) -> str:\n",
        "    if not subject_name:\n",
        "        return \"\"\n",
        "    q = _u.quote(subject_name)\n",
        "    return \"%s?q=%s\" % (REMOTE_NAME_ABS, q)\n",
        "\n",
        "website_urls = [_extract_find_url(subj) for subj in df[\"Subject\"].tolist()]\n",
        "\n",
        "export_df = pd.DataFrame({\n",
        "    \"Match to\"      : df[match_col].tolist(),\n",
        "    \"Name\"          : df[name_col].tolist(),\n",
        "    \"cM\"            : df[cm_col].tolist(),\n",
        "    \"Match Summary\" : [_html_to_text(v) for v in df[\"Match Summary\"].tolist()],\n",
        "    \"Website URL\"   : website_urls,\n",
        "    \"Lineage\"       : [_html_to_text(v) for v in df[LINEAGE_HEADER_SAFE].tolist()],\n",
        "})\n",
        "\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    with pd.ExcelWriter(LOCAL_XLSX) as _writer:\n",
        "        export_df.to_excel(_writer, index=False)\n",
        "print(\"[OK] Wrote exports: %s and %s\" % (os.path.abspath(LOCAL_CSV), os.path.abspath(LOCAL_XLSX)))\n",
        "\n",
        "# ---------- 7) Stylesheet content (includes print fix for scroll containers) ----------\n",
        "CSS_TEXT = \"\"\"/* yates.one-name.net - DNA pages (unified stylesheet)\n",
        "   Version: %s\n",
        "   Note: Typography, layout, colors, borders - centralized here. */\n",
        "\n",
        ":root {\n",
        "  --table-width-px: %dpx;\n",
        "  --brand-blue: #5b79b8;\n",
        "  --brand-blue-dark: #4668aa;\n",
        "  --line: #dddddd;\n",
        "  --line-strong: #999999;\n",
        "}\n",
        "\n",
        "html, body {\n",
        "  margin:0; padding:0;\n",
        "  font-family: \"Times New Roman\", Times, serif;\n",
        "  font-size: 16px; line-height: 1.35;\n",
        "  color:#111111; background:#ffffff;\n",
        "}\n",
        "\n",
        ".wrap {\n",
        "  max-width:100%%;\n",
        "  margin:0 auto;\n",
        "  background:#ffffff;\n",
        "  padding:16px;\n",
        "  padding-bottom:48px;\n",
        "}\n",
        ".centerline { text-align:center; }\n",
        "\n",
        ".downloads { text-align:center; margin:4px 0 10px 0; font-size: 13px; }\n",
        ".updated   { font-size: 12px; color:#555555; text-align:center; margin:2px 0 10px 0; }\n",
        "\n",
        ".left-align { text-align:left; }\n",
        "\n",
        "/* Simple header alignment helpers */\n",
        "th.center-header { text-align:center; }\n",
        "th.left-header   { text-align:left; }\n",
        "\n",
        "/* Wrapper for scroll container */\n",
        ".table-scroll-wrapper {\n",
        "  width:100%%;\n",
        "  max-width:100%%;\n",
        "  margin:0 auto;\n",
        "}\n",
        "\n",
        "/* Single scroll container (optimized: no fake top scrollbar) */\n",
        ".table-scroll {\n",
        "  width:100%%;\n",
        "  max-width:100%%;\n",
        "  max-height:80vh;\n",
        "  overflow-x:auto;\n",
        "  overflow-y:auto;\n",
        "  border:1px solid var(--line);\n",
        "  position:relative;\n",
        "  -webkit-overflow-scrolling:touch;\n",
        "  scrollbar-width:auto;\n",
        "  -ms-overflow-style:auto;\n",
        "}\n",
        "\n",
        "/* Table: let widths and content drive horizontal size */\n",
        "table.sortable {\n",
        "  border-collapse:separate;\n",
        "  border-spacing:0;\n",
        "}\n",
        "\n",
        "table.sortable th,\n",
        "table.sortable td {\n",
        "  border:1px solid var(--line);\n",
        "  padding:6px 8px;\n",
        "  vertical-align:top;\n",
        "  white-space:nowrap;\n",
        "}\n",
        "\n",
        "/* Sticky header row */\n",
        "table.sortable th {\n",
        "  background:#e3eaf8;\n",
        "  position:-webkit-sticky;\n",
        "  position:sticky;\n",
        "  top:0;\n",
        "  z-index:5;\n",
        "  box-shadow:0 1px 0 #cccccc;\n",
        "  cursor:pointer;\n",
        "}\n",
        "\n",
        "/* Sticky first column (Match to) for DNA Register table */\n",
        "table.dna-register-table th:nth-child(1),\n",
        "table.dna-register-table td:nth-child(1) {\n",
        "  position:-webkit-sticky;\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "table.dna-register-table th:nth-child(1) {\n",
        "  z-index:7;\n",
        "}\n",
        "\n",
        "/* First data row marker */\n",
        "#first-row td { border-top:2px solid var(--line-strong); }\n",
        "\n",
        "/* Back-to-top button */\n",
        ".back-to-top {\n",
        "  position:fixed; right:16px; bottom:16px; padding:6px 10px;\n",
        "  border:1px solid #3e5a97; background:var(--brand-blue);\n",
        "  color:#ffffff; cursor:pointer; border-radius:6px; display:none; z-index:9999;\n",
        "}\n",
        ".back-to-top:hover { background:var(--brand-blue-dark); }\n",
        "\n",
        "/* Controls */\n",
        ".controls { text-align:center; }\n",
        ".controls-spaced { margin:6px 0 10px 0; }\n",
        ".search { font-size: 14px; padding:5px 8px; }\n",
        "\n",
        "/* Old-school blue nav menu */\n",
        ".oldnav {\n",
        "  margin:8px auto 6px auto; padding:0; background:var(--brand-blue);\n",
        "  border-radius:6px; overflow:hidden; max-width: var(--table-width-px);\n",
        "}\n",
        ".oldnav ul { list-style:none; margin:0; padding:0; display:flex; flex-wrap:wrap; }\n",
        ".oldnav li { margin:0; padding:0; }\n",
        ".oldnav a, .oldnav a:link, .oldnav a:visited, .oldnav a:active { color:#ffffff !important; }\n",
        ".oldnav a {\n",
        "  display:block;\n",
        "  padding:8px 12px;\n",
        "  text-decoration:none;\n",
        "  white-space:nowrap;\n",
        "  border-right:1px solid #ffffff;\n",
        "  font-weight:600;\n",
        "}\n",
        ".oldnav li:last-child a { border-right:none; }\n",
        ".oldnav a:hover { background:var(--brand-blue-dark); color:#ffffff !important; }\n",
        "\n",
        "/* Responsive tweaks */\n",
        "@media screen and (min-width: 1200px) {\n",
        "  .wrap { max-width: var(--table-width-px); }\n",
        "}\n",
        "@media screen and (max-width: 1199px) {\n",
        "  .oldnav { border-radius:0; }\n",
        "}\n",
        "@media screen and (max-width: 700px) {\n",
        "  table.sortable th, table.sortable td { padding:5px 6px; }\n",
        "}\n",
        "\n",
        "/* Print: avoid scroll clipping for long tables (e.g., Cousin List, DNA register) */\n",
        "@media print {\n",
        "  .table-scroll-wrapper,\n",
        "  .table-scroll {\n",
        "    max-height:none !important;\n",
        "    overflow:visible !important;\n",
        "    border:none !important;\n",
        "  }\n",
        "  html, body {\n",
        "    overflow:visible !important;\n",
        "  }\n",
        "}\n",
        "\"\"\" % (\n",
        "    CSS_VERSION,\n",
        "    TABLE_TOTAL_WIDTH_PX,\n",
        ")\n",
        "\n",
        "os.makedirs(\"partials\", exist_ok=True)\n",
        "with open(STYLESHEET_LOCAL, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as _css:\n",
        "    _css.write(CSS_TEXT)\n",
        "print(\"[OK] Wrote stylesheet: %s\" % os.path.abspath(STYLESHEET_LOCAL))\n",
        "\n",
        "# ---------- 8) Main HTML ----------\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$HEAD_LINK\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<!-- Cell2 build: Version=2025.12.01-G1 | HIDE_COLS=2,3,5 | CousinListFullPrint=on | Scroll=single-bottom -->\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '')\n",
        "      .replace(/\\\\s+/g,' ')\n",
        "      .trim()\n",
        "      .toLowerCase();\n",
        "  }\n",
        "\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[colIndex]),\n",
        "          B = textOf(b.cells[colIndex]);\n",
        "\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "\n",
        "      if(!isNaN(nA) && !isNaN(nB)){\n",
        "        return asc ? (nA - nB) : (nB - nA);\n",
        "      }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      frag.appendChild(rows[i]);\n",
        "    }\n",
        "    tb.appendChild(frag);\n",
        "  }\n",
        "\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "\n",
        "    for(var i=0;i<ths.length;i++){\n",
        "      (function(idx){\n",
        "        var th  = ths[idx];\n",
        "        var dir = 'asc';\n",
        "        th.addEventListener('click', function(){\n",
        "          dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "\n",
        "          for (var j = 0; j < ths.length; j++){\n",
        "            ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "          }\n",
        "          th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "          sortTable(tbl, idx, dir);\n",
        "        }, false);\n",
        "      })(i);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "\n",
        "    var tb   = tbl.tBodies[0];\n",
        "    var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt  = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "    }\n",
        "\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "\n",
        "    box.addEventListener('input',  onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "\n",
        "    window.addEventListener('scroll', function(){\n",
        "      btn.style.display = (window.pageYOffset > 200 ? 'block' : 'none');\n",
        "    }, false);\n",
        "\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  function bindScrollSync(){\n",
        "    // No-op in the optimized version (no separate top-scroll container).\n",
        "    return;\n",
        "  }\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindSearch();\n",
        "    bindBackToTop();\n",
        "    bindScrollSync();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "# ---------- 8.1) Build HTML table manually with inline widths ----------\n",
        "website_links = []\n",
        "for subj in df[\"Subject\"].tolist():\n",
        "    url = _extract_find_url(subj)\n",
        "    if url:\n",
        "        website_links.append('<a href=\"%s\" target=\"_blank\" rel=\"noopener\">Website</a>' %\n",
        "                             _html.escape(url, quote=True))\n",
        "    else:\n",
        "        website_links.append(\"\")\n",
        "\n",
        "col_headers = [\n",
        "    (\"Match to\", \"center\"),\n",
        "    (\"Name\", \"center\"),\n",
        "    (\"cM\", \"center\"),\n",
        "    (\"Match Summary\", \"center\"),\n",
        "    (\"Website\", \"center\"),\n",
        "    (\"Yates DNA Ancestral Lines\", \"left\"),\n",
        "]\n",
        "\n",
        "col_data = [\n",
        "    display_match_to,\n",
        "    display_name,\n",
        "    df[cm_col].tolist(),\n",
        "    df[\"Match Summary\"].tolist(),\n",
        "    website_links,\n",
        "    df[LINEAGE_HEADER_SAFE].tolist(),\n",
        "]\n",
        "\n",
        "thead_cells = []\n",
        "for (idx, (hdr, align)) in enumerate(col_headers):\n",
        "    wpx = COL_WIDTHS[idx]\n",
        "    style_bits = [\"width:%dpx\" % wpx]\n",
        "    if idx in HIDE_COLS_ZERO_BASED:\n",
        "        style_bits.append(\"display:none\")\n",
        "    style_attr = \"; \".join(style_bits)\n",
        "    if align == \"center\":\n",
        "        cell_html = '<th class=\"center-header\" style=\"%s;\">%s</th>' % (style_attr, hdr)\n",
        "    else:\n",
        "        cell_html = '<th class=\"left-header\" style=\"%s;\">%s</th>' % (style_attr, hdr)\n",
        "    thead_cells.append(cell_html)\n",
        "thead_html = \"<thead>\\n  <tr>\" + \"\".join(thead_cells) + \"</tr>\\n</thead>\"\n",
        "\n",
        "nrows = len(df)\n",
        "tbody_lines = [\"<tbody>\"]\n",
        "for r in range(nrows):\n",
        "    tr_open = '  <tr id=\"first-row\">' if r == 0 else '  <tr>'\n",
        "    cells = []\n",
        "    for c in range(len(col_headers)):\n",
        "        wpx = COL_WIDTHS[c]\n",
        "        val = col_data[c][r]\n",
        "        val_str = \"\" if val is None else str(val)\n",
        "        style_bits = [\"width:%dpx\" % wpx]\n",
        "        if c in HIDE_COLS_ZERO_BASED:\n",
        "            style_bits.append(\"display:none\")\n",
        "        style_attr = \"; \".join(style_bits)\n",
        "        cells.append('<td style=\"%s;\">%s</td>' % (style_attr, val_str))\n",
        "    tbody_lines.append(tr_open + \"\".join(cells) + \"</tr>\")\n",
        "tbody_lines.append(\"</tbody>\")\n",
        "tbody_html = \"\\n\".join(tbody_lines)\n",
        "\n",
        "html_table = (\n",
        "    '<table border=\"1\" class=\"dataframe sortable dna-register-table\" id=\"refactor-table\">'\n",
        "    + thead_html +\n",
        "    \"\\n\" +\n",
        "    tbody_html +\n",
        "    \"</table>\"\n",
        ")\n",
        "\n",
        "# Optimized: single bottom scroll container (no fake top scroll shim)\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div class=\"table-scroll\" id=\"bottom-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (html_table,)\n",
        "\n",
        "# ---------- 8.2) Page assembly ----------\n",
        "if LAST_UPDATED_TEXT:\n",
        "    updated_label = 'Last updated: <span id=\"last-updated\">%s</span>' % _html.escape(LAST_UPDATED_TEXT)\n",
        "else:\n",
        "    updated_label = 'Last updated: <span id=\"last-updated\"></span>'\n",
        "\n",
        "_updated_parts = [updated_label]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "if SHOWING_STATIC:\n",
        "    _updated_parts.append('Showing: %s' % _html.escape(SHOWING_STATIC))\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">' +\n",
        "    ' &nbsp;|&nbsp; '.join(_updated_parts) +\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "# No \"Download:\" paragraph in the body; downloads live in nav\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "  '<div class=\"controls controls-spaced centerline\">'\n",
        "  '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" placeholder=\"Search&amp;hellip;\" />'\n",
        "  '</div>'\n",
        ")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK       = HEAD_LINK,\n",
        "    UPDATED_BLOCK   = UPDATED_BLOCK,\n",
        "    NAV_BLOCK       = NAV_BLOCK,\n",
        "    CONTROLS_BLOCK  = CONTROLS_BLOCK,\n",
        "    DOWNLOADS_BLOCK = DOWNLOADS_BLOCK,\n",
        "    SCROLL_WRAPPER  = SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"[OK] Saved canonical render: %s\" % os.path.abspath(LOCAL_HTML))\n",
        "\n",
        "# ---------- 9) Upload ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, STYLESHEET_LOCAL, _remote_path(STYLESHEET_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload stylesheet failed: %s\" % e)\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_CANON))\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, _remote_path(REMOTE_HTML_LEG))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload main HTML failed: %s\" % e)\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(LOCAL_CSV):\n",
        "                ftp_upload_overwrite(ftps, LOCAL_CSV, _remote_path(REMOTE_CSV))\n",
        "            if os.path.exists(LOCAL_XLSX):\n",
        "                ftp_upload_overwrite(ftps, LOCAL_XLSX, _remote_path(REMOTE_XLSX))\n",
        "            print(\"[OK] Uploaded CSV/XLSX -> /partials/\")\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload CSV/XLSX failed: %s\" % e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [\n",
        "            _remote_path(REMOTE_HTML_CANON),\n",
        "            _remote_path(REMOTE_HTML_LEG),\n",
        "            _remote_path(REMOTE_CSV),\n",
        "            _remote_path(REMOTE_XLSX),\n",
        "            _remote_path(STYLESHEET_REMOTE),\n",
        "        ]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.shtml\")\n",
        "        print(\"Legacy (ons_):    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.shtml\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "        print(\"Trees (Cell 3):   https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "        print(\"Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\")\n",
        "        print(\"\\nBust cache once if needed by appending ?v=%s to the URL.\" % CSS_VERSION)\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session: %s\" % e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "# ====== CUT STOP [1/1] CELL 2 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN77LDV7bAjE",
        "outputId": "02d1457e-798c-4005-8084-1977a81c4d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2_AllStyles_ExternalCSS | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
            "[LAYOUT] TABLE_TOTAL_WIDTH_PX=3480\n",
            "[LAYOUT] Column widths (px): 1=80 2=220 3=60 4=1200 5=120 6=1800\n",
            "[OK] Loaded CSV: 93 rows, 6 cols\n",
            "[OK] Loaded vitals from dna_vitals.csv -> autosomal=1,605, showing=93, last_updated_text=December 6, 2025 11:30 AM\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Wrote stylesheet: /content/partials/dna_tree_styles.css\n",
            "[OK] Saved canonical render: /content/yates_ancestor_register.shtml\n",
            "[PUT] partials/dna_tree_styles.css -> partials/dna_tree_styles.css\n",
            "[PUT] yates_ancestor_register.shtml -> partials/yates_ancestor_register.shtml\n",
            "[PUT] yates_ancestor_register.shtml -> partials/ons_yates_dna_register.shtml\n",
            "[PUT] yates_ancestor_register.csv -> partials/yates_ancestor_register.csv\n",
            "[PUT] yates_ancestor_register.xlsx -> partials/yates_ancestor_register.xlsx\n",
            "[OK] Uploaded CSV/XLSX -> /partials/\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/yates_ancestor_register.shtml : 107518\n",
            "partials/ons_yates_dna_register.shtml : 107518\n",
            "partials/yates_ancestor_register.csv : 40278\n",
            "partials/yates_ancestor_register.xlsx : 17969\n",
            "partials/dna_tree_styles.css : 3855\n",
            "\n",
            "--- Open URLs ---\n",
            "Canonical:        https://yates.one-name.net/partials/yates_ancestor_register.shtml\n",
            "Legacy (ons_):    https://yates.one-name.net/partials/ons_yates_dna_register.shtml\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n",
            "Trees (Cell 3):   https://yates.one-name.net/partials/just-trees.shtml\n",
            "Stylesheet:       https://yates.one-name.net/partials/dna_tree_styles.css\n",
            "\n",
            "Bust cache once if needed by appending ?v=v2025-12-01-g1 to the URL.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2b\n"
      ],
      "metadata": {
        "id": "-eiXmG11LSv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2b — Build + Upload Match/Lineage Count Partials (Counts only) ======\n",
        "# RON GOLDEN RULES -- CLIFF NOTES (v2025.12.06-G2)\n",
        "# • Complete & runnable Colab cell — one contiguous block.\n",
        "# • Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# • XHTML 1.0 Transitional; ALL typography/layout via /partials/dna_tree_styles.css (linked only).\n",
        "# • Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.12.06-G2 | Encoding=ISO-8859-15\n",
        "# • Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.12.06-G2 | Encoding=ISO-8859-15\")\n",
        "\n",
        "DOWNLOADS_BLOCK = \"\"  # moved into nav_block.shtml\n",
        "\n",
        "import os, re, posixpath, socket, traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "# Shared stylesheet link (must already be present on server from Cell 2)\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "# Shared nav include (SSI)\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "# vitals authority (built by Cell 1)\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "# TNG settings for cousin links (match vertical chart behavior in main register)\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "# Local partial paths\n",
        "MATCH_COUNT_LOCAL   = os.path.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_LOCAL = os.path.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_LOCAL  = os.path.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# Remote partial paths (server-side)\n",
        "MATCH_COUNT_REMOTE   = posixpath.join(\"partials\", \"match_count.shtml\")\n",
        "LINEAGE_COUNT_REMOTE = posixpath.join(\"partials\", \"lineage_count.shtml\")\n",
        "COUSIN_PRINT_REMOTE  = posixpath.join(\"partials\", \"cousin_list_print.htm\")\n",
        "\n",
        "# ---------- 1a) Load vitals authority from dna_vitals.csv ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert a UTC timestamp like\n",
        "      'YYYY-MM-DD HH:MM UTC'\n",
        "      'YYYY-MM-DD HH:MM:SS UTC'\n",
        "      'YYYY-MM-DDTHH:MM UTC'\n",
        "      'YYYY-MM-DDTHH:MM:SS UTC'\n",
        "    to 'Month D, YYYY H:MM AM/PM' in approx. US Eastern (UTC-5).\n",
        "    If parsing fails, return the original string.\n",
        "    \"\"\"\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# ---------- 3) Resolver (match_to_unmasked.csv on server) ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = ftp_download_if_exists(ftps, SERVER_MAPPING_BASENAME, SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) CSV + name helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\",\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) Count helpers + partial HTML shells ----------\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip().lower()\n",
        "    return t\n",
        "\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        + \"%s\\n\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        + \"<title>%s</title>\\n\" % _html.escape(title)\n",
        "        + \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        + \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title)\n",
        "        + \"<div class=\\\"updated centerline\\\">\"\n",
        "          \"Last updated: %s &nbsp;|&nbsp; \"\n",
        "          \"Showing: %s\"\n",
        "          \"</div>\\n\" % (\n",
        "              _html.escape(LAST_UPDATED_DISPLAY),\n",
        "              _html.escape(AUTOSOMAL_MATCHES_TEXT),\n",
        "          )\n",
        "        + NAV_BLOCK + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "          \"Showing: \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "          \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "          \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function ySelEachRow(tb, cb){\"\n",
        "        \" if(!tb) return;\"\n",
        "\n",
        "        \" var rows=tb.getElementsByTagName('tr');\"\n",
        "        \" for(var i=0;i<rows.length;i++){cb(rows[i]);}\"\n",
        "        \"}\"\n",
        "        \"function ySelClear(tr){\"\n",
        "        \" if(!tr) return;\"\n",
        "        \" tr.removeAttribute('data-selected');\"\n",
        "        \" var cls=tr.className||'';\"\n",
        "        \" cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,'');\"\n",
        "        \" tr.className=cls;\"\n",
        "        \" tr.style.backgroundColor='';\"\n",
        "        \"}\"\n",
        "        \"function ySelToggle(a){\"\n",
        "        \" var tr=a;\"\n",
        "        \" while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;}\"\n",
        "        \" if(!tr) return false;\"\n",
        "        \" var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \" if(sel){\"\n",
        "        \"  ySelClear(tr);\"\n",
        "        \" }else{\"\n",
        "        \"  tr.setAttribute('data-selected','1');\"\n",
        "        \"  var cls=tr.className||'';\"\n",
        "        \"  if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';}\"\n",
        "        \"  tr.style.backgroundColor='#fff2cc';\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelGetTBody(tbodyId){\"\n",
        "        \" var tb=document.getElementById(tbodyId);\"\n",
        "        \" if(tb) return tb;\"\n",
        "        \" var t=document.getElementById('ref-table');\"\n",
        "        \" if(!t) return null;\"\n",
        "        \" if(t.tBodies&&t.tBodies.length){return t.tBodies[0];}\"\n",
        "        \" return t;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowSelected(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){\"\n",
        "        \"  var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \"  tr.style.display=sel?'':'none';\"\n",
        "        \" });\"\n",
        "        \" var rl=document.getElementById('reg-list');\"\n",
        "        \" if(rl){\"\n",
        "        \"  var selVals=[];\"\n",
        "        \"  ySelEachRow(tb,function(tr){\"\n",
        "        \"    if(tr.getAttribute('data-selected')==='1'){\"\n",
        "        \"      var v=tr.getAttribute('data-filter')\"\n",
        "        \"        || tr.getAttribute('data-lineage')\"\n",
        "        \"        || tr.getAttribute('data-code')\"\n",
        "        \"        || tr.getAttribute('data-q')\"\n",
        "        \"        || '';\"\n",
        "        \"      if(v){selVals.push(v);}\"\n",
        "        \"    }\"\n",
        "        \"  });\"\n",
        "        \"  if(selVals.length===0){\"\n",
        "        \"    return false;\"\n",
        "        \"  }\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    var r=rows[i];\"\n",
        "        \"    var lv=r.getAttribute('data-filter')\"\n",
        "        \"      || r.getAttribute('data-lineage')\"\n",
        "        \"      || r.getAttribute('data-code')\"\n",
        "        \"      || '';\"\n",
        "        \"    var show=false;\"\n",
        "        \"    for(var j=0;j<selVals.length;j++){\"\n",
        "        \"      if(lv===selVals[j]){show=true; break;}\"\n",
        "        \"    }\"\n",
        "        \"    r.style.display=show?'':'none';\"\n",
        "        \"  }\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowAll(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display='';});\"\n",
        "        \" var rl=document.getElementById('reg-list');\"\n",
        "        \" if(rl){\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){rows[i].style.display='';}\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelReset(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);});\"\n",
        "        \" var rl=document.getElementById('reg-list');\"\n",
        "        \" if(rl){\"\n",
        "        \"  var rows=rl.getElementsByTagName('tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){rows[i].style.display='';}\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"window.ySelToggle=ySelToggle;\"\n",
        "        \"window.ySelShowSelected=ySelShowSelected;\"\n",
        "        \"window.ySelShowAll=ySelShowAll;\"\n",
        "        \"window.ySelReset=ySelReset;\"\n",
        "        \"})();\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(\n",
        "    row,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        name_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        name_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        name_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return subject_name_html, name_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Match Count partial ----------\n",
        "def build_match_count_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    codes_raw = main_df[match_col].astype(str).map(lambda x: x.strip())\n",
        "    keys_norm = codes_raw.map(_norm_code_for_count)\n",
        "\n",
        "    counts_series = keys_norm.value_counts(dropna=False)\n",
        "    counts = counts_series.reset_index()\n",
        "    if counts.shape[1] >= 2:\n",
        "        counts.columns = [\"norm_key\", \"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"] = counts.index.astype(str)\n",
        "        counts[\"Count\"] = counts_series.values\n",
        "        counts = counts[[\"norm_key\", \"Count\"]]\n",
        "\n",
        "    first_display = {}\n",
        "    raw_list = codes_raw.tolist()\n",
        "    norm_list = keys_norm.tolist()\n",
        "    for code_disp, k in zip(raw_list, norm_list):\n",
        "        if k not in first_display and str(k) != \"\":\n",
        "            first_display[k] = code_disp\n",
        "\n",
        "    counts[\"Code\"] = counts[\"norm_key\"].map(lambda k: first_display.get(k, k))\n",
        "    counts[\"Unmasked\"] = counts[\"norm_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "\n",
        "    counts = counts.sort_values(\n",
        "        by=[\"Code\", \"Count\"],\n",
        "        ascending=[True, False],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Match Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append(\n",
        "        '<th style=\"width:35%\">Match to</th>'\n",
        "        '<th style=\"width:35%\">First Ancestor</th>'\n",
        "        '<th style=\"width:30%\">Showing</th>'\n",
        "    )\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in counts.iterrows():\n",
        "        code = r.get(\"Code\", \"\")\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        norm_key = _norm_code_for_count(code)\n",
        "        unm = (r.get(\"Unmasked\", \"\") or \"\").strip()\n",
        "        label = (unm or code).strip()\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td></td>\"\n",
        "            \"<td class=\\\"count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td></tr>\"\n",
        "            % (\n",
        "                _html.escape(label, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(code),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected code(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>Name</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        code_raw = str(row.get(match_col, \"\")).strip()\n",
        "        if not code_raw:\n",
        "            continue\n",
        "        norm_key = _norm_code_for_count(code_raw)\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-code=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                _html.escape(norm_key, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Lineage Count partial ----------\n",
        "def build_lineage_count_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    first_series = (\n",
        "        main_df.get(\"First Ancestor\", pd.Series(dtype=str))\n",
        "        .astype(str)\n",
        "        .map(lambda x: x.strip())\n",
        "    )\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values(\n",
        "        [\"Count\", \"First Ancestor\"],\n",
        "        ascending=[False, True],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    html.append(_partial_head(\"Lineage Count\"))\n",
        "    html.append('<table id=\"ref-table\" class=\"sortable\" border=\"1\"><thead><tr>')\n",
        "    html.append(\n",
        "        '<th style=\"width:80%\">First Ancestor</th>'\n",
        "        '<th style=\"width:20%\">Showing</th>'\n",
        "    )\n",
        "    html.append(\"</tr></thead><tbody id=\\\"ref-tb\\\">\")\n",
        "\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        tr = (\n",
        "            \"<tr data-q=\\\"%s\\\" data-count=\\\"%d\\\" data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td class=\\\"count\\\">\"\n",
        "            \"<a href=\\\"#\\\" class=\\\"count-pick\\\" onclick=\\\"return ySelToggle(this);\\\" title=\\\"Toggle select\\\">%d</a>\"\n",
        "            \"</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                cnt,\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for selected lineage(s)</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>Name</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        first = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        if not first:\n",
        "            continue\n",
        "\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                match_to_html,\n",
        "                name_html,\n",
        "                cm_html,\n",
        "                header_html,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 9) Cousin printable partial ----------\n",
        "def build_cousin_print_partial(main_df: pd.DataFrame, id_col: str, match_col: str, name_col: str, cm_col: str, path_col: str) -> str:\n",
        "    rows = []\n",
        "\n",
        "    for _, row in main_df.iterrows():\n",
        "        subject_raw = row.get(match_col, \"\")\n",
        "        subject_name = normalize_person_name(MATCH_TO_UNMASKED.get(str(subject_raw).strip().lower(), subject_raw))\n",
        "        subject_name_html = \"<strong>%s</strong>\" % subject_name if subject_name else \"\"\n",
        "\n",
        "        pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "        matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "        if pid:\n",
        "            matchee_name_html = (\n",
        "                '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "                'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "                % (TNG_BASE, pid, TNG_TREE, matchee_name)\n",
        "            )\n",
        "        else:\n",
        "            matchee_name_html = matchee_name\n",
        "\n",
        "        cm_val = row.get(cm_col, \"0\")\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        gens_total = len(tokens)\n",
        "\n",
        "        if \"common_husband\" in main_df.columns and \"common_wife\" in main_df.columns:\n",
        "            husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "            wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "            if not husband_raw and not wife_raw:\n",
        "                husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "        else:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "        header_html = build_header(\n",
        "            subject_name_html or subject_name,\n",
        "            cm_val,\n",
        "            matchee_name_html,\n",
        "            gens_total,\n",
        "            husband_raw,\n",
        "            wife_raw,\n",
        "        )\n",
        "        rows.append(header_html)\n",
        "\n",
        "    rows_sorted = sorted(rows)\n",
        "\n",
        "    html_rows = [\n",
        "        '<table border=\"1\" id=\"refactor-table\" class=\"sortable\"><thead><tr><th>Match Summary</th></tr></thead><tbody>'\n",
        "    ]\n",
        "    for v in rows_sorted:\n",
        "        html_rows.append(\"<tr><td>%s</td></tr>\" % v)\n",
        "    html_rows.append(\"</tbody></table>\")\n",
        "\n",
        "    cousin_html = (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\" \"\n",
        "        \"\\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\"><head>\"\n",
        "        \"%s\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\"\n",
        "        \"<title>Cousin List (Printable)</title>\"\n",
        "        \"</head><body onload=\\\"window.print();\\\">\"\n",
        "        \"<div class=\\\"wrap\\\">\"\n",
        "        \"<h1 class=\\\"centerline\\\">Cousin List (Printable)</h1>\"\n",
        "        \"<div class=\\\"table-scroll\\\">%s</div>\"\n",
        "        \"</div></body></html>\"\n",
        "        % \"\".join(html_rows)\n",
        "    )\n",
        "    return cousin_html\n",
        "\n",
        "# ---------- 10) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for counts: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column (try headings like 'Match to' or 'Match').\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column for First Ancestor.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    # Rebuild First Ancestor column from lineage path (raw, not normalized)\n",
        "    first_ancestors = []\n",
        "    for _, row in df.iterrows():\n",
        "        tokens = split_tokens(row.get(path_col, \"\"))\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    mc_html = build_match_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        MATCH_COUNT_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(mc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(MATCH_COUNT_LOCAL))\n",
        "\n",
        "    lc_html = build_lineage_count_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        LINEAGE_COUNT_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(lc_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(LINEAGE_COUNT_LOCAL))\n",
        "\n",
        "    cousin_html = build_cousin_print_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        COUSIN_PRINT_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", os.path.abspath(COUSIN_PRINT_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; uploads skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, MATCH_COUNT_LOCAL, _remote_path(MATCH_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, LINEAGE_COUNT_LOCAL, _remote_path(LINEAGE_COUNT_REMOTE))\n",
        "            ftp_upload_overwrite(ftps, COUSIN_PRINT_LOCAL, _remote_path(COUSIN_PRINT_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload partials failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        for p in [\n",
        "            _remote_path(MATCH_COUNT_REMOTE),\n",
        "            _remote_path(LINEAGE_COUNT_REMOTE),\n",
        "            _remote_path(COUSIN_PRINT_REMOTE),\n",
        "        ]:\n",
        "            sz = ftp_size(ftps, p)\n",
        "            print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URLs ---\")\n",
        "        print(\"Match Count:      https://yates.one-name.net/partials/match_count.shtml\")\n",
        "        print(\"Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\")\n",
        "        print(\"Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2b ================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OBA73F4exq4",
        "outputId": "5a603535-1978-4da9-d170-05ea58a543a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2b_Counts | Version=2025.12.06-G2 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): December 6, 2025 11:30 AM\n",
            "[OK] Loaded CSV for counts: 93 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote partial: /content/partials/match_count.shtml\n",
            "[OK] Wrote partial: /content/partials/lineage_count.shtml\n",
            "[OK] Wrote partial: /content/partials/cousin_list_print.htm\n",
            "[PUT] partials/match_count.shtml -> partials/match_count.shtml\n",
            "[PUT] partials/lineage_count.shtml -> partials/lineage_count.shtml\n",
            "[PUT] partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/match_count.shtml : 65146\n",
            "partials/lineage_count.shtml : 69825\n",
            "partials/cousin_list_print.htm : 31260\n",
            "\n",
            "--- Open URLs ---\n",
            "Match Count:      https://yates.one-name.net/partials/match_count.shtml\n",
            "Lineage Count:    https://yates.one-name.net/partials/lineage_count.shtml\n",
            "Cousin Printable: https://yates.one-name.net/partials/cousin_list_print.htm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 2b_NetworkAuthority"
      ],
      "metadata": {
        "id": "MUDyjlbJbQoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2b_NetworkAuthority - Write dna_network_first_ancestors.txt ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Reads the same master CSV used by Cell 2b and derives a de-duplicated\n",
        "#   list of \"first ancestors\" from the lineage/ancestral-line column.\n",
        "# - Writes that list as /content/dna_network_first_ancestors.txt, one per line.\n",
        "# - This file is then consumed by Cell 2d (SaveNetworkAuthority) and Cell 2c\n",
        "#   (Match Specific Produced DNA Network).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2b_NetworkAuthority | Version=2025.12.06 | Encoding=ISO-8859-15 | DECLARED_LINES=160\n",
        "\n",
        "DECLARED_LINES = 160\n",
        "print(\n",
        "    \"[CONFIRM] Golden Rules active | \"\n",
        "    \"Cell=Cell2b_NetworkAuthority | \"\n",
        "    \"Version=2025.12.06 | \"\n",
        "    \"Encoding=ISO-8859-15 | \"\n",
        "    \"DECLARED_LINES=%d\" % DECLARED_LINES\n",
        ")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "\n",
        "# This should match the master CSV name used by Cell 2b.\n",
        "MASTER_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "# Authority file that Cell 2d expects and that Cell 2c already uses.\n",
        "AUTHORITY_PATH = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# ---------- 2) Helpers ----------\n",
        "\n",
        "def _clean_piece(text):\n",
        "    \"\"\"Normalize whitespace and tildes inside a lineage token.\"\"\"\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "# Same separator logic we used in Cell 2c: split a lineage path into tokens.\n",
        "SEP_RE = re.compile(\n",
        "    r\"\\s*(?:\"\n",
        "    r\"\\u2192\"          # unicode right arrow\n",
        "    r\"|&rarr;\"         # HTML right arrow\n",
        "    r\"|;|>|,\"\n",
        "    r\"|~{2,}\"          # multiple tildes\n",
        "    r\"|/{2,}\"          # double slash\n",
        "    r\"|\\|{2,}\"         # double pipe\n",
        "    r\")\\s*\"\n",
        ")\n",
        "\n",
        "def split_tokens(s):\n",
        "    \"\"\"Split a lineage path string into tokens using SEP_RE.\"\"\"\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    \"\"\"\n",
        "    Find a column in df using regex patterns and optional preferred names.\n",
        "    Returns the column name or None.\n",
        "    \"\"\"\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    # Preferred exact names first\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    # Otherwise use regex patterns\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# ---------- 3) Load master CSV ----------\n",
        "\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(MASTER_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "\n",
        "if df is None:\n",
        "    raise SystemExit(\n",
        "        \"[ERROR] Unable to read CSV: %s (%r)\" % (MASTER_CSV, _last_err)\n",
        "    )\n",
        "\n",
        "print(\n",
        "    \"[OK] Loaded master CSV for network authority: %d rows, %d cols\"\n",
        "    % (len(df), len(df.columns))\n",
        ")\n",
        "\n",
        "# ---------- 4) Identify lineage / ancestral-line column ----------\n",
        "\n",
        "line_col = find_col(\n",
        "    df,\n",
        "    patterns=[\n",
        "        r\"(yates\\s*dna\\s*ancestral\\s*line)\",\n",
        "        r\"(ancestral\\s*line)\",\n",
        "        r\"(lineage)\"\n",
        "    ],\n",
        "    prefer_exact=[\n",
        "        \"Yates DNA Ancestral Line\",\n",
        "        \"Ancestral Line\",\n",
        "        \"Lineage\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "if not line_col:\n",
        "    raise SystemExit(\n",
        "        \"[ERROR] Cannot find lineage/ancestral-line column in master CSV.\"\n",
        "    )\n",
        "\n",
        "print(\"[OK] Using lineage column:\", repr(line_col))\n",
        "\n",
        "# ---------- 5) Derive first ancestors and de-duplicate ----------\n",
        "\n",
        "first_ancestors = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    tokens = split_tokens(row.get(line_col, \"\"))\n",
        "    first = _clean_piece(tokens[0]) if tokens else \"\"\n",
        "    if first:\n",
        "        first_ancestors.append(first)\n",
        "\n",
        "total_raw = len(first_ancestors)\n",
        "unique_list = []\n",
        "seen = set()\n",
        "\n",
        "# Preserve original encounter order, but discard duplicates and empties.\n",
        "for anc in first_ancestors:\n",
        "    key = anc.strip()\n",
        "    if not key:\n",
        "        continue\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    unique_list.append(key)\n",
        "\n",
        "print(\"[INFO] Total first-ancestor tokens collected: %d\" % total_raw)\n",
        "print(\"[INFO] Unique non-empty first ancestors:    %d\" % len(unique_list))\n",
        "\n",
        "# Optional: show a short preview in the notebook\n",
        "for idx, anc in enumerate(unique_list[:25], start=1):\n",
        "    print(\"  %2d. %s\" % (idx, anc))\n",
        "\n",
        "if not unique_list:\n",
        "    print(\"[WARN] No non-empty first ancestors found. Authority file will NOT be written.\")\n",
        "else:\n",
        "    # ---------- 6) Write dna_network_first_ancestors.txt ----------\n",
        "    try:\n",
        "        # Ensure directory exists (AUTHORITY_PATH is just a filename in /content).\n",
        "        os.makedirs(os.path.dirname(AUTHORITY_PATH) or \".\", exist_ok=True)\n",
        "        with open(\n",
        "            AUTHORITY_PATH,\n",
        "            \"w\",\n",
        "            encoding=\"iso-8859-15\",\n",
        "            errors=\"xmlcharrefreplace\",\n",
        "        ) as f:\n",
        "            for anc in unique_list:\n",
        "                f.write(anc.strip() + \"\\n\")\n",
        "        print(\"[OK] Wrote authority file:\", os.path.abspath(AUTHORITY_PATH))\n",
        "        print(\"[OK] Lines written:\", len(unique_list))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] Failed to write authority file:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Cell2b_NetworkAuthority complete (authority file ready for Cell 2d) ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2b_NetworkAuthority - Write dna_network_first_ancestors.txt ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kpgsxWD3WTx",
        "outputId": "3fd942ab-dc56-45ca-9011-a6251d5335fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2b_NetworkAuthority | Version=2025.12.06 | Encoding=ISO-8859-15 | DECLARED_LINES=160\n",
            "[OK] Loaded master CSV for network authority: 93 rows, 6 cols\n",
            "[OK] Using lineage column: 'Yates DNA Ancestral Line'\n",
            "[INFO] Total first-ancestor tokens collected: 93\n",
            "[INFO] Unique non-empty first ancestors:    26\n",
            "   1. RobinsonWallaceAl&JonesSallieMcL\n",
            "   2. UnknownName&FordVeatriceM\n",
            "   3. YatesFrancis&TichborneJane\n",
            "   4. YatesJohn&BarfieldElizabeth\n",
            "   5. YatesJohn&GaterJoane\n",
            "   6. YatesThomas&SearchingStill\n",
            "   7. YatesThomas&SmithMary\n",
            "   8. YatesUriah&OakesSarah\n",
            "   9. YatesWilliam&BoothAnamariaE\n",
            "  10. YatesWilliam&EdwardsElizabeth\n",
            "  11. YatesWilliam&HouseHannah\n",
            "  12. YatesWilliam&NeedhamMary\n",
            "  13. YatesWilliam&ParkerSally\n",
            "  14. YatesWilliam&PikeEsther\n",
            "  15. YatesWilliam&RidyardAnn\n",
            "  16. YatesWilliam&SaltPhoebe\n",
            "  17. YatesWilliam&ThornburyAnne\n",
            "  18. YatesWilliamBa&BullockMartha\n",
            "  19. YatesWilliamCh&McManusEmilyMill\n",
            "  20. YatesWilliamJo&HolsteadSarahJane\n",
            "  21. YatesWilliamLe&LampingEllenIsab\n",
            "  22. YatesWilliamM&OsborneNancyAnn\n",
            "  23. YatesWilliamNi&HaysElizabeth\n",
            "  24. YatesWilliamPr&McKinneyElizabeth\n",
            "  25. YatesWilliamRo&DavisMalissa\n",
            "[OK] Wrote authority file: /content/dna_network_first_ancestors.txt\n",
            "[OK] Lines written: 26\n",
            "\n",
            "--- Cell2b_NetworkAuthority complete (authority file ready for Cell 2d) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2c"
      ],
      "metadata": {
        "id": "ah9XcKRcqLU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2c - Match Specific Produced DNA Network ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G4)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout via /partials/dna_tree_styles.css (linked only).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\")\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "DNA_NETWORK_LOCAL = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "DNA_NETWORK_REMOTE = posixpath.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "# Authority list is maintained on the server at /dna/network_first_ancestors.txt\n",
        "DNA_NETWORK_AUTH_REMOTE = \"dna/network_first_ancestors.txt\"\n",
        "DNA_NETWORK_AUTH_LOCAL = \"dna_network_first_ancestors.txt\"\n",
        "\n",
        "# ---------- 1a) Load vitals ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def ftp_download_if_exists(ftps, remote_name, local_name) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR %s\" % remote_name, f.write)\n",
        "        print(\"[PULL] %s -> %s\" % (remote_name, os.path.abspath(local_name)))\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name):\n",
        "                os.remove(local_name)\n",
        "        except Exception:\n",
        "            pass\n",
        "        print(\"[MISS] %s (%s)\" % (remote_name, e))\n",
        "        return False\n",
        "\n",
        "def sync_authority_from_server():\n",
        "    \"\"\"\n",
        "    Pull /dna/network_first_ancestors.txt from the server (if it exists)\n",
        "    into DNA_NETWORK_AUTH_LOCAL.\n",
        "    \"\"\"\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[INFO] FTP creds missing; authority sync skipped.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        remote = _remote_path(DNA_NETWORK_AUTH_REMOTE)\n",
        "        ok = ftp_download_if_exists(ftps, remote, DNA_NETWORK_AUTH_LOCAL)\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "        if ok:\n",
        "            print(\"[INFO] Authority file synced from server.\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Authority sync failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = False\n",
        "        try:\n",
        "            local_name = SERVER_MAPPING_LOCAL_CACHE\n",
        "            with open(local_name, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % SERVER_MAPPING_BASENAME, f.write)\n",
        "            print(\"[PULL] %s -> %s\" % (SERVER_MAPPING_BASENAME, os.path.abspath(local_name)))\n",
        "            ok = True\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                if os.path.exists(SERVER_MAPPING_LOCAL_CACHE):\n",
        "                    os.remove(SERVER_MAPPING_LOCAL_CACHE)\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"[MISS] %s (%s)\" % (SERVER_MAPPING_BASENAME, e))\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "# ---------- 4) Helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\",\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    if \"-\" in token:\n",
        "        token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_name_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_name_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) HTML shell ----------\n",
        "def _partial_head(title):\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        + \"%s\\n\" % HEAD_LINK\n",
        "        + \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        + \"<title>%s</title>\\n\" % _html.escape(title)\n",
        "        + \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        + \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title)\n",
        "        + \"<div class=\\\"updated centerline\\\">\"\n",
        "        \"Last updated: %s &nbsp;|&nbsp; \"\n",
        "        \"Showing: %s\"\n",
        "        \"</div>\\n\" % (\n",
        "            _html.escape(LAST_UPDATED_DISPLAY),\n",
        "            _html.escape(AUTOSOMAL_MATCHES_TEXT),\n",
        "        )\n",
        "        + NAV_BLOCK\n",
        "        + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "        \"Showing: \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelShowSelected('ref-tb');\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelShowAll('ref-tb');\\\">All</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return ySelReset('ref-tb');\\\">Reset</a>\"\n",
        "        \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \"function ySelEachRow(tb, cb){\"\n",
        "        \" if(!tb) return;\"\n",
        "        \" var rows=tb.getElementsByTagName('tr');\"\n",
        "        \" for(var i=0;i<rows.length;i++){cb(rows[i]);}\"\n",
        "        \"}\"\n",
        "        \"function ySelClear(tr){\"\n",
        "        \" if(!tr) return;\"\n",
        "        \" tr.removeAttribute('data-selected');\"\n",
        "        \" var cls=tr.className||'';\"\n",
        "        \" cls=cls.replace(/\\\\bsel-row\\\\b/g,'').replace(/\\\\s{2,}/g,' ').replace(/^\\\\s+|\\\\s+$/g,'');\"\n",
        "        \" tr.className=cls;\"\n",
        "        \" tr.style.backgroundColor='';\"\n",
        "        \"}\"\n",
        "        \"function ySelToggle(a){\"\n",
        "        \" var tr=a;\"\n",
        "        \" while(tr&&tr.tagName&&tr.tagName.toLowerCase()!=='tr'){tr=tr.parentNode;}\"\n",
        "        \" if(!tr) return false;\"\n",
        "        \" var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \" if(sel){\"\n",
        "        \"  ySelClear(tr);\"\n",
        "        \" }else{\"\n",
        "        \"  tr.setAttribute('data-selected','1');\"\n",
        "        \"  var cls=tr.className||'';\"\n",
        "        \"  if(cls.indexOf('sel-row')===-1){tr.className=(cls?(cls+' '):'')+'sel-row';}\"\n",
        "        \"  tr.style.backgroundColor='#fff2cc';\"\n",
        "        \" }\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelGetTBody(tbodyId){\"\n",
        "        \" var tb=document.getElementById(tbodyId);\"\n",
        "        \" return tb || null;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowSelected(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){\"\n",
        "        \"  var sel=tr.getAttribute('data-selected')==='1';\"\n",
        "        \"  tr.style.display=sel?'':'none';\"\n",
        "        \" });\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelShowAll(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display='';});\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"function ySelReset(tbodyId){\"\n",
        "        \" var tb=ySelGetTBody(tbodyId);\"\n",
        "        \" if(!tb) return false;\"\n",
        "        \" ySelEachRow(tb,function(tr){tr.style.display=''; ySelClear(tr);});\"\n",
        "        \" return false;\"\n",
        "        \"}\"\n",
        "        \"window.ySelToggle=ySelToggle;\"\n",
        "        \"window.ySelShowSelected=ySelShowSelected;\"\n",
        "        \"window.ySelShowAll=ySelShowAll;\"\n",
        "        \"window.ySelReset=ySelReset;\"\n",
        "        \"})();\"\n",
        "        \"(function(){\"\n",
        "        \" function collectFirstAncestors(){\"\n",
        "        \"  var rows=document.querySelectorAll('#ref-tb tr');\"\n",
        "        \"  var seen=Object.create(null);\"\n",
        "        \"  var out=[];\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    var cells=rows[i].getElementsByTagName('td');\"\n",
        "        \"    if(!cells.length) continue;\"\n",
        "        \"    var txt=(cells[0].textContent||cells[0].innerText||'').replace(/\\\\s+/g,' ').trim();\"\n",
        "        \"    if(!txt) continue;\"\n",
        "        \"    if(!seen[txt]){seen[txt]=true; out.push(txt);}\"\n",
        "        \"  }\"\n",
        "        \"  return out;\"\n",
        "        \" }\"\n",
        "        \" function setStatus(msg,isError){\"\n",
        "        \"  var span=document.getElementById('save-network-status');\"\n",
        "        \"  if(!span) return;\"\n",
        "        \"  span.textContent=msg;\"\n",
        "        \"  span.style.color=isError?'#990000':'#006600';\"\n",
        "        \" }\"\n",
        "        \" function onClickSaveNetwork(){\"\n",
        "        \"  var ancestors=collectFirstAncestors();\"\n",
        "        \"  if(!ancestors.length){\"\n",
        "        \"    setStatus('No ancestors found to save.',true);\"\n",
        "        \"    return;\"\n",
        "        \"  }\"\n",
        "        \"  setStatus('Saving '+ancestors.length+' ancestors...',false);\"\n",
        "        \"  fetch('/dna/save_network.php',{\"\n",
        "        \"    method:'POST',\"\n",
        "        \"    headers:{'Content-Type':'application/json'},\"\n",
        "        \"    body:JSON.stringify({ancestors:ancestors})\"\n",
        "        \"  }).then(function(resp){\"\n",
        "        \"    if(!resp.ok){throw new Error('HTTP '+resp.status);}\"\n",
        "        \"    return resp.json();\"\n",
        "        \"  }).then(function(data){\"\n",
        "        \"    if(data&&data.status==='ok'){\"\n",
        "        \"      var n=(typeof data.saved==='number')?data.saved:ancestors.length;\"\n",
        "        \"      setStatus('Saved '+n+' ancestors to network authority.',false);\"\n",
        "        \"    }else{\"\n",
        "        \"      setStatus('Unexpected response while saving network.',true);\"\n",
        "        \"    }\"\n",
        "        \"  }).catch(function(err){\"\n",
        "        \"    setStatus('Save failed: '+err,true);\"\n",
        "        \"  });\"\n",
        "        \" }\"\n",
        "        \" function init(){\"\n",
        "        \"  var btn=document.getElementById('save-network-btn');\"\n",
        "        \"  if(!btn) return;\"\n",
        "        \"  btn.addEventListener('click',onClickSaveNetwork,false);\"\n",
        "        \" }\"\n",
        "        \" if(document.readyState==='loading'){\"\n",
        "        \"  document.addEventListener('DOMContentLoaded',init,false);\"\n",
        "        \" }else{\"\n",
        "        \"  init();\"\n",
        "        \" }\"\n",
        "        \"})();\"\n",
        "        \"\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(\n",
        "    row,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    subject_raw = row.get(match_col, \"\")\n",
        "    key = str(subject_raw).strip().lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_raw)\n",
        "    subject_name = normalize_person_name(subject_unmasked)\n",
        "    subject_name_html = _html.escape(subject_name or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_html or subject_name,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return subject_name_html, matchee_html, _html.escape(str(cm_val).strip()), header_html\n",
        "\n",
        "# ---------- 7) Network authority + page builder ----------\n",
        "def _load_network_authority(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_network_first_ancestors.txt not found; using all First Ancestors.\")\n",
        "        return []\n",
        "    vals = []\n",
        "    with open(path, \"r\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as fh:\n",
        "        for line in fh:\n",
        "            t = line.strip()\n",
        "            if t:\n",
        "                vals.append(t)\n",
        "    if not vals:\n",
        "        print(\"[INFO] dna_network_first_ancestors.txt is empty; using all First Ancestors.\")\n",
        "        return []\n",
        "    print(\"[INFO] Loaded %d authority First Ancestors from %s\" % (len(vals), path))\n",
        "    if vals:\n",
        "        preview = vals[:10]\n",
        "        print(\"[INFO] First 10 authority entries:\")\n",
        "        for i, fa in enumerate(preview, 1):\n",
        "            print(\"  %2d. %s\" % (i, fa))\n",
        "    return vals\n",
        "\n",
        "def build_network_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        ") -> str:\n",
        "    # Build First Ancestor (raw, not normalized) and full lineage\n",
        "    first_ancestors = []\n",
        "    full_lineages = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        path_raw = str(row.get(path_col, \"\") or \"\")\n",
        "        tokens = split_tokens(path_raw)\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "        full_lineages.append(_clean_piece(path_raw))\n",
        "    df = main_df.copy()\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "    df[\"Full Lineage\"] = full_lineages\n",
        "\n",
        "    # Apply authority filter if present\n",
        "    auth_vals = _load_network_authority(DNA_NETWORK_AUTH_LOCAL)\n",
        "    if auth_vals:\n",
        "        auth_set = set(auth_vals)\n",
        "        before = len(df)\n",
        "        df = df[df[\"First Ancestor\"].isin(auth_set)].copy()\n",
        "        after = len(df)\n",
        "        print(\"[INFO] Filtered DNA network rows by authority list: %d -> %d\" % (before, after))\n",
        "        if after == 0:\n",
        "            print(\"[WARN] Authority filter eliminated all rows; falling back to full dataset.\")\n",
        "            df = main_df.copy()\n",
        "            df[\"First Ancestor\"] = first_ancestors\n",
        "            df[\"Full Lineage\"] = full_lineages\n",
        "    else:\n",
        "        print(\"[INFO] No authority filter applied; DNA network uses all rows.\")\n",
        "\n",
        "    # Summary counts for top table (deduplicated First Ancestors)\n",
        "    first_series = df[\"First Ancestor\"].astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values(\n",
        "        [\"Count\", \"First Ancestor\"],\n",
        "        ascending=[False, True],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    html = []\n",
        "    title = \"Match Specific Produced DNA Network\"\n",
        "    html.append(_partial_head(title))\n",
        "\n",
        "    # 7a) First Ancestor summary table (top, deduplicated)\n",
        "    html.append('<h2 class=\"centerline\">Match Specific Produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"ref-table\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th style=\"width:80%\">First Ancestor</th>'\n",
        "        '<th style=\"width:20%\">Showing</th>'\n",
        "        '</tr></thead><tbody id=\"ref-tb\">'\n",
        "    )\n",
        "    for _, r in lin_df.iterrows():\n",
        "        first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "        cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "        tr = (\n",
        "            \"<tr data-lineage=\\\"%s\\\" data-filter=\\\"%s\\\">\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td class=\\\"count\\\">%d</td>\"\n",
        "            \"</tr>\"\n",
        "            % (\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first, quote=True),\n",
        "                _html.escape(first),\n",
        "                cnt,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    # 7a.1) Button + status line to push current First Ancestors to /dna/save_network.php\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin:10px 0 15px 0;\">'\n",
        "        '<button id=\"save-network-btn\" type=\"button\">Update DNA Network Authority</button>'\n",
        "        '<span id=\"save-network-status\" style=\"margin-left:8px; font-size:90%;\"></span>'\n",
        "        '</div>'\n",
        "    )\n",
        "\n",
        "    # 7b) DNA Register rows table (below)\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for produced DNA network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>First Ancestor</th>'\n",
        "        '<th>cM</th>'\n",
        "        '<th>Full Lineage</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '</tr></thead><tbody>'\n",
        "    )\n",
        "    for _, row in df.iterrows():\n",
        "        match_to_html, name_html, cm_html, header_html = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "        first = _html.escape(str(row.get(\"First Ancestor\", \"\")).strip())\n",
        "        lineage = _html.escape(str(row.get(\"Full Lineage\", \"\")).strip())\n",
        "\n",
        "        tr = (\n",
        "            \"<tr>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"<td>%s</td>\"\n",
        "            \"</tr>\"\n",
        "            % (match_to_html, first, cm_html, lineage, header_html)\n",
        "        )\n",
        "        html.append(tr)\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html)\n",
        "\n",
        "# ---------- 8) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for DNA Network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\\\s*dna\\\\s*ancestral\\\\s*line|ancestral\\\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    # Sync the authority list from the server (dna/network_first_ancestors.txt)\n",
        "    sync_authority_from_server()\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    network_html = build_network_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "    with open(\n",
        "        DNA_NETWORK_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(network_html)\n",
        "    print(\"[OK] Wrote DNA Network partial:\", os.path.abspath(DNA_NETWORK_LOCAL))\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; upload of dna_network.shtml skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, DNA_NETWORK_LOCAL, _remote_path(DNA_NETWORK_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network.shtml failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p = _remote_path(DNA_NETWORK_REMOTE)\n",
        "        sz = ftp_size(ftps, p)\n",
        "        print(\"%s : %s\" % (p, sz if sz is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URL ---\")\n",
        "        print(\"DNA Network: https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session for dna_network.shtml:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2c - Match Specific Produced DNA Network ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cFg7B6P089G",
        "outputId": "4247a76d-2d99-498f-c27b-d6dd0847e065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2c_DNANetwork | Version=2025.12.06-G4 | Encoding=ISO-8859-15\n",
            "[VITALS] autosomal (after manual filter): 93\n",
            "[VITALS] last updated (display): December 6, 2025 5:41 PM\n",
            "[OK] Loaded CSV for DNA Network: 93 rows, 6 cols\n",
            "[PULL] dna/network_first_ancestors.txt -> /content/dna_network_first_ancestors.txt\n",
            "[INFO] Authority file synced from server.\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[INFO] Loaded 26 authority First Ancestors from dna_network_first_ancestors.txt\n",
            "[INFO] First 10 authority entries:\n",
            "   1. YatesFrancis&TichborneJane\n",
            "   2. YatesWilliam&ThornburyAnne\n",
            "   3. YatesWilliam&ParkerSally\n",
            "   4. YatesThomas&SearchingStill\n",
            "   5. YatesJohn&GaterJoane\n",
            "   6. YatesWilliam&EdwardsElizabeth\n",
            "   7. YatesWilliam&NeedhamMary\n",
            "   8. YatesWilliam&PikeEsther\n",
            "   9. YatesWilliam&SaltPhoebe\n",
            "  10. YatesWilliamBa&BullockMartha\n",
            "[INFO] Filtered DNA network rows by authority list: 93 -> 93\n",
            "[OK] Wrote DNA Network partial: /content/partials/dna_network.shtml\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 69939\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network: https://yates.one-name.net/partials/dna_network.shtml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2d"
      ],
      "metadata": {
        "id": "XM9k3wCyx4Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2d - Check Network Authority (Server Reader) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.06-G2)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - ASCII-only source; any writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - Deterministic audit banner + DECLARED_LINES.\n",
        "# - Purpose:\n",
        "#     * Read the current authority list from:\n",
        "#         https://yates.one-name.net/dna/network_first_ancestors.txt\n",
        "#     * De-duplicate it and print a clean, ordered list.\n",
        "# - This cell does NOT write or POST anything; the browser button does that.\n",
        "# ============================================================================\n",
        "\n",
        "import re\n",
        "import requests\n",
        "\n",
        "DECLARED_LINES = 80\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2d_CheckNetworkAuthority \"\n",
        "      \"| Version=2025.12.06-G2 | Encoding=ISO-8859-15 | DECLARED_LINES=%d\"\n",
        "      % DECLARED_LINES)\n",
        "\n",
        "AUTH_URL = \"https://yates.one-name.net/dna/network_first_ancestors.txt\"\n",
        "\n",
        "print(\"[INFO] Fetching authority from:\", AUTH_URL)\n",
        "\n",
        "try:\n",
        "    resp = requests.get(AUTH_URL, timeout=20)\n",
        "    print(\"[INFO] HTTP status:\", resp.status_code)\n",
        "    if resp.status_code != 200:\n",
        "        raise SystemExit(\"[ERROR] Could not fetch authority file.\")\n",
        "    raw_text = resp.text\n",
        "except Exception as e:\n",
        "    raise SystemExit(\"[ERROR] Request failed: %s\" % e)\n",
        "\n",
        "lines = []\n",
        "for ln in raw_text.splitlines():\n",
        "    s = ln.strip()\n",
        "    if not s:\n",
        "        continue\n",
        "    if s.startswith(\"#\"):\n",
        "        continue\n",
        "    lines.append(s)\n",
        "\n",
        "seen = {}\n",
        "unique = []\n",
        "for s in lines:\n",
        "    if s not in seen:\n",
        "        seen[s] = True\n",
        "        unique.append(s)\n",
        "\n",
        "print(\"[INFO] Total lines (including comments/blank):\", len(raw_text.splitlines()))\n",
        "print(\"[INFO] Unique First Ancestors:\", len(unique))\n",
        "\n",
        "for idx, val in enumerate(unique, 1):\n",
        "    print(\" %2d. %s\" % (idx, val))\n",
        "\n",
        "print(\"\\n--- Cell2d_CheckNetworkAuthority complete ---\")\n",
        "# ====== CUT STOP [1/1] CELL 2d - Check Network Authority ======================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBeIiTQwWxoa",
        "outputId": "40487688-7fa8-4f34-a9a5-14190e0c28eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2d_CheckNetworkAuthority | Version=2025.12.06-G2 | Encoding=ISO-8859-15 | DECLARED_LINES=80\n",
            "[INFO] Fetching authority from: https://yates.one-name.net/dna/network_first_ancestors.txt\n",
            "[INFO] HTTP status: 200\n",
            "[INFO] Total lines (including comments/blank): 26\n",
            "[INFO] Unique First Ancestors: 26\n",
            "  1. YatesFrancis&ampTichborneJane\n",
            "  2. YatesWilliam&ampThornburyAnne\n",
            "  3. YatesWilliam&ampParkerSally\n",
            "  4. YatesThomas&ampSearchingStill\n",
            "  5. YatesJohn&ampGaterJoane\n",
            "  6. YatesWilliam&ampEdwardsElizabeth\n",
            "  7. YatesWilliam&ampNeedhamMary\n",
            "  8. YatesWilliam&ampPikeEsther\n",
            "  9. YatesWilliam&ampSaltPhoebe\n",
            " 10. YatesWilliamBa&ampBullockMartha\n",
            " 11. YatesWilliamPr&ampMcKinneyElizabeth\n",
            " 12. RobinsonWallaceAl&ampJonesSallieMcL\n",
            " 13. UnknownName&ampFordVeatriceM\n",
            " 14. YatesJohn&ampBarfieldElizabeth\n",
            " 15. YatesThomas&ampSmithMary\n",
            " 16. YatesUriah&ampOakesSarah\n",
            " 17. YatesWilliam&ampBoothAnamariaE\n",
            " 18. YatesWilliam&ampHouseHannah\n",
            " 19. YatesWilliam&ampRidyardAnn\n",
            " 20. YatesWilliamCh&ampMcManusEmilyMill\n",
            " 21. YatesWilliamJo&ampHolsteadSarahJane\n",
            " 22. YatesWilliamLe&ampLampingEllenIsab\n",
            " 23. YatesWilliamM&ampOsborneNancyAnn\n",
            " 24. YatesWilliamNi&ampHaysElizabeth\n",
            " 25. YatesWilliamRo&ampDavisMalissa\n",
            " 26. YatesWilliamTh&ampWilkinsNancy\n",
            "\n",
            "--- Cell2d_CheckNetworkAuthority complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cell 2k"
      ],
      "metadata": {
        "id": "H2k15Th8cWfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 2k - Unified DNA Network View (Sticky + Exclude + Summary + CSV + Metrics) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.07-K16)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; all file writes use encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography/layout via /partials/dna_tree_styles.css (linked only, with a tiny scoped helper style here).\n",
        "# - Deterministic audit banner:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2025.12.07-K16 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2025.12.07-K16 | Encoding=ISO-8859-15\")\n",
        "\n",
        "DECLARED_LINES = 560\n",
        "print(\"[AUDIT] DECLARED_LINES={}\".format(DECLARED_LINES))\n",
        "\n",
        "import os\n",
        "import re\n",
        "import posixpath\n",
        "import socket\n",
        "import traceback\n",
        "from datetime import datetime, timedelta\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "\n",
        "# ---------- 0) Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "\n",
        "# ---------- 1) Config ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "FTP_DIR = (os.environ.get(\"FTP_DIR\", \"\") or \"\").strip()\n",
        "\n",
        "STYLESHEET_BASENAME = \"dna_tree_styles.css\"\n",
        "CSS_VERSION = \"v2025-11-12-max\"\n",
        "STYLESHEET_HREF = \"/partials/%s?%s\" % (STYLESHEET_BASENAME, CSS_VERSION)\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />' % STYLESHEET_HREF\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "VITALS_LOCAL = \"dna_vitals.csv\"\n",
        "\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "\n",
        "DNA_NETWORK_LOCAL = os.path.join(\"partials\", \"dna_network.shtml\")\n",
        "DNA_NETWORK_REMOTE = posixpath.join(\"partials\", \"dna_network.shtml\")\n",
        "\n",
        "# New: CSV snapshot of register\n",
        "REGISTER_CSV_LOCAL = \"dna_network_register.csv\"\n",
        "REGISTER_CSV_REMOTE = \"dna/dna_network_register.csv\"\n",
        "\n",
        "\n",
        "# ---------- 1a) Vitals ----------\n",
        "def _friendly_ts_from_utc(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"(unknown)\"\n",
        "    raw_clean = raw.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(raw_clean, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24 = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12 = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _load_vitals(path: str):\n",
        "    autosomal = \"(unknown)\"\n",
        "    last_updated_display = \"(unknown)\"\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[WARN] dna_vitals.csv not found; using '(unknown)' for header vitals.\")\n",
        "        return autosomal, last_updated_display\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    df_v = None\n",
        "    last_err = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df_v = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df_v = None\n",
        "    if df_v is None or \"line\" not in df_v.columns:\n",
        "        print(\"[WARN] Unable to read dna_vitals.csv or missing 'line' column:\", last_err)\n",
        "        return autosomal, last_updated_display\n",
        "    for raw_line in df_v[\"line\"].astype(str):\n",
        "        line = raw_line.strip()\n",
        "        low = line.lower()\n",
        "        if low.startswith(\"after manual filter, total records:\"):\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", line)\n",
        "            if m:\n",
        "                num_raw = m.group(1).replace(\",\", \"\")\n",
        "                try:\n",
        "                    autosomal = \"{:,}\".format(int(num_raw))\n",
        "                except Exception:\n",
        "                    autosomal = num_raw\n",
        "        elif low.startswith(\"last_updated_text:\"):\n",
        "            ts = line.split(\":\", 1)[1].strip()\n",
        "            last_updated_display = _friendly_ts_from_utc(ts)\n",
        "    print(\"[VITALS] autosomal (after manual filter): %s\" % autosomal)\n",
        "    print(\"[VITALS] last updated (display): %s\" % last_updated_display)\n",
        "    return autosomal, last_updated_display\n",
        "\n",
        "AUTOSOMAL_MATCHES_TEXT, LAST_UPDATED_DISPLAY = _load_vitals(VITALS_LOCAL)\n",
        "\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", 21)))\n",
        "    ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "    try:\n",
        "        ftps.prot_p()\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if FTP_DIR:\n",
        "        for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "            try:\n",
        "                ftps.mkd(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ensure_remote_dirs(ftps, remote_path):\n",
        "    if \"/\" not in remote_path:\n",
        "        return\n",
        "    pwd0 = ftps.pwd()\n",
        "    parts = [p for p in remote_path.split(\"/\")[:-1] if p and p != \".\"]\n",
        "    for seg in parts:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "    ftps.cwd(pwd0)\n",
        "\n",
        "def ftp_upload_overwrite(ftps, local_path, remote_name):\n",
        "    ensure_remote_dirs(ftps, remote_name)\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(\"STOR %s\" % remote_name, fh)\n",
        "    print(\"[PUT] %s -> %s\" % (local_path, remote_name))\n",
        "\n",
        "def ftp_size(ftps, remote_name):\n",
        "    try:\n",
        "        sz = ftps.size(remote_name)\n",
        "        return int(sz) if sz is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "# ---------- 3) Resolver ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\", \"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "    last = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read mapping CSV %s: %s\" % (path, last))\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\", \"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        try:\n",
        "            ftps.cwd(SERVER_PARTIALS_DIR)\n",
        "        except Exception:\n",
        "            pass\n",
        "        ok = False\n",
        "        try:\n",
        "            local_name = SERVER_MAPPING_LOCAL_CACHE\n",
        "            with open(local_name, \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR %s\" % SERVER_MAPPING_BASENAME, f.write)\n",
        "            print(\"[PULL] %s -> %s\" % (SERVER_MAPPING_BASENAME, os.path.abspath(local_name)))\n",
        "            ok = True\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                if os.path.exists(SERVER_MAPPING_LOCAL_CACHE):\n",
        "                    os.remove(SERVER_MAPPING_LOCAL_CACHE)\n",
        "            except Exception:\n",
        "                pass\n",
        "            print(\"[MISS] %s (%s)\" % (SERVER_MAPPING_BASENAME, e))\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /%s. Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "            % _remote_path(SERVER_MAPPING_REMOTE)\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(\"[OK] Resolver loaded: %d codes\" % len(df_map))\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = {}\n",
        "\n",
        "def _setup_resolver():\n",
        "    global MATCH_TO_UNMASKED\n",
        "    if not MATCH_TO_UNMASKED:\n",
        "        MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "\n",
        "# ---------- 4) Name helpers ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns:\n",
        "                return name\n",
        "            if name and name.lower() in lowmap:\n",
        "                return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s):\n",
        "        return []\n",
        "    if not isinstance(s, str):\n",
        "        s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r\"~+\", \" \", str(text))\n",
        "    t = re.sub(r\"\\s+\", \" \", t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\n",
        "    \"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\",\n",
        "}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token:\n",
        "        return token\n",
        "    token = re.sub(\n",
        "        r\"(^|\\b)([a-z])(['’])([a-z])\",\n",
        "        lambda m: m.group(1) + m.group(2).upper() + m.group(3) + m.group(4).upper(),\n",
        "        token.lower(),\n",
        "    )\n",
        "    if \"-\" in token:\n",
        "        token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\", lambda m: \"Mc\" + m.group(1).upper(), token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\" + m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name:\n",
        "        return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i > 0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i - 1].islower() and token[i].isupper():\n",
        "            idx = i\n",
        "            break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i\n",
        "                break\n",
        "    if idx is None:\n",
        "        return (token,)\n",
        "    surname = token[:idx]\n",
        "    given = token[idx:]\n",
        "    given_spaced = re.sub(r\"(?<!^)([A-Z])\", r\" \\1\", given)\n",
        "    return (\"%s %s\" % (given_spaced.strip(), surname.strip()),)\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (parts[0], parts[-1])).strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1:\n",
        "            return nm\n",
        "        return (\"%s %s\" % (ps[0], ps[-1])).strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates:\n",
        "        return surname\n",
        "    return (\"%s %s\" % (smart_titlecase(given_candidates[0]), surname)).strip()\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = \"%s %s\" % (first, last)\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens:\n",
        "        return (\"\", \"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2:\n",
        "        return (\"\", \"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1:\n",
        "        return \"parents\" if g == 1 else \"self\"\n",
        "    if g == 2:\n",
        "        return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1:\n",
        "        return \"great-grandparents\"\n",
        "    return \"%dx-great-grandparents\" % greats\n",
        "\n",
        "def build_header(subject_display_html, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = \"%d\" % int(round(float(cm_val)))\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        \"%s is a %s cM cousin match to %s, whose\" % (subject_display_html, cm_str, matchee_name_html),\n",
        "        \"%s (back %d Gens)\" % (degree_label, gens),\n",
        "        \"are\",\n",
        "        \"%s & %s.\" % (husband, wife),\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    s = re.sub(r\"\\.\\s*$\", \"\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "# ---------- 5) HTML shell ----------\n",
        "def _partial_head(title):\n",
        "    sticky_style = (\n",
        "        \"<style type=\\\"text/css\\\">\\n\"\n",
        "        \"#reg-list { border-collapse: collapse; }\\n\"\n",
        "        \"#reg-list th, #reg-list td { white-space: nowrap; }\\n\"\n",
        "        \"#reg-list thead th { position: sticky; top: 0; z-index: 3; }\\n\"\n",
        "        \"#reg-list th:first-child, #reg-list td:first-child {\\n\"\n",
        "        \"  position: sticky;\\n\"\n",
        "        \"  left: 0;\\n\"\n",
        "        \"  z-index: 2;\\n\"\n",
        "        \"  background: #ffffff;\\n\"\n",
        "        \"}\\n\"\n",
        "        \"</style>\\n\"\n",
        "    )\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \" \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        + HEAD_LINK + \"\\n\"\n",
        "        + sticky_style +\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        + \"<title>%s</title>\\n\" % _html.escape(title)\n",
        "        + \"</head>\\n<body>\\n<div class=\\\"wrap\\\">\\n\"\n",
        "        + \"<h1 class=\\\"centerline\\\">%s</h1>\\n\" % _html.escape(title)\n",
        "        + \"<div class=\\\"updated centerline\\\">\"\n",
        "        \"Last updated: %s &nbsp;|&nbsp; \"\n",
        "        \"Showing: %s\"\n",
        "        \"</div>\\n\" % (\n",
        "            _html.escape(LAST_UPDATED_DISPLAY),\n",
        "            _html.escape(AUTOSOMAL_MATCHES_TEXT),\n",
        "        )\n",
        "        + NAV_BLOCK\n",
        "        + \"\\n\"\n",
        "        + \"<div class=\\\"selection-menu centerline\\\">\"\n",
        "        \"View: \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return dnShowSelected();\\\">Selected</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return dnShowAll();\\\">All</a> &nbsp;|&nbsp; \"\n",
        "        \"<a href=\\\"#\\\" onclick=\\\"return dnReset();\\\">Reset</a>\"\n",
        "        \"</div>\\n\"\n",
        "        + \"<div class=\\\"table-scroll\\\">\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_tail():\n",
        "    return (\n",
        "        \"</div>\\n</div>\\n\"\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\"\n",
        "        \" var selectedMatches={};\"\n",
        "        \" var viewMode='all';\"\n",
        "        \" function hasSelection(){\"\n",
        "        \"  for(var k in selectedMatches){if(selectedMatches.hasOwnProperty(k)){return true;}}\"\n",
        "        \"  return false;\"\n",
        "        \" }\"\n",
        "        \" function setRowSelected(tr, sel){\"\n",
        "        \"  if(!tr) return;\"\n",
        "        \"  tr.setAttribute('data-selected', sel ? '1' : '0');\"\n",
        "        \"  tr.style.backgroundColor = sel ? '#fff2cc' : '';\"\n",
        "        \" }\"\n",
        "        \" function setParticipantsVisible(show){\"\n",
        "        \"  var wrap=document.getElementById('participants-wrapper');\"\n",
        "        \"  if(!wrap) return;\"\n",
        "        \"  wrap.style.display = show ? '' : 'none';\"\n",
        "        \" }\"\n",
        "        \" function setFirstAncVisible(show){\"\n",
        "        \"  var wrap=document.getElementById('first-anc-wrapper');\"\n",
        "        \"  if(!wrap) return;\"\n",
        "        \"  wrap.style.display = show ? '' : 'none';\"\n",
        "        \" }\"\n",
        "        \" function updateSummaryFromVisible(){\"\n",
        "        \"  var rows=document.querySelectorAll('#reg-tb tr');\"\n",
        "        \"  var participants={};\"\n",
        "        \"  var firstAncestors={};\"\n",
        "        \"  var visibleLines=0;\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    var r=rows[i];\"\n",
        "        \"    if(r.style.display==='none') continue;\"\n",
        "        \"    if(r.getAttribute('data-excluded')==='1') continue;\"\n",
        "        \"    visibleLines++;\"\n",
        "        \"    var mk=r.getAttribute('data-match')||'';\"\n",
        "        \"    var fa=r.getAttribute('data-first')||'';\"\n",
        "        \"    if(mk){participants[mk]=true;}\"\n",
        "        \"    if(fa){firstAncestors[fa]=true;}\"\n",
        "        \"  }\"\n",
        "        \"  var pCount=0, faCount=0, k;\"\n",
        "        \"  for(k in participants){if(participants.hasOwnProperty(k)){pCount++;}}\"\n",
        "        \"  for(k in firstAncestors){if(firstAncestors.hasOwnProperty(k)){faCount++;}}\"\n",
        "        \"  var selCount=0;\"\n",
        "        \"  for(k in selectedMatches){if(selectedMatches.hasOwnProperty(k)){selCount++;}}\"\n",
        "        \"  var elSel=document.getElementById('dn-sum-selected');\"\n",
        "        \"  var elP=document.getElementById('dn-sum-part');\"\n",
        "        \"  var elL=document.getElementById('dn-sum-lines');\"\n",
        "        \"  var elFA=document.getElementById('dn-sum-fa');\"\n",
        "        \"  if(elSel){elSel.textContent='Selected Participant: '+selCount;}\"\n",
        "        \"  if(elP){elP.textContent='Network Participants: '+pCount;}\"\n",
        "        \"  if(elL){elL.textContent='Network Ancestral lines: '+visibleLines;}\"\n",
        "        \"  if(elFA){elFA.textContent='Network First Ancestors: '+faCount;}\"\n",
        "        \" }\"\n",
        "        \" function updateRegister(){\"\n",
        "        \"  var regRows=document.querySelectorAll('#reg-tb tr');\"\n",
        "        \"  if(viewMode!=='selected' || !hasSelection()){\"\n",
        "        \"    for(var i=0;i<regRows.length;i++){regRows[i].style.display='';}\"\n",
        "        \"  }else{\"\n",
        "        \"    var focusFirst={};\"\n",
        "        \"    for(var i2=0;i2<regRows.length;i2++){\"\n",
        "        \"      var r2=regRows[i2];\"\n",
        "        \"      var mk=r2.getAttribute('data-match')||'';\"\n",
        "        \"      var fa=r2.getAttribute('data-first')||'';\"\n",
        "        \"      if(mk && fa && selectedMatches[mk]){focusFirst[fa]=true;}\"\n",
        "        \"    }\"\n",
        "        \"    for(var i3=0;i3<regRows.length;i3++){\"\n",
        "        \"      var r3=regRows[i3];\"\n",
        "        \"      var fa2=r3.getAttribute('data-first')||'';\"\n",
        "        \"      var show=!!focusFirst[fa2];\"\n",
        "        \"      r3.style.display = show ? '' : 'none';\"\n",
        "        \"    }\"\n",
        "        \"  }\"\n",
        "        \"  updateSummaryFromVisible();\"\n",
        "        \" }\"\n",
        "        \" function dnToggleMatchRow(tr){\"\n",
        "        \"  if(!tr) return false;\"\n",
        "        \"  var key=tr.getAttribute('data-match')||'';\"\n",
        "        \"  if(!key) return false;\"\n",
        "        \"  if(selectedMatches[key]){\"\n",
        "        \"    delete selectedMatches[key];\"\n",
        "        \"    setRowSelected(tr,false);\"\n",
        "        \"  }else{\"\n",
        "        \"    selectedMatches[key]=true;\"\n",
        "        \"    setRowSelected(tr,true);\"\n",
        "        \"  }\"\n",
        "        \"  updateRegister();\"\n",
        "        \"  return false;\"\n",
        "        \" }\"\n",
        "        \" function wireMatchRows(){\"\n",
        "        \"  var rows=document.querySelectorAll('#match-tb tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    (function(r){\"\n",
        "        \"      r.onclick=function(e){\"\n",
        "        \"        dnToggleMatchRow(r);\"\n",
        "        \"        if(e && e.preventDefault){e.preventDefault();}\"\n",
        "        \"      };\"\n",
        "        \"    })(rows[i]);\"\n",
        "        \"  }\"\n",
        "        \" }\"\n",
        "        \" function dnToggleFirstAnc(){\"\n",
        "        \"  var wrap=document.getElementById('first-anc-wrapper');\"\n",
        "        \"  if(!wrap) return false;\"\n",
        "        \"  var hidden = (wrap.style.display==='none');\"\n",
        "        \"  setFirstAncVisible(hidden);\"\n",
        "        \"  return false;\"\n",
        "        \" }\"\n",
        "        \" function dnShowSelected(){\"\n",
        "        \"  viewMode='selected';\"\n",
        "        \"  updateRegister();\"\n",
        "        \"  setParticipantsVisible(false);\"\n",
        "        \"  return false;\"\n",
        "        \" }\"\n",
        "        \" function dnShowAll(){\"\n",
        "        \"  viewMode='all';\"\n",
        "        \"  updateRegister();\"\n",
        "        \"  setParticipantsVisible(true);\"\n",
        "        \"  setFirstAncVisible(true);\"\n",
        "        \"  return false;\"\n",
        "        \" }\"\n",
        "        \" function dnReset(){\"\n",
        "        \"  selectedMatches={};\"\n",
        "        \"  viewMode='all';\"\n",
        "        \"  var rows=document.querySelectorAll('#match-tb tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){setRowSelected(rows[i],false);}\"\n",
        "        \"  var rrows=document.querySelectorAll('#reg-tb tr');\"\n",
        "        \"  for(var j=0;j<rrows.length;j++){\"\n",
        "        \"    rrows[j].style.display='';\"\n",
        "        \"    rrows[j].setAttribute('data-excluded','0');\"\n",
        "        \"    rrows[j].style.opacity='';\"\n",
        "        \"    var c=rrows[j].querySelector('.dn-include-cell');\"\n",
        "        \"    if(c){c.textContent='Yes';}\"\n",
        "        \"  }\"\n",
        "        \"  updateSummaryFromVisible();\"\n",
        "        \"  setParticipantsVisible(true);\"\n",
        "        \"  setFirstAncVisible(true);\"\n",
        "        \"  return false;\"\n",
        "        \" }\"\n",
        "        \" function toggleExcludeRow(tr){\"\n",
        "        \"  if(!tr) return;\"\n",
        "        \"  var cur = tr.getAttribute('data-excluded')==='1';\"\n",
        "        \"  var next = !cur;\"\n",
        "        \"  tr.setAttribute('data-excluded', next ? '1' : '0');\"\n",
        "        \"  tr.style.opacity = next ? '0.45' : '';\"\n",
        "        \"  var cell = tr.querySelector('.dn-include-cell');\"\n",
        "        \"  if(cell){cell.textContent = next ? 'No' : 'Yes';}\"\n",
        "        \"  updateSummaryFromVisible();\"\n",
        "        \" }\"\n",
        "        \" function wireRegisterRows(){\"\n",
        "        \"  var rows=document.querySelectorAll('#reg-tb tr');\"\n",
        "        \"  for(var i=0;i<rows.length;i++){\"\n",
        "        \"    (function(r){\"\n",
        "        \"      var cell=r.querySelector('.dn-include-cell');\"\n",
        "        \"      if(!cell) return;\"\n",
        "        \"      cell.onclick=function(e){\"\n",
        "        \"        toggleExcludeRow(r);\"\n",
        "        \"        if(e && e.preventDefault){e.preventDefault();}\"\n",
        "        \"        return false;\"\n",
        "        \"      };\"\n",
        "        \"    })(rows[i]);\"\n",
        "        \"  }\"\n",
        "        \" }\"\n",
        "        \" window.dnShowSelected=dnShowSelected;\"\n",
        "        \" window.dnShowAll=dnShowAll;\"\n",
        "        \" window.dnReset=dnReset;\"\n",
        "        \" window.dnToggleFirstAnc=dnToggleFirstAnc;\"\n",
        "        \" function init(){\"\n",
        "        \"  wireMatchRows();\"\n",
        "        \"  wireRegisterRows();\"\n",
        "        \"  setParticipantsVisible(true);\"\n",
        "        \"  setFirstAncVisible(true);\"\n",
        "        \"  updateRegister();\"\n",
        "        \" }\"\n",
        "        \" if(document.readyState==='loading'){\"\n",
        "        \"  document.addEventListener('DOMContentLoaded',init,false);\"\n",
        "        \" }else{\"\n",
        "        \"  init();\"\n",
        "        \" }\"\n",
        "        \"})();\"\n",
        "        \"\\n//]]>\\n</script>\\n</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------- 6) DNA-register-style row builder ----------\n",
        "def build_register_row(\n",
        "    row,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    # subject_raw = code as in CSV (authority)\n",
        "    subject_raw = str(row.get(match_col, \"\") or \"\")\n",
        "    subject_code = subject_raw.strip()\n",
        "    key = subject_code.lower()\n",
        "    subject_unmasked = MATCH_TO_UNMASKED.get(key, subject_code)\n",
        "\n",
        "    # For the Match-to column we want the code.\n",
        "    match_to_html = _html.escape(subject_code or \"\")\n",
        "\n",
        "    # For the header narrative we want a human name (unmasked if available).\n",
        "    subject_display = normalize_person_name(subject_unmasked or subject_code)\n",
        "    subject_display_html = _html.escape(subject_display or \"\")\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_display\n",
        "    if pid:\n",
        "        matchee_html = (\n",
        "            '<a href=\"%s/verticalchart.php?personID=%s&tree=%s&parentset=0&display=vertical&generations=15\" '\n",
        "            'target=\"_blank\" rel=\"noopener\">%s</a>'\n",
        "            % (TNG_BASE, pid, TNG_TREE, _html.escape(matchee_name or \"\", quote=False))\n",
        "        )\n",
        "    else:\n",
        "        matchee_html = _html.escape(matchee_name or \"\", quote=False)\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "\n",
        "    gens_total = len(tokens)\n",
        "\n",
        "    if \"common_husband\" in row.index and \"common_wife\" in row.index:\n",
        "        husband_raw = str(row.get(\"common_husband\", \"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\", \"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_display_html or subject_display,\n",
        "        cm_val,\n",
        "        matchee_html,\n",
        "        gens_total,\n",
        "        husband_raw,\n",
        "        wife_raw,\n",
        "    )\n",
        "\n",
        "    return match_to_html, matchee_html, _html.escape(str(cm_val).strip()), header_html, key\n",
        "\n",
        "\n",
        "# ---------- 7) Unified page + CSV builder ----------\n",
        "def build_network_partial(\n",
        "    main_df: pd.DataFrame,\n",
        "    id_col: str,\n",
        "    match_col: str,\n",
        "    name_col: str,\n",
        "    cm_col: str,\n",
        "    path_col: str,\n",
        "):\n",
        "    first_ancestors = []\n",
        "    full_lineages = []\n",
        "    for _, row in main_df.iterrows():\n",
        "        path_raw = str(row.get(path_col, \"\") or \"\")\n",
        "        tokens = split_tokens(path_raw)\n",
        "        first_ancestors.append(_clean_piece(tokens[0]) if tokens else \"\")\n",
        "        full_lineages.append(_clean_piece(path_raw))\n",
        "    df = main_df.copy()\n",
        "    df[\"First Ancestor\"] = first_ancestors\n",
        "    df[\"Full Lineage\"] = full_lineages\n",
        "\n",
        "    first_series = df[\"First Ancestor\"].astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\", \"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\", \"Count\"]]\n",
        "\n",
        "    lin_df = lin_df.sort_values(\n",
        "        [\"Count\", \"First Ancestor\"],\n",
        "        ascending=[False, True],\n",
        "        kind=\"mergesort\",\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    # Participants table\n",
        "    part_df = df[[match_col]].copy()\n",
        "    part_df[\"match_key\"] = part_df[match_col].astype(str).str.strip().str.lower()\n",
        "    part_df[\"match_code_raw\"] = part_df[match_col].astype(str).str.strip()\n",
        "\n",
        "    unmasked_series = part_df[\"match_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "    part_df[\"match_label\"] = unmasked_series\n",
        "    mask_empty = part_df[\"match_label\"] == \"\"\n",
        "    part_df.loc[mask_empty, \"match_label\"] = part_df.loc[mask_empty, \"match_code_raw\"]\n",
        "\n",
        "    part_df = part_df[part_df[\"match_key\"] != \"\"]\n",
        "    if part_df.empty:\n",
        "        p_counts = pd.DataFrame(columns=[\"match_key\", \"match_code_raw\", \"match_label\", \"rows\"])\n",
        "    else:\n",
        "        grp = part_df.groupby(\"match_key\")\n",
        "        rows_series = grp.size().rename(\"rows\")\n",
        "        code_series = grp[\"match_code_raw\"].first()\n",
        "        label_series = grp[\"match_label\"].first()\n",
        "        p_counts = pd.concat([code_series, label_series, rows_series], axis=1).reset_index()\n",
        "        p_counts.columns = [\"match_key\", \"match_code_raw\", \"match_label\", \"rows\"]\n",
        "        p_counts = p_counts.sort_values(\n",
        "            [\"match_key\"],\n",
        "            ascending=[True],\n",
        "            kind=\"mergesort\",\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "    # Full-study metrics from the entire produced network\n",
        "    full_participants = int(p_counts.shape[0])\n",
        "    full_lines = int(first_series[first_series != \"\"].shape[0])\n",
        "    full_first_anc = int(len(vc.index))\n",
        "\n",
        "    html = []\n",
        "    title = \"Match Specific Produced DNA Network\"\n",
        "    html.append(_partial_head(title))\n",
        "\n",
        "    html.append('<h2 class=\"centerline\">Match Specific Produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<p class=\"centerline\">This unified view shows participants (matches), First Ancestors '\n",
        "        'produced by the current DNA Network, and the detailed DNA Register rows below. '\n",
        "        'Click rows in the participants table to focus on a subset; use the Selected / All / Reset '\n",
        "        'links above to change the view. In the register, you can mark individual lines as '\n",
        "        'excluded from the proof.</p>'\n",
        "    )\n",
        "\n",
        "    # Participants block\n",
        "    html.append('<div id=\"participants-wrapper\">')\n",
        "    html.append('<h3>Network participants (matches)</h3>')\n",
        "    if p_counts.empty:\n",
        "        html.append('<p><em>No participants could be derived from the produced DNA network.</em></p>')\n",
        "    else:\n",
        "        html.append(\n",
        "            '<table id=\"participants-table\" class=\"sortable\" border=\"1\">'\n",
        "            '<thead><tr>'\n",
        "            '<th>#</th>'\n",
        "            '<th>Code</th>'\n",
        "            '<th>Participant</th>'\n",
        "            '<th>Network rows</th>'\n",
        "            '</tr></thead><tbody id=\"match-tb\">'\n",
        "        )\n",
        "        idx = 0\n",
        "        for _, r in p_counts.iterrows():\n",
        "            idx += 1\n",
        "            mkey = str(r.get(\"match_key\", \"\")).strip().lower()\n",
        "            code_raw = str(r.get(\"match_code_raw\", \"\")).strip()\n",
        "            label = str(r.get(\"match_label\", \"\")).strip()\n",
        "            rows_count = int(str(r.get(\"rows\", \"0\")).strip() or \"0\")\n",
        "\n",
        "            tr = (\n",
        "                '<tr data-match=\"%s\">'\n",
        "                '<td>%d</td>'\n",
        "                '<td>%s</td>'\n",
        "                '<td>%s</td>'\n",
        "                '<td class=\"count\">%d</td>'\n",
        "                '</tr>'\n",
        "                % (\n",
        "                    _html.escape(mkey, quote=True),\n",
        "                    idx,\n",
        "                    _html.escape(code_raw),\n",
        "                    _html.escape(label),\n",
        "                    rows_count,\n",
        "                )\n",
        "            )\n",
        "            html.append(tr)\n",
        "        html.append(\"</tbody></table>\")\n",
        "    html.append(\"</div>\")  # participants-wrapper\n",
        "\n",
        "    # First Ancestor table + toggle\n",
        "    html.append('<h3>First Ancestors produced by this DNA Network</h3>')\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin-bottom:4px;\">'\n",
        "        '<a href=\"#\" onclick=\"return dnToggleFirstAnc();\">Hide / show First Ancestors table</a>'\n",
        "        '</div>'\n",
        "    )\n",
        "    html.append('<div id=\"first-anc-wrapper\">')\n",
        "    if lin_df.empty:\n",
        "        html.append('<p><em>No First Ancestors could be derived from the produced DNA network.</em></p>')\n",
        "    else:\n",
        "        html.append(\n",
        "            '<table id=\"first-anc-table\" class=\"sortable\" border=\"1\">'\n",
        "            '<thead><tr>'\n",
        "            '<th style=\"width:80%\">First Ancestor</th>'\n",
        "            '<th style=\"width:20%\">Showing</th>'\n",
        "            '</tr></thead><tbody id=\"anc-tb\">'\n",
        "        )\n",
        "        for _, r in lin_df.iterrows():\n",
        "            first = str(r.get(\"First Ancestor\", \"\")).strip()\n",
        "            cnt = int(str(r.get(\"Count\", \"0\")).strip() or \"0\")\n",
        "            tr = (\n",
        "                '<tr data-first=\"%s\">'\n",
        "                '<td>%s</td>'\n",
        "                '<td class=\"count\">%d</td>'\n",
        "                '</tr>'\n",
        "                % (\n",
        "                    _html.escape(first, quote=True),\n",
        "                    _html.escape(first),\n",
        "                    cnt,\n",
        "                )\n",
        "            )\n",
        "            html.append(tr)\n",
        "        html.append(\"</tbody></table>\")\n",
        "    html.append(\"</div>\")  # first-anc-wrapper\n",
        "\n",
        "    # Summary block + CSV link (dynamic per current view)\n",
        "    html.append(\n",
        "        '<table id=\"dn-summary\" class=\"summary-block\" border=\"0\" '\n",
        "        'style=\"margin:10px auto 6px auto;\">'\n",
        "        '<tr>'\n",
        "        '<td><strong>Current proof scope:</strong></td>'\n",
        "        '<td>'\n",
        "        '<span id=\"dn-sum-selected\">Selected Participant: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-part\">Network Participants: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-lines\">Network Ancestral lines: 0</span> &nbsp;|&nbsp; '\n",
        "        '<span id=\"dn-sum-fa\">Network First Ancestors: 0</span> &nbsp;|&nbsp; '\n",
        "        '<a href=\"/dna/dna_network_register.csv\">Download register CSV</a>'\n",
        "        '</td>'\n",
        "        '</tr>'\n",
        "        '</table>'\n",
        "    )\n",
        "\n",
        "    # Fixed full-study metrics line (entire network dataset)\n",
        "    html.append(\n",
        "        '<div class=\"centerline\" style=\"margin:2px 0 10px 0; font-size:90%%;\">'\n",
        "        'Full study scope (all network lines): '\n",
        "        'Participants: %d  |  Ancestral lines: %d  |  First Ancestors: %d'\n",
        "        '</div>' % (full_participants, full_lines, full_first_anc)\n",
        "    )\n",
        "\n",
        "    # Register rows table + CSV rows collection\n",
        "    html.append('<h2 class=\"centerline\">DNA Register rows for produced DNA Network</h2>')\n",
        "    html.append(\n",
        "        '<table id=\"reg-list\" class=\"sortable\" border=\"1\">'\n",
        "        '<thead><tr>'\n",
        "        '<th>Match to</th>'\n",
        "        '<th>First Ancestor</th>'\n",
        "        '<th>Include in proof</th>'\n",
        "        '<th>Match Summary</th>'\n",
        "        '<th>Full Lineage</th>'\n",
        "        '</tr></thead><tbody id=\"reg-tb\">'\n",
        "    )\n",
        "    csv_rows = []\n",
        "    tag_re = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        match_to_html, name_html, cm_html, header_html, mkey = build_register_row(\n",
        "            row, id_col, match_col, name_col, cm_col, path_col\n",
        "        )\n",
        "        first_raw = str(row.get(\"First Ancestor\", \"\")).strip()\n",
        "        first = _html.escape(first_raw)\n",
        "        lineage_raw = str(row.get(\"Full Lineage\", \"\")).strip()\n",
        "        lineage = _html.escape(lineage_raw)\n",
        "\n",
        "        tr = (\n",
        "            '<tr data-match=\"%s\" data-first=\"%s\" data-excluded=\"0\">'\n",
        "            '<td>%s</td>'\n",
        "            '<td>%s</td>'\n",
        "            '<td class=\"dn-include-cell\">Yes</td>'\n",
        "            '<td>%s</td>'\n",
        "            '<td>%s</td>'\n",
        "            '</tr>'\n",
        "            % (\n",
        "                _html.escape(mkey, quote=True),\n",
        "                first,\n",
        "                match_to_html,   # code\n",
        "                first,\n",
        "                header_html,     # summary (HTML)\n",
        "                lineage,\n",
        "            )\n",
        "        )\n",
        "        html.append(tr)\n",
        "\n",
        "        # CSV row: strip any HTML tags from summary\n",
        "        summary_plain = tag_re.sub(\"\", header_html)\n",
        "        csv_rows.append({\n",
        "            \"Match code\": mkey,\n",
        "            \"First Ancestor\": first_raw,\n",
        "            \"Include in proof\": \"Yes\",\n",
        "            \"Match Summary\": summary_plain,\n",
        "            \"Full Lineage\": lineage_raw,\n",
        "        })\n",
        "\n",
        "    html.append(\"</tbody></table>\")\n",
        "\n",
        "    html.append(_partial_tail())\n",
        "    return \"\".join(html), csv_rows\n",
        "\n",
        "\n",
        "# ---------- 8) Main driver ----------\n",
        "def main():\n",
        "    encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\")\n",
        "    last_err = None\n",
        "    df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(CSV_IN, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            last_err = ex\n",
        "            df = None\n",
        "    if df is None:\n",
        "        raise RuntimeError(\"Unable to read CSV: %s (%s)\" % (CSV_IN, last_err))\n",
        "\n",
        "    print(\"[OK] Loaded CSV for unified DNA Network: %d rows, %d cols\" % (len(df), len(df.columns)))\n",
        "\n",
        "    id_col = find_col(df, [r\"^(id#|personid)$\"], [\"ID#\", \"ID\", \"PersonID\", \"personID\"])\n",
        "    match_col = find_col(df, [r\"^match\\\\s*to$\"], [\"Match to\", \"Match\", \"match_to\", \"Match_to\"])\n",
        "    name_col = find_col(df, [r\"^name$\"], [\"Name\"])\n",
        "    cm_col = find_col(df, [r\"^(c\\\\s*:?m|cm)$\", r\"centi.?morgan\"], [\"cM\", \"cm\"])\n",
        "    path_col = find_col(\n",
        "        df,\n",
        "        [r\"(yates\\\\s*dna\\\\s*ancestral\\\\s*line|ancestral\\\\s*line|lineage)\"],\n",
        "        [\"Yates DNA Ancestral Line\", \"Ancestral Line\", \"Lineage\"],\n",
        "    )\n",
        "\n",
        "    if not match_col:\n",
        "        raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "    if not path_col:\n",
        "        raise ValueError(\"CSV missing lineage/path column.\")\n",
        "    if not name_col:\n",
        "        raise ValueError(\"CSV missing 'Name' column.\")\n",
        "    if not cm_col:\n",
        "        raise ValueError(\"CSV missing 'cM' column.\")\n",
        "    if not id_col:\n",
        "        raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "\n",
        "    _setup_resolver()\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    network_html, csv_rows = build_network_partial(df, id_col, match_col, name_col, cm_col, path_col)\n",
        "\n",
        "    # Write HTML\n",
        "    with open(\n",
        "        DNA_NETWORK_LOCAL,\n",
        "        \"w\",\n",
        "        encoding=\"iso-8859-15\",\n",
        "        errors=\"xmlcharrefreplace\",\n",
        "    ) as f:\n",
        "        f.write(network_html)\n",
        "    print(\"[OK] Wrote unified DNA Network partial:\", os.path.abspath(DNA_NETWORK_LOCAL))\n",
        "\n",
        "    # Write CSV snapshot of register\n",
        "    if csv_rows:\n",
        "        reg_df = pd.DataFrame(csv_rows, columns=[\n",
        "            \"Match code\",\n",
        "            \"First Ancestor\",\n",
        "            \"Include in proof\",\n",
        "            \"Match Summary\",\n",
        "            \"Full Lineage\",\n",
        "        ])\n",
        "        reg_df.to_csv(\n",
        "            REGISTER_CSV_LOCAL,\n",
        "            index=False,\n",
        "            encoding=\"iso-8859-15\",\n",
        "            errors=\"xmlcharrefreplace\",\n",
        "        )\n",
        "        print(\"[OK] Wrote register CSV:\", os.path.abspath(REGISTER_CSV_LOCAL))\n",
        "    else:\n",
        "        print(\"[WARN] No register rows; CSV not written.\")\n",
        "\n",
        "    if not all(os.environ.get(k) for k in [\"FTP_HOST\", \"FTP_USER\", \"FTP_PASS\"]):\n",
        "        print(\"[SKIP] Missing FTP creds; upload of dna_network.shtml and CSV skipped.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, DNA_NETWORK_LOCAL, _remote_path(DNA_NETWORK_REMOTE))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Upload dna_network.shtml failed:\", e)\n",
        "\n",
        "        if csv_rows:\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, REGISTER_CSV_LOCAL, _remote_path(REGISTER_CSV_REMOTE))\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] Upload register CSV failed:\", e)\n",
        "\n",
        "        print(\"\\n--- SIZE Verification (if supported) ---\")\n",
        "        p_html = _remote_path(DNA_NETWORK_REMOTE)\n",
        "        sz_html = ftp_size(ftps, p_html)\n",
        "        print(\"%s : %s\" % (p_html, sz_html if sz_html is not None else \"(SIZE unsupported)\"))\n",
        "        if csv_rows:\n",
        "            p_csv = _remote_path(REGISTER_CSV_REMOTE)\n",
        "            sz_csv = ftp_size(ftps, p_csv)\n",
        "            print(\"%s : %s\" % (p_csv, sz_csv if sz_csv is not None else \"(SIZE unsupported)\"))\n",
        "\n",
        "        try:\n",
        "            ftps.quit()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        print(\"\\n--- Open URL ---\")\n",
        "        print(\"DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\")\n",
        "        if csv_rows:\n",
        "            print(\"Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\")\n",
        "    except Exception as e:\n",
        "        print(\"[FAIL] FTP session for dna_network.shtml / register CSV:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "# ====== CUT STOP [1/1] CELL 2k - Unified DNA Network View (Sticky + Exclude + Summary + CSV + Metrics) ======\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIuAfy5MldAe",
        "outputId": "02e603e8-f943-4b3d-a6a2-3e461efaaeb8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell2k_DNANetworkUnified | Version=2025.12.07-K16 | Encoding=ISO-8859-15\n",
            "[AUDIT] DECLARED_LINES=560\n",
            "[VITALS] autosomal (after manual filter): 1,605\n",
            "[VITALS] last updated (display): December 7, 2025 7:29 PM\n",
            "[OK] Loaded CSV for unified DNA Network: 1605 rows, 6 cols\n",
            "[PULL] match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "[OK] Resolver loaded: 83 codes\n",
            "[OK] Wrote unified DNA Network partial: /content/partials/dna_network.shtml\n",
            "[OK] Wrote register CSV: /content/dna_network_register.csv\n",
            "[PUT] partials/dna_network.shtml -> partials/dna_network.shtml\n",
            "[PUT] dna_network_register.csv -> dna/dna_network_register.csv\n",
            "\n",
            "--- SIZE Verification (if supported) ---\n",
            "partials/dna_network.shtml : 1384733\n",
            "dna/dna_network_register.csv : 753663\n",
            "\n",
            "--- Open URL ---\n",
            "DNA Network (unified): https://yates.one-name.net/partials/dna_network.shtml\n",
            "Register CSV: https://yates.one-name.net/dna/dna_network_register.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "INiJljOS1kRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CUT START [1/1] CELL 3 - Ancestor Register (Old-school Blue Menu; WHITE menu text; .shtml + SSI) ======\n",
        "# RON GOLDEN RULES - CLIFF NOTES (v2025.12.01-G1)\n",
        "# - Complete & runnable Colab cell, one contiguous block.\n",
        "# - Source ASCII-only; outputs written with encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\".\n",
        "# - XHTML 1.0 Transitional; typography comes ONLY from /partials/dna_tree_styles.css.\n",
        "# - Deterministic audit:\n",
        "#   [CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
        "# - Enforce ISO-8859-15 printable chars on writes.\n",
        "\n",
        "print(\"[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\")\n",
        "\n",
        "# ---------- Imports ----------\n",
        "import os, re, socket, posixpath, traceback\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from ftplib import FTP_TLS\n",
        "from string import Template as _T\n",
        "\n",
        "# Downloads paragraph is now suppressed (links live in nav_block.shtml)\n",
        "DOWNLOADS_BLOCK = \"\"\n",
        "\n",
        "# ---------- Secrets ----------\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ[\"FTP_HOST\"] = userdata.get(\"FTP_HOST\")\n",
        "    os.environ[\"FTP_USER\"] = userdata.get(\"FTP_USER\")\n",
        "    os.environ[\"FTP_PASS\"] = userdata.get(\"FTP_PASS\")\n",
        "    try:\n",
        "        os.environ[\"FTP_PORT\"] = userdata.get(\"FTP_PORT\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    try:\n",
        "        os.environ[\"FTP_DIR\"] = userdata.get(\"FTP_DIR\")\n",
        "    except Exception:\n",
        "        os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "except Exception:\n",
        "    os.environ.setdefault(\"FTP_HOST\", \"\")\n",
        "    os.environ.setdefault(\"FTP_USER\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PASS\", \"\")\n",
        "    os.environ.setdefault(\"FTP_PORT\", \"21\")\n",
        "    os.environ.setdefault(\"FTP_DIR\", \"\")\n",
        "\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip().strip(\"/\")\n",
        "\n",
        "# ---------- Config / Paths ----------\n",
        "INPUT_CSV = \"final_combined_df_with_value_labels.csv\"\n",
        "\n",
        "EXPORT_BASENAME = \"yates_ancestor_register\"\n",
        "LOCAL_CSV   = EXPORT_BASENAME + \".csv\"\n",
        "LOCAL_XLSX  = EXPORT_BASENAME + \".xlsx\"\n",
        "REMOTE_CSV  = posixpath.join(\"partials\", LOCAL_CSV)\n",
        "REMOTE_XLSX = posixpath.join(\"partials\", LOCAL_XLSX)\n",
        "\n",
        "# This page is now .shtml so Apache will parse SSI\n",
        "OUTPUT_NAME = \"just-trees.shtml\"\n",
        "REMOTE_HTML = posixpath.join(\"partials\", OUTPUT_NAME)\n",
        "\n",
        "# Stylesheet + cache buster (shared with Cell 2)\n",
        "STYLESHEET_HREF = \"/partials/dna_tree_styles.css\"\n",
        "CSS_VERSION     = \"v2025-11-23-g3\"\n",
        "HEAD_LINK = '<link rel=\"stylesheet\" type=\"text/css\" href=\"%s?%s\" />' % (STYLESHEET_HREF, CSS_VERSION)\n",
        "\n",
        "# Layout knob (used for top-scroll inner width)\n",
        "TABLE_WIDTH_PX = 5550\n",
        "\n",
        "# ---------- Load CSV (robust) ----------\n",
        "df = None\n",
        "_last_err = None\n",
        "for enc in (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"iso-8859-15\", \"latin1\"):\n",
        "    try:\n",
        "        df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, encoding=enc)\n",
        "        break\n",
        "    except Exception as e:\n",
        "        _last_err = e\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise SystemExit(\"[ERROR] Unable to read CSV: %s (%r)\" % (INPUT_CSV, _last_err))\n",
        "print(\"[OK] Loaded CSV: %s rows=%d, cols=%d\" % (INPUT_CSV, len(df), len(df.columns)))\n",
        "\n",
        "# Ensure haplogroup present (harmless for this view)\n",
        "if \"haplogroup\" not in df.columns:\n",
        "    df[\"haplogroup\"] = \"\"\n",
        "else:\n",
        "    df[\"haplogroup\"] = df[\"haplogroup\"].fillna(\"\")\n",
        "\n",
        "# ---------- Resolver: Column B (masked) -> Column C (unmasked) ----------\n",
        "A_IDX = 0\n",
        "B_IDX = 1\n",
        "C_IDX = 2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\", \" \")\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
        "    return t.lower()\n",
        "\n",
        "# Prefer local-first resolver cached by Cell 1; fall back to server\n",
        "LOCAL_RESOLVER = \"match_to_unmasked.csv\"\n",
        "if not os.path.exists(LOCAL_RESOLVER) and os.path.exists(\"/content/partials/match_to_unmasked.csv\"):\n",
        "    LOCAL_RESOLVER = \"/content/partials/match_to_unmasked.csv\"\n",
        "\n",
        "def _pull_resolver_if_needed(local_path):\n",
        "    if os.path.exists(local_path):\n",
        "        print(\"Using resolver:\", os.path.abspath(local_path))\n",
        "        return local_path\n",
        "    print(\"Resolver not found locally; attempting server pull ...\")\n",
        "    try:\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(os.environ.get(\"FTP_HOST\", \"\"), int(os.environ.get(\"FTP_PORT\", \"21\")))\n",
        "            ftps.login(os.environ.get(\"FTP_USER\", \"\"), os.environ.get(\"FTP_PASS\", \"\"))\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "            if FTP_DIR:\n",
        "                for p in [p for p in FTP_DIR.split(\"/\") if p]:\n",
        "                    try:\n",
        "                        ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try:\n",
        "                            ftps.mkd(p)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        ftps.cwd(p)\n",
        "            try:\n",
        "                ftps.cwd(\"partials\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            with open(\"match_to_unmasked.csv\", \"wb\") as f:\n",
        "                ftps.retrbinary(\"RETR match_to_unmasked.csv\", f.write)\n",
        "        print(\"[OK] Pulled resolver from server -> match_to_unmasked.csv\")\n",
        "        return \"match_to_unmasked.csv\"\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not pull resolver from server:\", e)\n",
        "        return local_path\n",
        "\n",
        "LOCAL_RESOLVER = _pull_resolver_if_needed(LOCAL_RESOLVER)\n",
        "\n",
        "def _load_resolver_to_map(path):\n",
        "    last = None\n",
        "    m = None\n",
        "    for enc in (\"utf-8-sig\", \"iso-8859-15\", \"utf-8\", \"cp1252\", \"latin1\"):\n",
        "        try:\n",
        "            m = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "            m = None\n",
        "    if m is None:\n",
        "        print(\"[WARN] Resolver not loaded:\", last)\n",
        "        return {}\n",
        "    cols = {c.lower(): c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        print(\"[WARN] Resolver missing 'code'/'unmasked' cols; skipping map.\")\n",
        "        return {}\n",
        "    m = m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"] = m[cols[\"code\"]].map(_norm_code)\n",
        "    m[\"__val__\"] = m[cols[\"unmasked\"]].astype(str)\n",
        "    m = m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map = _load_resolver_to_map(LOCAL_RESOLVER) if os.path.exists(LOCAL_RESOLVER) else {}\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw = df.iloc[:, B_IDX].astype(str)\n",
        "masked_key = masked_raw.map(_norm_code)\n",
        "resolved   = masked_key.map(resolver_map)\n",
        "df.iloc[:, C_IDX] = resolved.fillna(\"\")\n",
        "\n",
        "print(\n",
        "    \"[OK] Column B -> C mapping: %d / %d  unmatched: %d\"\n",
        "    % (int(resolved.notna().sum()), len(df), len(df) - int(resolved.notna().sum()))\n",
        ")\n",
        "\n",
        "# ---------- Load vitals from dna_vitals.csv (friendly date + autosomal count) ----------\n",
        "VITALS_CSV = \"dna_vitals.csv\"\n",
        "LAST_UPDATED_TEXT  = \"\"\n",
        "AUTOSOMAL_MATCHES  = \"\"\n",
        "SHOWING_STATIC     = \"\"\n",
        "\n",
        "def _friendly_ts_from_utc(raw):\n",
        "    \"\"\"\n",
        "    Convert a UTC timestamp like\n",
        "      'YYYY-MM-DD HH:MM UTC'\n",
        "      'YYYY-MM-DD HH:MM:SS UTC'\n",
        "      'YYYY-MM-DDTHH:MM UTC'\n",
        "      'YYYY-MM-DDTHH:MM:SS UTC'\n",
        "    to 'Month D, YYYY H:MM AM/PM' in approximate US Eastern (UTC-5).\n",
        "    On failure, return the original string.\n",
        "    \"\"\"\n",
        "    s = str(raw or \"\").strip()\n",
        "    if not s:\n",
        "        return \"(unknown)\"\n",
        "    s = s.replace(\"UTC\", \"\").replace(\"utc\", \"\").strip()\n",
        "    fmts = [\n",
        "        \"%Y-%m-%d %H:%M\",\n",
        "        \"%Y-%m-%d %H:%M:%S\",\n",
        "        \"%Y-%m-%dT%H:%M\",\n",
        "        \"%Y-%m-%dT%H:%M:%S\",\n",
        "    ]\n",
        "    dt_utc = None\n",
        "    for fmt in fmts:\n",
        "        try:\n",
        "            dt_utc = datetime.strptime(s, fmt)\n",
        "            break\n",
        "        except Exception:\n",
        "            dt_utc = None\n",
        "    if dt_utc is None:\n",
        "        return raw\n",
        "    dt_est = dt_utc - timedelta(hours=5)\n",
        "    months = [\n",
        "        \"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\n",
        "        \"July\",\"August\",\"September\",\"October\",\"November\",\"December\"\n",
        "    ]\n",
        "    month_name = months[dt_est.month - 1]\n",
        "    h24  = dt_est.hour\n",
        "    ampm = \"AM\" if h24 < 12 else \"PM\"\n",
        "    h12  = h24 % 12\n",
        "    if h12 == 0:\n",
        "        h12 = 12\n",
        "    return \"%s %d, %d %d:%02d %s\" % (\n",
        "        month_name,\n",
        "        dt_est.day,\n",
        "        dt_est.year,\n",
        "        h12,\n",
        "        dt_est.minute,\n",
        "        ampm,\n",
        "    )\n",
        "\n",
        "def _format_num_with_commas(raw_val):\n",
        "    \"\"\"Normalize a numeric string (allowing commas) and format with thousands commas.\"\"\"\n",
        "    if raw_val is None:\n",
        "        return \"\"\n",
        "    s = str(raw_val)\n",
        "    # Strip everything except digits and optional leading minus\n",
        "    s_digits = re.sub(r\"[^0-9\\-]\", \"\", s)\n",
        "    if not s_digits:\n",
        "        return \"\"\n",
        "    try:\n",
        "        n = int(s_digits)\n",
        "        return \"{:,}\".format(n)\n",
        "    except Exception:\n",
        "        return s_digits\n",
        "\n",
        "def _load_vitals(path):\n",
        "    global LAST_UPDATED_TEXT, AUTOSOMAL_MATCHES, SHOWING_STATIC\n",
        "    LAST_UPDATED_TEXT = \"\"\n",
        "    AUTOSOMAL_MATCHES = \"\"\n",
        "    SHOWING_STATIC    = \"\"\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(\"[INFO] dna_vitals.csv not found; LAST_UPDATED_TEXT and AUTOSOMAL_MATCHES will be blank.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        vdf = pd.read_csv(path, dtype=str, encoding=\"iso-8859-15\", keep_default_na=False)\n",
        "    except Exception:\n",
        "        encs = (\"utf-8-sig\", \"utf-8\", \"cp1252\", \"latin1\")\n",
        "        last = None\n",
        "        vdf  = None\n",
        "        for enc in encs:\n",
        "            try:\n",
        "                vdf = pd.read_csv(path, dtype=str, encoding=enc, keep_default_na=False)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                last = e\n",
        "        if vdf is None:\n",
        "            print(\"[WARN] Unable to read dna_vitals.csv: %s\" % last)\n",
        "            return\n",
        "\n",
        "    flat = []\n",
        "    for row in vdf.astype(str).values.tolist():\n",
        "        for cell in row:\n",
        "            flat.append(str(cell))\n",
        "\n",
        "    autosomal_raw = None\n",
        "    showing_raw   = None\n",
        "    last_text     = None\n",
        "\n",
        "    for cell in flat:\n",
        "        s = str(cell)\n",
        "        if \"Records tagged and filtered by NPFX\" in s and autosomal_raw is None:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", s)\n",
        "            if m:\n",
        "                autosomal_raw = m.group(1)\n",
        "        if \"After manual filter, total records\" in s and showing_raw is None:\n",
        "            m = re.search(r\"(\\d[\\d,]*)\", s)\n",
        "            if m:\n",
        "                showing_raw = m.group(1)\n",
        "        if \"LAST_UPDATED_TEXT\" in s and last_text is None:\n",
        "            m = re.search(r\"LAST_UPDATED_TEXT\\s*:\\s*(.+)\", s)\n",
        "            if m:\n",
        "                last_text = m.group(1).strip()\n",
        "\n",
        "    # Fallback: grab first two numeric-looking tokens if labels were not found\n",
        "    if autosomal_raw is None or showing_raw is None or last_text is None:\n",
        "        all_text = \" \".join(flat)\n",
        "        nums = re.findall(r\"\\d[\\d,]*\", all_text)\n",
        "        if autosomal_raw is None and len(nums) >= 1:\n",
        "            autosomal_raw = nums[0]\n",
        "        if showing_raw is None and len(nums) >= 2:\n",
        "            showing_raw = nums[1]\n",
        "\n",
        "    if last_text is not None:\n",
        "        last_text = _friendly_ts_from_utc(last_text)\n",
        "\n",
        "    AUTOSOMAL_MATCHES = _format_num_with_commas(autosomal_raw)\n",
        "    SHOWING_STATIC    = _format_num_with_commas(showing_raw)\n",
        "    LAST_UPDATED_TEXT = last_text or \"\"\n",
        "\n",
        "    print(\"[OK] Vitals from %s -> autosomal=%s, showing=%s, last_updated_text=%s\"\n",
        "          % (path,\n",
        "             AUTOSOMAL_MATCHES or \"?\",\n",
        "             SHOWING_STATIC or \"?\",\n",
        "             LAST_UPDATED_TEXT or \"(blank)\"))\n",
        "\n",
        "_load_vitals(VITALS_CSV)\n",
        "\n",
        "# ---------- Blocks (updated, nav via SSI, controls) ----------\n",
        "if LAST_UPDATED_TEXT:\n",
        "    updated_str = (\n",
        "        'Last updated: <span id=\"last-updated\">%s</span>' %\n",
        "        _html.escape(LAST_UPDATED_TEXT)\n",
        "    )\n",
        "else:\n",
        "    updated_str = 'Last updated: <span id=\"last-updated\"></span>'\n",
        "\n",
        "_updated_parts = [updated_str]\n",
        "if AUTOSOMAL_MATCHES:\n",
        "    _updated_parts.append('Autosomal matches: %s' % _html.escape(AUTOSOMAL_MATCHES))\n",
        "# Showing is still the live, on-page filtered count\n",
        "_updated_parts.append('Showing: <span id=\"showing-count\"></span>')\n",
        "\n",
        "UPDATED_BLOCK = (\n",
        "    '<div class=\"updated centerline\">' +\n",
        "    ' &nbsp;|&nbsp; '.join(_updated_parts) +\n",
        "    '</div>'\n",
        ")\n",
        "\n",
        "NAV_BLOCK = '<!--#include virtual=\"/partials/nav_block.shtml\" -->'\n",
        "\n",
        "CONTROLS_BLOCK = (\n",
        "    '<div class=\"controls centerline\" style=\"margin:6px 0 10px 0;\">'\n",
        "    '<input type=\"text\" id=\"search-box\" class=\"search\" size=\"28\" value=\"\" '\n",
        "    'placeholder=\"Search&amp;hellip;\" />'\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ---------- HTML table ----------\n",
        "visible_cols = [c for c in df.columns if c]\n",
        "\n",
        "table_html = df.to_html(\n",
        "    index=False,\n",
        "    columns=visible_cols,\n",
        "    escape=False,\n",
        "    border=1,\n",
        "    classes=\"dataframe sortable\"\n",
        ")\n",
        "\n",
        "if 'id=\"refactor-table\"' not in table_html:\n",
        "    table_html = re.sub(r\"<table([^>]*)>\", r'<table\\1 id=\"refactor-table\">', table_html, count=1)\n",
        "\n",
        "if 'class=\"dataframe sortable\"' not in table_html and \"sortable\" not in table_html:\n",
        "    table_html = table_html.replace('class=\"dataframe\"', 'class=\"dataframe sortable\"', 1)\n",
        "\n",
        "table_html = table_html.replace(\"<tbody>\\n<tr>\", \"<tbody>\\n<tr id=\\\"first-row\\\">\", 1)\n",
        "\n",
        "SCROLL_WRAPPER = (\n",
        "    '<div class=\"table-scroll-wrapper\">'\n",
        "    '<div id=\"top-scroll\" class=\"scroll-sync-top\">'\n",
        "    '<div class=\"scroll-sync-top-inner\" style=\"width:%dpx;\"></div>'\n",
        "    '</div>'\n",
        "    '<div id=\"bottom-scroll\" class=\"table-scroll\">%s</div>'\n",
        "    '</div>'\n",
        ") % (TABLE_WIDTH_PX, table_html)\n",
        "\n",
        "# ---------- XHTML page template ----------\n",
        "page_tpl = _T(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register (Trees View)</title>\n",
        "$HEAD_LINK\n",
        "<style type=\"text/css\">\n",
        "/* Sticky second column (index 2) for Trees table */\n",
        "#refactor-table th:nth-child(2),\n",
        "#refactor-table td:nth-child(2){\n",
        "  position:sticky;\n",
        "  left:0;\n",
        "  z-index:6;\n",
        "  background:#ffffff;\n",
        "}\n",
        "#refactor-table th:nth-child(2){\n",
        "  z-index:7;\n",
        "}\n",
        "</style>\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1 class=\"centerline\">Ancestor Register (Trees View)</h1>\n",
        "  $DOWNLOADS_BLOCK\n",
        "  $UPDATED_BLOCK\n",
        "  $NAV_BLOCK\n",
        "  $CONTROLS_BLOCK\n",
        "  $SCROLL_WRAPPER\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){\n",
        "    return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase();\n",
        "  }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb = tbl && tbl.tBodies ? tbl.tBodies[0] : null;\n",
        "    if(!tb) return;\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    var asc  = (dir === 'asc');\n",
        "    rows.sort(function(a,b){\n",
        "      var A = textOf(a.cells[colIndex]), B = textOf(b.cells[colIndex]);\n",
        "      var nA = parseFloat(A.replace(/[^0-9.\\\\-]/g,'')),\n",
        "          nB = parseFloat(B.replace(/[^0-9.\\\\-]/g,''));\n",
        "      if(!isNaN(nA) && !isNaN(nB)){ return asc ? (nA-nB) : (nB-nA); }\n",
        "      if (A < B) return asc ? -1 : 1;\n",
        "      if (A > B) return asc ?  1 : -1;\n",
        "      return 0;\n",
        "    });\n",
        "    var frag = document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "  function bindHeaderSort(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths = tbl.tHead.rows[0].cells;\n",
        "    if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dir = 'asc';\n",
        "      th.addEventListener('click', function(){\n",
        "        dir = (dir === 'asc') ? 'desc' : 'asc';\n",
        "        for (var j = 0; j < ths.length; j++){\n",
        "          ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+\\\\(asc\\\\)|\\\\s+\\\\(desc\\\\)/,'');\n",
        "        }\n",
        "        th.innerHTML += (dir === 'asc' ? ' (asc)' : ' (desc)');\n",
        "        sortTable(tbl, idx, dir);\n",
        "      }, false);\n",
        "    })(i);\n",
        "  }\n",
        "  function formatWithCommas(n){\n",
        "    try{\n",
        "      var x = parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10);\n",
        "      if(isNaN(x)) return '';\n",
        "      return x.toLocaleString('en-US');\n",
        "    }catch(e){ return String(n||''); }\n",
        "  }\n",
        "  function visibleRowCount(){\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows = tbl.tBodies[0].rows, n = 0;\n",
        "    for(var i=0;i<rows.length;i++){\n",
        "      if(rows[i].style.display !== 'none') n++;\n",
        "    }\n",
        "    return n;\n",
        "  }\n",
        "  function updateShowing(){\n",
        "    var el = document.getElementById('showing-count');\n",
        "    if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "  function getParam(name){\n",
        "    var m = location.search.match(new RegExp('[?&]'+name+'=([^&]+)'));\n",
        "    return m ? decodeURIComponent(m[1].replace(/\\\\+/g,' ')) : '';\n",
        "  }\n",
        "  function bindSearch(){\n",
        "    var box = document.getElementById('search-box');\n",
        "    var tbl = document.getElementById('refactor-table');\n",
        "    if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb = tbl.tBodies[0];\n",
        "    var rows = [].slice.call(tb.rows || []);\n",
        "    function rowText(tr){\n",
        "      var t = '';\n",
        "      for(var i=0;i<tr.cells.length;i++){\n",
        "        t += ' ' + (tr.cells[i].textContent || tr.cells[i].innerText || '');\n",
        "      }\n",
        "      return t.replace(/\\\\s+/g,' ').toLowerCase();\n",
        "    }\n",
        "    function apply(q){\n",
        "      q = String(q || '').toLowerCase();\n",
        "      for(var i=0;i<rows.length;i++){\n",
        "        var txt = rowText(rows[i]);\n",
        "        var show = !q || txt.indexOf(q) > -1;\n",
        "        rows[i].style.display = show ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to = null;\n",
        "    function onInput(){\n",
        "      if(to) clearTimeout(to);\n",
        "      to = setTimeout(function(){ apply(box.value); }, 60);\n",
        "    }\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "    var q0 = getParam('q');\n",
        "    if(q0){\n",
        "      box.value = q0;\n",
        "      apply(q0);\n",
        "      try{ history.replaceState(null,'',location.pathname); }catch(e){}\n",
        "    } else {\n",
        "      box.value = '';\n",
        "      apply('');\n",
        "    }\n",
        "  }\n",
        "  function bindBackToTop(){\n",
        "    var btn = document.getElementById('back-to-top');\n",
        "    if(!btn) return;\n",
        "    function toggle(){ btn.style.display = (window.scrollY > 200 ? 'block' : 'none'); }\n",
        "    toggle();\n",
        "    window.addEventListener('scroll', toggle, {passive:true});\n",
        "    btn.addEventListener('click', function(){\n",
        "      try{\n",
        "        window.scrollTo({top:0, behavior:'smooth'});\n",
        "      } catch(e){\n",
        "        window.scrollTo(0,0);\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "  function bindSyncedScrollbars(){\n",
        "    var topScroll    = document.getElementById('top-scroll');\n",
        "    var bottomScroll = document.getElementById('bottom-scroll');\n",
        "    if(!(topScroll && bottomScroll)) return;\n",
        "    var syncing = false;\n",
        "    topScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      bottomScroll.scrollLeft = topScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "    bottomScroll.addEventListener('scroll', function(){\n",
        "      if(syncing) return;\n",
        "      syncing = true;\n",
        "      topScroll.scrollLeft = bottomScroll.scrollLeft;\n",
        "      syncing = false;\n",
        "    }, false);\n",
        "  }\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    bindHeaderSort();\n",
        "    bindBackToTop();\n",
        "    bindSearch();\n",
        "    bindSyncedScrollbars();\n",
        "    updateShowing();\n",
        "  });\n",
        "})();\n",
        "//]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    HEAD_LINK=HEAD_LINK,\n",
        "    DOWNLOADS_BLOCK=DOWNLOADS_BLOCK,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    NAV_BLOCK=NAV_BLOCK,\n",
        "    CONTROLS_BLOCK=CONTROLS_BLOCK,\n",
        "    SCROLL_WRAPPER=SCROLL_WRAPPER,\n",
        ")\n",
        "\n",
        "# ---------- Exports ----------\n",
        "export_df = df.copy()\n",
        "export_df.to_csv(LOCAL_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "try:\n",
        "    export_df.to_excel(LOCAL_XLSX, index=False)\n",
        "except Exception:\n",
        "    from pandas import ExcelWriter\n",
        "    with ExcelWriter(LOCAL_XLSX) as _w:\n",
        "        export_df.to_excel(_w, index=False)\n",
        "print(\"[OK] Wrote exports:\", os.path.abspath(LOCAL_CSV), \"and\", os.path.abspath(LOCAL_XLSX))\n",
        "\n",
        "# ---------- Save page locally ----------\n",
        "try:\n",
        "    with open(OUTPUT_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", os.path.abspath(OUTPUT_NAME))\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Save failed:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# ---------- Upload to /partials ----------\n",
        "def _ftps_ensure_dir(ftps, path):\n",
        "    if not path:\n",
        "        return\n",
        "    for seg in [p for p in path.split(\"/\") if p]:\n",
        "        try:\n",
        "            ftps.cwd(seg)\n",
        "        except Exception:\n",
        "            try:\n",
        "                ftps.mkd(seg)\n",
        "            except Exception:\n",
        "                pass\n",
        "            ftps.cwd(seg)\n",
        "\n",
        "ftp_host = os.environ.get(\"FTP_HOST\")\n",
        "ftp_user = os.environ.get(\"FTP_USER\")\n",
        "ftp_pass = os.environ.get(\"FTP_PASS\")\n",
        "ftp_port = int(os.environ.get(\"FTP_PORT\", \"21\") or \"21\")\n",
        "\n",
        "if ftp_host and ftp_user and ftp_pass:\n",
        "    print(\"[INFO] Attempting FTP upload ...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, ftp_port)\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try:\n",
        "                ftps.prot_p()\n",
        "            except Exception:\n",
        "                pass\n",
        "            try:\n",
        "                ftps.set_pasv(True)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            _ftps_ensure_dir(ftps, FTP_DIR)\n",
        "            _ftps_ensure_dir(ftps, \"partials\")\n",
        "\n",
        "            with open(OUTPUT_NAME, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_HTML), fh)\n",
        "            print(\"[OK] Uploaded HTML -> /partials/%s\" % os.path.basename(REMOTE_HTML))\n",
        "\n",
        "            with open(LOCAL_CSV, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_CSV), fh)\n",
        "            with open(LOCAL_XLSX, \"rb\") as fh:\n",
        "                ftps.storbinary(\"STOR \" + os.path.basename(REMOTE_XLSX), fh)\n",
        "            print(\"[OK] Uploaded exports -> /partials/ (%s, %s)\" % (LOCAL_CSV, LOCAL_XLSX))\n",
        "\n",
        "            print(\"\\n--- Open URLs ---\")\n",
        "            print(\"Trees page:       https://yates.one-name.net/partials/just-trees.shtml\")\n",
        "            print(\"CSV export:       https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_CSV))\n",
        "            print(\"Excel export:     https://yates.one-name.net/partials/%s\" % os.path.basename(LOCAL_XLSX))\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload (missing credentials).\")\n",
        "\n",
        "print(\"\\n--- Cell 3 Complete (.shtml + SSI nav; top+bottom scroll; sticky col 2; sortable/searchable with live 'Showing' count; LAST_UPDATED_TEXT + Autosomal matches from dna_vitals.csv, now comma-formatted; exports + upload ready) ---\")\n",
        "# ====== CUT STOP  [1/1] CELL 3 ==================================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SPp7EfhUfkE",
        "outputId": "5c73d8ac-ca6d-43eb-fc53-f823e6b2741a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CONFIRM] Golden Rules active | Cell=Cell3_OldSchoolMenu_WhiteText | Version=2025.12.01-G1 | Encoding=ISO-8859-15\n",
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=93, cols=6\n",
            "Using resolver: /content/match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 93 / 93  unmatched: 0\n",
            "[OK] Vitals from dna_vitals.csv -> autosomal=1,605, showing=93, last_updated_text=December 6, 2025 11:30 AM\n",
            "[OK] Wrote exports: /content/yates_ancestor_register.csv and /content/yates_ancestor_register.xlsx\n",
            "[OK] Saved locally: /content/just-trees.shtml\n",
            "[INFO] Attempting FTP upload ...\n",
            "[OK] Uploaded HTML -> /partials/just-trees.shtml\n",
            "[OK] Uploaded exports -> /partials/ (yates_ancestor_register.csv, yates_ancestor_register.xlsx)\n",
            "\n",
            "--- Open URLs ---\n",
            "Trees page:       https://yates.one-name.net/partials/just-trees.shtml\n",
            "CSV export:       https://yates.one-name.net/partials/yates_ancestor_register.csv\n",
            "Excel export:     https://yates.one-name.net/partials/yates_ancestor_register.xlsx\n",
            "\n",
            "--- Cell 3 Complete (.shtml + SSI nav; top+bottom scroll; sticky col 2; sortable/searchable with live 'Showing' count; LAST_UPDATED_TEXT + Autosomal matches from dna_vitals.csv, now comma-formatted; exports + upload ready) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# debug"
      ],
      "metadata": {
        "id": "9G7Y0HwjtZIt"
      }
    }
  ]
}