{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OmSPVbuZEg8WxkfC-PoJLIdBLJo1HC9n",
      "authorship_tag": "ABX9TyOs94Z41r+zSbfLVpwJ2+iT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/A_v_33_00_YatesStudy_stable_2024_gedcom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik3CzJtgxuXH",
        "outputId": "ddbfa80e-651a-4f88-f349-ff053ded5161"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVWCghMxefi",
        "outputId": "80b580c4-1876-424c-a373-6c5ca39c4662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 55730 total records\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "filtered_ids.xlsx not found. Skipping second-level manual filter.\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "Excel file without hotlinks saved at: /content/2_data_NO_Hotlinks.xlsx\n",
            "Filtered data saved to /content/3_generations_fq-qi.xlsx\n",
            "Excel file loaded with hotlinks from /content/2_data_NO_Hotlinks.xlsx contains 1045 records.\n",
            "Excel file with exclusion IDs from /content/exclude_ids.xlsx contains 382 records.\n",
            "Filtered data saved to /content/3_generations_fq-qi.xlsx with 663 records.\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10] # Use slicing syntax to extract the first 10 characters of the first_name variable\n",
        "    last_name = last_name[:10].rstrip('/') # Use slicing syntax to extract the first 10 characters of the last_name variable\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            import pandas as pd  # Assuming you haven't imported it yet\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def apply_manual_filter(self):\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            import pandas as pd\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "                manual_filtered_ids = set(df['ID'].astype(str))  # Ensure IDs are strings\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids)}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                # Debug output to verify IDs before filtering\n",
        "                print(\"IDs before manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) in manual_filtered_ids]\n",
        "                print(\"IDs after manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def check_and_apply_exclusion_filter(self):\n",
        "        \"\"\"Apply exclusion filter if '/exclude_ids.xlsx' is present.\"\"\"\n",
        "        file_path = '/content/exclude_ids.xlsx'  # Updated path\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df_exclude = pd.read_excel(file_path)\n",
        "                if 'ID' in df_exclude.columns:\n",
        "                    exclude_ids = set(df_exclude['ID'].astype(str))  # Ensure conversion to string\n",
        "                    print(f\"Exclusion filter IDs loaded: {len(exclude_ids)}\")\n",
        "                    print(f\"Sample of IDs to exclude: {list(exclude_ids)[:5]}\")  # Print some sample IDs\n",
        "                    # Apply the exclusion filter\n",
        "                    initial_count = len(self.filter_pool)\n",
        "                    self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) not in exclude_ids]\n",
        "                    print(f\"Excluded {initial_count - len(self.filter_pool)} records based on exclusion IDs.\")\n",
        "                else:\n",
        "                    print(\"Column 'ID' not found in the Excel file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to apply exclusion filter: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"No exclusion filter applied, '{file_path}' not found. Check the path and ensure the file is uploaded to Colab.\")\n",
        "\n",
        "\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list,add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "#    print(f\"Distant ancestors paths for {individual_id}: {paths}\")\n",
        "\n",
        "    return paths\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "#global generation_table\n",
        "#global visited_pairs\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "\n",
        "    return matching_table\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "# Main Loop\n",
        "for dataset in filtered_datasets:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    visited_pairs = set()\n",
        "    generation_table = []\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "#    filtered_ancestral_line_str = \"|\".join(filtered_ancestral_line_names)\n",
        "#    print(f\"Filtered Ancestral Line for {individual_id}: {filtered_ancestral_line_str}\")\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Global variables\n",
        "visited_pairs = set()\n",
        "combined_df_rows = []  # Initialize your empty combined_df_rows list\n",
        "\n",
        "# Main Loop\n",
        "for dataset in gedcom_instance.filter_pool:  # Assuming filter_pool is iterable\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    # Process Individual and Get Data\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(individual_id, gedcom_instance, records)\n",
        "    cm = individual_data['cM']\n",
        "    sort = individual_data['Sort']\n",
        "    individual_name = extract_name(records.get(individual_id, ''))\n",
        "    # Append to DataFrame Rows\n",
        "    combined_df_rows.append([individual_id, sort, individual_name, cm, filtered_ancestral_line_str])\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create DataFrame\n",
        "columns = ['ID#', 'Match to', 'Name', 'cM', 'Yates DNA Ancestral Line']\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "#**********************************************************************************WORKING\n",
        "# Function to remove the named prefix from the 'Yates DNA Ancestral Line' column\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row['Yates DNA Ancestral Line']\n",
        "    prefix_to_remove = 'YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~'\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row['Yates DNA Ancestral Line'] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "# Apply the function to remove the prefix\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "#**********************************************************************************WORKING\n",
        "\n",
        "# Function to add hotlinks\n",
        "def create_hotlink(row):\n",
        "    url_base = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "    additional_params = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "    if pd.notnull(row['ID#']):\n",
        "        return f'<a href=\"{url_base}{row[\"ID#\"]}{additional_params}\">{row[\"ID#\"]}</a>'\n",
        "    return ''  # Return an empty string for null values\n",
        "\n",
        "# Apply the hotlink function\n",
        "combined_df['Link'] = combined_df.apply(create_hotlink, axis=1)\n",
        "\n",
        "# Define the columns to be used in the final DataFrame\n",
        "ordered_columns = ['Match to', 'Name', 'cM', 'Link', 'Yates DNA Ancestral Line']\n",
        "combined_df = combined_df[ordered_columns]\n",
        "\n",
        "# Sort the DataFrame\n",
        "combined_df.sort_values(by=['Yates DNA Ancestral Line', 'Match to'], ascending=[False, False], inplace=True)\n",
        "\n",
        "# Save the version with hotlinks\n",
        "output_excel_with_links = f'/content/1_data_with-Hotlinks.xlsx'\n",
        "combined_df.to_excel(output_excel_with_links, index=False)\n",
        "#print(f\"Excel file with hotlinks saved at: {output_excel_with_links}\")\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Assuming combined_df is already defined and includes the 'Link' column with IDs in a hyperlink format\n",
        "\n",
        "#**********************************************************************************WORKING\n",
        "# Create a copy of the DataFrame to modify for no-hotlinks version\n",
        "combined_df_no_links = combined_df.copy()\n",
        "\n",
        "# Assuming you want to simply remove the HTML link and retain the plain ID in the 'Link' column,\n",
        "# you can strip the HTML if it's structured consistently. If it's already plain text or you don't need to change,\n",
        "# skip this step or adjust accordingly.\n",
        "def strip_links(link_text):\n",
        "    # Example to strip a simple HTML tag; adjust regex as needed for actual data format\n",
        "    if '<a href=' in link_text:\n",
        "        return link_text.split('\">')[-1].split('</a>')[0]  # Adjust based on actual format\n",
        "    return link_text  # Return as is if no HTML tag\n",
        "\n",
        "# Apply function to strip HTML from 'Link' if necessary\n",
        "combined_df_no_links['Link'] = combined_df_no_links['Link'].apply(strip_links)\n",
        "\n",
        "# Define the output file path with a timestamp for uniqueness\n",
        "output_excel_no_links = f'/content/2_data_NO_Hotlinks.xlsx'\n",
        "\n",
        "# Try to save the DataFrame without hotlinks to an Excel file\n",
        "try:\n",
        "    combined_df_no_links.to_excel(output_excel_no_links, index=False)\n",
        "    print(f\"Excel file without hotlinks saved at: {output_excel_no_links}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to save the Excel file without hotlinks: {e}\")\n",
        "#**********************************************************************************WORKING\n",
        "\n",
        "#**********************************************************************************WORKING\n",
        "import pandas as pd\n",
        "\n",
        "def remove_excluded_records(main_file_path, exclude_file_path, output_file_path):\n",
        "    # Load the main data file\n",
        "    main_df = pd.read_excel(main_file_path)\n",
        "\n",
        "    # Load the exclusion list\n",
        "    exclude_df = pd.read_excel(exclude_file_path)\n",
        "\n",
        "    # Assuming there's a common column in both dataframes such as 'Link' or 'ID'\n",
        "    # Adjust the column name as per your actual data structure\n",
        "    common_column = 'Link'  # or it could be 'ID' or another unique identifier\n",
        "\n",
        "    # Convert the exclusion dataframe column to a list for filtering\n",
        "    excluded_links = exclude_df[common_column].tolist()\n",
        "\n",
        "    # Filter out the excluded records\n",
        "    filtered_df = main_df[~main_df[common_column].isin(excluded_links)]\n",
        "\n",
        "    # Save the filtered data to a new Excel file\n",
        "    filtered_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"Filtered data saved to {output_file_path}\")\n",
        "\n",
        "# Specify file paths\n",
        "main_file_path = '/content/2_data_NO_Hotlinks.xlsx'\n",
        "exclude_file_path = '/content/exclude_ids.xlsx'\n",
        "output_file_path = '/content/3_generations_fq-qi.xlsx'\n",
        "\n",
        "# Call the function with the specified paths\n",
        "remove_excluded_records(main_file_path, exclude_file_path, output_file_path)\n",
        "#**********************************************************************************WORKING\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def remove_excluded_records(main_file_path, exclude_file_path, output_file_path):\n",
        "    # Load the main data file\n",
        "    main_df = pd.read_excel(main_file_path)\n",
        "    print(f\"Excel file loaded with hotlinks from {main_file_path} contains {len(main_df)} records.\")\n",
        "\n",
        "    # Load the exclusion list\n",
        "    exclude_df = pd.read_excel(exclude_file_path)\n",
        "    print(f\"Excel file with exclusion IDs from {exclude_file_path} contains {len(exclude_df)} records.\")\n",
        "\n",
        "    # Assuming there's a common column in both dataframes such as 'Link' or 'ID'\n",
        "    # Adjust the column name as per your actual data structure\n",
        "    common_column = 'Link'  # or it could be 'ID' or another unique identifier\n",
        "\n",
        "    # Convert the exclusion dataframe column to a list for filtering\n",
        "    excluded_links = exclude_df[common_column].tolist()\n",
        "\n",
        "    # Filter out the excluded records\n",
        "    filtered_df = main_df[~main_df[common_column].isin(excluded_links)]\n",
        "\n",
        "    # Save the filtered data to a new Excel file\n",
        "    filtered_df.to_excel(output_file_path, index=False)\n",
        "    print(f\"Filtered data saved to {output_file_path} with {len(filtered_df)} records.\")\n",
        "\n",
        "# Specify file paths\n",
        "main_file_path = '/content/2_data_NO_Hotlinks.xlsx'\n",
        "exclude_file_path = '/content/exclude_ids.xlsx'\n",
        "output_file_path = '/content/3_generations_fq-qi.xlsx'\n",
        "\n",
        "# Call the function with the specified paths\n",
        "remove_excluded_records(main_file_path, exclude_file_path, output_file_path)\n",
        "\n",
        "\n",
        "##########################################################################################################\n",
        "##########################################################################################################\n",
        "##########################################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Super visualize\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "\n",
        "def process_data(filepath):\n",
        "    \"\"\"Load data, process it to expand 'Yates DNA Ancestral Line', calculate FQ and QI, sort by frequency, and save results with visual separators.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "    parents_stack = []\n",
        "    starting_plane = 0\n",
        "\n",
        "    # Process each row to expand and calculate\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Yates DNA Ancestral Line'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            plane = i + starting_plane\n",
        "            if len(parents_stack) > i:\n",
        "                parents_stack[i] = node\n",
        "            else:\n",
        "                parents_stack.append(node)\n",
        "            parent = parents_stack[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Gen #': plane,\n",
        "                'Offspring & Spouse': node,\n",
        "                'Parents': parent,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Gen #' and 'Parents' and calculate FQ and QI\n",
        "    results_df = expanded_df.groupby(['Gen #', 'Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        QI=('cM', 'mean')\n",
        "    ).reset_index()\n",
        "    results_df['QI'] = pd.to_numeric(results_df['QI'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    # Filter results where FQ is greater than or equal to 3\n",
        "    final_results_df = results_df[results_df['FQ'] >= 3]\n",
        "\n",
        "    # Sort data by 'Gen #', 'FQ', and then 'Offspring & Spouse' for proper ranking\n",
        "    final_results_df.sort_values(by=['Gen #', 'FQ', 'Offspring & Spouse'], ascending=[True, False, True], inplace=True)\n",
        "\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Gen #', 'Parents', 'Offspring & Spouse', 'FQ', 'QI']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Initialize variables to track last seen parent and generation\n",
        "    last_gen = None\n",
        "    last_parent = None\n",
        "    parent_seen = set()\n",
        "\n",
        "    for r in dataframe_to_rows(final_results_df, index=False, header=False):\n",
        "        current_gen, current_parent = r[0], r[1]\n",
        "\n",
        "        if current_parent not in parent_seen:\n",
        "            parent_seen.add(current_parent)\n",
        "            ws.append(r)\n",
        "        else:\n",
        "            r[1] = ''  # Clear parent name for repeated entries within the same generation\n",
        "            ws.append(r)\n",
        "\n",
        "    # Adding borders, alignment, and fill colors for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    thick = Side(border_style=\"thick\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "\n",
        "    # Define fill colors for different generations\n",
        "    colors = ['FFCCCB', 'CCFFCC']  # Alternate colors for different generations\n",
        "    color_idx = 0\n",
        "\n",
        "    # Apply borders, alignment, and fill colors within the sheet\n",
        "    last_gen = None\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        current_gen = row[0].value\n",
        "        if last_gen is not None and current_gen != last_gen:\n",
        "            color_idx = (color_idx + 1) % len(colors)\n",
        "            for cell in row:\n",
        "                cell.border = Border(top=thick, left=thin, right=thin, bottom=thin)\n",
        "        else:\n",
        "            for cell in row:\n",
        "                cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "\n",
        "        fill = PatternFill(start_color=colors[color_idx], end_color=colors[color_idx], fill_type=\"solid\")\n",
        "        for cell in row:\n",
        "            cell.alignment = center_alignment\n",
        "            cell.fill = fill\n",
        "\n",
        "        last_gen = current_gen\n",
        "\n",
        "    # Save the workbook\n",
        "    output_file = '/content/4_generations_chart_styled.xlsx'\n",
        "    wb.save(output_file)\n",
        "    print(f\"Results successfully saved with styled borders to {output_file}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/3_generations_fq-qi.xlsx'\n",
        "\n",
        "# Process the data\n",
        "process_data(input_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHHa7ykzng9O",
        "outputId": "0eee6ce0-9935-405d-d731-75e114f45fa3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-a6ce81fe657c>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_results_df.sort_values(by=['Gen #', 'FQ', 'Offspring & Spouse'], ascending=[True, False, True], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results successfully saved with styled borders to /content/4_generations_chart_styled.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define the prefix globally\n",
        "prefix_to_remove = 'YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~'\n",
        "\n",
        "def remove_prefix(row):\n",
        "    \"\"\"Function to remove the named prefix from the 'Yates DNA Ancestral Line' column.\"\"\"\n",
        "    ancestral_line = row['Yates DNA Ancestral Line']\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row['Yates DNA Ancestral Line'] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "# Load your DataFrame here\n",
        "# combined_df = pd.read_excel('path_to_your_data.xlsx')\n",
        "\n",
        "# Apply the function to remove the prefix\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "\n",
        "# Verification step: Check if any rows still start with the prefix\n",
        "sample_check = combined_df['Yates DNA Ancestral Line'].str.startswith(prefix_to_remove).any()\n",
        "if not sample_check:\n",
        "    print(\"All prefixes successfully removed.\")\n",
        "else:\n",
        "    print(\"Some prefixes not removed, please check the data.\")\n",
        "\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"Loads data from an Excel file and applies prefix removal.\"\"\"\n",
        "    df = pd.read_excel(file_path)\n",
        "    df = df.apply(remove_prefix, axis=1)\n",
        "    return df\n",
        "\n",
        "def search_for_specific_line(df, line_to_search):\n",
        "    \"\"\"Searches for a specific line in the 'Yates DNA Ancestral Line' after prefix removal.\"\"\"\n",
        "    found = df[df['Yates DNA Ancestral Line'].str.contains(line_to_search, na=False)]\n",
        "    if not found.empty:\n",
        "        print(\"Line still found after prefix removal:\", found)\n",
        "    else:\n",
        "        print(\"Line not found, prefix removal successful.\")\n",
        "\n",
        "def main():\n",
        "    main_data = load_data('/content/main_data_dict.xlsx')\n",
        "    target_line = 'YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~YatesJohnThom&HatfieldeElizabeth~~~YatesRobertJo&DysonMary~~~YatesJosephCh&MarcelisHuybertje~~~YatesRobertJo&DeGraafMargaret~~~YatesJames&JonesMaryAnn~~~YatesDavid&DavisHepsathe~~~YatesJohnFran&StreetRachel~~~YatesWilliam&HartleyRachelMi~~~GriffithCarlAlvi&YatesMaudAlzo~~~GriffithAlvinAll&TracyRoseMary~~~ShuckMelvinEa&GriffithLorettaJ~~~ShuckHarley'\n",
        "    search_for_specific_line(main_data, target_line)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bszQYqspm4",
        "outputId": "28f6a106-3913-411e-a692-94ffe450aa01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All prefixes successfully removed.\n",
            "Line not found, prefix removal successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "def load_data_to_dict(file_path, columns):\n",
        "    \"\"\"Loads Excel data directly into a dictionary.\"\"\"\n",
        "    df = pd.read_excel(file_path, usecols=columns)\n",
        "    print(f\"Initial data load: {len(df)} rows\")\n",
        "    return df.to_dict('records')\n",
        "\n",
        "def extract_id_from_link(data_dict):\n",
        "    \"\"\"Extracts ID from an HTML anchor tag stored in each dictionary entry.\"\"\"\n",
        "    pattern = re.compile(r'personID=(I\\d+)')\n",
        "    count_with_ids = 0\n",
        "    for entry in data_dict:\n",
        "        match = pattern.search(entry['Link'])\n",
        "        if match:\n",
        "            entry['ID'] = match.group(1)\n",
        "            count_with_ids += 1\n",
        "    print(f\"Extracted IDs from HTML: {count_with_ids} rows have IDs\")\n",
        "    return data_dict\n",
        "\n",
        "def apply_exclusion_filter(data_dict, exclude_file_path):\n",
        "    \"\"\"Filters out excluded records based on IDs and returns two dictionaries.\"\"\"\n",
        "    if os.path.exists(exclude_file_path):\n",
        "        df_exclude = pd.read_excel(exclude_file_path)\n",
        "        exclude_ids = set(df_exclude['ID'].astype(str))\n",
        "        remaining = [entry for entry in data_dict if entry.get('ID') not in exclude_ids]\n",
        "        excluded = [entry for entry in data_dict if entry.get('ID') in exclude_ids]\n",
        "        print(f\"Excluded records count: {len(excluded)}\")\n",
        "        print(f\"Expected remaining records count: {len(remaining)}\")\n",
        "        return remaining, excluded\n",
        "    else:\n",
        "        print(f\"{exclude_file_path} not found, no exclusion filter applied.\")\n",
        "        return data_dict, []\n",
        "\n",
        "def save_and_reload_data(remaining_data, filename='remaining_data.xlsx'):\n",
        "    \"\"\"Saves the DataFrame to an Excel file and reloads it to ensure data integrity.\"\"\"\n",
        "    df = pd.DataFrame(remaining_data)\n",
        "    df.to_excel(filename, index=False)\n",
        "    print(f\"Data saved to {filename}.\")\n",
        "    return pd.read_excel(filename)\n",
        "\n",
        "def process_data(data_dict):\n",
        "    \"\"\"Converts dictionary back to DataFrame and performs data processing.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "    print(f\"Data loaded into DataFrame for processing: {len(df)} rows\")\n",
        "\n",
        "    # Assuming your 'Yates DNA Ancestral Line' expansion and filtering logic is correct\n",
        "    expanded_data = []\n",
        "    parents_stack = []\n",
        "    starting_plane = 8\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Yates DNA Ancestral Line'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            plane = i + starting_plane\n",
        "            if len(parents_stack) > i:\n",
        "                parents_stack[i] = node\n",
        "            else:\n",
        "                parents_stack.append(node)\n",
        "            parent = parents_stack[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Gen #': plane,\n",
        "                'Offspring & Spouse': node,\n",
        "                'Parents': parent,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID')\n",
        "            })\n",
        "\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "    filtered_df = expanded_df[expanded_df['Gen #'] >= 8]\n",
        "    results_df = filtered_df.groupby(['Gen #', 'Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        QI=('cM', 'mean')\n",
        "    ).reset_index()\n",
        "    results_df['QI'] = pd.to_numeric(results_df['QI'], errors='coerce').fillna(0).astype(int)\n",
        "\n",
        "    final_results_df = results_df[results_df['FQ'] >= 3]\n",
        "    final_results_df.set_index(['Gen #', 'Parents', 'Offspring & Spouse'], inplace=True)\n",
        "    print(f\"Final data ready for output: {len(final_results_df)} rows\")\n",
        "\n",
        "    return final_results_df\n",
        "\n",
        "def main():\n",
        "    file_path = '/content/main_data_dict.xlsx'\n",
        "    if os.path.exists(file_path):\n",
        "        data_dict = load_data_to_dict(file_path, [\"cM\", \"Yates DNA Ancestral Line\", \"Link\"])\n",
        "        data_dict = extract_id_from_link(data_dict)\n",
        "        remaining_data, excluded_data = apply_exclusion_filter(data_dict, '/content/exclude_ids.xlsx')\n",
        "        remaining_df = save_and_reload_data(remaining_data)\n",
        "        final_df = process_data(remaining_df)\n",
        "        final_df.to_excel('generations_fq-qi.xlsx', index=True)\n",
        "        print(\"Final output saved. generations_fq-qi.xlsx.\")\n",
        "    else:\n",
        "        print(f\"File {file_path} not found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "#***************************************************************************************************************\n",
        "#***************************************************************************************************************\n",
        "#***************************************************************************************************************"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNm70lf3TQPm",
        "outputId": "e6142148-030a-4be6-b97a-c1f6986514ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data load: 1045 rows\n",
            "Extracted IDs from HTML: 1045 rows have IDs\n",
            "Excluded records count: 334\n",
            "Expected remaining records count: 711\n",
            "Data saved to remaining_data.xlsx.\n",
            "Data loaded into DataFrame for processing: 711 rows\n",
            "Final data ready for output: 288 rows\n",
            "Final output saved. generations_fq-qi.xlsx.\n"
          ]
        }
      ]
    }
  ]
}