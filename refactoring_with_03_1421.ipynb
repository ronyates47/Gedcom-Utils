{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPDDDK+kAWae/K+FE6FiDo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/refactoring_with_03_1421.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "640ed5be-75a0-4231-9703-ff56e74e8525",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Downloading XlsxWriter-3.2.2-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 01_28_2033(works!)_2025_stable\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script with Enhanced Optimizations\n",
        "-----------------------------------------------------------\n",
        "This script parses a GEDCOM file, calculates a \"Value\" from shared ancestral segments,\n",
        "computes standardized metrics (Z-Score, Robust Z-Score, Percentile Rank), and creates a\n",
        "Composite Score and descriptor (High, Moderate, Medium, Low, or Very Low) for each record.\n",
        "It uses enhanced parallel processing with chunking, increased caching, and attempts to\n",
        "minimize memory overhead. The final DataFrame (with \"Name\" suppressed) is exported to CSV\n",
        "and HTML (with the Yates DNA Ancestral Line left aligned).\n",
        "\"\"\"\n",
        "\n",
        "# Standard Libraries\n",
        "import csv, glob, logging, functools, os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Global variables used in ancestral processing\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "################################################################################\n",
        "#                                GedcomDataset Class                           #\n",
        "################################################################################\n",
        "class GedcomDataset:\n",
        "    \"\"\"Stores and extracts details for an individual from a GEDCOM file.\"\"\"\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        \"\"\"Store a detail value (e.g., NAME, NPFX) under the given key.\"\"\"\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        \"\"\"Return the individual's ID (without '@') and set the anchor name.\"\"\"\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        \"\"\"Return the anchor name.\"\"\"\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        \"\"\"Return the NPFX field value.\"\"\"\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        \"\"\"Extract and return the cM value from the NPFX field.\"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        \"\"\"Extract and return the sort value (used as 'Match to') from the NPFX field.\"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        \"\"\"Extract and return the YDNA value from the NPFX field.\"\"\"\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        \"\"\"Return the FAMC value without '@' symbols.\"\"\"\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "################################################################################\n",
        "#                           Utility Functions                                  #\n",
        "################################################################################\n",
        "def extract_name(record):\n",
        "    \"\"\"\n",
        "    Extracts the first and last name from a GEDCOM record.\n",
        "    Returns a concatenation of last and first names, or \"UnknownName\" if not found.\n",
        "    \"\"\"\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    if name_start == 5 or name_end == -1:\n",
        "        return \"UnknownName\"\n",
        "    name = record[name_start:name_end].strip()\n",
        "    if '/' not in name:\n",
        "        return name[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    return last_name[:10].rstrip('/').replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}\n",
        "\n",
        "################################################################################\n",
        "#                               Gedcom Class                                   #\n",
        "################################################################################\n",
        "class Gedcom:\n",
        "    \"\"\"\n",
        "    Parses a GEDCOM file and builds a filter pool of individuals with NPFX data.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        return file_name.replace(' ', '_').lower()\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        \"\"\"Parses the GEDCOM file and filters individuals with NPFX data.\"\"\"\n",
        "        global name_to_id\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "                logger.info(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "################################################################################\n",
        "# Caching: Increase lru_cache maxsize for compute_value_for_line\n",
        "################################################################################\n",
        "@functools.lru_cache(maxsize=5000)\n",
        "def compute_value_for_line(ancestral_line, sorted_segments_hashable):\n",
        "    \"\"\"\n",
        "    Compute the Value for an ancestral line using precomputed shared segments.\n",
        "    The second parameter is a frozenset (hashable) version of the shared segments dictionary.\n",
        "    Returns an integer score.\n",
        "    \"\"\"\n",
        "    sorted_segments = dict(sorted_segments_hashable)\n",
        "    if pd.isna(ancestral_line) or ancestral_line.strip() == \"\":\n",
        "        return 0\n",
        "    lines = [line.strip() for line in ancestral_line.split(\"~~~\") if line.strip()]\n",
        "    value = 0\n",
        "    lines_copy = lines.copy()\n",
        "    for segment, freq in sorted_segments.items():\n",
        "        segment_list = list(segment)\n",
        "        counter_seg = Counter(segment_list)\n",
        "        counter_lines = Counter(lines_copy)\n",
        "        if all(counter_lines[k] >= counter_seg[k] for k in counter_seg):\n",
        "            value += freq\n",
        "            for k, cnt in counter_seg.items():\n",
        "                for _ in range(cnt):\n",
        "                    lines_copy.remove(k)\n",
        "    value += len(lines_copy)\n",
        "    return value\n",
        "\n",
        "################################################################################\n",
        "# Other Utility Functions: extract_id, find_parents, etc.\n",
        "################################################################################\n",
        "def extract_id(record):\n",
        "    \"\"\"Extracts the ID from a GEDCOM record.\"\"\"\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    if id_start == 0 or id_end == -1:\n",
        "        return \"UnknownID\"\n",
        "    return record[id_start:id_end].strip()\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in records:\n",
        "        return [path]\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "    return paths\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    \"\"\"Calculates a score based on ancestral paths containing 'Yates'.\"\"\"\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    \"\"\"Processes an individual to build its ancestral line and related details.\"\"\"\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            ydna_value = dataset.get_extractable_YDNA()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()\n",
        "            break\n",
        "    else:\n",
        "        cm_value = ''\n",
        "        sort_value = ''\n",
        "        ydna_value = ''\n",
        "        anchor_gen1 = None\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "    if anchor_gen1 in filtered_ancestral_line_names:\n",
        "        raise ValueError(f\"anchor_gen1 ({anchor_gen1}) was mistakenly included in the ancestral line.\")\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'YDNA': ydna_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "################################################################################\n",
        "# Parallel Processing: Wrapper for processing a single record\n",
        "################################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, records):\n",
        "    \"\"\"\n",
        "    Wrapper to process an individual record.\n",
        "    Returns a list: [individual_id, sort, name, cM, filtered ancestral line string].\n",
        "    \"\"\"\n",
        "    individual_data, filtered_line = process_individual(individual_id, gedcom_instance, records)\n",
        "    cm = individual_data[\"cM\"]\n",
        "    sort = individual_data[\"Sort\"]\n",
        "    name = extract_name(records.get(individual_id, \"\"))\n",
        "    return [individual_id, sort, name, cm, filtered_line]\n",
        "\n",
        "################################################################################\n",
        "# Utility: Function to split list into chunks\n",
        "################################################################################\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "################################################################################\n",
        "# Main Execution\n",
        "################################################################################\n",
        "def main():\n",
        "    # Define select_gedcom_file within main\n",
        "    def select_gedcom_file():\n",
        "        gedcom_files = glob.glob('*.ged')\n",
        "        if not gedcom_files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return gedcom_files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom_file()\n",
        "    if gedcom_file_path:\n",
        "        gedcom_instance = Gedcom(gedcom_file_path)\n",
        "        gedcom_instance.parse_gedcom()\n",
        "        individual_ids = [dataset.get_gen_person() for dataset in gedcom_instance.filter_pool]\n",
        "        print(f\"Records tagged and filtered by NPFX: {len(individual_ids)}\")\n",
        "        def extract_id(record):\n",
        "            id_start = record.find('@') + 1\n",
        "            id_end = record.find('@', id_start)\n",
        "            if id_start == 0 or id_end == -1:\n",
        "                return \"UnknownID\"\n",
        "            return record[id_start:id_end].strip()\n",
        "        with open(gedcom_file_path, 'r', encoding='utf-8') as file:\n",
        "            data = file.read()\n",
        "        data = data.split('\\n0 ')\n",
        "        records = {extract_id(record): record for record in data}\n",
        "    else:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    # Parallel processing: Process records in parallel in chunks.\n",
        "    combined_df_rows = []\n",
        "    chunk_size = 50  # Adjust chunk size as needed\n",
        "    # We'll use a ProcessPoolExecutor with an increased number of workers if available.\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Using %s workers for parallel processing.\", max_workers)\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            # Use functools.partial to fix gedcom_instance and records\n",
        "            process_func = functools.partial(process_record_wrapper, gedcom_instance=gedcom_instance, records=records)\n",
        "            results = list(executor.map(process_func, chunk))\n",
        "            combined_df_rows.extend(results)\n",
        "\n",
        "    ################################################################################\n",
        "    # Create and Populate Main DataFrame (combined_df)\n",
        "    ################################################################################\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "    # Build a value_store dictionary (if needed)\n",
        "    value_store = {}\n",
        "    for _, row in combined_df.iterrows():\n",
        "        value_store[row[\"ID#\"]] = {\n",
        "            \"Match to\": row[\"Match to\"],\n",
        "            \"Name\": row[\"Name\"],\n",
        "            \"cM\": row[\"cM\"],\n",
        "            \"Yates DNA Ancestral Line\": row[\"Yates DNA Ancestral Line\"],\n",
        "            \"Value\": None\n",
        "        }\n",
        "\n",
        "    def remove_prefix(row):\n",
        "        \"\"\"Remove unwanted prefix from the ancestral line.\"\"\"\n",
        "        ancestral_line = row[\"Yates DNA Ancestral Line\"]\n",
        "        prefix_to_remove = \"YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~\"\n",
        "        if ancestral_line.startswith(prefix_to_remove):\n",
        "            row[\"Yates DNA Ancestral Line\"] = ancestral_line[len(prefix_to_remove):]\n",
        "        return row\n",
        "\n",
        "    combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "    ordered_columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\"]\n",
        "    combined_df = combined_df[ordered_columns]\n",
        "    combined_df.index += 1\n",
        "    combined_df.sort_values(by=[\"Match to\", \"Yates DNA Ancestral Line\"], ascending=[False, True], inplace=True)\n",
        "\n",
        "    ################################################################################\n",
        "    # Optimize Segmentation Calculation: Vectorize \"Value\" Calculation with Caching\n",
        "    ################################################################################\n",
        "    def identify_all_shared_segments(df, ancestral_col, min_shared=2, min_size=2):\n",
        "        \"\"\"\n",
        "        Identify all shared segments in the ancestral line.\n",
        "        Returns a dictionary with segment tuples as keys and frequencies as values.\n",
        "        \"\"\"\n",
        "        segment_counts = defaultdict(int)\n",
        "        for _, row in df.iterrows():\n",
        "            if pd.isna(row[ancestral_col]):\n",
        "                continue\n",
        "            lines = [line.strip() for line in row[ancestral_col].split(\"~~~\") if line.strip()]\n",
        "            for size in range(min_size, len(lines) + 1):\n",
        "                for subset in combinations(sorted(lines), size):\n",
        "                    segment_counts[subset] += 1\n",
        "        shared_segments = {segment: count for segment, count in segment_counts.items() if count >= min_shared}\n",
        "        return dict(sorted(shared_segments.items(), key=lambda x: len(x[0]), reverse=True))\n",
        "\n",
        "    df_process = combined_df[[\"ID#\", \"Yates DNA Ancestral Line\"]].copy()\n",
        "    df_process[\"Value\"] = 0\n",
        "    ancestral_col = \"Yates DNA Ancestral Line\"\n",
        "    shared_segments_found = identify_all_shared_segments(df_process, ancestral_col, min_shared=2, min_size=2)\n",
        "    seg_df = pd.DataFrame(\n",
        "        [(\"~~~\".join(seg), freq) for seg, freq in shared_segments_found.items()],\n",
        "        columns=[\"Segment\", \"Frequency\"]\n",
        "    ).sort_values(by=\"Frequency\", ascending=False)\n",
        "    # Convert shared_segments_found to a hashable type\n",
        "    shared_segments_hashable = frozenset(shared_segments_found.items())\n",
        "    df_process[\"Value\"] = df_process[ancestral_col].apply(lambda line: compute_value_for_line(str(line), shared_segments_hashable))\n",
        "    id_to_value_map = df_process.set_index(\"ID#\")[\"Value\"].to_dict()\n",
        "    merged_df = combined_df.merge(df_process[[\"ID#\", \"Value\"]], on=\"ID#\", how=\"left\", suffixes=(\"\", \"_new\"))\n",
        "    if \"Value_new\" in merged_df.columns:\n",
        "        merged_df[\"Value\"] = merged_df[\"Value_new\"]\n",
        "        merged_df.drop(columns=[\"Value_new\"], inplace=True)\n",
        "    combined_df = merged_df\n",
        "    combined_df.sort_values(by=\"Yates DNA Ancestral Line\", ascending=False, inplace=True)\n",
        "\n",
        "    ################################################################################\n",
        "    # Calculate Additional Scores: Z-Score, Robust Z-Score, and Percentile Rank\n",
        "    ################################################################################\n",
        "    combined_df[\"Value\"] = pd.to_numeric(combined_df[\"Value\"], errors=\"coerce\")\n",
        "    mean_value = combined_df[\"Value\"].mean()\n",
        "    std_value = combined_df[\"Value\"].std()\n",
        "    combined_df[\"Z-Score\"] = (combined_df[\"Value\"] - mean_value) / std_value\n",
        "    median_value = combined_df[\"Value\"].median()\n",
        "    mad_value = np.median(np.abs(combined_df[\"Value\"] - median_value))\n",
        "    if mad_value == 0:\n",
        "        combined_df[\"Robust Z-Score\"] = 0\n",
        "    else:\n",
        "        combined_df[\"Robust Z-Score\"] = (combined_df[\"Value\"] - median_value) / (mad_value * 1.4826)\n",
        "    combined_df[\"Percentile Rank\"] = combined_df[\"Value\"].rank(pct=True) * 100\n",
        "\n",
        "    ################################################################################\n",
        "    # Composite Metric Integration:\n",
        "    # Compute \"Composite Score\" as the average of the absolute Z-Score,\n",
        "    # absolute Robust Z-Score, and normalized Percentile Rank,\n",
        "    # then assign a descriptor.\n",
        "    ################################################################################\n",
        "    combined_df[\"Composite Score\"] = (\n",
        "        combined_df[\"Z-Score\"].abs() +\n",
        "        combined_df[\"Robust Z-Score\"].abs() +\n",
        "        (combined_df[\"Percentile Rank\"] / 100)\n",
        "    ) / 3\n",
        "    combined_df[\"Composite Score\"] = combined_df[\"Composite Score\"].round(2)\n",
        "\n",
        "    def assign_descriptor(score):\n",
        "        \"\"\"Assigns a descriptor based on the composite score.\"\"\"\n",
        "        if score >= 2.0:\n",
        "            return \"High\"\n",
        "        elif score >= 1.5:\n",
        "            return \"Moderate\"\n",
        "        elif score >= 1.0:\n",
        "            return \"Medium\"\n",
        "        elif score >= 0.5:\n",
        "            return \"Low\"\n",
        "        else:\n",
        "            return \"Very Low\"\n",
        "\n",
        "    combined_df[\"Composite Significance\"] = combined_df[\"Composite Score\"].apply(assign_descriptor)\n",
        "\n",
        "    ################################################################################\n",
        "    # Repair Final DataFrame Columns:\n",
        "    # Final order: \"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"\n",
        "    # (Note: \"Name\" is suppressed from the output.)\n",
        "    ################################################################################\n",
        "    final_columns = [\"ID#\", \"Match to\", \"Value\", \"Composite Score\", \"Composite Significance\", \"Yates DNA Ancestral Line\"]\n",
        "    combined_df = combined_df[final_columns]\n",
        "    logger.info(\"Final DataFrame columns: %s\", combined_df.columns.tolist())\n",
        "    print(combined_df.head(10))\n",
        "\n",
        "    ################################################################################\n",
        "    # Export the final DataFrame with the additional scores to CSV\n",
        "    ################################################################################\n",
        "    output_filename = \"final_combined_df_with_composite_scores.csv\"\n",
        "    combined_df.to_csv(output_filename, index=False)\n",
        "    logger.info(\"Final DataFrame with composite scores exported to '%s'.\", output_filename)\n",
        "\n",
        "    ################################################################################\n",
        "    # Export the Final DataFrame to HTML with left-aligned Yates DNA Ancestral Line\n",
        "    ################################################################################\n",
        "    html_filename = \"HTML_combined_df_with_composite_scores.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table {\n",
        "      width: 100%;\n",
        "      border-collapse: collapse;\n",
        "      margin: 20px 0;\n",
        "    }\n",
        "    table, th, td {\n",
        "      border: 1px solid #333;\n",
        "    }\n",
        "    th, td {\n",
        "      padding: 8px 12px;\n",
        "      text-align: center;\n",
        "    }\n",
        "    th {\n",
        "      background-color: #f2f2f2;\n",
        "    }\n",
        "    /* Left-align the 6th column (\"Yates DNA Ancestral Line\") */\n",
        "    td:nth-child(6) {\n",
        "      text-align: left;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    html_content = css_style + combined_df.to_html(\n",
        "        index=False,\n",
        "        columns=final_columns,\n",
        "        escape=False\n",
        "    )\n",
        "    with open(html_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Final DataFrame exported to HTML file '%s'.\", html_filename)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qlkx9p7MGJH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "381d4c20-2b96-4dc0-95be-ff96b70d9b27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 499\n",
            "After manual filter, total records: 500\n",
            "Records tagged and filtered by NPFX: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%prun main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k-BQ3CDuBCJv",
        "outputId": "287c211e-d11a-4495-f7f8-2f54b7fe2409"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 58271 total records\n",
            "Records tagged and filtered by NPFX: 1301\n",
            "Records with YDNA information: 76\n",
            "Autosomal matches: 1225\n",
            "Manual filter IDs loaded: 749\n",
            "After manual filter, total records: 750\n",
            "Records tagged and filtered by NPFX: 750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process ForkProcess-3:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 249, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 103, in get\n",
            "    res = self._recv_bytes()\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 437, in _recv_bytes\n",
            "    return self._recv(size)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 402, in _recv\n",
            "    buf.write(chunk)\n",
            "KeyboardInterrupt\n",
            "Exception in thread Thread-11:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 347, in run\n",
            "    self.terminate_broken(cause)\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 499, in terminate_broken\n",
            "    work_item.future.set_exception(bpe)\n",
            "  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 559, in set_exception\n",
            "    raise InvalidStateError('{}: {!r}'.format(self._state, self))\n",
            "concurrent.futures._base.InvalidStateError: CANCELLED: <Future at 0x7d582bd0edd0 state=cancelled>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a8d5cf3feb55>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'main()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-48>\u001b[0m in \u001b[0;36mprun\u001b[0;34m(self, parameter_s, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mprun\u001b[0;34m(self, parameter_s, cell)\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0marg_str\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0marg_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_with_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_with_profiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36m_run_with_profiler\u001b[0;34m(self, code, opts, namespace)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mprof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mprof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0msys_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/cProfile.py\u001b[0m in \u001b[0;36mrunctx\u001b[0;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-03a17fa695cb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# Use functools.partial to fix gedcom_instance and records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mprocess_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_record_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgedcom_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgedcom_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mcombined_df_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process ForkProcess-4:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install line_profiler\n",
        "%lprun -f compute_value_for_line compute_value_for_line(\"YatesWilliam~~~YatesLevi~~~YatesAmbrose\", frozenset({(('YatesWilliam', 'YatesLevi', 'YatesAmbrose'), 5)}))\n"
      ],
      "metadata": {
        "id": "fc55DXRmBHoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "compute_value_for_line(\"YatesWilliam~~~YatesLevi~~~YatesAmbrose\", frozenset({(('YatesWilliam', 'YatesLevi', 'YatesAmbrose'), 5)}))\n"
      ],
      "metadata": {
        "id": "z5z8ZEL_BQZe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}