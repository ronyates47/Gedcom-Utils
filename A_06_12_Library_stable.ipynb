{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWcuoUa2jPCUFzKess81j6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/A_06_12_Library_stable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "2b7c69b9-6371-4596-8ff0-b05781cd3b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1605\n",
        "\n",
        "#5-Make_Parents_Trailing_Descendants\n",
        "\n",
        "#makes************trailing_descendants\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill, Font\n",
        "\n",
        "# Load the provided root_parent_master_processed.xlsx file\n",
        "root_parent_master_processed_path = '/content/root_parent_master_processed.xlsx'  # Update the path as needed\n",
        "root_parent_master_processed_df = pd.read_excel(root_parent_master_processed_path)\n",
        "\n",
        "# Load the root parents from the processed file into a dictionary\n",
        "root_parent_master_updated = root_parent_master_processed_df.set_index('root_parent_master').T.to_dict()\n",
        "\n",
        "# Count and print the full complement of root parent pairs\n",
        "full_complement_count = len(root_parent_master_updated)\n",
        "print(f\"Full complement of root parent pairs: {full_complement_count}\")\n",
        "\n",
        "# Define the fq_threshold value\n",
        "fq_threshold = 5  # Set this value as needed\n",
        "\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(df, root_parent_master):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Add 'DI' column by extracting the first two characters from the 'Parents' column\n",
        "    grouped_df['DI'] = grouped_df['Parents'].apply(lambda x: x[:2] if pd.notnull(x) else '')\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs and root_parent_master dictionary\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in root_parent_master else '', axis=1)\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    grouped_df['Prefix'] = grouped_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    grouped_df['Prefix'] = grouped_df['Prefix'].astype(str)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to fq_threshold\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse', 'Prefix']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= fq_threshold].copy()\n",
        "\n",
        "    return final_results_df, len(df)\n",
        "\n",
        "# Load the provided data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Update the path as needed\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Process the data\n",
        "processed_data_df, total_records = process_data(df, root_parent_master_updated)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(lambda x: x.sort_values(by='FQ', ascending=False)).reset_index(drop=True)\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "# Sort the data\n",
        "sorted_data_df = sort_all_segments(processed_data_df.to_dict(orient='list'))\n",
        "\n",
        "def assemble_descendants(df, total_records, fq_threshold):\n",
        "    \"\"\"Assemble descendants starting from root parents and include FQ, QI, and % Total.\"\"\"\n",
        "    assembled_data = []\n",
        "    di_dict = {}  # Dictionary to store DI values for each ancestor\n",
        "\n",
        "    for root, root_data in root_parent_master_updated.items():\n",
        "        if root_data['FQ'] >= fq_threshold:  # Filter root parents by FQ threshold\n",
        "            di_dict[root] = root[:2]  # Store the DI value for the root\n",
        "            current_generation = [(root, 0)]\n",
        "            assembled_data.append({\n",
        "                'Root': 'Yes',\n",
        "                'Parents and Trailing Descendants': root,\n",
        "                'FQ': root_data['FQ'],\n",
        "                'QI': root_data['QI'],\n",
        "                '% Total': f\"{round(root_data['FQ'] / total_records * 100)}%\",\n",
        "                'DI': di_dict[root]\n",
        "            })\n",
        "            while current_generation:\n",
        "                next_generation = []\n",
        "                for parent, depth in current_generation:\n",
        "                    children = df[df['Parents'] == parent]\n",
        "                    for _, child_row in children.iterrows():\n",
        "                        # Use the DI value from the parent stored in the dictionary\n",
        "                        di = di_dict[parent] if parent in di_dict else child_row['Parents'][:2]\n",
        "                        fq = child_row['FQ']\n",
        "                        qi = child_row['QI']\n",
        "                        percent_total = f\"{round(fq / total_records * 100)}%\"\n",
        "                        descendant = f\"{'  ' * (depth + 1)}→ {child_row['Offspring & Spouse']}\"\n",
        "                        assembled_data.append({\n",
        "                            'Root': '',\n",
        "                            'Parents and Trailing Descendants': descendant,\n",
        "                            'FQ': fq,\n",
        "                            'QI': qi,\n",
        "                            '% Total': percent_total,\n",
        "                            'DI': di\n",
        "                        })\n",
        "                        next_generation.append((child_row['Offspring & Spouse'], depth + 1))\n",
        "                        di_dict[child_row['Offspring & Spouse']] = di  # Propagate the DI value to the next generation\n",
        "                current_generation = next_generation\n",
        "\n",
        "    assembled_df = pd.DataFrame(assembled_data)\n",
        "\n",
        "    # Correctly assign the DI by splitting the first two characters from 'Parents and Trailing Descendants'\n",
        "    assembled_df['DI'] = assembled_df['Parents and Trailing Descendants'].apply(lambda x: x.split('→')[-1].strip()[:2])\n",
        "\n",
        "    # Handle duplicates by ensuring unique DI values\n",
        "    assembled_df['DI'] = assembled_df.apply(lambda row: row['DI'] if row['DI'] != '' else row['Parents and Trailing Descendants'][:2], axis=1)\n",
        "\n",
        "    return assembled_df\n",
        "\n",
        "# Assemble descendants\n",
        "descendants_df = assemble_descendants(pd.DataFrame(sorted_data_df), total_records, fq_threshold)\n",
        "\n",
        "# Print the count of records each time\n",
        "print(f\"Total Records: {total_records}\")\n",
        "print(f\"Filtered root parents count (FQ >= {fq_threshold}): {len(descendants_df[descendants_df['Root'] == 'Yes'])}\")\n",
        "\n",
        "# Output the entire DataFrame to the console\n",
        "print(descendants_df.to_string(index=False))\n",
        "\n",
        "def save_to_excel_with_styles(df, output_path):\n",
        "    \"\"\"Save the assembled data to an Excel file with the desired format and styles.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents and Trailing Descendants', 'FQ', 'QI', '% Total', 'DI']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    header_font = Font(bold=True)\n",
        "    thick = Side(border_style=\"thick\", color=\"000000\")\n",
        "\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "        cell.font = header_font\n",
        "        cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)\n",
        "\n",
        "    # Add data rows to the worksheet\n",
        "    for r in dataframe_to_rows(df, index=False, header=False):\n",
        "        ws.append(r)\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    col_indices = {'FQ': 3, 'QI': 4, '% Total': 5, 'DI': 6}\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=\n",
        "thin, bottom=thin)\n",
        "            if cell.column in col_indices.values():\n",
        "                cell.border = Border(top=thin, left=thick, right=thick, bottom=thin)\n",
        "                cell.alignment = center_alignment\n",
        "            elif cell.column == 2:\n",
        "                cell.alignment = left_alignment\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(output_path)\n",
        "    print(f\"Results successfully saved to {output_path}\")\n",
        "\n",
        "# Save the assembled data to an Excel file with styles\n",
        "output_file_path = '/content/trailing_descendants-2.xlsx'  # Update the path as needed\n",
        "save_to_excel_with_styles(descendants_df, output_file_path)\n",
        "\n",
        "#5-Make_Parents_Trailing_Descendants"
      ],
      "metadata": {
        "id": "LYNzlvPdZwD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defined_names\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update this path as needed\n",
        "data = pd.read_excel(file_path, sheet_name='DNA_Study_Library')\n",
        "\n",
        "# Extract unique First Pair names\n",
        "unique_first_pairs = data['First Pair'].unique()\n",
        "\n",
        "# Function to split the First Pair and extract names\n",
        "def split_first_pair(pair):\n",
        "    parent1, parent2 = pair.split('&')\n",
        "    parent1_name = ''.join([i for i in parent1 if not i.isdigit()])  # Remove digits from name\n",
        "    parent2_name = ''.join([i for i in parent2 if not i.isdigit()])\n",
        "    return parent1_name, parent2_name\n",
        "\n",
        "# Function to format names in \"firstname lastname\" order\n",
        "def format_name(name):\n",
        "    if 'Yates' in name:\n",
        "        parts = name.split('Yates')\n",
        "        if len(parts) == 2:\n",
        "            lastname = 'Yates'\n",
        "            firstname = parts[1]\n",
        "            formatted_name = f\"{firstname.capitalize()} {lastname.capitalize()}\"\n",
        "        else:\n",
        "            formatted_name = name\n",
        "    else:\n",
        "        # Handle non-Yates names\n",
        "        parts = re.findall('[A-Z][^A-Z]*', name)  # Split by uppercase letters\n",
        "        if len(parts) > 1:\n",
        "            lastname = parts[0]\n",
        "            firstname = ' '.join(parts[1:])\n",
        "            formatted_name = f\"{firstname.capitalize()} {lastname.capitalize()}\"\n",
        "        else:\n",
        "            formatted_name = name.capitalize()  # If the pattern doesn't match, return the name as is\n",
        "\n",
        "    return formatted_name\n",
        "\n",
        "# Process each unique First Pair\n",
        "report_data = []\n",
        "for pair in unique_first_pairs:\n",
        "    parent1_name, parent2_name = split_first_pair(pair)\n",
        "    parent1_name_fl = format_name(parent1_name)\n",
        "    parent2_name_fl = format_name(parent2_name)\n",
        "    report_data.append({'First Pair': pair,\n",
        "                        'Parent 1': parent1_name_fl,\n",
        "                        'Parent 2': parent2_name_fl})\n",
        "\n",
        "# Create DataFrame for the report\n",
        "report_df_fl = pd.DataFrame(report_data)\n",
        "\n",
        "# Save the report to an Excel file\n",
        "output_excel_path = '/content/defined_names.xlsx'\n",
        "report_df_fl.to_excel(output_excel_path, index=False)\n",
        "\n",
        "# Provide a link to download the file in Colab\n",
        "from google.colab import files\n",
        "files.download(output_excel_path)\n"
      ],
      "metadata": {
        "id": "rNKH_ilMjZJb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "24b11439-5d68-49ae-dd14-149591bedbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b8830706-8178-4d35-875c-7018746ff391\", \"defined_names.xlsx\", 10567)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [Section 1: Main Script](#section-1a-main script-html_output)\n",
        "- [Section 1: Main Script](#section-1b-main script-parsing & libraries)\n",
        "- [Section 2: Data Cleaning](#section-2-root_parent_processed_list)\n",
        "- [Section 3: Data Analysis](#section-3-root_parent_for_chart)\n",
        "- [Section 4: Data Visualization](#section-4-parents&offspring_chart)\n",
        "- [Section 5: Model Training](#section-5-Make_Parents_Trailing_Descendants)\n",
        "- [Section 6: Model Evaluation](#section-6-Draft_Graph)\n",
        "- [Section 7: Chart of First Parents and Sort Names]\n",
        "- [Section 8: Chart of Sort Name and First Parents]\n",
        "- [Section 9: Matrix-2_Sort Name and First Parents]\n",
        "- [Section 10: Matrix-2_Sort Name and First Parents.xlsx]\n",
        "- [Section 11: 31YatesAbraham&42SearchingStill]\n"
      ],
      "metadata": {
        "id": "M1j-9PnxtsjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing 2\n",
        "\n",
        "# Define the fq_threshold value\n",
        "fq_threshold = 1\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "import os\n",
        "\n",
        "# Hard-coded interval scheme\n",
        "interval_scheme = [\n",
        "    (1000, 1), (1025, 2), (1050, 3), (1075, 4), (1100, 5),\n",
        "    (1125, 6), (1150, 7), (1175, 8), (1200, 9), (1225, 10),\n",
        "    (1250, 11), (1275, 12), (1300, 13), (1325, 14), (1350, 15),\n",
        "    (1375, 16), (1400, 17), (1425, 18), (1450, 19), (1475, 20),\n",
        "    (1500, 21), (1525, 22), (1550, 23), (1575, 24), (1600, 25),\n",
        "    (1625, 26), (1650, 27), (1675, 28), (1700, 29), (1725, 30),\n",
        "    (1750, 31), (1775, 32), (1800, 33), (1825, 34), (1850, 35),\n",
        "    (1875, 36), (1900, 37), (1925, 38), (1950, 39), (1975, 40),\n",
        "    (2000, 41), (2025, 42), (2050, 43), (2075, 44)\n",
        "]\n",
        "\n",
        "# Function to assign interval based on birthdate\n",
        "def assign_interval(birth_date, interval_scheme):\n",
        "    if pd.isnull(birth_date):\n",
        "        return 99\n",
        "    try:\n",
        "        birth_year = int(birth_date[-4:])\n",
        "    except ValueError:\n",
        "        return 99\n",
        "    for year, interval in interval_scheme:\n",
        "        if birth_year <= year:\n",
        "            return interval\n",
        "    return 99\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "    def get_birth_date(self):\n",
        "        return self.extractable_detail.get('BIRTH_DATE', '')\n",
        "\n",
        "    def get_fams(self):\n",
        "        return self.extractable_detail.get('FAMS', '').strip('@')\n",
        "\n",
        "# Set of excluded record IDs\n",
        "excluded_ids = {\n",
        "    'I47640', 'I55585', 'I47666', 'I55586', 'I47570', 'I55587',\n",
        "    'I47569', 'I47641', 'I47571', 'I47572', 'I47549', 'I47550',\n",
        "    'I47573', 'I55588', 'I47548', 'I47551', 'I47553', 'I24502', 'I24503',\n",
        "    'I47659', 'I48070', 'I47660', 'I48129', 'I48130', 'I48126', 'I48127'\n",
        "}\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:30]\n",
        "    last_name = last_name[:30].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                individual_id = tag.strip('@')\n",
        "                if individual_id in excluded_ids:\n",
        "                    continue  # Skip processing for the excluded IDs\n",
        "\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC', 'FAMS', 'BIRT', 'SEX']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'DATE' and current_key == 'BIRT':\n",
        "                    current_dataset.add_extractable_detail('BIRTH_DATE', value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Remove excluded IDs from filter_pool\n",
        "        self.filter_pool = [ds for ds in self.filter_pool if ds.get_gen_person() not in excluded_ids]\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def apply_manual_filter(self):\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "                manual_filtered_ids = set(df['ID'].astype(str))  # Ensure IDs are strings\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids)}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                # Debug output to verify IDs before filtering\n",
        "                print(\"IDs before manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) in manual_filtered_ids]\n",
        "                print(\"IDs after manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def check_and_apply_exclusion_filter(self):\n",
        "        \"\"\"Apply exclusion filter if '/exclude_ids.xlsx' is present.\"\"\"\n",
        "        file_path = '/content/exclude_ids.xlsx'  # Updated path\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df_exclude = pd.read_excel(file_path)\n",
        "                if 'ID' in df_exclude.columns:\n",
        "                    exclude_ids = set(df_exclude['ID'].astype(str))  # Ensure conversion to string\n",
        "                    print(f\"Exclusion filter IDs loaded: {len(exclude_ids)}\")\n",
        "                    print(f\"Sample of IDs to exclude: {list(exclude_ids)[:5]}\")  # Print some sample IDs\n",
        "                    # Apply the exclusion filter\n",
        "                    initial_count = len(self.filter_pool)\n",
        "                    self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) not in exclude_ids]\n",
        "                    print(f\"Excluded {initial_count - len(self.filter_pool)} records based on exclusion IDs.\")\n",
        "                else:\n",
        "                    print(\"Column 'ID' not found in the Excel file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to apply exclusion filter: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"No exclusion filter applied, '{file_path}' not found. Check the path and ensure the file is uploaded to Colab.\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list, add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "# Function to find parents, ensuring excluded IDs are not processed\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id in excluded_ids or individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "# Function to find distant ancestors, ensuring excluded IDs are not processed\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    if individual_id in excluded_ids:\n",
        "        return []\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id and father_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id and mother_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Example usage after parsing and filtering\n",
        "gedcom_instance.parse_gedcom()\n",
        "\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records, interval_scheme):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    birth_date = None  # Initialize birth_date\n",
        "\n",
        "    # Extract cm_value, sort_value, anchor_gen1, and birth_date\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            birth_date = dataset.extractable_detail.get('BIRTH_DATE')  # Get birth date\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "        birth_date = 'N/A'  # Set default if not found\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    ancestors_data = []\n",
        "\n",
        "    # Extract ancestor information and concatenate name with Date Interval\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        ancestor_details = []\n",
        "        for ancestor_id in pair:\n",
        "            if ancestor_id in records:\n",
        "                ancestor_record = records[ancestor_id]\n",
        "                ancestor_name = extract_name(ancestor_record)\n",
        "                ancestor_sex = 'M' if '1 SEX M' in ancestor_record else 'F' if '1 SEX F' in ancestor_record else ''\n",
        "                birth_date_start = ancestor_record.find('2 DATE ') + 7\n",
        "                birth_date_end = ancestor_record.find('\\n', birth_date_start)\n",
        "                ancestor_birth_date = ancestor_record[birth_date_start:birth_date_end].strip() if birth_date_start != -1 else 'N/A'\n",
        "                ancestor_date_interval = assign_interval(ancestor_birth_date, interval_scheme)\n",
        "                ancestor_details.append({\n",
        "                    'ID': ancestor_id,\n",
        "                    'Name': ancestor_name,\n",
        "                    'Sex': ancestor_sex,\n",
        "                    'Birth Date': ancestor_birth_date,\n",
        "                    'Date Interval': ancestor_date_interval\n",
        "                })\n",
        "        if ancestor_details:\n",
        "            name_with_interval_1 = f\"{ancestor_details[0]['Date Interval']}{ancestor_details[0]['Name']}\"\n",
        "            name_with_interval_2 = f\"{ancestor_details[1]['Date Interval']}{ancestor_details[1]['Name']}\" if len(ancestor_details) > 1 else '99Unknown'\n",
        "            combined_name_with_interval = f\"{name_with_interval_1}&{name_with_interval_2}\"\n",
        "            ancestors_data.append(combined_name_with_interval)\n",
        "\n",
        "    # Reverse the order to start with the oldest ancestor\n",
        "    ancestors_data.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(ancestors_data)\n",
        "\n",
        "    # Assign date interval\n",
        "    date_interval = assign_interval(birth_date, interval_scheme)\n",
        "\n",
        "    # Create first-pair by splitting the ancestors full line\n",
        "    first_pair = ancestors_data[0] if ancestors_data else ''\n",
        "\n",
        "    individual_data = {\n",
        "        'Sort': sort_value,\n",
        "        'ID': individual_id,\n",
        "        'Name': extract_name(records[individual_id]),\n",
        "        'Sex': dataset.extractable_detail.get('SEX', ''),\n",
        "        'Birth Date': birth_date,\n",
        "        'Date Interval': date_interval,\n",
        "        'FAMS': dataset.get_fams(),\n",
        "        'FAMC': dataset.get_extractable_FAMC(),\n",
        "        'cM': cm_value,\n",
        "        'First Pair': first_pair,  # Add first-pair here\n",
        "        'Ancestors': filtered_ancestral_line_str  # Add ancestors data to individual data\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "# Initialize the dictionary to store individual data\n",
        "DNA_Study_Library = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJZNgpivobYC",
        "outputId": "c278c106-c8d9-4ff0-faed-4d095817adb4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 1\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 56025 total records\n",
            "Records tagged and filtered by NPFX: 1084\n",
            "Manual filter IDs loaded: 198\n",
            "After manual filter, total records: 199\n",
            "Records tagged and filtered by NPFX: 199\n",
            "GEDCOM contained 56025 total records\n",
            "Records tagged and filtered by NPFX: 1084\n",
            "Manual filter IDs loaded: 198\n",
            "After manual filter, total records: 597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing\n",
        "\n",
        "# Define the fq_threshold value\n",
        "fq_threshold = 1\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "import os\n",
        "\n",
        "# Hard-coded interval scheme\n",
        "interval_scheme = [\n",
        "    (1000, 1), (1025, 2), (1050, 3), (1075, 4), (1100, 5),\n",
        "    (1125, 6), (1150, 7), (1175, 8), (1200, 9), (1225, 10),\n",
        "    (1250, 11), (1275, 12), (1300, 13), (1325, 14), (1350, 15),\n",
        "    (1375, 16), (1400, 17), (1425, 18), (1450, 19), (1475, 20),\n",
        "    (1500, 21), (1525, 22), (1550, 23), (1575, 24), (1600, 25),\n",
        "    (1625, 26), (1650, 27), (1675, 28), (1700, 29), (1725, 30),\n",
        "    (1750, 31), (1775, 32), (1800, 33), (1825, 34), (1850, 35),\n",
        "    (1875, 36), (1900, 37), (1925, 38), (1950, 39), (1975, 40),\n",
        "    (2000, 41), (2025, 42), (2050, 43), (2075, 44)\n",
        "]\n",
        "\n",
        "# Function to assign interval based on birthdate\n",
        "def assign_interval(birth_date, interval_scheme):\n",
        "    if pd.isnull(birth_date):\n",
        "        return 99\n",
        "    try:\n",
        "        birth_year = int(birth_date[-4:])\n",
        "    except ValueError:\n",
        "        return 99\n",
        "    for year, interval in interval_scheme:\n",
        "        if birth_year <= year:\n",
        "            return interval\n",
        "    return 99\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "    def get_birth_date(self):\n",
        "        return self.extractable_detail.get('BIRTH_DATE', '')\n",
        "\n",
        "    def get_fams(self):\n",
        "        return self.extractable_detail.get('FAMS', '').strip('@')\n",
        "\n",
        "# Set of excluded record IDs\n",
        "excluded_ids = {\n",
        "    'I47640', 'I55585', 'I47666', 'I55586', 'I47570', 'I55587',\n",
        "    'I47569', 'I47641', 'I47571', 'I47572', 'I47549', 'I47550',\n",
        "    'I47573', 'I55588', 'I47548', 'I47551', 'I47553', 'I24502', 'I24503',\n",
        "    'I47659', 'I48070', 'I47660', 'I48129', 'I48130', 'I48126', 'I48127'\n",
        "}\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:30]\n",
        "    last_name = last_name[:30].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                individual_id = tag.strip('@')\n",
        "                if individual_id in excluded_ids:\n",
        "                    continue  # Skip processing for the excluded IDs\n",
        "\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC', 'FAMS', 'BIRT', 'SEX']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'DATE' and current_key == 'BIRT':\n",
        "                    current_dataset.add_extractable_detail('BIRTH_DATE', value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Remove excluded IDs from filter_pool\n",
        "        self.filter_pool = [ds for ds in self.filter_pool if ds.get_gen_person() not in excluded_ids]\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def apply_manual_filter(self):\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "                manual_filtered_ids = set(df['ID'].astype(str))  # Ensure IDs are strings\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids)}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                # Debug output to verify IDs before filtering\n",
        "                print(\"IDs before manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) in manual_filtered_ids]\n",
        "                print(\"IDs after manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def check_and_apply_exclusion_filter(self):\n",
        "        \"\"\"Apply exclusion filter if '/exclude_ids.xlsx' is present.\"\"\"\n",
        "        file_path = '/content/exclude_ids.xlsx'  # Updated path\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df_exclude = pd.read_excel(file_path)\n",
        "                if 'ID' in df_exclude.columns:\n",
        "                    exclude_ids = set(df_exclude['ID'].astype(str))  # Ensure conversion to string\n",
        "                    print(f\"Exclusion filter IDs loaded: {len(exclude_ids)}\")\n",
        "                    print(f\"Sample of IDs to exclude: {list(exclude_ids)[:5]}\")  # Print some sample IDs\n",
        "                    # Apply the exclusion filter\n",
        "                    initial_count = len(self.filter_pool)\n",
        "                    self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) not in exclude_ids]\n",
        "                    print(f\"Excluded {initial_count - len(self.filter_pool)} records based on exclusion IDs.\")\n",
        "                else:\n",
        "                    print(\"Column 'ID' not found in the Excel file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to apply exclusion filter: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"No exclusion filter applied, '{file_path}' not found. Check the path and ensure the file is uploaded to Colab.\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list, add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "# Function to find parents, ensuring excluded IDs are not processed\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id in excluded_ids or individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "# Function to find distant ancestors, ensuring excluded IDs are not processed\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    if individual_id in excluded_ids:\n",
        "        return []\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id and father_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id and mother_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Other parts of the script remain the same\n",
        "\n",
        "# Example usage after parsing and filtering\n",
        "gedcom_instance.parse_gedcom()\n",
        "\n",
        "# Save the filter_pool to an Excel file\n",
        "#print_filter_pool_to_excel(gedcom_instance.filter_pool)\n",
        "\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records, interval_scheme):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    birth_date = None  # Initialize birth_date\n",
        "\n",
        "    # Extract cm_value, sort_value, anchor_gen1, and birth_date\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            birth_date = dataset.extractable_detail.get('BIRTH_DATE')  # Get birth date\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "        birth_date = 'N/A'  # Set default if not found\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    ancestors_data = []\n",
        "\n",
        "    # Extract ancestor information and concatenate name with Date Interval\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        ancestor_details = []\n",
        "        for ancestor_id in pair:\n",
        "            if ancestor_id in records:\n",
        "                ancestor_record = records[ancestor_id]\n",
        "                ancestor_name = extract_name(ancestor_record)\n",
        "                ancestor_sex = 'M' if '1 SEX M' in ancestor_record else 'F' if '1 SEX F' in ancestor_record else ''\n",
        "                birth_date_start = ancestor_record.find('2 DATE ') + 7\n",
        "                birth_date_end = ancestor_record.find('\\n', birth_date_start)\n",
        "                ancestor_birth_date = ancestor_record[birth_date_start:birth_date_end].strip() if birth_date_start != -1 else 'N/A'\n",
        "                ancestor_date_interval = assign_interval(ancestor_birth_date, interval_scheme)\n",
        "                ancestor_details.append({\n",
        "                    'ID': ancestor_id,\n",
        "                    'Name': ancestor_name,\n",
        "                    'Sex': ancestor_sex,\n",
        "                    'Birth Date': ancestor_birth_date,\n",
        "                    'Date Interval': ancestor_date_interval\n",
        "                })\n",
        "        if ancestor_details:\n",
        "            name_with_interval_1 = f\"{ancestor_details[0]['Date Interval']}{ancestor_details[0]['Name']}\"\n",
        "            name_with_interval_2 = f\"{ancestor_details[1]['Date Interval']}{ancestor_details[1]['Name']}\" if len(ancestor_details) > 1 else '99Unknown'\n",
        "            combined_name_with_interval = f\"{name_with_interval_1}&{name_with_interval_2}\"\n",
        "            ancestors_data.append(combined_name_with_interval)\n",
        "\n",
        "    # Reverse the order to start with the oldest ancestor\n",
        "    ancestors_data.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(ancestors_data)\n",
        "\n",
        "    # Assign date interval\n",
        "    date_interval = assign_interval(birth_date, interval_scheme)\n",
        "\n",
        "    # Create first-pair by splitting the ancestors full line\n",
        "    first_pair = ancestors_data[0] if ancestors_data else ''\n",
        "\n",
        "    individual_data = {\n",
        "        'Sort': sort_value,\n",
        "        'ID': individual_id,\n",
        "        'Name': extract_name(records[individual_id]),\n",
        "        'Sex': dataset.extractable_detail.get('SEX', ''),\n",
        "        'Birth Date': birth_date,\n",
        "        'Date Interval': date_interval,\n",
        "        'FAMS': dataset.get_fams(),\n",
        "        'FAMC': dataset.get_extractable_FAMC(),\n",
        "        'cM': cm_value,\n",
        "        'First Pair': first_pair,  # Add first-pair here\n",
        "        'Ancestors': filtered_ancestral_line_str  # Add ancestors data to individual data\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "# Initialize the dictionary to store individual data\n",
        "DNA_Study_Library = {}\n",
        "Granular_Data = []\n",
        "\n",
        "# Process individuals and populate the dictionary\n",
        "for dataset in gedcom_instance.filter_pool:  # Assuming filter_pool is iterable\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    # Process Individual and Get Data\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(individual_id, gedcom_instance, records, interval_scheme)\n",
        "\n",
        "    # Store the data in the dictionary\n",
        "    DNA_Study_Library[individual_id] = individual_data\n",
        "\n",
        "    # Collect granular data\n",
        "    for ancestor_pair in generation_table:\n",
        "        generation, pair = ancestor_pair\n",
        "        id1, id2 = pair\n",
        "        if id1 in records and id2 in records:\n",
        "            id1_name = extract_name(records[id1])\n",
        "            id2_name = extract_name(records[id2])\n",
        "            Granular_Data.append({\n",
        "                'Generation': generation,\n",
        "                'ID1': id1,\n",
        "                'Name1': id1_name,\n",
        "                'ID2': id2,\n",
        "                'Name2': id2_name,\n",
        "                'Individual ID': individual_id,\n",
        "                'Individual Name': extract_name(records[individual_id])\n",
        "            })\n",
        "\n",
        "# Function to save the data to an Excel file\n",
        "def save_to_excel(data_dict, filename):\n",
        "    # Convert the dictionary to a pandas DataFrame\n",
        "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
        "\n",
        "    # Save the DataFrame to an Excel file using openpyxl\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, sheet_name='DNA_Study_Library', index=False)  # index=False to prevent writing the index\n",
        "\n",
        "        # Apply some basic formatting\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['DNA_Study_Library']\n",
        "\n",
        "        # Adjust column width\n",
        "        for column in worksheet.columns:\n",
        "            max_length = 0\n",
        "            column = [cell for cell in column]\n",
        "            for cell in column:\n",
        "                try:\n",
        "                    if len(str(cell.value)) > max_length:\n",
        "                        max_length = len(cell.value)\n",
        "                except:\n",
        "                    pass\n",
        "            adjusted_width = (max_length + 2)\n",
        "            worksheet.column_dimensions[column[0].column_letter].width = adjusted_width\n",
        "\n",
        "# Save the DNA Study Library to an Excel file\n",
        "save_to_excel(DNA_Study_Library, 'DNA_Study_Library.xlsx')\n",
        "\n",
        "# Save the granular data to an Excel file\n",
        "granular_df = pd.DataFrame(Granular_Data)\n",
        "granular_df.to_excel('Granular_Data.xlsx', index=False)\n",
        "\n",
        "print(\"Data has been saved to DNA_Study_Library.xlsx and Granular_Data.xlsx\")\n",
        "\n",
        "# End of the script\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMxJNG4hceuP",
        "outputId": "131f6992-d06e-44b2-eefa-5677753b95dc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 1\n",
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 56025 total records\n",
            "Records tagged and filtered by NPFX: 1084\n",
            "Manual filter IDs loaded: 198\n",
            "After manual filter, total records: 199\n",
            "Records tagged and filtered by NPFX: 199\n",
            "GEDCOM contained 56025 total records\n",
            "Records tagged and filtered by NPFX: 1084\n",
            "Manual filter IDs loaded: 198\n",
            "After manual filter, total records: 597\n",
            "Data has been saved to DNA_Study_Library.xlsx and Granular_Data.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVWCghMxefi",
        "outputId": "f80002e4-5b1b-4b3d-d69d-7e44be822c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 56051 total records\n",
            "Records tagged and filtered by NPFX: 1084\n",
            "Manual filter IDs loaded: 198\n",
            "After manual filter, total records: 199\n",
            "Records tagged and filtered by NPFX: 199\n"
          ]
        }
      ],
      "source": [
        "#section-1a-main script-html_output\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "\n",
        "anchor_gen1 = None\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10] # Use slicing syntax to extract the first 10 characters of the first_name variable\n",
        "    last_name = last_name[:10].rstrip('/') # Use slicing syntax to extract the first 10 characters of the last_name variable\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "#class Gedcom:\n",
        "#    def __init__(self, file_name):\n",
        "#        self.file_name = file_name\n",
        "#        self.gedcom_datasets = []\n",
        "#        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            import pandas as pd  # Assuming you haven't imported it yet\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list,add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:10]\n",
        "    last_name = last_name[:10].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "#    print(f\"Distant ancestors paths for {individual_id}: {paths}\")\n",
        "\n",
        "    return paths\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "#global generation_table\n",
        "#global visited_pairs\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "\n",
        "    return matching_table\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "# Main Loop\n",
        "for dataset in filtered_datasets:\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    visited_pairs = set()\n",
        "    generation_table = []\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "#    filtered_ancestral_line_str = \"|\".join(filtered_ancestral_line_names)\n",
        "#    print(f\"Filtered Ancestral Line for {individual_id}: {filtered_ancestral_line_str}\")\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        name_pair = [extract_name(records.get(id, '')) for id in pair]\n",
        "        formatted_name_pair = f\"{name_pair[0]}&{name_pair[1]}\"\n",
        "        filtered_ancestral_line_names.append(formatted_name_pair)\n",
        "\n",
        "    filtered_ancestral_line_names.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(filtered_ancestral_line_names)\n",
        "\n",
        "    individual_data = {\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'Filtered Ancestral Line': filtered_ancestral_line_str\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Global variables\n",
        "visited_pairs = set()\n",
        "combined_df_rows = []  # Initialize your empty combined_df_rows list\n",
        "\n",
        "# Main Loop\n",
        "for dataset in gedcom_instance.filter_pool:  # Assuming filter_pool is iterable\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    # Process Individual and Get Data\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(individual_id, gedcom_instance, records)\n",
        "    cm = individual_data['cM']\n",
        "    sort = individual_data['Sort']\n",
        "    individual_name = extract_name(records.get(individual_id, ''))\n",
        "    # Append to DataFrame Rows\n",
        "    combined_df_rows.append([individual_id, sort, individual_name, cm, filtered_ancestral_line_str])\n",
        "\n",
        "# Create DataFrame\n",
        "columns = ['ID#', 'Match to', 'Name', 'cM', 'Yates DNA Ancestral Line']\n",
        "combined_df = pd.DataFrame(combined_df_rows, columns=columns)\n",
        "\n",
        "# Initialize Link column\n",
        "combined_df['Link'] = combined_df['ID#']\n",
        "\n",
        "# Function to remove the named prefix from the 'Yates DNA Ancestral Line' column\n",
        "def remove_prefix(row):\n",
        "    ancestral_line = row['Yates DNA Ancestral Line']\n",
        "    prefix_to_remove = 'YatesJohn&HydeAlice~~~YatesThomas&WhiteFrances~~~'\n",
        "    if ancestral_line.startswith(prefix_to_remove):\n",
        "        row['Yates DNA Ancestral Line'] = ancestral_line[len(prefix_to_remove):]\n",
        "    return row\n",
        "\n",
        "# Apply the function to remove the prefix\n",
        "combined_df = combined_df.apply(remove_prefix, axis=1)\n",
        "\n",
        "# Function to add hotlinks\n",
        "def create_hotlink(row):\n",
        "    url_base = \"https://yates.one-name.net/tng/verticalchart.php?personID=\"\n",
        "    additional_params = \"&tree=tree1&parentset=0&display=vertical&generations=15\"\n",
        "    if pd.notnull(row['Link']) and pd.notnull(row['ID#']):\n",
        "        if '*' in row['Link']:\n",
        "            return f'<a href=\"{url_base}{row[\"ID#\"]}{additional_params}\">{row[\"Link\"]}</a>'\n",
        "        return f'<a href=\"{url_base}{row[\"ID#\"]}{additional_params}\">{row[\"ID#\"]}</a>'\n",
        "    return ''  # Return an empty string for null values\n",
        "\n",
        "# Ensure the 'ID#' column exists\n",
        "if 'ID#' not in combined_df.columns:\n",
        "    print(\"Error: 'ID#' column not found in DataFrame\")\n",
        "else:\n",
        "    # Apply the hotlink function\n",
        "    combined_df['Link'] = combined_df.apply(create_hotlink, axis=1)\n",
        "    # Drop the 'ID#' column\n",
        "    combined_df = combined_df.drop(columns=['ID#'])\n",
        "\n",
        "# Assume combined_df is your DataFrame\n",
        "ordered_columns = ['Match to', 'Name', 'cM', 'Link', 'Yates DNA Ancestral Line']\n",
        "combined_df = combined_df[ordered_columns]\n",
        "\n",
        "# Update index and sort\n",
        "combined_df.index += 1\n",
        "combined_df.sort_values(by=['Match to', 'Yates DNA Ancestral Line'], ascending=[False, True], inplace=True)\n",
        "\n",
        "# Assume combined_df is your DataFrame\n",
        "ordered_columns = ['Match to', 'Name', 'cM', 'Link', 'Yates DNA Ancestral Line']\n",
        "combined_df = combined_df[ordered_columns]\n",
        "\n",
        "combined_df.sort_values(by=['Yates DNA Ancestral Line', 'Match to'], ascending=[False, False], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Assuming combined_df is your DataFrame\n",
        "\n",
        "# Get the current date and time in the format of YYYY-MM-DD_HHMMSS\n",
        "current_datetime = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
        "\n",
        "# Append the current date and time to your output file names\n",
        "output_html_path = f'/content/htmloutput_{current_datetime}.html'\n",
        "combined_df.to_html(output_html_path, index=False, classes='sortable', escape=False)  # Set escape=False to render HTML links\n",
        "\n",
        "#output_excel_path = f'/content/xlsxoutput_{current_datetime}.xlsx'\n",
        "#combined_df.to_excel(output_excel_path, index=False)\n",
        "\n",
        "# Ensure that combined_df is defined and datetime module is imported\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "##########################################################################################################\n",
        "##########################################################################################################\n",
        "##########################################################################################################\n",
        "\n",
        "#section-1a-main script-html_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section-1b-main script-parsing & libraries\n",
        "\n",
        "#makes********DNA_Study_Library\n",
        "\n",
        "fq_threshold = 1  # Define the fq_threshold value\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "import os\n",
        "\n",
        "# Hard-coded interval scheme\n",
        "interval_scheme = [\n",
        "    (1000, 1), (1025, 2), (1050, 3), (1075, 4), (1100, 5),\n",
        "    (1125, 6), (1150, 7), (1175, 8), (1200, 9), (1225, 10),\n",
        "    (1250, 11), (1275, 12), (1300, 13), (1325, 14), (1350, 15),\n",
        "    (1375, 16), (1400, 17), (1425, 18), (1450, 19), (1475, 20),\n",
        "    (1500, 21), (1525, 22), (1550, 23), (1575, 24), (1600, 25),\n",
        "    (1625, 26), (1650, 27), (1675, 28), (1700, 29), (1725, 30),\n",
        "    (1750, 31), (1775, 32), (1800, 33), (1825, 34), (1850, 35),\n",
        "    (1875, 36), (1900, 37), (1925, 38), (1950, 39), (1975, 40),\n",
        "    (2000, 41), (2025, 42), (2050, 43), (2075, 44)\n",
        "]\n",
        "\n",
        "# Function to assign interval based on birthdate\n",
        "def assign_interval(birth_date, interval_scheme):\n",
        "    if pd.isnull(birth_date):\n",
        "        return 99\n",
        "    try:\n",
        "        birth_year = int(birth_date[-4:])\n",
        "    except ValueError:\n",
        "        return 99\n",
        "    for year, interval in interval_scheme:\n",
        "        if birth_year <= year:\n",
        "            return interval\n",
        "    return 99\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "    def get_birth_date(self):\n",
        "        return self.extractable_detail.get('BIRTH_DATE', '')\n",
        "\n",
        "    def get_fams(self):\n",
        "        return self.extractable_detail.get('FAMS', '').strip('@')\n",
        "\n",
        "# Set of excluded record IDs\n",
        "excluded_ids = {\n",
        "    'I47640', 'I55585', 'I47666', 'I55586', 'I47570', 'I55587',\n",
        "    'I47569', 'I47641', 'I47571', 'I47572', 'I47549', 'I47550',\n",
        "    'I47573', 'I55588', 'I47548', 'I47551', 'I47553', 'I24502', 'I24503',\n",
        "    'I47659', 'I48070', 'I47660', 'I48129', 'I48130', 'I48126', 'I48127'\n",
        "}\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:30]\n",
        "    last_name = last_name[:30].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                individual_id = tag.strip('@')\n",
        "                if individual_id in excluded_ids:\n",
        "                    continue  # Skip processing for the excluded IDs\n",
        "\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC', 'FAMS', 'BIRT', 'SEX']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'DATE' and current_key == 'BIRT':\n",
        "                    current_dataset.add_extractable_detail('BIRTH_DATE', value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Remove excluded IDs from filter_pool\n",
        "        self.filter_pool = [ds for ds in self.filter_pool if ds.get_gen_person() not in excluded_ids]\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def apply_manual_filter(self):\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "                manual_filtered_ids = set(df['ID'].astype(str))  # Ensure IDs are strings\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids)}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                # Debug output to verify IDs before filtering\n",
        "                print(\"IDs before manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) in manual_filtered_ids]\n",
        "                print(\"IDs after manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def check_and_apply_exclusion_filter(self):\n",
        "        \"\"\"Apply exclusion filter if '/exclude_ids.xlsx' is present.\"\"\"\n",
        "        file_path = '/content/exclude_ids.xlsx'  # Updated path\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df_exclude = pd.read_excel(file_path)\n",
        "                if 'ID' in df_exclude.columns:\n",
        "                    exclude_ids = set(df_exclude['ID'].astype(str))  # Ensure conversion to string\n",
        "                    print(f\"Exclusion filter IDs loaded: {len(exclude_ids)}\")\n",
        "                    print(f\"Sample of IDs to exclude: {list(exclude_ids)[:5]}\")  # Print some sample IDs\n",
        "                    # Apply the exclusion filter\n",
        "                    initial_count = len(self.filter_pool)\n",
        "                    self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) not in exclude_ids]\n",
        "                    print(f\"Excluded {initial_count - len(self.filter_pool)} records based on exclusion IDs.\")\n",
        "                else:\n",
        "                    print(\"Column 'ID' not found in the Excel file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to apply exclusion filter: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"No exclusion filter applied, '{file_path}' not found. Check the path and ensure the file is uploaded to Colab.\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list, add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "# Function to find parents, ensuring excluded IDs are not processed\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id in excluded_ids or individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "# Function to find distant ancestors, ensuring excluded IDs are not processed\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    if individual_id in excluded_ids:\n",
        "        return []\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id and father_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id and mother_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Other parts of the script remain the same\n",
        "\n",
        "# Example usage after parsing and filtering\n",
        "gedcom_instance.parse_gedcom()\n",
        "\n",
        "# Save the filter_pool to an Excel file\n",
        "#print_filter_pool_to_excel(gedcom_instance.filter_pool)\n",
        "\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records, interval_scheme):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    birth_date = None  # Initialize birth_date\n",
        "\n",
        "    # Extract cm_value, sort_value, anchor_gen1, and birth_date\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            birth_date = dataset.extractable_detail.get('BIRTH_DATE')  # Get birth date\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "        birth_date = 'N/A'  # Set default if not found\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    ancestors_data = []\n",
        "\n",
        "    # Extract ancestor information and concatenate name with Date Interval\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        ancestor_details = []\n",
        "        for ancestor_id in pair:\n",
        "            if ancestor_id in records:\n",
        "                ancestor_record = records[ancestor_id]\n",
        "                ancestor_name = extract_name(ancestor_record)\n",
        "                ancestor_sex = 'M' if '1 SEX M' in ancestor_record else 'F' if '1 SEX F' in ancestor_record else ''\n",
        "                birth_date_start = ancestor_record.find('2 DATE ') + 7\n",
        "                birth_date_end = ancestor_record.find('\\n', birth_date_start)\n",
        "                ancestor_birth_date = ancestor_record[birth_date_start:birth_date_end].strip() if birth_date_start != -1 else 'N/A'\n",
        "                ancestor_date_interval = assign_interval(ancestor_birth_date, interval_scheme)\n",
        "                ancestor_details.append({\n",
        "                    'ID': ancestor_id,\n",
        "                    'Name': ancestor_name,\n",
        "                    'Sex': ancestor_sex,\n",
        "                    'Birth Date': ancestor_birth_date,\n",
        "                    'Date Interval': ancestor_date_interval\n",
        "                })\n",
        "        if ancestor_details:\n",
        "            name_with_interval_1 = f\"{ancestor_details[0]['Date Interval']}{ancestor_details[0]['Name']}\"\n",
        "            name_with_interval_2 = f\"{ancestor_details[1]['Date Interval']}{ancestor_details[1]['Name']}\" if len(ancestor_details) > 1 else '99Unknown'\n",
        "            combined_name_with_interval = f\"{name_with_interval_1}&{name_with_interval_2}\"\n",
        "            ancestors_data.append(combined_name_with_interval)\n",
        "\n",
        "    # Reverse the order to start with the oldest ancestor\n",
        "    ancestors_data.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(ancestors_data)\n",
        "\n",
        "    # Assign date interval\n",
        "    date_interval = assign_interval(birth_date, interval_scheme)\n",
        "\n",
        "    # Create first-pair by splitting the ancestors full line\n",
        "    first_pair = ancestors_data[0] if ancestors_data else ''\n",
        "\n",
        "    individual_data = {\n",
        "        'Sort': sort_value,\n",
        "        'ID': individual_id,\n",
        "        'Name': extract_name(records[individual_id]),\n",
        "        'Sex': dataset.extractable_detail.get('SEX', ''),\n",
        "        'Birth Date': birth_date,\n",
        "        'Date Interval': date_interval,\n",
        "        'FAMS': dataset.get_fams(),\n",
        "        'FAMC': dataset.get_extractable_FAMC(),\n",
        "        'cM': cm_value,\n",
        "        'First Pair': first_pair,  # Add first-pair here\n",
        "        'Ancestors': filtered_ancestral_line_str  # Add ancestors data to individual data\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "# Initialize the dictionary to store individual data\n",
        "DNA_Study_Library = {}\n",
        "\n",
        "# Process individuals and populate the dictionary\n",
        "for dataset in gedcom_instance.filter_pool:  # Assuming filter_pool is iterable\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    # Process Individual and Get Data\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(individual_id, gedcom_instance, records, interval_scheme)\n",
        "\n",
        "    # Store the data in the dictionary\n",
        "    DNA_Study_Library[individual_id] = individual_data\n",
        "\n",
        "\n",
        "\n",
        "# Function to save the data to an Excel file\n",
        "def save_to_excel(data_dict, filename):\n",
        "    # Convert the dictionary to a pandas DataFrame\n",
        "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
        "\n",
        "    # Save the DataFrame to an Excel file using openpyxl\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, sheet_name='DNA_Study_Library', index=False)  # index=False to prevent writing the index\n",
        "\n",
        "        # Apply some basic formatting\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['DNA_Study_Library']\n",
        "\n",
        "        # Adjust column width\n",
        "        for column in worksheet.columns:\n",
        "            max_length = 0\n",
        "            column = [cell for cell in column]\n",
        "            for cell in column:\n",
        "                try:\n",
        "                    if len(str(cell.value)) > max_length:\n",
        "                        max_length = len(cell.value)\n",
        "                except:\n",
        "                    pass\n",
        "            adjusted_width = (max_length + 2)\n",
        "            worksheet.column_dimensions[column[0].column_letter].width = adjusted_width\n",
        "\n",
        "# Save the DNA Study Library to an Excel file\n",
        "save_to_excel(DNA_Study_Library, 'DNA_Study_Library.xlsx')\n",
        "\n",
        "print(\"Data has been saved to DNA_Study_Library.xlsx\")\n",
        "\n",
        "\n",
        "#section-1b-main script-parsing & libraries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "bs4rp5lwCrdu",
        "outputId": "91a576bb-6271-411b-8376-65c4ca8f9a6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gedcom'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-302ea33300cb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgedcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindividual\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndividualElement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgedcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gedcom'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section-2-root_parent_processed_list\n",
        "\n",
        "#makes**********root_parent_master_processed\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Convert the DataFrame to a dictionary\n",
        "DNA_Study_Library = df.to_dict(orient='records')\n",
        "\n",
        "def calculate_root_counts(DNA_Study_Library):\n",
        "    # Initialize a dictionary to store the counts and sum of cMs of each root node-parent pair\n",
        "    root_count_dict = {}\n",
        "\n",
        "    # Populate the root_count_dict with the counts and sum of cMs of each root node-parent pair\n",
        "    for data in DNA_Study_Library:\n",
        "        first_pair = data['First Pair']\n",
        "        try:\n",
        "            cM_value = int(data['cM'])  # Ensure cM value is an integer\n",
        "        except ValueError:\n",
        "            print(f\"Invalid cM value for {first_pair}: {data['cM']}\")\n",
        "            continue\n",
        "\n",
        "        if first_pair not in root_count_dict:\n",
        "            root_count_dict[first_pair] = {\n",
        "                'count': 1,\n",
        "                'Sum cM': cM_value\n",
        "            }\n",
        "        else:\n",
        "            root_count_dict[first_pair]['count'] += 1\n",
        "            root_count_dict[first_pair]['Sum cM'] += cM_value\n",
        "\n",
        "    # Create the new data structure with the required columns\n",
        "    output_data = []\n",
        "\n",
        "    for first_pair, info in root_count_dict.items():\n",
        "        QI = round(info['Sum cM'] / info['count']) if info['count'] != 0 else 0\n",
        "        output_data.append({\n",
        "            'root_parent_master': first_pair,\n",
        "            'FQ': info['count'],\n",
        "            'QI': QI\n",
        "        })\n",
        "\n",
        "    # Sort the data by 'root_parent_master' alphabetically\n",
        "    output_data.sort(key=lambda x: x['root_parent_master'])\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Function to print the data in the desired console format\n",
        "def print_root_counts(output_data):\n",
        "    print(f\"{'Result':<20}\")\n",
        "    print(pd.DataFrame(output_data, columns=['root_parent_master', 'FQ', 'QI']).to_string(index=True))\n",
        "\n",
        "# Calculate the root counts\n",
        "output_data = calculate_root_counts(DNA_Study_Library)\n",
        "\n",
        "# Print the report in the required format\n",
        "print_root_counts(output_data)\n",
        "\n",
        "# Save the output data to a root_parent_master dictionary\n",
        "root_parent_master = {row['root_parent_master']: row for row in output_data}\n",
        "\n",
        "# Function to save the output data to an Excel file\n",
        "def save_to_excel(output_data, filename):\n",
        "    df = pd.DataFrame(output_data, columns=['root_parent_master', 'FQ', 'QI'])\n",
        "    df.to_excel(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Save the output data to an Excel file in the required format\n",
        "save_to_excel(output_data, '/content/root_parent_master_processed.xlsx')\n",
        "\n",
        "#section-2-root_parent_processed_list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYxXimU7hBkj",
        "outputId": "3fceb80b-e943-44e5-ecfd-caf54b74a5a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result              \n",
            "                       root_parent_master   FQ  QI\n",
            "0          23YatesFrancis&23TichborneJane  178  18\n",
            "1  24YatesJohnThomas&24HatfieldeElizabeth   21  19\n",
            "Data saved to /content/root_parent_master_processed.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section-3-root_parent_for_chart\n",
        "\n",
        "#makes********root_parent_master\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def calculate_root_counts(DNA_Study_Library, fq_limit=5):\n",
        "    # Initialize a dictionary to store the counts and sum of cMs of each root node-parent pair\n",
        "    root_count_dict = {}\n",
        "\n",
        "    # Populate the root_count_dict with the counts and sum of cMs of each root node-parent pair\n",
        "    for data in DNA_Study_Library:\n",
        "        first_pair = data['First Pair']\n",
        "        try:\n",
        "            cM_value = int(data['cM'])  # Ensure cM value is an integer\n",
        "        except ValueError:\n",
        "            print(f\"Invalid cM value for {first_pair}: {data['cM']}\")\n",
        "            continue\n",
        "\n",
        "        if first_pair not in root_count_dict:\n",
        "            root_count_dict[first_pair] = {\n",
        "                'count': 1,\n",
        "                'Sum cM': cM_value\n",
        "            }\n",
        "        else:\n",
        "            root_count_dict[first_pair]['count'] += 1\n",
        "            root_count_dict[first_pair]['Sum cM'] += cM_value\n",
        "\n",
        "    # Create the new data structure with the required columns\n",
        "    output_data = []\n",
        "\n",
        "    for first_pair, info in root_count_dict.items():\n",
        "        if info['count'] >= fq_limit:  # Apply the FQ limiter\n",
        "            QI = round(info['Sum cM'] / info['count']) if info['count'] != 0 else 0\n",
        "            output_data.append({\n",
        "                'Parents': 'Unsure Parents',\n",
        "                'FQ': info['count'],\n",
        "                'QI': QI,\n",
        "                'root_parent_master': first_pair,\n",
        "                '# of Nodes': 0,  # Placeholder for now\n",
        "                'Sum cM': info['Sum cM']\n",
        "            })\n",
        "\n",
        "    # Sort the data by 'root_parent_master' alphabetically\n",
        "    output_data.sort(key=lambda x: x['root_parent_master'])\n",
        "\n",
        "    # Assign node numbers\n",
        "    for idx, row in enumerate(output_data):\n",
        "        row['# of Nodes'] = idx + 1\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Function to print the data\n",
        "def print_root_counts(output_data, limit=None):\n",
        "    print(f\"{'Parents':<15}{'FQ':<5}{'QI':<5}{'root_parent_master':<45}{'# of Nodes':<14}{'Sum cM':<8}\")\n",
        "    for idx, row in enumerate(output_data):\n",
        "        if limit is not None and idx >= limit:  # Limit the number of records printed\n",
        "            break\n",
        "        print(f\"{row['Parents']:<15}{row['FQ']:<5}{row['QI']:<5}{row['root_parent_master']:<45}{row['# of Nodes']:<14}{row['Sum cM']:<8}\")\n",
        "\n",
        "# Calculate the root counts with FQ limit\n",
        "fq_limit = 1\n",
        "output_data = calculate_root_counts(DNA_Study_Library, fq_limit=fq_limit)\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_limit}\")\n",
        "\n",
        "# Print the report\n",
        "print_root_counts(output_data)\n",
        "\n",
        "# Save the output data to a root_parent_master dictionary\n",
        "root_parent_master = {row['root_parent_master']: row for row in output_data}\n",
        "\n",
        "# Function to save the output data to an Excel file\n",
        "def save_to_excel(output_data, filename):\n",
        "    df = pd.DataFrame(output_data)\n",
        "    df.to_excel(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Save the output data to an Excel file\n",
        "save_to_excel(output_data, '/content/root_parent_master.xlsx')\n",
        "\n",
        "#section-3-root_parent_for_chart\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRWCAsuvJrHX",
        "outputId": "d5a94b27-e09f-4a1b-c37f-c77122fc6290"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 1\n",
            "Parents        FQ   QI   root_parent_master                           # of Nodes    Sum cM  \n",
            "Unsure Parents 178  18   23YatesFrancis&23TichborneJane               1             3119    \n",
            "Unsure Parents 21   19   24YatesJohnThomas&24HatfieldeElizabeth       2             394     \n",
            "Data saved to /content/root_parent_master.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#section-4-parents&offspring_chart\n",
        "\n",
        "#makes*********parents&offspring_chart\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook, drawing\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "\n",
        "# Preloaded dictionary\n",
        "# root_parent_master = {row['root_parent_master']: row for row in output_data}\n",
        "\n",
        "# Print the value to confirm it's set correctly\n",
        "\n",
        "fq_threshold = 2  # Define the fq_threshold value\n",
        "\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(filepath, root_parent_master):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Fill NaN values in 'Ancestors' column with empty strings and ensure all values are strings\n",
        "    df['Ancestors'] = df['Ancestors'].fillna('').astype(str)\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs and root_parent_master dictionary\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in root_parent_master else '', axis=1)\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    grouped_df['Prefix'] = grouped_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    grouped_df['Prefix'] = grouped_df['Prefix'].astype(str)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to 10\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse', 'Prefix']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= fq_threshold].copy()\n",
        "\n",
        "    return final_results_df\n",
        "\n",
        "# def sort_within_segment(group):\n",
        "#     \"\"\"Sort the group by the FQ in descending order within each segment.\"\"\"\n",
        "#     return group.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    # sorted_df = df.groupby('Prefix', group_keys=False).apply(sort_within_segment).reset_index(drop=True)\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(lambda x: x).reset_index(drop=True)\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "def print_data_to_console(sorted_df, root_parent_master):\n",
        "    \"\"\"Print the sorted data to the console, inserting rows from the preloaded dictionary.\"\"\"\n",
        "    headers = ['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']\n",
        "    print(f\"{'Root':<5}{'Parents':<20}{'FQ':<5}{'QI':<5}{'Offspring & Spouse':<45}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    last_parent = None\n",
        "    for _, row in sorted_df.iterrows():\n",
        "        root_key = row['Parents']\n",
        "        if root_key in root_parent_master:\n",
        "            root_row = root_parent_master[root_key]\n",
        "            if last_parent != 'Unsure Parents':\n",
        "                print(f\"{'Yes':<5}{'Unsure Parents':<20}{root_row['FQ']:<5}{root_row['QI']:<5}{root_row['root_parent_master']:<45}\")\n",
        "                last_parent = 'Unsure Parents'\n",
        "        if last_parent == row['Parents']:\n",
        "            row['Parents'] = ''\n",
        "        else:\n",
        "            last_parent = row['Parents']\n",
        "        print(f\"{row['Root']:<5}{row['Parents']:<20}{row['FQ']:<5}{row['QI']:<5}{row['Offspring & Spouse']:<45}\")\n",
        "\n",
        "def save_data_to_excel(sorted_df, root_parent_master):\n",
        "    \"\"\"Save the sorted data to an Excel file with styled segments.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "\n",
        "    # Initialize variables to track last seen parent initial and generation\n",
        "    last_parent_initial = None\n",
        "    parent_seen = set()\n",
        "\n",
        "    # Adding the rows and assigning interval offset for staggered node-parents\n",
        "    last_parent = None\n",
        "    for r in dataframe_to_rows(sorted_df, index=False, header=False):\n",
        "        current_parent = r[1]\n",
        "\n",
        "        # Calculate interval offset\n",
        "        if current_parent not in parent_seen:\n",
        "            parent_seen.add(current_parent)\n",
        "            current_parent_prefix = current_parent[:2] if current_parent else ''\n",
        "            if last_parent_initial is not None and current_parent_prefix != last_parent_initial:\n",
        "                # Insert a black row at the end of the previous segment\n",
        "                ws.append([''] * len(headers))\n",
        "                black_row_index = ws.max_row\n",
        "                ws.row_dimensions[black_row_index].height = 5\n",
        "                for cell in ws[black_row_index]:\n",
        "                    cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "            last_parent_initial = current_parent_prefix\n",
        "            if current_parent in root_parent_master:\n",
        "                root_row = root_parent_master[current_parent]\n",
        "                if last_parent != 'Unsure Parents':\n",
        "                    ws.append(['Yes', 'Unsure Parents', root_row['FQ'], root_row['QI'], root_row['root_parent_master']])\n",
        "                    last_parent = 'Unsure Parents'\n",
        "        if last_parent == current_parent:\n",
        "            r[1] = ''  # Clear parent name for repeated entries within the same generation\n",
        "        else:\n",
        "            last_parent = current_parent\n",
        "        ws.append(r)\n",
        "\n",
        "    # Add a black row at the end of the last segment\n",
        "    ws.append([''] * len(headers))\n",
        "    black_row_index = ws.max_row\n",
        "    ws.row_dimensions[black_row_index].height = 5\n",
        "    for cell in ws[black_row_index]:\n",
        "        cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    right_alignment = Alignment(horizontal=\"right\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "\n",
        "        row[1].alignment = right_alignment  # Right-align Parents column\n",
        "        row[4].alignment = left_alignment  # Left-align Offspring & Spouse column\n",
        "\n",
        "    # Replace 'Yes' with an arrow image\n",
        "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=1):\n",
        "        for cell in row:\n",
        "            if cell.value == 'Yes':\n",
        "                if ws.cell(row=cell.row, column=2).value == 'Unsure Parents':  # Check if the Parents column is 'Unsure Parents'\n",
        "                    img = drawing.image.Image('/content/arrow.png')\n",
        "                    img.width, img.height = (25, 25)  # Increase size to 25x25\n",
        "                    img.anchor = cell.coordinate\n",
        "                    ws.add_image(img)\n",
        "                cell.value = ''  # Clear the cell value\n",
        "\n",
        "    # Save the workbook\n",
        "    output_file = '/content/parents&offspring_chart.xlsx'\n",
        "    wb.save(output_file)\n",
        "    print(f\"Results successfully saved with styled colors to {output_file}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Replace with the correct path\n",
        "\n",
        "# Process the data and save it in a dictionary\n",
        "processed_data_dict = process_data(input_file_path, root_parent_master).to_dict(orient='list')\n",
        "\n",
        "# Sort all segments within the data\n",
        "sorted_data = sort_all_segments(processed_data_dict)\n",
        "\n",
        "# Print the sorted data to the console, inserting rows from the preloaded dictionary\n",
        "#print_data_to_console(sorted_data, root_parent_master)\n",
        "\n",
        "# Save the sorted data to an Excel file with formatting\n",
        "save_data_to_excel(sorted_data, root_parent_master)\n",
        "\n",
        "#section-4-parents&offspring_chart\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "begbE-62zZzh",
        "outputId": "9f1adbbe-5d11-4755-e253-7364962c9ba5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Frequency Quotient threshold is set to: 2\n",
            "Results successfully saved with styled colors to /content/parents&offspring_chart.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5-Make_Parents_Trailing_Descendants\n",
        "\n",
        "#makes************trailing_descendants\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill, Font\n",
        "\n",
        "# Load the provided root_parent_master_processed.xlsx file\n",
        "root_parent_master_processed_path = '/content/root_parent_master_processed.xlsx'  # Update the path as needed\n",
        "root_parent_master_processed_df = pd.read_excel(root_parent_master_processed_path)\n",
        "\n",
        "# Load the root parents from the processed file into a dictionary\n",
        "root_parent_master_updated = root_parent_master_processed_df.set_index('root_parent_master').T.to_dict()\n",
        "\n",
        "# Count and print the full complement of root parent pairs\n",
        "full_complement_count = len(root_parent_master_updated)\n",
        "print(f\"Full complement of root parent pairs: {full_complement_count}\")\n",
        "\n",
        "# Define the fq_threshold value\n",
        "fq_threshold = 5  # Set this value as needed\n",
        "\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(df, root_parent_master):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Add 'DI' column by extracting the first two characters from the 'Parents' column\n",
        "    grouped_df['DI'] = grouped_df['Parents'].apply(lambda x: x[:2] if pd.notnull(x) else '')\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs and root_parent_master dictionary\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in root_parent_master else '', axis=1)\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    grouped_df['Prefix'] = grouped_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    grouped_df['Prefix'] = grouped_df['Prefix'].astype(str)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to fq_threshold\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse', 'Prefix']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= fq_threshold].copy()\n",
        "\n",
        "    return final_results_df, len(df)\n",
        "\n",
        "# Load the provided data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Update the path as needed\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Process the data\n",
        "processed_data_df, total_records = process_data(df, root_parent_master_updated)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(lambda x: x.sort_values(by='FQ', ascending=False)).reset_index(drop=True)\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "# Sort the data\n",
        "sorted_data_df = sort_all_segments(processed_data_df.to_dict(orient='list'))\n",
        "\n",
        "def assemble_descendants(df, total_records, fq_threshold):\n",
        "    \"\"\"Assemble descendants starting from root parents and include FQ, QI, and % Total.\"\"\"\n",
        "    assembled_data = []\n",
        "    di_dict = {}  # Dictionary to store DI values for each ancestor\n",
        "\n",
        "    for root, root_data in root_parent_master_updated.items():\n",
        "        if root_data['FQ'] >= fq_threshold:  # Filter root parents by FQ threshold\n",
        "            di_dict[root] = root[:2]  # Store the DI value for the root\n",
        "            current_generation = [(root, 0)]\n",
        "            assembled_data.append({\n",
        "                'Root': 'Yes',\n",
        "                'Parents and Trailing Descendants': root,\n",
        "                'FQ': root_data['FQ'],\n",
        "                'QI': root_data['QI'],\n",
        "                '% Total': f\"{round(root_data['FQ'] / total_records * 100)}%\",\n",
        "                'DI': di_dict[root]\n",
        "            })\n",
        "            while current_generation:\n",
        "                next_generation = []\n",
        "                for parent, depth in current_generation:\n",
        "                    children = df[df['Parents'] == parent]\n",
        "                    for _, child_row in children.iterrows():\n",
        "                        # Use the DI value from the parent stored in the dictionary\n",
        "                        di = di_dict[parent] if parent in di_dict else child_row['Parents'][:2]\n",
        "                        fq = child_row['FQ']\n",
        "                        qi = child_row['QI']\n",
        "                        percent_total = f\"{round(fq / total_records * 100)}%\"\n",
        "                        descendant = f\"{'  ' * (depth + 1)}→ {child_row['Offspring & Spouse']}\"\n",
        "                        assembled_data.append({\n",
        "                            'Root': '',\n",
        "                            'Parents and Trailing Descendants': descendant,\n",
        "                            'FQ': fq,\n",
        "                            'QI': qi,\n",
        "                            '% Total': percent_total,\n",
        "                            'DI': di\n",
        "                        })\n",
        "                        next_generation.append((child_row['Offspring & Spouse'], depth + 1))\n",
        "                        di_dict[child_row['Offspring & Spouse']] = di  # Propagate the DI value to the next generation\n",
        "                current_generation = next_generation\n",
        "\n",
        "    assembled_df = pd.DataFrame(assembled_data)\n",
        "\n",
        "    # Correctly assign the DI by splitting the first two characters from 'Parents and Trailing Descendants'\n",
        "    assembled_df['DI'] = assembled_df['Parents and Trailing Descendants'].apply(lambda x: x.split('→')[-1].strip()[:2])\n",
        "\n",
        "    # Handle duplicates by ensuring unique DI values\n",
        "    assembled_df['DI'] = assembled_df.apply(lambda row: row['DI'] if row['DI'] != '' else row['Parents and Trailing Descendants'][:2], axis=1)\n",
        "\n",
        "    return assembled_df\n",
        "\n",
        "# Assemble descendants\n",
        "descendants_df = assemble_descendants(pd.DataFrame(sorted_data_df), total_records, fq_threshold)\n",
        "\n",
        "# Print the count of records each time\n",
        "print(f\"Total Records: {total_records}\")\n",
        "print(f\"Filtered root parents count (FQ >= {fq_threshold}): {len(descendants_df[descendants_df['Root'] == 'Yes'])}\")\n",
        "\n",
        "# Output the entire DataFrame to the console\n",
        "print(descendants_df.to_string(index=False))\n",
        "\n",
        "def save_to_excel_with_styles(df, output_path):\n",
        "    \"\"\"Save the assembled data to an Excel file with the desired format and styles.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents and Trailing Descendants', 'FQ', 'QI', '% Total', 'DI']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    header_font = Font(bold=True)\n",
        "    thick = Side(border_style=\"thick\", color=\"000000\")\n",
        "\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "        cell.font = header_font\n",
        "        cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)\n",
        "\n",
        "    # Add data rows to the worksheet\n",
        "    for r in dataframe_to_rows(df, index=False, header=False):\n",
        "        ws.append(r)\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    col_indices = {'FQ': 3, 'QI': 4, '% Total': 5, 'DI': 6}\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=\n",
        "thin, bottom=thin)\n",
        "            if cell.column in col_indices.values():\n",
        "                cell.border = Border(top=thin, left=thick, right=thick, bottom=thin)\n",
        "                cell.alignment = center_alignment\n",
        "            elif cell.column == 2:\n",
        "                cell.alignment = left_alignment\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(output_path)\n",
        "    print(f\"Results successfully saved to {output_path}\")\n",
        "\n",
        "# Save the assembled data to an Excel file with styles\n",
        "output_file_path = '/content/trailing_descendants-2.xlsx'  # Update the path as needed\n",
        "save_to_excel_with_styles(descendants_df, output_file_path)\n",
        "\n",
        "#5-Make_Parents_Trailing_Descendants\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHx8Hw70R0yW",
        "outputId": "cad27987-526b-4a75-d4de-36ce21ff4163"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full complement of root parent pairs: 2\n",
            "The Frequency Quotient threshold is set to: 5\n",
            "Total Records: 199\n",
            "Filtered root parents count (FQ >= 5): 2\n",
            "Root                                    Parents and Trailing Descendants  FQ  QI % Total DI\n",
            " Yes                                      23YatesFrancis&23TichborneJane 178  18     89% 23\n",
            "                                       → 24YatesThomas&25StephensDorothy 178  18     89% 24\n",
            "                                → 26YatesJohn&26TettershallMaryElizabeth 149  17     75% 26\n",
            "                                        → 26YatesThomas&42SearchingStill  29  19     15% 26\n",
            "                                             → 27YatesGeorge&27WellsMary 139  17     70% 27\n",
            "                                         → 27YatesJohn&27ParkerElizabeth   8  18      4% 27\n",
            "                                           → 28YatesJames&28WebsterAgnes  29  19     15% 28\n",
            "                                       → 28YatesGeorge&29WarfieldRachael  87  17     44% 28\n",
            "                                         → 28YatesJohn&28TuckerElizabeth  49  17     25% 28\n",
            "                                → 28YatesMartin&28DabridgecourtElizabeth   8  18      4% 28\n",
            "                                   → 29YatesJosephWebster&29RiggMargaret  17  16      9% 29\n",
            "                                       → 30WildmanJacob&30YatesElizabeth   6  28      3% 30\n",
            "                                            → 30YatesGeorge&29GuineyAnne  62  18     31% 30\n",
            "                                         → 30YatesSamuel&30GouldJohannah  23  16     12% 30\n",
            "                                        → 29YatesJohn&30KilgoreElizabeth  45  17     23% 29\n",
            "                                       → 29YatesMartin&30FrenchAnnaMaria   8  18      4% 29\n",
            "                                       → 31YatesWilliam&31GibsonNancyAnn  16  16      8% 31\n",
            "                                  → 31YatesGeorge&31LewisFrancesFielding  30  18     15% 31\n",
            "                                             → 31YatesWilliam&31TappMary  20  17     10% 31\n",
            "                                  → 30YatesMichael&30MarshallMarthaPatsy   8  16      4% 30\n",
            "                                  → 31YatesGeorge&31TuckerElizabethBetty   9  17      5% 31\n",
            "                                        → 31MoneyNicholas&31YatesMaryAnn   5  18      3% 31\n",
            "                                → 31YatesJoshua&31BoydstonMehitableNancy   5  16      3% 31\n",
            "                                             → 32YatesJohn&32RoperJemima  14  17      7% 32\n",
            "                                         → 31YatesStephen&32ParsonsLydia  11  16      6% 31\n",
            "                                       → 32YatesThomas&32RagsdaleRebecca   9  15      5% 32\n",
            "                                            → 31YatesEdward&31FrenchMary   6  20      3% 31\n",
            "                                             → 32YatesJames&32LucasSarah   5  16      3% 32\n",
            "                                   → 31YatesJohn&32GainesElizabethBetsey  11  16      6% 31\n",
            "                                     → 32YatesCharlesLewis&32GoodloeMary   5  17      3% 32\n",
            "                                      → 32YatesGeorge&33BrowningMaryWade   5  20      3% 32\n",
            "                                    → 32YatesWilliam&32WimberlyMaryPolly   8  23      4% 32\n",
            "                                       → 32YatesBenjamin&32ShockleySarah   5  11      3% 32\n",
            "                                 → 32EstesWilliamMarshall&32YatesMarthaL   5  13      3% 32\n",
            "                                            → 32YatesJohn&32DeelCourtney   5  17      3% 32\n",
            "                                        → 32TunnellStephen&32MoneyKeziah   5  18      3% 32\n",
            "                      → 32YatesGeorgeWoodford&32BrowningElizabethLarimer   6  17      3% 32\n",
            "                                       → 34YatesWilliam&34SwainElizabeth   6  26      3% 34\n",
            "                       → 34DobbsSamuelWinfield&34YatesCatherineElizabeth   6  26      3% 34\n",
            " Yes                              24YatesJohnThomas&24HatfieldeElizabeth  21  19     11% 24\n",
            "                                        → 25YatesJohn&25JobeGaitherJoane  17  20      9% 25\n",
            "                           → 31SheltonWilliamJosiah&31YatesHannahEveline   8  19      4% 31\n",
            "                                            → 26YatesRichard&26SmithJane   7  23      4% 26\n",
            "                                               → 28SoperJohn&28YatesMary   6  21      3% 28\n",
            "Results successfully saved to /content/trailing_descendants-2.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 draft\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill, Font\n",
        "\n",
        "# Load the provided root_parent_master_processed.xlsx file\n",
        "root_parent_master_processed_path = '/content/root_parent_master_processed.xlsx'  # Update the path as needed\n",
        "root_parent_master_processed_df = pd.read_excel(root_parent_master_processed_path)\n",
        "\n",
        "# Load the root parents from the processed file into a dictionary\n",
        "root_parent_master_updated = root_parent_master_processed_df.set_index('root_parent_master').T.to_dict()\n",
        "\n",
        "# Count and print the full complement of root parent pairs\n",
        "full_complement_count = len(root_parent_master_updated)\n",
        "print(f\"Full complement of root parent pairs: {full_complement_count}\")\n",
        "\n",
        "# Define the fq_threshold value\n",
        "fq_threshold = 160  # Set this value as needed\n",
        "\n",
        "print(f\"The Frequency Quotient threshold is set to: {fq_threshold}\")\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(df, root_parent_master):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    first_parents = parse_ancestors(df)\n",
        "    expanded_data = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "    grouped_df['DI'] = grouped_df['Parents'].apply(lambda x: x[:2] if pd.notnull(x) else '')\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in root_parent_master else '', axis=1)\n",
        "    grouped_df['Prefix'] = grouped_df['Parents'].str.extract(r'(\\d+)')\n",
        "    grouped_df['Prefix'] = grouped_df['Prefix'].astype(str)\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse', 'Prefix', 'DI']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= fq_threshold].copy()\n",
        "    return final_results_df, len(df)\n",
        "\n",
        "# Load the provided data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Update the path as needed\n",
        "df = pd.read_excel(input_file_path)\n",
        "\n",
        "# Process the data\n",
        "processed_data_df, total_records = process_data(df, root_parent_master_updated)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(lambda x: x.sort_values(by='FQ', ascending=False)).reset_index(drop=True)\n",
        "    return sorted_df\n",
        "\n",
        "# Sort the data\n",
        "sorted_data_df = sort_all_segments(processed_data_df.to_dict(orient='list'))\n",
        "\n",
        "def assemble_descendants(df, total_records, fq_threshold):\n",
        "    \"\"\"Assemble descendants starting from root parents and include FQ, QI, and % Total.\"\"\"\n",
        "    assembled_data = []\n",
        "    di_dict = {}  # Dictionary to store DI values for each ancestor\n",
        "\n",
        "    for root, root_data in root_parent_master_updated.items():\n",
        "        if root_data['FQ'] >= fq_threshold:\n",
        "            di_dict[root] = root[:2]  # Store the DI value for the root\n",
        "            current_generation = [(root, 0)]\n",
        "            assembled_data.append({\n",
        "                'Root': 'Yes',\n",
        "                'Parents and Trailing Descendants': root,\n",
        "                'FQ': root_data['FQ'],\n",
        "                'QI': root_data['QI'],\n",
        "                '% Total': f\"{round(root_data['FQ'] / total_records * 100)}%\",\n",
        "                'DI': int(di_dict[root])  # Convert DI to integer\n",
        "            })\n",
        "            while current_generation:\n",
        "                next_generation = []\n",
        "                for parent, depth in current_generation:\n",
        "                    children = df[df['Parents'] == parent]\n",
        "                    for _, child_row in children.iterrows():\n",
        "                        di = int(di_dict[parent]) if parent in di_dict else int(child_row['Parents'][:2])\n",
        "                        fq = child_row['FQ']\n",
        "                        qi = child_row['QI']\n",
        "                        percent_total = f\"{round(fq / total_records * 100)}%\"\n",
        "                        descendant = f\"{'  ' * (depth + 1)}→ {child_row['Offspring & Spouse']}\"\n",
        "                        assembled_data.append({\n",
        "                            'Root': '',\n",
        "                            'Parents and Trailing Descendants': descendant,\n",
        "                            'FQ': fq,\n",
        "                            'QI': qi,\n",
        "                            '% Total': percent_total,\n",
        "                            'DI': di  # Convert DI to integer\n",
        "                        })\n",
        "                        next_generation.append((child_row['Offspring & Spouse'], depth + 1))\n",
        "                        di_dict[child_row['Offspring & Spouse']] = di  # Propagate the DI value to the next generation\n",
        "                current_generation = next_generation\n",
        "\n",
        "    assembled_df = pd.DataFrame(assembled_data)\n",
        "    assembled_df['DI'] = assembled_df['Parents and Trailing Descendants'].apply(lambda x: int(x.split('→')[-1].strip()[:2]))\n",
        "    assembled_df['DI'] = assembled_df.apply(lambda row: int(row['DI']) if row['DI'] != '' else int(row['Parents and Trailing Descendants'][:2]), axis=1)\n",
        "    assembled_df['FQ'] = assembled_df['FQ'].astype(int)\n",
        "    assembled_df['QI'] = assembled_df['QI'].astype(int)\n",
        "    return assembled_df\n",
        "\n",
        "# Assemble descendants\n",
        "descendants_df = assemble_descendants(pd.DataFrame(sorted_data_df), total_records, fq_threshold)\n",
        "\n",
        "# Print the count of records each time\n",
        "print(f\"Total Records: {total_records}\")\n",
        "print(f\"Filtered root parents count (FQ >= {fq_threshold}): {len(descendants_df[descendants_df['Root'] == 'Yes'])}\")\n",
        "\n",
        "# Output the entire DataFrame to the console\n",
        "print(descendants_df.to_string(index=False))\n",
        "\n",
        "# Save specified columns to a list\n",
        "def save_columns_to_list(df, columns):\n",
        "    data_list = df[columns].values.tolist()\n",
        "    return data_list\n",
        "\n",
        "# Specify columns to be saved\n",
        "columns_to_save = ['Root', 'Parents and Trailing Descendants', 'FQ', 'DI']\n",
        "graph_data = save_columns_to_list(descendants_df, columns_to_save)\n",
        "\n",
        "# Print the data list for verification\n",
        "print(graph_data)\n",
        "\n",
        "def save_to_excel_with_styles(df, output_path):\n",
        "    \"\"\"Save the assembled data to an Excel file with the desired format and styles.\"\"\"\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "    headers = ['Root', 'Parents and Trailing Descendants', 'FQ', 'QI', '% Total', 'DI']\n",
        "    ws.append(headers)\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    header_font = Font(bold=True)\n",
        "    thick = Side(border_style=\"thick\", color=\"000000\")\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "        cell.font = header_font\n",
        "        cell.border = Border(top=thick, left=thick, right=thick, bottom=thick)\n",
        "    for r in dataframe_to_rows(df, index=False, header=False):\n",
        "        ws.append(r)\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "    col_indices = {'FQ': 3, 'QI': 4, '% Total': 5, 'DI': 6}\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "            if cell.column in col_indices.values():\n",
        "                cell.border = Border(top=thin, left=thick, right=thick, bottom=thick)\n",
        "                cell.alignment = center_alignment\n",
        "            elif cell.column == 2:\n",
        "                cell.alignment = left_alignment\n",
        "    wb.save(output_path)\n",
        "    print(f\"Results successfully saved to {output_path}\")\n",
        "\n",
        "# Save the assembled data to an Excel file with styles\n",
        "output_file_path = '/content/trailing_descendants.xlsx'  # Update the path as needed\n",
        "save_to_excel_with_styles(descendants_df, output_file_path)\n",
        "\n",
        "# Plotting the graph\n",
        "interval_scheme = [\n",
        "    (1000, 1), (1025, 2), (1050, 3), (1075, 4), (1100, 5),\n",
        "    (1125, 6), (1150, 7), (1175, 8), (1200, 9), (1225, 10),\n",
        "    (1250, 11), (1275, 12), (1300, 13), (1325, 14), (1350, 15),\n",
        "    (1375, 16), (1400, 17), (1425, 18), (1450, 19), (1475, 20),\n",
        "    (1500, 21), (1525, 22), (1550, 23), (1575, 24), (1600, 25),\n",
        "    (1625, 26), (1650, 27), (1675, 28), (1700, 29), (1725, 30),\n",
        "    (1750, 31), (1775, 32), (1800, 33), (1825, 34), (1850, 35),\n",
        "    (1875, 36), (1900, 37), (1925, 38), (1950, 39), (1975, 40),\n",
        "    (2000, 41), (2025, 42), (2050, 43), (2075, 44)\n",
        "]\n",
        "\n",
        "# Create a mapping for intervals to years\n",
        "interval_to_year = {value: year for year, value in interval_scheme}\n",
        "\n",
        "# Convert DI to integers for proper plotting\n",
        "descendants_df['DI'] = descendants_df['DI'].astype(int)\n",
        "\n",
        "# Plotting the data\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "for i, (root, *data) in enumerate(graph_data):\n",
        "    years = [interval_to_year[di] for _, _, _, di in graph_data]\n",
        "    fqs = [fq for _, _, fq, _ in graph_data]\n",
        "    ax.plot(years, fqs, marker='o', label=f'Root Line {i+1}')\n",
        "\n",
        "ax.set_title('Frequency Quotient Over Time for Root Parents and Their Descendants')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Frequency Quotient (FQ)')\n",
        "ax.legend(title='Root Lines')\n",
        "\n",
        "# Display the plot\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Verify the output Excel file\n",
        "output_file_path\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "f3bQdokiyI8G",
        "outputId": "6b02149f-d39a-4a8b-9e31-29d4386f07ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full complement of root parent pairs: 2\n",
            "The Frequency Quotient threshold is set to: 160\n",
            "Total Records: 199\n",
            "Filtered root parents count (FQ >= 160): 1\n",
            "Root    Parents and Trailing Descendants  FQ  QI % Total  DI\n",
            " Yes      23YatesFrancis&23TichborneJane 178  18     89%  23\n",
            "       → 24YatesThomas&25StephensDorothy 178  18     89%  24\n",
            "[['Yes', '23YatesFrancis&23TichborneJane', 178, 23], ['', '  → 24YatesThomas&25StephensDorothy', 178, 24]]\n",
            "Results successfully saved to /content/trailing_descendants.xlsx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/trailing_descendants.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 7: Chart of First Parents and Sort Names\n",
        "\n",
        "#makes_/content/1-data-firstp_list_matchnames.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update this path as necessary\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Initialize a dictionary to store the results\n",
        "results_dict = {}\n",
        "\n",
        "# Set the frequency threshold\n",
        "frequency_threshold = 1\n",
        "\n",
        "# Function to validate cM calculations\n",
        "def validate_cm_calculation(grouped_data, original_data):\n",
        "    validation_results = []\n",
        "    for sort_name in grouped_data['Sort'].unique():\n",
        "        original_sum_cm = original_data[original_data['Sort'] == sort_name]['cM'].sum()\n",
        "        calculated_sum_cm = grouped_data[grouped_data['Sort'] == sort_name]['Sum_cM'].values[0]\n",
        "        validation_results.append((sort_name, original_sum_cm, calculated_sum_cm))\n",
        "    return validation_results\n",
        "\n",
        "# Calculate statistics for each unique 'First Pair'\n",
        "unique_first_pairs = df['First Pair'].unique()\n",
        "\n",
        "for first_pair in unique_first_pairs:\n",
        "    fp_data = df[df['First Pair'] == first_pair]\n",
        "    total_occurrences = fp_data['Sort'].count()\n",
        "\n",
        "    # Aggregate the data by 'Sort' to handle duplicates\n",
        "    name_stats = fp_data.groupby('Sort').agg({'cM': ['count', 'sum']}).reset_index()\n",
        "    name_stats.columns = ['Sort', 'FQ', 'Sum_cM']\n",
        "    name_stats['Mean_cM'] = (name_stats['Sum_cM'] / name_stats['FQ']).round().astype(int)\n",
        "    name_stats['Percentage'] = (name_stats['FQ'] / total_occurrences * 100).round().astype(int)\n",
        "\n",
        "    # Validate cM calculations\n",
        "    validation_results = validate_cm_calculation(name_stats, fp_data)\n",
        "    for sort_name, original_sum, calculated_sum in validation_results:\n",
        "        if original_sum != calculated_sum:\n",
        "            print(f\"Validation Error: {sort_name} has mismatched cM values. Original: {original_sum}, Calculated: {calculated_sum}\")\n",
        "\n",
        "    # Filter based on the frequency threshold\n",
        "    name_stats = name_stats[name_stats['FQ'] > frequency_threshold]\n",
        "\n",
        "    # Sort the results by FQ\n",
        "    name_stats_sorted = name_stats.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "    # Store the results in the dictionary\n",
        "    results_dict[first_pair] = name_stats_sorted\n",
        "\n",
        "# Create a new workbook and select the active worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Define the header format\n",
        "headers = [\"Root\", \"First Pairs\", \"FQ\", \"QI\", \"% Total\"]\n",
        "ws.append(headers)\n",
        "\n",
        "# Apply header formatting\n",
        "header_font = Font(bold=True)\n",
        "center_alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "for col in range(1, len(headers) + 1):\n",
        "    ws.cell(row=1, column=col).font = header_font\n",
        "    ws.cell(row=1, column=col).alignment = center_alignment\n",
        "\n",
        "# Add the data to the worksheet\n",
        "current_first_pair = None\n",
        "row_index = 2\n",
        "for first_pair, stats in results_dict.items():\n",
        "    if not stats.empty:\n",
        "        ws.append([\"\", \"\", \"\", \"\", \"\"])  # Add a blank line before each new First Pair grouping\n",
        "        total_fq = stats['FQ'].sum()\n",
        "        total_percentage = 100  # Since it's the whole group, it's 100%\n",
        "        ws.append([\"Yes\", first_pair, total_fq, \"\", f\"{total_percentage}%\"])\n",
        "        for col in range(1, len(headers) + 1):\n",
        "            ws.cell(row=row_index, column=col).alignment = center_alignment\n",
        "        row_index += 1\n",
        "\n",
        "        for _, row in stats.iterrows():\n",
        "            sort_name = row['Sort']\n",
        "            fq = row['FQ']\n",
        "            mean_cm = row['Mean_cM']\n",
        "            percentage = row['Percentage']\n",
        "            ws.append([\"\", f\"→ {sort_name}\", fq, mean_cm, f\"{percentage}%\"])\n",
        "            for col in range(1, len(headers) + 1):\n",
        "                ws.cell(row=row_index, column=col).alignment = center_alignment\n",
        "            row_index += 1\n",
        "\n",
        "# Save the workbook\n",
        "output_path = '/content/1-data-firstp_list_matchnames.xlsx'\n",
        "wb.save(output_path)\n",
        "\n",
        "output_path\n",
        "\n",
        "# Section 7: Chart of First Parents and Sort Names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NQYOPKf3VQaP",
        "outputId": "aaf45651-cf85-46ff-df5e-efdb63c23885"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/1-data-firstp_list_matchnames.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 8: Chart of Sort Name and First Parents\n",
        "\n",
        "# makes_/content/2-data-matchnames_list_firstp_.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Group the data by \"Sort\" name and \"First Pair\"\n",
        "grouped = df.groupby(['Sort', 'First Pair']).agg({'cM': ['count', 'mean']}).reset_index()\n",
        "grouped.columns = ['Sort', 'First Pair', 'FQ', 'Mean_cM']\n",
        "\n",
        "# Create a new workbook and select the active worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Apply header formatting\n",
        "headers = [\"Sort Name\", \"First Pair\", \"FQ\", \"Mean cM\", \"%\"]\n",
        "ws.append(headers)\n",
        "\n",
        "header_font = Font(bold=True)\n",
        "center_alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "for col in range(1, len(headers) + 1):\n",
        "    ws.cell(row=1, column=col).font = header_font\n",
        "    ws.cell(row=1, column=col).alignment = center_alignment\n",
        "\n",
        "# Add the data to the worksheet\n",
        "current_sort_name = None\n",
        "row_index = 2\n",
        "\n",
        "for sort_name in grouped['Sort'].unique():\n",
        "    current_group = grouped[grouped['Sort'] == sort_name]\n",
        "    total_fq = current_group['FQ'].sum()\n",
        "\n",
        "    ws.append([sort_name, \"\", \"\", \"\", \"\"])\n",
        "    row_index += 1\n",
        "\n",
        "    for _, row in current_group.iterrows():\n",
        "        first_pair = row['First Pair']\n",
        "        fq = row['FQ']\n",
        "        mean_cm = row['Mean_cM']\n",
        "        percentage = (fq / total_fq) * 100\n",
        "        ws.append([\"\", first_pair, fq, round(mean_cm), f\"{round(percentage)}%\"])\n",
        "        for col in range(1, len(headers) + 1):\n",
        "            ws.cell(row=row_index, column=col).alignment = center_alignment\n",
        "        row_index += 1\n",
        "\n",
        "# Save the workbook\n",
        "output_path = '/content/2-data-matchnames_list_firstp_.xlsx'\n",
        "wb.save(output_path)\n",
        "\n",
        "output_path\n",
        "# Section 8: Chart of Sort Name and First Parents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "r_U4shRdYUoO",
        "outputId": "d2469a68-631f-48ca-af5f-b046e9b0d532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/2-data-matchnames_list_firstp_.xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 9: Matrix-2_Sort Name and First Parents\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update the path if necessary\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Group the data by \"Sort\" name and \"First Pair\" and calculate the FQ\n",
        "grouped = df.groupby(['Sort', 'First Pair']).agg({'cM': 'count'}).reset_index()\n",
        "grouped.columns = ['Sort', 'First Pair', 'FQ']\n",
        "\n",
        "# Create a pivot table with 'First Pair' as rows and 'Sort' as columns\n",
        "pivot_table = grouped.pivot(index='First Pair', columns='Sort', values='FQ').fillna(0)\n",
        "\n",
        "# Add summaries for rows and columns\n",
        "pivot_table['Total FQ'] = pivot_table.sum(axis=1)\n",
        "pivot_table.loc['Total FQ Summary'] = pivot_table.sum(axis=0)\n",
        "\n",
        "# Apply the first limitation: filter the pivot table to only include 'First Pair' rows with 'Total FQ' >= 2\n",
        "pivot_table_first_limit = pivot_table[pivot_table['Total FQ'] >= 2]\n",
        "\n",
        "# Sort the pivot table by 'Total FQ' in descending order\n",
        "pivot_table_first_limit_sorted = pivot_table_first_limit.sort_values(by='Total FQ', ascending=False)\n",
        "\n",
        "# Save the first iteration to a dictionary\n",
        "first_iteration_dict = pivot_table_first_limit_sorted.to_dict()\n",
        "\n",
        "# Apply the second limitation: filter the pivot table to only include 'Sort Name' columns with 'Total FQ' >= 4\n",
        "column_sums = pivot_table_first_limit_sorted.loc['Total FQ Summary']\n",
        "filtered_columns = column_sums[column_sums >= 4].index.tolist()\n",
        "\n",
        "# Specify certain \"Sort Name\" columns not to display\n",
        "exclude_columns = ['milocan', 'addison,david', 'zimkat', 'adamssarah', 'yates,nancy', 'walclif', 'mccollummike']\n",
        "filtered_columns = [col for col in filtered_columns if col not in exclude_columns]\n",
        "\n",
        "# Ensure 'Total FQ' is the last column after sorting columns\n",
        "pivot_table_second_limit = pivot_table_first_limit_sorted[filtered_columns + ['Total FQ']]\n",
        "column_sums = pivot_table_second_limit.loc['Total FQ Summary'].sort_values(ascending=False)\n",
        "sorted_columns = column_sums.index.tolist()\n",
        "sorted_columns.remove('Total FQ')\n",
        "pivot_table_final = pivot_table_second_limit[sorted_columns + ['Total FQ']]\n",
        "\n",
        "# Print the final sorted pivot table with summaries\n",
        "print(\"Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\")\n",
        "print(pivot_table_final.to_string())\n",
        "\n",
        "# Section 9: Matrix-2_Sort Name and First Parents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-k7x9Mu30Sc",
        "outputId": "5e870e7a-c48c-4dd9-98ce-b384511ef37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\n",
            "Sort                                             Total FQ  Total FQ  yates,ronald  marmar  wishardglen  yates,patricial  klingal  yates,timothyb  fridine  weeksjerri  yatesjamesrob  girtain,alma  birdwelljac  yates,andreal  cagilaba,leigh  yates,johnh  hendricksjas  littleil  girtain,kathryn  girtain,andy  cherylod  yates,timothyj  stetlerkar  leedon  harper,mable  yates,robertd  evansdei  franch,mike  henryche  girtain,theresa  yatesjohnh  solyons  girtja  yatesjohnrob  Total FQ  Total FQ\n",
            "First Pair                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "Total FQ Summary                                   1045.0    1045.0         121.0    66.0         54.0             50.0     46.0            43.0     41.0        40.0           39.0          33.0         33.0           30.0            30.0         30.0          27.0      25.0             24.0          23.0      22.0            21.0        20.0    20.0          20.0           19.0      17.0         15.0      14.0             13.0        11.0      9.0     4.0           4.0    1045.0    1045.0\n",
            "23YatesFrancis&23TichborneJane                      507.0     507.0          40.0    45.0         20.0             25.0     31.0            17.0     10.0        17.0           13.0          12.0         17.0           13.0            19.0         20.0          23.0      15.0             11.0          10.0      19.0            13.0        13.0     4.0          14.0            9.0      13.0          4.0       5.0              5.0         0.0      9.0     1.0           4.0     507.0     507.0\n",
            "30YatesBenjamin&42SearchingStill                    159.0     159.0          57.0     1.0         25.0             10.0      0.0             8.0      1.0        10.0           20.0           0.0          8.0            5.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             2.0         0.0     0.0           0.0            0.0       1.0          2.0       5.0              1.0         0.0      0.0     0.0           0.0     159.0     159.0\n",
            "24YatesJohnThomas&24HatfieldeElizabeth               62.0      62.0           3.0     0.0          1.0              4.0      2.0             3.0      9.0         1.0            2.0           2.0          1.0            2.0             1.0          2.0           0.0       0.0              0.0           3.0       0.0             3.0         5.0     1.0           4.0            4.0       0.0          1.0       1.0              1.0         0.0      0.0     0.0           0.0      62.0      62.0\n",
            "31YatesAbraham&42SearchingStill                      24.0      24.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           9.0          0.0            0.0             0.0          0.0           0.0       0.0              3.0           2.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         8.0      0.0     1.0           0.0      24.0      24.0\n",
            "32YatesJames&33SanfordSarah                          21.0      21.0           3.0     0.0          0.0              3.0      1.0             0.0      0.0         1.0            3.0           0.0          0.0            1.0             1.0          3.0           0.0       0.0              0.0           0.0       0.0             0.0         1.0     0.0           1.0            1.0       0.0          0.0       0.0              1.0         0.0      0.0     0.0           0.0      21.0      21.0\n",
            "31YatesJohn&31BarfieldElizabeth                      16.0      16.0           2.0     0.0          2.0              0.0      0.0             1.0      0.0         1.0            1.0           0.0          0.0            1.0             3.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              1.0         0.0      0.0     1.0           0.0      16.0      16.0\n",
            "33YatesWilliam&34ParkerSally                         13.0      13.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             1.0         0.0    12.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0      13.0      13.0\n",
            "27YatesWilliam&27BondMargaretCatherine               12.0      12.0           2.0     1.0          0.0              0.0      0.0             1.0      1.0         1.0            0.0           0.0          0.0            0.0             0.0          1.0           0.0       0.0              1.0           1.0       1.0             0.0         1.0     0.0           0.0            1.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0      12.0      12.0\n",
            "30YatesWilliam&30ThornburyAnne                       12.0      12.0           2.0     1.0          1.0              0.0      2.0             2.0      0.0         0.0            0.0           0.0          0.0            1.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              1.0         0.0      0.0     0.0           0.0      12.0      12.0\n",
            "26YatesJohn&27StrattonUrslyMehitable                  9.0       9.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           2.0          0.0            0.0             1.0          0.0           0.0       0.0              0.0           0.0       0.0             1.0         0.0     0.0           1.0            0.0       0.0          2.0       0.0              0.0         0.0      0.0     0.0           0.0       9.0       9.0\n",
            "28YatesThomas&28DruryMonica                           8.0       8.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            1.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            4.0       1.0          0.0       0.0              2.0         0.0      0.0     0.0           0.0       8.0       8.0\n",
            "32YatesJoseph&32LeighMary                             8.0       8.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       8.0       8.0\n",
            "33YatesWilliam&33EdwardsElizabeth                     7.0       7.0           0.0     0.0          0.0              0.0      0.0             0.0      7.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       7.0       7.0\n",
            "31RoperJamesDavid&31YatesSarahAnn                     6.0       6.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           0.0          2.0            2.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             1.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       6.0       6.0\n",
            "30YatesJohn&30BoswellAnn                              5.0       5.0           1.0     1.0          0.0              0.0      1.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     1.0           0.0       5.0       5.0\n",
            "32YatesJoseph&32AtmarSarah                            5.0       5.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             4.0          1.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "29YatesThomas&42SearchingStill                        5.0       5.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              1.0           2.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "29YatesJames&30McNayJane                              5.0       5.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       1.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          1.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "42YatesPhillip&42SearchingStill                       5.0       5.0           1.0     3.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "34ShepherdJonathan&34YatesEmilyBetty                  4.0       4.0           0.0     0.0          0.0              0.0      4.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "35YatesJohnWesley&35EvansAnn                          4.0       4.0           0.0     0.0          0.0              0.0      0.0             0.0      4.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "26YatesJohn&27StrattonMary                            4.0       4.0           0.0     0.0          1.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "32YatesJoshuaHardy&32StewartMarthaAnn                 4.0       4.0           0.0     0.0          0.0              0.0      0.0             1.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           1.0       0.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "29YatesMartin&42Eliz                                  4.0       4.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         1.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       1.0              0.0           0.0       1.0             0.0         0.0     0.0           0.0            0.0       0.0          1.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "29PhillipsWilliam&29YatesAnn                          4.0       4.0           2.0     0.0          0.0              1.0      0.0             0.0      1.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "29YatesRichard&42Mary                                 4.0       4.0           1.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "34YatesWilliam&34PikeEsther                           3.0       3.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            2.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "34YatesSamuelPorter&35BridgesMartha                   3.0       3.0           0.0     0.0          0.0              0.0      0.0             2.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "33YatesWilliam&33SaltPhoebe                           3.0       3.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           2.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         1.0      0.0     0.0           0.0       3.0       3.0\n",
            "32SwiftAbsalom&33YatesMaryElizabeth                   3.0       3.0           0.0     1.0          2.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "32YatesJohnPryor&33MartinSarah                        3.0       3.0           0.0     0.0          0.0              0.0      1.0             1.0      0.0         1.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "36YatesHarryGeorge&37SheldonGraceAmy                  3.0       3.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       3.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "32YatesWilliam&33NeedhamMary                          3.0       3.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          1.0           0.0       0.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              1.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "32YatesJacob&32VandenberghElizabeth                   3.0       3.0           1.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             1.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     1.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "42YatesJoseph&42MaghullHelen                          2.0       2.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "42YatesNathanJames&42SearchingStill                   2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          2.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "35YatesJames&36SuydamIsobelleElizabeth                2.0       2.0           0.0     0.0          0.0              0.0      0.0             2.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "35YatesEdwardSamuel&35KirkpatrickSarahElizabeth       2.0       2.0           0.0     1.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesWilliamPrice&34McKinneyElizabethAnn            2.0       2.0           1.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          1.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesWilliamBasil&34BullockMartha                   2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       2.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesDarlingBarber&34OliverMaryEmily                2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesThomas&34EvansMary                             2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      2.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesSoloman&34McCammackElizabeth                   2.0       2.0           0.0     0.0          0.0              0.0      1.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           1.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "32YatesJames&33FlournoyMaryAnn                        2.0       2.0           0.0     1.0          1.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33YatesWilliamThomas&33ShelhorseMaryPolly             2.0       2.0           0.0     2.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "31HandJonathan&31YatesRebecca                         2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              2.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33YatesJohn&42SearchingStill                          2.0       2.0           1.0     0.0          0.0              0.0      0.0             1.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33TolerElishaPhilpot&33YatesElizabeth                 2.0       2.0           1.0     1.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33GirtainJonathon&33BarberRebecca                     2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           2.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33CowdenWilliam&33YatesCatherine                      2.0       2.0           0.0     1.0          0.0              0.0      1.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "32YatesHenry&32McManusNancy                           2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       2.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "32YatesLoyd&32BrasfieldMaryPolly                      2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       1.0             0.0         0.0     0.0           0.0            0.0       1.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update the path if necessary\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Group the data by \"Sort\" name and \"First Pair\" and calculate the FQ\n",
        "grouped = df.groupby(['Sort', 'First Pair']).agg({'cM': 'count'}).reset_index()\n",
        "grouped.columns = ['Sort', 'First Pair', 'FQ']\n",
        "\n",
        "# Create a pivot table with 'First Pair' as rows and 'Sort' as columns\n",
        "pivot_table = grouped.pivot(index='First Pair', columns='Sort', values='FQ').fillna(0)\n",
        "\n",
        "# Add summaries for rows and columns\n",
        "pivot_table['Total FQ'] = pivot_table.sum(axis=1)\n",
        "pivot_table.loc['Total FQ Summary'] = pivot_table.sum(axis=0)\n",
        "\n",
        "# Apply the first limitation: filter the pivot table to only include 'First Pair' rows with 'Total FQ' >= 2\n",
        "pivot_table_first_limit = pivot_table[pivot_table['Total FQ'] >= 2]\n",
        "\n",
        "# Sort the pivot table by 'Total FQ' in descending order\n",
        "pivot_table_first_limit_sorted = pivot_table_first_limit.sort_values(by='Total FQ', ascending=False)\n",
        "\n",
        "# Save the first iteration to a dictionary\n",
        "first_iteration_dict = pivot_table_first_limit_sorted.to_dict()\n",
        "\n",
        "# Apply the second limitation: filter the pivot table to only include 'Sort Name' columns with 'Total FQ' >= 4\n",
        "column_sums = pivot_table_first_limit_sorted.loc['Total FQ Summary']\n",
        "filtered_columns = column_sums[column_sums >= 4].index.tolist()\n",
        "\n",
        "# Specify certain \"Sort Name\" columns not to display\n",
        "exclude_columns = ['milocan', 'addison,david', 'zimkat', 'adamssarah', 'yates,nancy', 'walclif', 'mccollummike']\n",
        "filtered_columns = [col for col in filtered_columns if col not in exclude_columns]\n",
        "\n",
        "# Ensure 'Total FQ' is the last column after sorting columns\n",
        "pivot_table_second_limit = pivot_table_first_limit_sorted[filtered_columns + ['Total FQ']]\n",
        "column_sums = pivot_table_second_limit.loc['Total FQ Summary'].sort_values(ascending=False)\n",
        "sorted_columns = column_sums.index.tolist()\n",
        "sorted_columns.remove('Total FQ')\n",
        "pivot_table_final = pivot_table_second_limit[sorted_columns + ['Total FQ']]\n",
        "\n",
        "# Print the final sorted pivot table with summaries\n",
        "print(\"Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\")\n",
        "print(pivot_table_final.to_string())\n",
        "\n",
        "# Save the final result to an Excel file with formatting\n",
        "output_path = '/content/Matrix-2_Sort Name and First Parents.xlsx'\n",
        "\n",
        "# Create a new workbook and select the active worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Apply header formatting\n",
        "headers = [\"First Pair\"] + sorted_columns + [\"Total FQ\"]\n",
        "ws.append(headers)\n",
        "\n",
        "header_font = Font(bold=True)\n",
        "center_alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "for col in range(1, len(headers) + 1):\n",
        "    ws.cell(row=1, column=col).font = header_font\n",
        "    ws.cell(row=1, column=col).alignment = center_alignment\n",
        "\n",
        "# Add the data to the worksheet\n",
        "for row in pivot_table_final.itertuples():\n",
        "    ws.append([row.Index] + list(row[1:]))\n",
        "\n",
        "for row in ws.iter_rows(min_row=2, max_col=ws.max_column, max_row=ws.max_row):\n",
        "    for cell in row:\n",
        "        cell.alignment = center_alignment\n",
        "\n",
        "# Save the workbook\n",
        "wb.save(output_path)\n",
        "\n",
        "# Output the path for download\n",
        "print(f\"Saved the final result to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypOi0T9q6XmZ",
        "outputId": "51e811d9-3e30-4fd3-dca0-363814211071"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\n",
            "Sort                                    Total FQ  Total FQ  yates,ronald  yates,patricial  wishardglen  yates,timothyb  birdwelljac  weeksjerri  yates,timothyj  yates,andreal  yatesjamesrob  franch,mike  Total FQ  Total FQ\n",
            "First Pair                                                                                                                                                                                                                    \n",
            "Total FQ Summary                             199       199            43               29           21              19           18          18              16             15             15            5       199       199\n",
            "23YatesFrancis&23TichborneJane               178       178            40               25           20              16           17          17              13             13             13            4       178       178\n",
            "24YatesJohnThomas&24HatfieldeElizabeth        21        21             3                4            1               3            1           1               3              2              2            1        21        21\n",
            "Saved the final result to /content/Matrix-2_Sort Name and First Parents.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update the path if necessary\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Group the data by \"Sort\" name and \"First Pair\" and calculate the FQ\n",
        "grouped = df.groupby(['Sort', 'First Pair']).agg({'cM': 'count'}).reset_index()\n",
        "grouped.columns = ['Sort', 'First Pair', 'FQ']\n",
        "\n",
        "# Create a pivot table with 'First Pair' as rows and 'Sort' as columns\n",
        "pivot_table = grouped.pivot(index='First Pair', columns='Sort', values='FQ').fillna(0)\n",
        "\n",
        "# Add summaries for rows and columns\n",
        "pivot_table['Total FQ'] = pivot_table.sum(axis=1)\n",
        "pivot_table.loc['Total FQ Summary'] = pivot_table.sum(axis=0)\n",
        "\n",
        "# Apply the first limitation: filter the pivot table to only include 'First Pair' rows with 'Total FQ' >= 2\n",
        "pivot_table_first_limit = pivot_table[pivot_table['Total FQ'] >= 2]\n",
        "\n",
        "# Sort the pivot table by 'Total FQ' in descending order\n",
        "pivot_table_first_limit_sorted = pivot_table_first_limit.sort_values(by='Total FQ', ascending=False)\n",
        "\n",
        "# Save the first iteration to a dictionary\n",
        "first_iteration_dict = pivot_table_first_limit_sorted.to_dict()\n",
        "\n",
        "# Apply the second limitation: filter the pivot table to only include 'Sort Name' columns with 'Total FQ' >= 4\n",
        "column_sums = pivot_table_first_limit_sorted.loc['Total FQ Summary']\n",
        "filtered_columns = column_sums[column_sums >= 4].index.tolist()\n",
        "\n",
        "# Specify certain \"Sort Name\" columns not to display\n",
        "exclude_columns = ['milocan', 'addison,david', 'zimkat', 'adamssarah', 'yates,nancy', 'walclif', 'mccollummike']\n",
        "filtered_columns = [col for col in filtered_columns if col not in exclude_columns]\n",
        "\n",
        "# Ensure 'Total FQ' is the last column after sorting columns\n",
        "pivot_table_second_limit = pivot_table_first_limit_sorted[filtered_columns + ['Total FQ']]\n",
        "column_sums = pivot_table_second_limit.loc['Total FQ Summary'].sort_values(ascending=False)\n",
        "sorted_columns = column_sums.index.tolist()\n",
        "sorted_columns.remove('Total FQ')\n",
        "pivot_table_final = pivot_table_second_limit[sorted_columns + ['Total FQ']]\n",
        "\n",
        "# Print the final sorted pivot table with summaries\n",
        "print(\"Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\")\n",
        "print(pivot_table_final.to_string())\n",
        "\n",
        "# Save the final result to an Excel file with formatting\n",
        "output_path = '/content/Matrix-2_Sort Name and First Parents.xlsx'\n",
        "\n",
        "# Create a new workbook and select the active worksheet\n",
        "wb = Workbook()\n",
        "ws = wb.active\n",
        "\n",
        "# Apply header formatting\n",
        "headers = [\"First Pair\"] + sorted_columns + [\"Total FQ\"]\n",
        "ws.append(headers)\n",
        "\n",
        "header_font = Font(bold=True)\n",
        "center_alignment = Alignment(horizontal=\"center\")\n",
        "\n",
        "for col in range(1, len(headers) + 1):\n",
        "    ws.cell(row=1, column=col).font = header_font\n",
        "    ws.cell(row=1, column=col).alignment = center_alignment\n",
        "\n",
        "# Add the data to the worksheet\n",
        "for row in pivot_table_final.itertuples():\n",
        "    ws.append([row.Index] + list(row[1:]))\n",
        "\n",
        "for row in ws.iter_rows(min_row=2, max_col=ws.max_column, max_row=ws.max_row):\n",
        "    for cell in row:\n",
        "        cell.alignment = center_alignment\n",
        "\n",
        "# Save the workbook\n",
        "wb.save(output_path)\n",
        "\n",
        "# Output the path for download\n",
        "print(f\"Saved the final result to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPr-SLH98cyd",
        "outputId": "2e8061f0-53a2-491a-8078-fa97dc4655c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\n",
            "Sort                                             Total FQ  Total FQ  yates,ronald  marmar  wishardglen  yates,patricial  klingal  yates,timothyb  fridine  weeksjerri  yatesjamesrob  girtain,alma  birdwelljac  yates,andreal  cagilaba,leigh  yates,johnh  hendricksjas  littleil  girtain,kathryn  girtain,andy  cherylod  yates,timothyj  stetlerkar  leedon  harper,mable  yates,robertd  evansdei  franch,mike  henryche  girtain,theresa  yatesjohnh  solyons  girtja  yatesjohnrob  Total FQ  Total FQ\n",
            "First Pair                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "Total FQ Summary                                   1045.0    1045.0         121.0    66.0         54.0             50.0     46.0            43.0     41.0        40.0           39.0          33.0         33.0           30.0            30.0         30.0          27.0      25.0             24.0          23.0      22.0            21.0        20.0    20.0          20.0           19.0      17.0         15.0      14.0             13.0        11.0      9.0     4.0           4.0    1045.0    1045.0\n",
            "23YatesFrancis&23TichborneJane                      507.0     507.0          40.0    45.0         20.0             25.0     31.0            17.0     10.0        17.0           13.0          12.0         17.0           13.0            19.0         20.0          23.0      15.0             11.0          10.0      19.0            13.0        13.0     4.0          14.0            9.0      13.0          4.0       5.0              5.0         0.0      9.0     1.0           4.0     507.0     507.0\n",
            "30YatesBenjamin&42SearchingStill                    159.0     159.0          57.0     1.0         25.0             10.0      0.0             8.0      1.0        10.0           20.0           0.0          8.0            5.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             2.0         0.0     0.0           0.0            0.0       1.0          2.0       5.0              1.0         0.0      0.0     0.0           0.0     159.0     159.0\n",
            "24YatesJohnThomas&24HatfieldeElizabeth               62.0      62.0           3.0     0.0          1.0              4.0      2.0             3.0      9.0         1.0            2.0           2.0          1.0            2.0             1.0          2.0           0.0       0.0              0.0           3.0       0.0             3.0         5.0     1.0           4.0            4.0       0.0          1.0       1.0              1.0         0.0      0.0     0.0           0.0      62.0      62.0\n",
            "31YatesAbraham&42SearchingStill                      24.0      24.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           9.0          0.0            0.0             0.0          0.0           0.0       0.0              3.0           2.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         8.0      0.0     1.0           0.0      24.0      24.0\n",
            "32YatesJames&33SanfordSarah                          21.0      21.0           3.0     0.0          0.0              3.0      1.0             0.0      0.0         1.0            3.0           0.0          0.0            1.0             1.0          3.0           0.0       0.0              0.0           0.0       0.0             0.0         1.0     0.0           1.0            1.0       0.0          0.0       0.0              1.0         0.0      0.0     0.0           0.0      21.0      21.0\n",
            "31YatesJohn&31BarfieldElizabeth                      16.0      16.0           2.0     0.0          2.0              0.0      0.0             1.0      0.0         1.0            1.0           0.0          0.0            1.0             3.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              1.0         0.0      0.0     1.0           0.0      16.0      16.0\n",
            "33YatesWilliam&34ParkerSally                         13.0      13.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             1.0         0.0    12.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0      13.0      13.0\n",
            "27YatesWilliam&27BondMargaretCatherine               12.0      12.0           2.0     1.0          0.0              0.0      0.0             1.0      1.0         1.0            0.0           0.0          0.0            0.0             0.0          1.0           0.0       0.0              1.0           1.0       1.0             0.0         1.0     0.0           0.0            1.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0      12.0      12.0\n",
            "30YatesWilliam&30ThornburyAnne                       12.0      12.0           2.0     1.0          1.0              0.0      2.0             2.0      0.0         0.0            0.0           0.0          0.0            1.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              1.0         0.0      0.0     0.0           0.0      12.0      12.0\n",
            "26YatesJohn&27StrattonUrslyMehitable                  9.0       9.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           2.0          0.0            0.0             1.0          0.0           0.0       0.0              0.0           0.0       0.0             1.0         0.0     0.0           1.0            0.0       0.0          2.0       0.0              0.0         0.0      0.0     0.0           0.0       9.0       9.0\n",
            "28YatesThomas&28DruryMonica                           8.0       8.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            1.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            4.0       1.0          0.0       0.0              2.0         0.0      0.0     0.0           0.0       8.0       8.0\n",
            "32YatesJoseph&32LeighMary                             8.0       8.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       8.0       8.0\n",
            "33YatesWilliam&33EdwardsElizabeth                     7.0       7.0           0.0     0.0          0.0              0.0      0.0             0.0      7.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       7.0       7.0\n",
            "31RoperJamesDavid&31YatesSarahAnn                     6.0       6.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           0.0          2.0            2.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             1.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       6.0       6.0\n",
            "30YatesJohn&30BoswellAnn                              5.0       5.0           1.0     1.0          0.0              0.0      1.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     1.0           0.0       5.0       5.0\n",
            "32YatesJoseph&32AtmarSarah                            5.0       5.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             4.0          1.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "29YatesThomas&42SearchingStill                        5.0       5.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              1.0           2.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "29YatesJames&30McNayJane                              5.0       5.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       1.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          1.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "42YatesPhillip&42SearchingStill                       5.0       5.0           1.0     3.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       5.0       5.0\n",
            "34ShepherdJonathan&34YatesEmilyBetty                  4.0       4.0           0.0     0.0          0.0              0.0      4.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "35YatesJohnWesley&35EvansAnn                          4.0       4.0           0.0     0.0          0.0              0.0      0.0             0.0      4.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "26YatesJohn&27StrattonMary                            4.0       4.0           0.0     0.0          1.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "32YatesJoshuaHardy&32StewartMarthaAnn                 4.0       4.0           0.0     0.0          0.0              0.0      0.0             1.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           1.0       0.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "29YatesMartin&42Eliz                                  4.0       4.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         1.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       1.0              0.0           0.0       1.0             0.0         0.0     0.0           0.0            0.0       0.0          1.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "29PhillipsWilliam&29YatesAnn                          4.0       4.0           2.0     0.0          0.0              1.0      0.0             0.0      1.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "29YatesRichard&42Mary                                 4.0       4.0           1.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       4.0       4.0\n",
            "34YatesWilliam&34PikeEsther                           3.0       3.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            2.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "34YatesSamuelPorter&35BridgesMartha                   3.0       3.0           0.0     0.0          0.0              0.0      0.0             2.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "33YatesWilliam&33SaltPhoebe                           3.0       3.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           2.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         1.0      0.0     0.0           0.0       3.0       3.0\n",
            "32SwiftAbsalom&33YatesMaryElizabeth                   3.0       3.0           0.0     1.0          2.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "32YatesJohnPryor&33MartinSarah                        3.0       3.0           0.0     0.0          0.0              0.0      1.0             1.0      0.0         1.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "36YatesHarryGeorge&37SheldonGraceAmy                  3.0       3.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       3.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "32YatesWilliam&33NeedhamMary                          3.0       3.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          1.0           0.0       0.0              0.0           1.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              1.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "32YatesJacob&32VandenberghElizabeth                   3.0       3.0           1.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             1.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     1.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       3.0       3.0\n",
            "42YatesJoseph&42MaghullHelen                          2.0       2.0           0.0     0.0          0.0              1.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "42YatesNathanJames&42SearchingStill                   2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          2.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "35YatesJames&36SuydamIsobelleElizabeth                2.0       2.0           0.0     0.0          0.0              0.0      0.0             2.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "35YatesEdwardSamuel&35KirkpatrickSarahElizabeth       2.0       2.0           0.0     1.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesWilliamPrice&34McKinneyElizabethAnn            2.0       2.0           1.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          1.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesWilliamBasil&34BullockMartha                   2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       2.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesDarlingBarber&34OliverMaryEmily                2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              1.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesThomas&34EvansMary                             2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      2.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "34YatesSoloman&34McCammackElizabeth                   2.0       2.0           0.0     0.0          0.0              0.0      1.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           1.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "32YatesJames&33FlournoyMaryAnn                        2.0       2.0           0.0     1.0          1.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33YatesWilliamThomas&33ShelhorseMaryPolly             2.0       2.0           0.0     2.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "31HandJonathan&31YatesRebecca                         2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              2.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33YatesJohn&42SearchingStill                          2.0       2.0           1.0     0.0          0.0              0.0      0.0             1.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33TolerElishaPhilpot&33YatesElizabeth                 2.0       2.0           1.0     1.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33GirtainJonathon&33BarberRebecca                     2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           2.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "33CowdenWilliam&33YatesCatherine                      2.0       2.0           0.0     1.0          0.0              0.0      1.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "32YatesHenry&32McManusNancy                           2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       0.0             0.0         0.0     0.0           0.0            0.0       0.0          0.0       2.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "32YatesLoyd&32BrasfieldMaryPolly                      2.0       2.0           0.0     0.0          0.0              0.0      0.0             0.0      0.0         0.0            0.0           0.0          0.0            0.0             0.0          0.0           0.0       0.0              0.0           0.0       1.0             0.0         0.0     0.0           0.0            0.0       1.0          0.0       0.0              0.0         0.0      0.0     0.0           0.0       2.0       2.0\n",
            "Saved the final result to /content/Matrix-2_Sort Name and First Parents.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 10: Matrix-2_Sort Name and First Parents.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.styles import Font, Alignment\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update the path if necessary\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Group the data by \"Sort\" name and \"First Pair\" and calculate the FQ\n",
        "grouped = df.groupby(['Sort', 'First Pair']).agg({'cM': 'count'}).reset_index()\n",
        "grouped.columns = ['Sort', 'First Pair', 'FQ']\n",
        "\n",
        "# Create a pivot table with 'First Pair' as rows and 'Sort' as columns\n",
        "pivot_table = grouped.pivot(index='First Pair', columns='Sort', values='FQ').fillna(0)\n",
        "\n",
        "# Add summaries for rows and columns\n",
        "pivot_table['Total FQ'] = pivot_table.sum(axis=1)\n",
        "pivot_table.loc['Total FQ Summary'] = pivot_table.sum(axis=0)\n",
        "\n",
        "# Apply the first limitation: filter the pivot table to only include 'First Pair' rows with 'Total FQ' >= 2\n",
        "pivot_table_first_limit = pivot_table[pivot_table['Total FQ'] >= 2]\n",
        "\n",
        "# Sort the pivot table by 'Total FQ' in descending order\n",
        "pivot_table_first_limit_sorted = pivot_table_first_limit.sort_values(by='Total FQ', ascending=False)\n",
        "\n",
        "# Save the first iteration to a dictionary\n",
        "first_iteration_dict = pivot_table_first_limit_sorted.to_dict()\n",
        "\n",
        "# Apply the second limitation: filter the pivot table to only include 'Sort Name' columns with 'Total FQ' >= 4\n",
        "column_sums = pivot_table_first_limit_sorted.loc['Total FQ Summary']\n",
        "filtered_columns = column_sums[column_sums >= 4].index.tolist()\n",
        "\n",
        "# Specify certain \"Sort Name\" columns not to display\n",
        "exclude_columns = ['milocan', 'addison,david', 'zimkat', 'adamssarah', 'yates,nancy', 'walclif', 'mccollummike']\n",
        "filtered_columns = [col for col in filtered_columns if col not in exclude_columns]\n",
        "\n",
        "# Ensure 'Total FQ' is the last column after sorting columns\n",
        "pivot_table_second_limit = pivot_table_first_limit_sorted[filtered_columns + ['Total FQ']]\n",
        "pivot_table_final = pivot_table_second_limit.drop('Total FQ Summary', axis=0)\n",
        "\n",
        "# Print the final sorted pivot table with summaries\n",
        "print(\"Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\")\n",
        "print(pivot_table_final.to_string())\n",
        "\n",
        "# Save the final result to an Excel file with formatting\n",
        "output_path = '/content/Matrix-2_Sort Name and First Parents.xlsx'\n",
        "pivot_table_final.to_excel(output_path)\n",
        "\n",
        "print(\"Final output saved successfully at:\", output_path)\n",
        "\n",
        "# Section 10: Matrix-2_Sort Name and First Parents.xlsx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKZWJNUL93yk",
        "outputId": "fdcd237c-d954-4eb7-d05a-034f10817044"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Sorted Pivot Table with Summaries (FQ >= 2 and Sort Name FQ >= 4, Excluding Specified Columns):\n",
            "Sort                                    birdwelljac  franch,mike  weeksjerri  wishardglen  yates,andreal  yates,patricial  yates,ronald  yates,timothyb  yates,timothyj  yatesjamesrob  Total FQ  Total FQ\n",
            "First Pair                                                                                                                                                                                                \n",
            "23YatesFrancis&23TichborneJane                   17            4          17           20             13               25            40              16              13             13       178       178\n",
            "24YatesJohnThomas&24HatfieldeElizabeth            1            1           1            1              2                4             3               3               3              2        21        21\n",
            "Final output saved successfully at: /content/Matrix-2_Sort Name and First Parents.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from openpyxl import Workbook\n",
        "\n",
        "# Load the data from the provided Excel file\n",
        "file_path = '/content/DNA_Study_Library.xlsx'  # Update the path if necessary\n",
        "df = pd.read_excel(file_path, usecols=[\"Sort\", \"cM\", \"First Pair\"])\n",
        "\n",
        "# Group the data by \"Sort\" name and \"First Pair\" and calculate the FQ\n",
        "grouped = df.groupby(['Sort', 'First Pair']).agg({'cM': 'count'}).reset_index()\n",
        "grouped.columns = ['Sort', 'First Pair', 'FQ']\n",
        "\n",
        "# Create a pivot table with 'First Pair' as rows and 'Sort' as columns\n",
        "pivot_table = grouped.pivot(index='First Pair', columns='Sort', values='FQ').fillna(0)\n",
        "\n",
        "# Add summaries for rows and columns\n",
        "pivot_table['Total FQ'] = pivot_table.sum(axis=1)\n",
        "pivot_table.loc['Total FQ Summary'] = pivot_table.sum(axis=0)\n",
        "\n",
        "# Apply the first limitation: filter the pivot table to only include 'First Pair' rows with 'Total FQ' >= 2\n",
        "pivot_table_first_limit = pivot_table[pivot_table['Total FQ'] >= 2]\n",
        "\n",
        "# Sort the pivot table by 'Total FQ' in descending order\n",
        "pivot_table_first_limit_sorted = pivot_table_first_limit.sort_values(by='Total FQ', ascending=False)\n",
        "\n",
        "# Save the first iteration to a dictionary\n",
        "first_iteration_dict = pivot_table_first_limit_sorted.to_dict()\n",
        "\n",
        "# Apply the second limitation: filter the pivot table to only include 'Sort Name' columns with 'Total FQ' >= 4\n",
        "column_sums = pivot_table_first_limit_sorted.loc['Total FQ Summary']\n",
        "filtered_columns = column_sums[column_sums >= 4].index.tolist()\n",
        "\n",
        "# Specify certain \"Sort Name\" columns not to display\n",
        "exclude_columns = ['milocan', 'addison,david', 'zimkat', 'adamssarah', 'yates,nancy', 'walclif', 'mccollummike']\n",
        "filtered_columns = [col for col in filtered_columns if col not in exclude_columns]\n",
        "\n",
        "# Ensure 'Total FQ' is the last column after sorting columns\n",
        "pivot_table_second_limit = pivot_table_first_limit_sorted[filtered_columns + ['Total FQ']]\n",
        "pivot_table_final = pivot_table_second_limit.drop('Total FQ Summary', axis=0)\n",
        "\n",
        "# Calculate percentages for the second sheet\n",
        "percentage_pivot_table = pivot_table_final.div(pivot_table_final.sum(axis=0), axis=1) * 100\n",
        "\n",
        "# Save the final result to an Excel file with two sheets: one for the original values and one for the percentages\n",
        "output_path = '/content/Matrix-2_Sort_Name_and_First_Parents.xlsx'\n",
        "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "    pivot_table_final.to_excel(writer, sheet_name='FQ Values')\n",
        "    percentage_pivot_table.to_excel(writer, sheet_name='Percentage Values')\n",
        "\n",
        "print(\"Final output saved successfully at:\", output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0hXS0f7_vaZ",
        "outputId": "529537d9-5364-4148-bace-b65af7bfeeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output saved successfully at: /content/Matrix-2_Sort_Name_and_First_Parents.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard distribution compared to actual\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# List of unique first pairs\n",
        "first_pairs = [\n",
        "    \"23YatesFrancis&23TichborneJane\", \"30YatesBenjamin&42SearchingStill\",\n",
        "    \"24YatesJohnThomas&24HatfieldeElizabeth\", \"31YatesAbraham&42SearchingStill\",\n",
        "    \"32YatesJames&33SanfordSarah\", \"31YatesJohn&31BarfieldElizabeth\",\n",
        "    \"33YatesWilliam&34ParkerSally\", \"27YatesWilliam&27BondMargaretCatherine\",\n",
        "    \"30YatesWilliam&30ThornburyAnne\", \"26YatesJohn&27StrattonUrslyMehitable\",\n",
        "    \"28YatesThomas&28DruryMonica\", \"32YatesJoseph&32LeighMary\",\n",
        "    \"33YatesWilliam&33EdwardsElizabeth\", \"31RoperJamesDavid&31YatesSarahAnn\",\n",
        "    \"30YatesJohn&30BoswellAnn\", \"32YatesJoseph&32AtmarSarah\",\n",
        "    \"29YatesThomas&42SearchingStill\", \"29YatesJames&30McNayJane\",\n",
        "    \"42YatesPhillip&42SearchingStill\", \"34ShepherdJonathan&34YatesEmilyBetty\",\n",
        "    \"35YatesJohnWesley&35EvansAnn\", \"26YatesJohn&27StrattonMary\",\n",
        "    \"32YatesJoshuaHardy&32StewartMarthaAnn\", \"29YatesMartin&42Eliz\",\n",
        "    \"29PhillipsWilliam&29YatesAnn\", \"29YatesRichard&42Mary\",\n",
        "    \"34YatesWilliam&34PikeEsther\", \"34YatesSamuelPorter&35BridgesMartha\",\n",
        "    \"33YatesWilliam&33SaltPhoebe\", \"32SwiftAbsalom&33YatesMaryElizabeth\",\n",
        "    \"32YatesJohnPryor&33MartinSarah\", \"36YatesHarryGeorge&37SheldonGraceAmy\",\n",
        "    \"32YatesWilliam&33NeedhamMary\", \"32YatesJacob&32VandenberghElizabeth\",\n",
        "    \"42YatesJoseph&42MaghullHelen\", \"42YatesNathanJames&42SearchingStill\",\n",
        "    \"35YatesJames&36SuydamIsobelleElizabeth\", \"35YatesEdwardSamuel&35KirkpatrickSarahElizabeth\",\n",
        "    \"34YatesWilliamPrice&34McKinneyElizabethAnn\", \"34YatesWilliamBasil&34BullockMartha\",\n",
        "    \"34YatesDarlingBarber&34OliverMaryEmily\", \"34YatesThomas&34EvansMary\",\n",
        "    \"34YatesSoloman&34McCammackElizabeth\", \"32YatesJames&33FlournoyMaryAnn\",\n",
        "    \"33YatesWilliamThomas&33ShelhorseMaryPolly\", \"31HandJonathan&31YatesRebecca\",\n",
        "    \"33YatesJohn&42SearchingStill\", \"33TolerElishaPhilpot&33YatesElizabeth\",\n",
        "    \"33GirtainJonathon&33BarberRebecca\", \"33CowdenWilliam&33YatesCatherine\",\n",
        "    \"32YatesHenry&32McManusNancy\", \"32YatesLoyd&32BrasfieldMaryPolly\"\n",
        "]\n",
        "\n",
        "# Actual FQ values\n",
        "actual_fq_values = [\n",
        "    40, 57, 3, 0, 3, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
        "    0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
        "    0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0\n",
        "]\n",
        "\n",
        "# Total FQ for Yates, Ronald\n",
        "total_fq_ronald = 121\n",
        "\n",
        "# Calculate the expected FQ per first pair\n",
        "expected_fq_per_pair = total_fq_ronald / len(first_pairs)\n",
        "\n",
        "# Create a DataFrame to display the expected and actual FQ distribution\n",
        "distribution_df = pd.DataFrame({\n",
        "    'First Pair': first_pairs,\n",
        "    'Expected FQ': [expected_fq_per_pair] * len(first_pairs),\n",
        "    'Actual FQ': actual_fq_values\n",
        "})\n",
        "\n",
        "print(distribution_df)\n",
        "# Standard distribution compared to actual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiHHePlyGNbC",
        "outputId": "9df996e6-a567-4ca2-f85e-665f32ed2dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         First Pair  Expected FQ  Actual FQ\n",
            "0                    23YatesFrancis&23TichborneJane     2.326923         40\n",
            "1                  30YatesBenjamin&42SearchingStill     2.326923         57\n",
            "2            24YatesJohnThomas&24HatfieldeElizabeth     2.326923          3\n",
            "3                   31YatesAbraham&42SearchingStill     2.326923          0\n",
            "4                       32YatesJames&33SanfordSarah     2.326923          3\n",
            "5                   31YatesJohn&31BarfieldElizabeth     2.326923          2\n",
            "6                      33YatesWilliam&34ParkerSally     2.326923          0\n",
            "7            27YatesWilliam&27BondMargaretCatherine     2.326923          2\n",
            "8                    30YatesWilliam&30ThornburyAnne     2.326923          2\n",
            "9              26YatesJohn&27StrattonUrslyMehitable     2.326923          0\n",
            "10                      28YatesThomas&28DruryMonica     2.326923          0\n",
            "11                        32YatesJoseph&32LeighMary     2.326923          0\n",
            "12                33YatesWilliam&33EdwardsElizabeth     2.326923          0\n",
            "13                31RoperJamesDavid&31YatesSarahAnn     2.326923          0\n",
            "14                         30YatesJohn&30BoswellAnn     2.326923          1\n",
            "15                       32YatesJoseph&32AtmarSarah     2.326923          0\n",
            "16                   29YatesThomas&42SearchingStill     2.326923          0\n",
            "17                         29YatesJames&30McNayJane     2.326923          0\n",
            "18                  42YatesPhillip&42SearchingStill     2.326923          1\n",
            "19             34ShepherdJonathan&34YatesEmilyBetty     2.326923          0\n",
            "20                     35YatesJohnWesley&35EvansAnn     2.326923          0\n",
            "21                       26YatesJohn&27StrattonMary     2.326923          0\n",
            "22            32YatesJoshuaHardy&32StewartMarthaAnn     2.326923          0\n",
            "23                             29YatesMartin&42Eliz     2.326923          0\n",
            "24                     29PhillipsWilliam&29YatesAnn     2.326923          2\n",
            "25                            29YatesRichard&42Mary     2.326923          1\n",
            "26                      34YatesWilliam&34PikeEsther     2.326923          0\n",
            "27              34YatesSamuelPorter&35BridgesMartha     2.326923          0\n",
            "28                      33YatesWilliam&33SaltPhoebe     2.326923          0\n",
            "29              32SwiftAbsalom&33YatesMaryElizabeth     2.326923          0\n",
            "30                   32YatesJohnPryor&33MartinSarah     2.326923          0\n",
            "31             36YatesHarryGeorge&37SheldonGraceAmy     2.326923          0\n",
            "32                     32YatesWilliam&33NeedhamMary     2.326923          0\n",
            "33              32YatesJacob&32VandenberghElizabeth     2.326923          1\n",
            "34                     42YatesJoseph&42MaghullHelen     2.326923          0\n",
            "35              42YatesNathanJames&42SearchingStill     2.326923          0\n",
            "36           35YatesJames&36SuydamIsobelleElizabeth     2.326923          0\n",
            "37  35YatesEdwardSamuel&35KirkpatrickSarahElizabeth     2.326923          0\n",
            "38       34YatesWilliamPrice&34McKinneyElizabethAnn     2.326923          1\n",
            "39              34YatesWilliamBasil&34BullockMartha     2.326923          0\n",
            "40           34YatesDarlingBarber&34OliverMaryEmily     2.326923          0\n",
            "41                        34YatesThomas&34EvansMary     2.326923          0\n",
            "42              34YatesSoloman&34McCammackElizabeth     2.326923          0\n",
            "43                   32YatesJames&33FlournoyMaryAnn     2.326923          0\n",
            "44        33YatesWilliamThomas&33ShelhorseMaryPolly     2.326923          0\n",
            "45                    31HandJonathan&31YatesRebecca     2.326923          0\n",
            "46                     33YatesJohn&42SearchingStill     2.326923          1\n",
            "47            33TolerElishaPhilpot&33YatesElizabeth     2.326923          1\n",
            "48                33GirtainJonathon&33BarberRebecca     2.326923          0\n",
            "49                 33CowdenWilliam&33YatesCatherine     2.326923          0\n",
            "50                      32YatesHenry&32McManusNancy     2.326923          0\n",
            "51                 32YatesLoyd&32BrasfieldMaryPolly     2.326923          0\n"
          ]
        }
      ]
    }
  ]
}