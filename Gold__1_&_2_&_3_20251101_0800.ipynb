{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNNUbhaHEC/NSxPdOY7jUIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/Gold__1_%26_2_%26_3_20251101_0800.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "6d64e831-ecdd-47dc-f9d6-615e5048106b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.12/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.16.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from mlxtend) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
            "ERROR: unknown command \"caas_jupyter_tools\"\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl\n",
        "!pip install xlsxwriter\n",
        "!pip install mlxtend\n",
        "!pip caas_jupyter_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ron Rules-QUICK CODE CARD (v2025.10.27-Refined)\n",
        "# - Complete & runnable in Colab; ISO-8859-15 (ASCII-only in source).\n",
        "# - Punctuation in strings use HTML entities (&rsquo; &ldquo; &rdquo; &mdash; &rarr;).\n",
        "# - Deliver Python code (inline, executable, CUT-ready section)\n",
        "# - XHTML 1.0 Transitional; old-school friendly; Times New Roman body.\n",
        "# - Use CUT markers; five # spacer lines follow the STOP marker."
      ],
      "metadata": {
        "id": "g3hSp6RQHgPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1"
      ],
      "metadata": {
        "id": "JvOlmbj91AGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 20250513-cell2 is good to use; adding more lineage functionality next\n",
        "#!/usr/bin/env python\n",
        "# RON RULES — QUICK CODE CARD (v2025.10.27-Refined)\n",
        "# - Complete & runnable in Colab; ISO-8859-15 (ASCII-only in source).\n",
        "# - Punctuation in strings use HTML entities (&rsquo; &ldquo; &rdquo; &mdash; &rarr;).\n",
        "# - Deliver Python code (inline, executable, CUT-ready section)\n",
        "# - XHTML 1.0 Transitional; old-school friendly; Times New Roman body.\n",
        "# - Use CUT markers; five # spacer lines follow the STOP marker.\n",
        "\"\"\"\n",
        "GEDCOM Composite Score Script using:\n",
        " - Chunk-based Parallel Processing for Speed (Stage 1: genealogical line creation)\n",
        " - A Trie-based approach, then final \"Value\" = 5 * (number of couples with node.count >=2) + (total couples)\n",
        "\n",
        "For ancestral lines where none of the couples are repeated (a one-off line), the Value is still computed.\n",
        "Now, instead of composite scoring, two new columns are added:\n",
        "  - Value Range (the numeric bracket)\n",
        "  - Value Label (a descriptive label)\n",
        "\n",
        "Exports final CSV/HTML sorted by \"Yates DNA Ancestral Line\", including a 'haplogroup' column.\n",
        "\"\"\"\n",
        "import csv\n",
        "import glob\n",
        "import logging\n",
        "import functools\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "###############################################################################\n",
        "# Global Variables\n",
        "###############################################################################\n",
        "anchor_gen1 = None\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "###############################################################################\n",
        "# Trie Data Structure\n",
        "###############################################################################\n",
        "class TrieNode:\n",
        "    \"\"\"A simple Trie node for storing a couple and counting how many lines pass here.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.children = {}\n",
        "\n",
        "class Trie:\n",
        "    def __init__(self):\n",
        "        self.root = TrieNode()\n",
        "\n",
        "    def insert_line(self, couples_list):\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple not in current.children:\n",
        "                current.children[couple] = TrieNode()\n",
        "            current = current.children[couple]\n",
        "            current.count += 1\n",
        "\n",
        "    def get_couple_count(self, couples_list):\n",
        "        counts = []\n",
        "        current = self.root\n",
        "        for couple in couples_list:\n",
        "            if couple in current.children:\n",
        "                current = current.children[couple]\n",
        "                counts.append(current.count)\n",
        "            else:\n",
        "                counts.append(0)\n",
        "                break\n",
        "        return counts\n",
        "\n",
        "###############################################################################\n",
        "# Utility: chunk generator\n",
        "###############################################################################\n",
        "def chunks(lst, n):\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "###############################################################################\n",
        "# GedcomDataset\n",
        "###############################################################################\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1\n",
        "        anchor_gen1 = self.anchor_gen1\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        elif '**' in npfx_value:\n",
        "            cm_value = npfx_value.split('**')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_part = npfx_value.split('&')[1]\n",
        "            if '**' in sort_part:\n",
        "                sort_value = sort_part.split('**')[0].strip()\n",
        "            else:\n",
        "                sort_value = sort_part.strip()\n",
        "            return sort_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_YDNA(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '**' in npfx_value:\n",
        "            ydna_value = npfx_value.split('**')[1].strip()\n",
        "            return ydna_value\n",
        "        return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "###############################################################################\n",
        "# Gedcom Class\n",
        "###############################################################################\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        ydna_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC']:\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "                    if '**' in value:\n",
        "                        ydna_count += 1\n",
        "\n",
        "        autosomal_count = npfx_count - ydna_count\n",
        "        print(f\"GEDCOM contained {total_count} total records\")\n",
        "        print(f\"Records tagged and filtered by NPFX: {npfx_count}\")\n",
        "        print(f\"Records with YDNA information: {ydna_count}\")\n",
        "        print(f\"Autosomal matches: {autosomal_count}\")\n",
        "\n",
        "        for ds in self.gedcom_datasets:\n",
        "            if ds.get_extractable_NPFX():\n",
        "                self.filter_pool.append(ds)\n",
        "\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                logger.warning(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                self.filter_pool = [d for d in self.filter_pool if d.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "                logger.info(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "        return autosomal_count\n",
        "\n",
        "###############################################################################\n",
        "# quick_extract_name\n",
        "###############################################################################\n",
        "def quick_extract_name(full_text):\n",
        "    name_marker = \"\\n1 NAME \"\n",
        "    idx = full_text.find(name_marker)\n",
        "    if idx == -1:\n",
        "        if full_text.startswith(\"1 NAME \"):\n",
        "            idx = 0\n",
        "        else:\n",
        "            return \"UnknownName\"\n",
        "    start = idx + len(name_marker)\n",
        "    end = full_text.find('\\n', start)\n",
        "    if end == -1:\n",
        "        end = len(full_text)\n",
        "    name_line = full_text[start:end].strip()\n",
        "    if '/' not in name_line:\n",
        "        return name_line[:10].replace(\" \", \"\")\n",
        "    first_name, last_name = name_line.split('/', 1)\n",
        "    last_name = last_name.replace(\"/\", \"\").strip()\n",
        "    return last_name[:10].replace(\" \", \"\") + first_name[:10].replace(\" \", \"\")\n",
        "\n",
        "###############################################################################\n",
        "# Parents & Ancestors\n",
        "###############################################################################\n",
        "def find_parents(individual_id, generation, parents_map):\n",
        "    global visited_pairs, generation_table\n",
        "    if individual_id not in parents_map:\n",
        "        return\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return\n",
        "    pair = (father_id, mother_id)\n",
        "    if pair not in visited_pairs:\n",
        "        visited_pairs.add(pair)\n",
        "        generation_table.append((generation, pair))\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation+1, parents_map)\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation+1, parents_map)\n",
        "\n",
        "def find_distant_ancestors(individual_id, parents_map, path=None):\n",
        "    if path is None:\n",
        "        path = []\n",
        "    path.append(individual_id)\n",
        "    if individual_id not in parents_map:\n",
        "        return [path]\n",
        "    father_id, mother_id = parents_map[individual_id]\n",
        "    if not father_id and not mother_id:\n",
        "        return [path]\n",
        "    paths = []\n",
        "    if father_id:\n",
        "        paths.extend(find_distant_ancestors(father_id, parents_map, path[:]))\n",
        "    if mother_id:\n",
        "        paths.extend(find_distant_ancestors(mother_id, parents_map, path[:]))\n",
        "    return paths if paths else [path]\n",
        "\n",
        "###############################################################################\n",
        "# filter_ancestral_line\n",
        "###############################################################################\n",
        "def filter_ancestral_line(winning_path_ids, generation_table_local, names_map):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table_local:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    matching_table.sort(key=lambda x: x[0])\n",
        "    lines = []\n",
        "    for gen, pair in matching_table:\n",
        "        name_pair = [names_map.get(pid, \"UnknownName\") for pid in pair]\n",
        "        lines.append(f\"{name_pair[0]}&{name_pair[1]}\")\n",
        "    lines.reverse()\n",
        "    return \"~~~\".join(lines)\n",
        "\n",
        "###############################################################################\n",
        "# process_record_wrapper (parallel) - STAGE 1\n",
        "###############################################################################\n",
        "def process_record_wrapper(individual_id, gedcom_instance, parents_map, names_map):\n",
        "    global generation_table, visited_pairs, anchor_gen1\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, parents_map)\n",
        "    distant_anc_paths = find_distant_ancestors(individual_id, parents_map)\n",
        "\n",
        "    best_score = None\n",
        "    best_path = None\n",
        "    for path in distant_anc_paths:\n",
        "        name_path = [names_map.get(pid, \"UnknownName\") for pid in path]\n",
        "        score = sum((idx+1) for idx, nm in enumerate(name_path) if 'Yates' in nm)\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_path = path\n",
        "\n",
        "    if not best_path:\n",
        "        best_path = []\n",
        "\n",
        "    best_path_cleaned = [pid for pid in best_path if pid != individual_id]\n",
        "    line_str = filter_ancestral_line(set(best_path_cleaned), generation_table, names_map)\n",
        "\n",
        "    cm_value = ''\n",
        "    sort_value = ''\n",
        "    ydna_value = ''\n",
        "    for ds in gedcom_instance.filter_pool:\n",
        "        if ds.get_gen_person() == individual_id:\n",
        "            cm_value = ds.get_extractable_cm()\n",
        "            sort_value = ds.get_extractable_sort()\n",
        "            ydna_value = ds.get_extractable_YDNA()\n",
        "            break\n",
        "\n",
        "    short_name = names_map.get(individual_id, \"UnknownName\")\n",
        "    # Return columns: ID#, Match to, Name, cM, Yates DNA Ancestral Line, haplogroup\n",
        "    return [individual_id, sort_value, short_name, cm_value, line_str, ydna_value]\n",
        "\n",
        "###############################################################################\n",
        "# main()\n",
        "###############################################################################\n",
        "def main():\n",
        "    def select_gedcom():\n",
        "        files = glob.glob(\"*.ged\")\n",
        "        if not files:\n",
        "            print(\"No GEDCOM files found.\")\n",
        "            return None\n",
        "        print(\"Automatically selecting the first GEDCOM file.\")\n",
        "        return files[0]\n",
        "\n",
        "    gedcom_file_path = select_gedcom()\n",
        "    if not gedcom_file_path:\n",
        "        print(\"No GEDCOM file selected; exiting.\")\n",
        "        return\n",
        "\n",
        "    ged = Gedcom(gedcom_file_path)\n",
        "    autosomal_count = ged.parse_gedcom()\n",
        "    filter_count = len(ged.filter_pool)\n",
        "\n",
        "    with open(\"autosomal_count.txt\", \"w\") as f:\n",
        "        f.write(str(autosomal_count))\n",
        "\n",
        "    print(\"Records tagged and filtered by NPFX:\", filter_count)\n",
        "\n",
        "    with open(gedcom_file_path, 'r', encoding='utf-8') as f:\n",
        "        raw_data = f.read()\n",
        "\n",
        "    blocks = raw_data.split('\\n0 ')\n",
        "    all_records = {}\n",
        "    for blk in blocks:\n",
        "        blk = blk.strip()\n",
        "        if not blk:\n",
        "            continue\n",
        "        flend = blk.find('\\n')\n",
        "        if flend == -1:\n",
        "            flend = len(blk)\n",
        "        first_line = blk[:flend]\n",
        "        if '@' in first_line:\n",
        "            start = first_line.find('@') + 1\n",
        "            end = first_line.find('@', start)\n",
        "            rec_id = first_line[start:end].strip()\n",
        "            all_records[rec_id] = blk\n",
        "\n",
        "    parents_map = {}\n",
        "    names_map = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        nm = quick_extract_name(\"\\n\" + txt)\n",
        "        names_map[rec_id] = nm\n",
        "\n",
        "    families = {}\n",
        "    for rec_id, txt in all_records.items():\n",
        "        if 'FAM' in txt[:50]:\n",
        "            father_idx = txt.find('1 HUSB @')\n",
        "            husb_id = txt[father_idx+len('1 HUSB @'):txt.find('@', father_idx+len('1 HUSB @'))] if father_idx != -1 else None\n",
        "            wife_idx = txt.find('1 WIFE @')\n",
        "            wife_id = txt[wife_idx+len('1 WIFE @'):txt.find('@', wife_idx+len('1 WIFE @'))] if wife_idx != -1 else None\n",
        "            kids = [ln.split('@')[1] for ln in txt.split('\\n') if ln.strip().startswith('1 CHIL @')]\n",
        "            families[rec_id] = (husb_id, wife_id, kids)\n",
        "\n",
        "    for fam_id, (f_id, m_id, k_list) in families.items():\n",
        "        for kid in k_list:\n",
        "            parents_map[kid] = (f_id, m_id)\n",
        "\n",
        "    individual_ids = [d.get_gen_person() for d in ged.filter_pool]\n",
        "    print(f\"Processing {len(individual_ids)} individuals with chunk-based parallel...\")\n",
        "\n",
        "    combined_rows = []\n",
        "    chunk_size = 50\n",
        "    max_workers = os.cpu_count() or 4\n",
        "    logger.info(\"Starting chunk-based parallel processing with %d workers.\", max_workers)\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor, tqdm(total=len(individual_ids), desc=\"Building Yates Lines (Stage 1)\") as pbar:\n",
        "        for chunk in chunks(individual_ids, chunk_size):\n",
        "            func = functools.partial(process_record_wrapper, gedcom_instance=ged, parents_map=parents_map, names_map=names_map)\n",
        "            results = list(executor.map(func, chunk))\n",
        "            combined_rows.extend(results)\n",
        "            pbar.update(len(chunk))\n",
        "\n",
        "    columns = [\"ID#\", \"Match to\", \"Name\", \"cM\", \"Yates DNA Ancestral Line\", \"haplogroup\"]\n",
        "    df = pd.DataFrame(combined_rows, columns=columns)\n",
        "    df.index += 1\n",
        "\n",
        "    def remove_specific_prefix(row):\n",
        "        prefix = \"YatesJohn&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesWilliam&SearchingStill~~~YatesEdmund&CornellMargaret~~~YatesRichard&AshendonJoan~~~YatesJohn&HydeAlice~~~YatesThomas&FauconerElizabeth~~~\"\n",
        "        if row[\"Yates DNA Ancestral Line\"].startswith(prefix):\n",
        "            row[\"Yates DNA Ancestral Line\"] = row[\"Yates DNA Ancestral Line\"][len(prefix):]\n",
        "        return row\n",
        "\n",
        "    df = df.apply(remove_specific_prefix, axis=1)\n",
        "\n",
        "    logger.info(\"Building Trie from reversed lines...\")\n",
        "    trie = Trie()\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.notna(line_str) and line_str.strip():\n",
        "            trie.insert_line([x.strip() for x in line_str.split(\"~~~\") if x.strip()])\n",
        "\n",
        "    values, prefix_counts = [], []\n",
        "    logger.info(\"Computing 'Value' = 5*(#couples with node.count >=2) + (total couples) ...\")\n",
        "    for _, row in df.iterrows():\n",
        "        line_str = row[\"Yates DNA Ancestral Line\"]\n",
        "        if pd.isna(line_str) or not line_str.strip():\n",
        "            values.append(0)\n",
        "            prefix_counts.append(0)\n",
        "        else:\n",
        "            couples_list = [x.strip() for x in line_str.split(\"~~~\") if x.strip()]\n",
        "            node_counts = trie.get_couple_count(couples_list)\n",
        "            prefix_count = sum(1 for c in node_counts if c >= 2)\n",
        "            values.append(5 * prefix_count + len(couples_list))\n",
        "            prefix_counts.append(prefix_count)\n",
        "\n",
        "    df[\"Value\"], df[\"PrefixCount\"] = values, prefix_counts\n",
        "\n",
        "    def assign_value_range_label(val):\n",
        "        try:\n",
        "            v = float(val)\n",
        "        except:\n",
        "            return \"\", \"\"\n",
        "        if v >= 60: return \">=60\", \"1-likely correct\"\n",
        "        if 47 <= v <= 59: return \"59~47\", \"2-lines forming\"\n",
        "        if 34 <= v <= 46: return \"46~34\", \"3-patterns emerging\"\n",
        "        if 21 <= v <= 33: return \"33~21\", \"4-notable patterns\"\n",
        "        if 8 <= v <= 20: return \"20~8\", \"5-patterns stable\"\n",
        "        if 1 <= v <= 7:  return f\"{v:.0f}\", \"6-need research\"\n",
        "        return f\"{v:.0f}\", \"0-uncategorized\"\n",
        "\n",
        "    ranges, labels = zip(*(assign_value_range_label(v) for v in df[\"Value\"]))\n",
        "    df[\"Value Range\"], df[\"Value Label\"] = ranges, labels\n",
        "\n",
        "    df.sort_values(by=[\"Yates DNA Ancestral Line\"], inplace=True)\n",
        "    df.drop(\"PrefixCount\", axis=1, inplace=True)\n",
        "\n",
        "    csv_name = \"final_combined_df_with_value_labels.csv\"\n",
        "    df.to_csv(csv_name, index=False)\n",
        "    logger.info(\"Exported final DataFrame to '%s'.\", csv_name)\n",
        "\n",
        "    html_name = \"HTML_combined_df_with_value_labels.html\"\n",
        "    css_style = \"\"\"\n",
        "    <style>\n",
        "    table { width: 100%; border-collapse: collapse; margin: 20px 0; }\n",
        "    table, th, td { border: 1px solid #333; }\n",
        "    th, td { padding: 8px 12px; text-align: center; }\n",
        "    th { background-color: #f2f2f2; }\n",
        "    /* Left-align the last column */\n",
        "    td:nth-child(7) { text-align: left; }\n",
        "    </style>\n",
        "    \"\"\"\n",
        "    final_cols = [\"ID#\", \"cM\", \"haplogroup\", \"Match to\", \"Value Range\", \"Value Label\", \"Yates DNA Ancestral Line\"]\n",
        "    html_content = css_style + df.to_html(index=False, columns=final_cols, escape=False)\n",
        "    with open(html_name, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "    logger.info(\"Exported HTML to '%s'.\", html_name)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "    try:\n",
        "        display(Javascript('alert(\"✅ GEDCOM processing (and HTML export) is complete!\");'))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import smtplib, ssl\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "def send_email(subject, body, to_addr):\n",
        "    smtp_server = 'smtp.gmail.com'\n",
        "    port = 465\n",
        "    sender = os.environ['GMAIL_USER']\n",
        "    password = os.environ['GMAIL_APP_PASSWORD']\n",
        "    msg = MIMEText(body)\n",
        "    msg['Subject'] = subject\n",
        "    msg['From'] = sender\n",
        "    msg['To'] = to_addr\n",
        "    context = ssl.create_default_context()\n",
        "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
        "        server.login(sender, password)\n",
        "        server.send_message(msg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "Qh13Q-WUmVu3",
        "outputId": "5b49092c-32aa-475a-aa41-983604ae70d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 62184 total records\n",
            "Records tagged and filtered by NPFX: 1556\n",
            "Records with YDNA information: 1\n",
            "Autosomal matches: 1555\n",
            "After manual filter, total records: 7\n",
            "Records tagged and filtered by NPFX: 7\n",
            "Processing 7 individuals with chunk-based parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building Yates Lines (Stage 1): 100%|██████████| 7/7 [00:03<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "alert(\"✅ GEDCOM processing (and HTML export) is complete!\");"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2"
      ],
      "metadata": {
        "id": "s1Oa3qUz0_Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2\n",
        "# ====== CUT START [1/1] INDEPENDENT CELL 2 — Build Register (Load -> Map -> HTML -> Save/Upload) ==\n",
        "# RON RULES — QUICK CODE CARD (v2025.10.31-SelectByName)\n",
        "# - Python code (inline, executable, CUT-ready section); ISO-8859-15 (ASCII-only in source).\n",
        "# - Punctuation in strings use HTML entities (&rsquo; &ldquo; &rdquo; &mdash; &rarr;).\n",
        "# - Deliver full runnable code; no fabrication; no JSON/triple-quote artifacts.\n",
        "# - XHTML 1.0 Transitional; old-school friendly; Times New Roman body.\n",
        "# - Use CUT markers; five # spacer lines follow the STOP marker.\n",
        "\n",
        "# ---------- 0) Imports & Secrets ----------\n",
        "import os, re, io, posixpath, socket, traceback, urllib.parse as _u\n",
        "from ftplib import FTP_TLS\n",
        "import pandas as pd\n",
        "import html as _html\n",
        "from string import Template\n",
        "\n",
        "# Load secrets if running in Colab; otherwise respect existing env\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try: os.environ['FTP_DIR'] = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR', '')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_DIR', '')\n",
        "\n",
        "# ---------- 1) Config (filenames, layout, toggles) ----------\n",
        "CSV_IN = \"final_combined_df_with_value_labels.csv\"\n",
        "LOCAL_HTML = \"ons_yates_dna_register.htm\"\n",
        "REMOTE_HTML = \"ons_yates_dna_register.htm\"\n",
        "LOCAL_COUNT_FILE = \"/content/autosomal_count.txt\"\n",
        "REMOTE_COUNT_NAME = \"autosomal_count.txt\"\n",
        "FTP_DIR = os.environ.get(\"FTP_DIR\", \"\").strip()\n",
        "COUNT_PUBLIC_URL = (f\"/{FTP_DIR}/{REMOTE_COUNT_NAME}\" if FTP_DIR else f\"/{REMOTE_COUNT_NAME}\")\n",
        "TNG_BASE = \"https://yates.one-name.net/tng\"\n",
        "TNG_TREE = \"tree1\"\n",
        "HOME_URL = \"https://yates.one-name.net/ons_yates_dna_register.htm\"\n",
        "TABLE_WIDTH_PX = 3150\n",
        "COL_A_PX = 1100\n",
        "ARROW_ENTITY = \"&rarr;\"\n",
        "REMOVE_PERIOD_AT_END = True\n",
        "FIND_COL_PX = 118\n",
        "UPLOAD_COLUMN_A = True\n",
        "\n",
        "# Resolver on server\n",
        "SERVER_PARTIALS_DIR = \"partials\"\n",
        "SERVER_MAPPING_BASENAME = \"match_to_unmasked.csv\"\n",
        "SERVER_MAPPING_REMOTE = posixpath.join(SERVER_PARTIALS_DIR, SERVER_MAPPING_BASENAME)\n",
        "SERVER_MAPPING_LOCAL_CACHE = \"match_to_unmasked.server.csv\"\n",
        "\n",
        "# ---------- 2) FTP helpers ----------\n",
        "FTP_TIMEOUT = int(os.environ.get(\"FTP_TIMEOUT\", \"30\"))\n",
        "FTP_PASSIVE = True\n",
        "\n",
        "def ftp_connect() -> FTP_TLS:\n",
        "    ftps = FTP_TLS(timeout=FTP_TIMEOUT)\n",
        "    socket.setdefaulttimeout(FTP_TIMEOUT)\n",
        "    ftps.connect(os.environ.get('FTP_HOST',''), int(os.environ.get('FTP_PORT', 21)))\n",
        "    ftps.login(os.environ.get('FTP_USER',''), os.environ.get('FTP_PASS',''))\n",
        "    try: ftps.prot_p()\n",
        "    except Exception: pass\n",
        "    try: ftps.set_pasv(FTP_PASSIVE)\n",
        "    except Exception: pass\n",
        "    if FTP_DIR:\n",
        "        try:\n",
        "            ftps.cwd(FTP_DIR)\n",
        "        except Exception:\n",
        "            parts = [p for p in FTP_DIR.split(\"/\") if p]\n",
        "            for p in parts:\n",
        "                try: ftps.mkd(p)\n",
        "                except Exception: pass\n",
        "                ftps.cwd(p)\n",
        "    return ftps\n",
        "\n",
        "def _remote_path(name: str) -> str:\n",
        "    return posixpath.join(FTP_DIR, name) if FTP_DIR else name\n",
        "\n",
        "def ftp_download_if_exists(ftps: FTP_TLS, remote_name: str, local_name: str) -> bool:\n",
        "    try:\n",
        "        with open(local_name, \"wb\") as f:\n",
        "            ftps.retrbinary(f\"RETR {remote_name}\", f.write)\n",
        "        print(f\"Pulled: {remote_name} -> {os.path.abspath(local_name)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            if os.path.exists(local_name): os.remove(local_name)\n",
        "        except Exception: pass\n",
        "        print(f\"Skip pull (not found): {remote_name} ({e})\")\n",
        "        return False\n",
        "\n",
        "def ftp_upload_overwrite(ftps: FTP_TLS, local_path: str, remote_name: str):\n",
        "    with open(local_path, \"rb\") as fh:\n",
        "        ftps.storbinary(f\"STOR {remote_name}\", fh)\n",
        "    print(f\"Uploaded: {local_path} -> {remote_name}\")\n",
        "\n",
        "# ---------- 3) Resolver load (masked code -> unmasked display) ----------\n",
        "def _read_mapping_csv(path: str) -> pd.DataFrame:\n",
        "    encs = (\"iso-8859-15\",\"utf-8-sig\",\"utf-8\",\"cp1252\",\"latin1\")\n",
        "    last = None; df = None\n",
        "    for enc in encs:\n",
        "        try:\n",
        "            df = pd.read_csv(path, encoding=enc, dtype=str, keep_default_na=False)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last = e\n",
        "    if df is None:\n",
        "        raise RuntimeError(f\"Unable to read mapping CSV {path}: {last}\")\n",
        "    if df.shape[1] < 2:\n",
        "        raise RuntimeError(\"Mapping CSV must have at least two columns: code, unmasked\")\n",
        "    df = df.iloc[:, :2].copy()\n",
        "    df.columns = [\"code\",\"unmasked\"]\n",
        "    df[\"code\"] = df[\"code\"].astype(str).str.strip().str.lower()\n",
        "    df[\"unmasked\"] = df[\"unmasked\"].astype(str).str.strip()\n",
        "    df = df[df[\"code\"] != \"\"].drop_duplicates(subset=[\"code\"], keep=\"first\")\n",
        "    if df.empty:\n",
        "        raise RuntimeError(\"Mapping CSV empty after normalization.\")\n",
        "    return df\n",
        "\n",
        "def load_resolver_from_server() -> dict:\n",
        "    with ftp_connect() as ftps:\n",
        "        ok = ftp_download_if_exists(ftps, _remote_path(SERVER_MAPPING_REMOTE), SERVER_MAPPING_LOCAL_CACHE)\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "    if not ok:\n",
        "        raise RuntimeError(\n",
        "            \"Resolver not found on server: /\" + _remote_path(SERVER_MAPPING_REMOTE) +\n",
        "            \". Upload match_to_unmasked.csv into /partials/ and re-run.\"\n",
        "        )\n",
        "    df_map = _read_mapping_csv(SERVER_MAPPING_LOCAL_CACHE)\n",
        "    print(f\"Loaded resolver: {len(df_map)} codes\")\n",
        "    return dict(zip(df_map[\"code\"], df_map[\"unmasked\"]))\n",
        "\n",
        "MATCH_TO_UNMASKED = load_resolver_from_server()\n",
        "\n",
        "def resolve_match_to(code: str) -> str:\n",
        "    if not isinstance(code, str): return \"\"\n",
        "    return MATCH_TO_UNMASKED.get(code.strip().lower(), code)\n",
        "\n",
        "# ---------- 4) Name utilities ----------\n",
        "SEP_RE = re.compile(r\"\\s*(?:\\u2192|&rarr;|;|>|,|~{2,}|/{2,}|\\|{2,})\\s*\")\n",
        "\n",
        "def split_tokens(s):\n",
        "    if pd.isna(s): return []\n",
        "    if not isinstance(s, str): s = str(s)\n",
        "    return [p.strip() for p in SEP_RE.split(s) if str(p).strip()]\n",
        "\n",
        "def _clean_piece(text: str) -> str:\n",
        "    t = re.sub(r'~+', ' ', str(text))\n",
        "    t = re.sub(r'\\s+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "_PARTICLES = {\"de\",\"del\",\"della\",\"der\",\"van\",\"von\",\"da\",\"dos\",\"das\",\"di\",\"la\",\"le\",\"du\",\"of\"}\n",
        "\n",
        "def _smart_title(token: str) -> str:\n",
        "    if not token: return token\n",
        "    token = re.sub(r\"(^|\\b)([a-z])(['’])([a-z])\", lambda m: m.group(1)+m.group(2).upper()+m.group(3)+m.group(4).upper(), token.lower())\n",
        "    token = \"-\".join([w.capitalize() for w in token.split(\"-\")])\n",
        "    token = re.sub(r\"\\bmc([a-z])\",  lambda m: \"Mc\"+m.group(1).upper(),  token)\n",
        "    token = re.sub(r\"\\bmac([a-z])\", lambda m: \"Mac\"+m.group(1).upper(), token)\n",
        "    return token\n",
        "\n",
        "def smart_titlecase(name: str) -> str:\n",
        "    name = _clean_piece(name)\n",
        "    if not name: return name\n",
        "    if \",\" in name:\n",
        "        last, first = [p.strip() for p in name.split(\",\", 1)]\n",
        "        pieces = (first + \" \" + last).split()\n",
        "    else:\n",
        "        pieces = name.split()\n",
        "    out = []\n",
        "    for i, w in enumerate(pieces):\n",
        "        out.append(w.lower() if (i>0 and w.lower() in _PARTICLES) else _smart_title(w))\n",
        "    return \" \".join(out)\n",
        "\n",
        "def surname_given_from_token(token):\n",
        "    token = token.strip()\n",
        "    if not token: return (token,)\n",
        "    idx = None\n",
        "    for i in range(1, len(token)):\n",
        "        if token[i-1].islower() and token[i].isupper():\n",
        "            idx = i; break\n",
        "    if idx is None:\n",
        "        for i in range(1, len(token)):\n",
        "            if token[i].isupper():\n",
        "                idx = i; break\n",
        "    if idx is None: return (token,)\n",
        "    surname = token[:idx]; given = token[idx:]\n",
        "    given_spaced = re.sub(r'(?<!^)([A-Z])', r' \\1', given)\n",
        "    return (f\"{given_spaced.strip()} {surname.strip()}\",)\n",
        "\n",
        "def normalize_person_name(s: str) -> str:\n",
        "    if pd.isna(s): return \"\"\n",
        "    s = _clean_piece(str(s))\n",
        "    if \",\" in s:\n",
        "        last, first = [p.strip() for p in s.split(\",\", 1)]\n",
        "        s = f\"{first} {last}\"\n",
        "    if \" \" not in s and s.isalpha():\n",
        "        return smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return smart_titlecase(s)\n",
        "\n",
        "def truncate_first(name: str, n: int = 4) -> str:\n",
        "    name = name.strip()\n",
        "    if not name: return name\n",
        "    parts = name.split()\n",
        "    return parts[0][:n] if len(parts) == 1 else f\"{parts[0][:n]} {parts[-1]}\"\n",
        "\n",
        "def derive_common_from_first_token(tokens):\n",
        "    if not tokens: return (\"\",\"\")\n",
        "    first = _clean_piece(tokens[0])\n",
        "    parts = re.split(r\"\\s*(?:&| and )\\s*\", first, maxsplit=1, flags=re.I)\n",
        "    if len(parts) != 2: return (\"\",\"\")\n",
        "    def _norm(s):\n",
        "        return smart_titlecase(s) if \" \" in s else smart_titlecase(surname_given_from_token(s)[0])\n",
        "    return (_norm(parts[0]), _norm(parts[1]))\n",
        "\n",
        "def degree_label_from_generations(g):\n",
        "    if g <= 1: return (\"parents\" if g == 1 else \"self\")\n",
        "    if g == 2: return \"grandparents\"\n",
        "    greats = g - 2\n",
        "    if greats == 1: return \"great-grandparents\"\n",
        "    return f\"{greats}x-great-grandparents\"\n",
        "\n",
        "def build_header(subject_name, cm_val, matchee_name_html, gens, husband, wife):\n",
        "    try:\n",
        "        cm_str = f\"{int(round(float(cm_val)))}\"\n",
        "    except Exception:\n",
        "        cm_str = (str(cm_val).strip() or \"0\")\n",
        "    degree_label = degree_label_from_generations(gens)\n",
        "    parts = [\n",
        "        f\"{subject_name} is a {cm_str} cM cousin match to {matchee_name_html}, whose\",\n",
        "        f\"{degree_label} (back {gens} Gens)\",\n",
        "        \"are\",\n",
        "        f\"{husband} & {wife}.\"\n",
        "    ]\n",
        "    s = \" \".join(parts)\n",
        "    if REMOVE_PERIOD_AT_END: s = re.sub(r'\\.\\s*$', '', s)\n",
        "    return s\n",
        "\n",
        "# ---------- 5) Read main CSV; detect columns ----------\n",
        "def find_col(df, patterns, prefer_exact=None):\n",
        "    cols = list(df.columns)\n",
        "    lowmap = {c.lower(): c for c in cols}\n",
        "    if prefer_exact:\n",
        "        for name in prefer_exact:\n",
        "            if name in df.columns: return name\n",
        "            if name and name.lower() in lowmap: return lowmap[name.lower()]\n",
        "    for pat in patterns:\n",
        "        rx = re.compile(pat, re.I)\n",
        "        for c in cols:\n",
        "            if rx.search(c): return c\n",
        "    return None\n",
        "\n",
        "_encs = (\"utf-8-sig\",\"utf-8\",\"cp1252\",\"iso-8859-15\",\"latin1\")\n",
        "_last_err = None\n",
        "df = None\n",
        "for _e in _encs:\n",
        "    try:\n",
        "        df = pd.read_csv(CSV_IN, encoding=_e, dtype=str, keep_default_na=False)\n",
        "        break\n",
        "    except Exception as _ex:\n",
        "        _last_err = _ex\n",
        "        df = None\n",
        "if df is None:\n",
        "    raise RuntimeError(f\"Unable to read CSV: {CSV_IN} ({_last_err})\")\n",
        "print(f\"Loaded main CSV: {len(df)} rows, {len(df.columns)} cols\")\n",
        "\n",
        "id_col = find_col(df, [r'^(id#|personid)$'], [\"ID#\",\"ID\",\"PersonID\",\"personID\"])\n",
        "match_to_col = find_col(df, [r'^match\\s*to$'], [\"Match to\",\"Match\"])\n",
        "name_col = find_col(df, [r'^name$'], [\"Name\"])\n",
        "cm_col = find_col(df, [r'^(c\\s*:?m|cm)$', r'centi.?morgan'], [\"cM\",\"cm\"])\n",
        "path_col = find_col(df, [r'(yates\\s*dna\\s*ancestral\\s*line|ancestral\\s*line|lineage)'],\n",
        "                    [\"Yates DNA Ancestral Line\",\"Ancestral Line\",\"Lineage\"])\n",
        "\n",
        "if not id_col: raise ValueError(\"CSV missing an ID#/PersonID column.\")\n",
        "if not match_to_col: raise ValueError(\"CSV missing 'Match to' column.\")\n",
        "if not name_col: raise ValueError(\"CSV missing 'Name' column.\")\n",
        "if not cm_col: raise ValueError(\"CSV missing 'cM' column.\")\n",
        "if not path_col: raise ValueError(\"CSV missing lineage/path column.\")\n",
        "\n",
        "# ---------- 6) Transform rows -> display_df ----------\n",
        "ID_PAT = re.compile(r\"\\bI\\d+\\b\", re.I)\n",
        "def extract_person_id(s: str) -> str:\n",
        "    m = ID_PAT.search(str(s or \"\"))\n",
        "    return m.group(0).upper() if m else \"\"\n",
        "\n",
        "_CAMEL_WORDS = re.compile(r\"[A-Z][a-z]*|[A-Z]+(?![a-z])|[a-z]+\")\n",
        "def _truncate_alpha(s: str, n: int) -> str:\n",
        "    return re.sub(r\"[^A-Za-z]\", \"\", s)[:n]\n",
        "\n",
        "def norm_matchee_name(raw: str) -> str:\n",
        "    raw = str(raw or \"\").strip()\n",
        "    if not raw: return \"\"\n",
        "    if \" \" in raw or \",\" in raw:\n",
        "        nm = smart_titlecase(raw)\n",
        "        parts = nm.split()\n",
        "        if len(parts) == 1: return nm\n",
        "        given = parts[0]; surname = parts[-1]\n",
        "        return f\"{_truncate_alpha(given, 7)} {surname}\".strip()\n",
        "    words = _CAMEL_WORDS.findall(raw)\n",
        "    while words and len(words[0]) == 1:\n",
        "        words.pop(0)\n",
        "    if not words:\n",
        "        nm = smart_titlecase(surname_given_from_token(raw)[0])\n",
        "        ps = nm.split()\n",
        "        if len(ps) == 1: return nm\n",
        "        return f\"{_truncate_alpha(ps[0], 7)} {ps[-1]}\".strip()\n",
        "    surname = smart_titlecase(words[0])\n",
        "    given_candidates = [w for w in words[1:] if w.lower() != surname.lower()]\n",
        "    if not given_candidates: return surname\n",
        "    given = smart_titlecase(given_candidates[0])\n",
        "    return f\"{_truncate_alpha(given, 7)} {surname}\".strip()\n",
        "\n",
        "headers, lineages, findcol = [], [], []\n",
        "subjects, first_ancestors = [], []\n",
        "REMOTE_NAME_ABS = \"/\" + REMOTE_HTML\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_raw = row.get(match_to_col, \"\")\n",
        "    subject_name = normalize_person_name(resolve_match_to(subject_raw))\n",
        "    subject_name_b = f\"<strong>{subject_name}</strong>\" if subject_name else subject_name\n",
        "\n",
        "    pid = extract_person_id(row.get(id_col, \"\"))\n",
        "    matchee_name = norm_matchee_name(row.get(name_col, \"\")) or subject_name\n",
        "\n",
        "    if pid:\n",
        "        matchee_name_html = (\n",
        "            f'<a href=\"{TNG_BASE}/verticalchart.php?personID={pid}&tree={TNG_TREE}&parentset=0&display=vertical&generations=15\" '\n",
        "            f'target=\"_blank\">{matchee_name}</a>'\n",
        "        )\n",
        "    else:\n",
        "        matchee_name_html = matchee_name\n",
        "\n",
        "    cm_val = row.get(cm_col, \"0\")\n",
        "\n",
        "    tokens = split_tokens(row.get(path_col, \"\"))\n",
        "    gens_total = len(tokens)\n",
        "    tokens_disp = tokens[:7]\n",
        "\n",
        "    if \"common_husband\" in df.columns and \"common_wife\" in df.columns:\n",
        "        husband_raw = str(row.get(\"common_husband\",\"\")).strip()\n",
        "        wife_raw = str(row.get(\"common_wife\",\"\")).strip()\n",
        "        if not husband_raw and not wife_raw:\n",
        "            husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "    else:\n",
        "        husband_raw, wife_raw = derive_common_from_first_token(tokens)\n",
        "\n",
        "    header_html = build_header(\n",
        "        subject_name_b,\n",
        "        cm_val,\n",
        "        matchee_name_html,\n",
        "        gens_total,\n",
        "        truncate_first(husband_raw, 7) if husband_raw else \"\",\n",
        "        truncate_first(wife_raw, 7) if wife_raw else \"\"\n",
        "    )\n",
        "\n",
        "    if tokens_disp:\n",
        "        tokens_disp[0] = f\"<strong>{tokens_disp[0]}</strong>\"\n",
        "    sep = f\" {ARROW_ENTITY} \"\n",
        "    lineage_text = sep.join(tokens_disp) if tokens_disp else \"\"\n",
        "\n",
        "    q = _u.quote(subject_name)\n",
        "    quick = (\n",
        "        f'<a class=\"find-btn\" href=\"{REMOTE_NAME_ABS}?q={q}\" target=\"_blank\" rel=\"noopener\" '\n",
        "        f'title=\"Open a filtered view for {subject_name}\">Find</a>'\n",
        "    )\n",
        "\n",
        "    headers.append(header_html)\n",
        "    lineages.append(lineage_text)\n",
        "    findcol.append(quick)\n",
        "    subjects.append(subject_name)\n",
        "    first_ancestors.append(tokens[0] if tokens else \"\")\n",
        "\n",
        "LINEAGE_HEADER_SAFE = \"Lineage (Starting with oldest ancestor, the line is:)\"\n",
        "df[\"Match Summary\"] = headers\n",
        "df[LINEAGE_HEADER_SAFE] = lineages\n",
        "df[\"Find\"] = findcol\n",
        "df[\"Subject\"] = subjects\n",
        "df[\"First Ancestor\"] = [ _clean_piece(x) for x in first_ancestors ]\n",
        "display_df = df[[\"Find\",\"Match Summary\", LINEAGE_HEADER_SAFE]]\n",
        "\n",
        "# Column A CSV (optional upload)\n",
        "MATCH_COUSINS_CSV = \"the_match_cousins.csv\"\n",
        "display_df[[\"Match Summary\"]].to_csv(MATCH_COUSINS_CSV, index=False, encoding=\"iso-8859-15\")\n",
        "print(\"Wrote:\", os.path.abspath(MATCH_COUSINS_CSV))\n",
        "\n",
        "# ---------- 7) HTML (Times, sticky headers, sortable, back-to-top) ----------\n",
        "html_table = display_df.to_html(index=False, escape=False, classes=\"sortable\")\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">', 1\n",
        ")\n",
        "html_table = html_table.replace('<tbody>\\n<tr>', '<tbody>\\n<tr id=\"first-row\">', 1)\n",
        "\n",
        "# Header: rename \"Find\" -> \"Select:\" (no header checkbox)\n",
        "html_table = html_table.replace(\"<th>Find</th>\", '<th>Select:</th>', 1)\n",
        "\n",
        "# Sort-hint headers\n",
        "html_table = html_table.replace(\"<th>Match Summary</th>\", \"><th>Match Summary&ndash;click to sort</th>\", 1).replace(\"><th>Match Summary&ndash;click to sort</th>\", \"<th>Match Summary&ndash;click to sort</th>\", 1)\n",
        "html_table = html_table.replace(f\"<th>{_html.escape(LINEAGE_HEADER_SAFE)}</th>\", \"<th>Lineage (Starting with oldest ancestor&ndash;click to sort)</th>\", 1)\n",
        "\n",
        "colgroup_html = (\n",
        "    \"<colgroup>\\n\"\n",
        "    f\"  <col style=\\\"width:{FIND_COL_PX}px;\\\" />\\n\"\n",
        "    f\"  <col style=\\\"width:{COL_A_PX}px;\\\" />\\n\"\n",
        "    \"  <col />\\n\"\n",
        "    \"</colgroup>\\n\"\n",
        ")\n",
        "html_table = html_table.replace(\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">',\n",
        "    '<table border=\"1\" class=\"dataframe sortable\" id=\"refactor-table\">\\n' + colgroup_html, 1\n",
        ")\n",
        "\n",
        "TABLE_CSS = (\n",
        "    \"<style type=\\\"text/css\\\">\\n\"\n",
        "    \"  html { scroll-behavior: smooth; }\\n\"\n",
        "    \"  body { font-family: 'Times New Roman', Georgia, serif; font-size:100%; background:#ffffff; color:#222; margin:0; padding:0; line-height:1.5; }\\n\"\n",
        "    f\"  .wrap {{ max-width:{TABLE_WIDTH_PX}px; margin:0 auto; background:#ffffff; padding:20px; padding-bottom:48px; }}\\n\"\n",
        "    \"  a { color:#154b8b; text-decoration:none; } a:hover { text-decoration:underline; }\\n\"\n",
        "    \"  h1 { margin:0 0 6px 0; font-size:26px; line-height:1.2; text-align:center; }\\n\"\n",
        "    \"  .updated { font-size:12px; color:#555; text-align:center; margin:2px 0 10px 0; }\\n\"\n",
        "    \"  .sortbar { margin:6px 0 10px 0; font-size:13px; background:#ffffff; padding:6px 8px; border-radius:6px; display:flex; flex-wrap:wrap; gap:5px; align-items:center; border:1px solid #ddd; }\\n\"\n",
        "    \"  .btn { display:inline-block; border:1px solid #5b79b8; background:#5b79b8; color:#fff; padding:4px 9px; text-decoration:none; cursor:pointer; border-radius:5px; line-height:1.2; }\\n\"\n",
        "    \"  .btn:hover { background:#4668aa; }\\n\"\n",
        "    \"  input.btn.search { background:#fff; color:#111; border-color:#bbb; }\\n\"\n",
        "    \"  .find-cell { white-space:nowrap; }\\n\"\n",
        "    \"  .selbox { margin-right:6px; vertical-align:middle; }\\n\"\n",
        "    \"  .table-scroll { max-height:70vh; overflow-y:auto; overflow-x:auto; border:1px solid #ddd; }\\n\"\n",
        "    f\"  table.sortable {{ border-collapse:collapse; width:{TABLE_WIDTH_PX}px; table-layout:fixed; }}\\n\"\n",
        "    \"  table.sortable th, table.sortable td { border:1px solid #ddd; padding:6px 8px; vertical-align:top; }\\n\"\n",
        "    \"  table.sortable th { background:#e3eaf8; text-align:left; position:sticky; top:0; z-index:2; box-shadow:0 1px 0 #ccc; cursor:pointer; }\\n\"\n",
        "    \"  #first-row td { border-top:2px solid #999; }\\n\"\n",
        "    \"  .back-to-top { position:fixed; right:16px; bottom:16px; padding:6px 10px; border:1px solid #3e5a97; background:#5b79b8; color:#fff; cursor:pointer; border-radius:6px; font-size:12px; display:none; z-index:9999; }\\n\"\n",
        "    \"  .back-to-top:hover { background:#4668aa; }\\n\"\n",
        "    \"  #dynamicContent { margin:10px 0 14px 0; }\\n\"\n",
        "    \"  @media screen and (max-width: 820px) { .wrap { padding:12px; } h1 { font-size:22px; } }\\n\"\n",
        "    \"</style>\\n\"\n",
        ")\n",
        "\n",
        "# Toolbar (no Email). Add Show Selected / Show All for multi-name aggregation.\n",
        "DYNAMIC_BLOCK = (\n",
        "    \"<div class=\\\"sortbar\\\">\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/gengen/dna_cousin_surname_study.htm\\\" target=\\\"_blank\\\">Study Details</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/gengen/dna_theory_of_the_case.htm\\\" target=\\\"_blank\\\">Theory in Action</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"gengen/images/cousin-calculator.jpg\\\" target=\\\"_blank\\\">Cousin Connection</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"gengen/images/Shared_cM_Project_v4.jpg\\\" target=\\\"_blank\\\">Cousin by DNA</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"partials/match_count.htm\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Match Count</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"partials/lineage_count.htm\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Lineage Count</a>\\n\"\n",
        "    \"  <a class=\\\"btn\\\" href=\\\"https://yates.one-name.net/yates_ancestor_register.htm\\\">Ancestor Register</a>\\n\"\n",
        "    \"  <span class=\\\"btn\\\" id=\\\"show-selected\\\" title=\\\"Show all rows for the checked name(s)\\\">Show Selected</span>\\n\"\n",
        "    \"  <span class=\\\"btn\\\" id=\\\"show-all\\\" title=\\\"Show all rows\\\">Show All</span>\\n\"\n",
        "    \"  <span class=\\\"btn\\\" id=\\\"print-cousin-list\\\" style=\\\"cursor:pointer;\\\" title=\\\"Open a printable list of the *currently visible* rows\\\">Cousin List (Printable)</span>\\n\"\n",
        "    \"  <span class=\\\"btn\\\" id=\\\"clear-selected\\\">Clear</span>\\n\"\n",
        "    \"  <input type=\\\"text\\\" id=\\\"search-box\\\" class=\\\"btn search\\\" size=\\\"24\\\" value=\\\"\\\" placeholder=\\\"Search&hellip;\\\" />\\n\"\n",
        "    \"</div>\\n\"\n",
        "    \"<div id=\\\"dynamicContent\\\"></div>\\n\"\n",
        ")\n",
        "\n",
        "JS_COUNT_URL = COUNT_PUBLIC_URL.replace(\"'\", \"%27\")\n",
        "UPDATED_BLOCK = (\n",
        "    \"<div class=\\\"updated\\\">\"\n",
        "    f\"<a href=\\\"{HOME_URL}\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Home</a>\"\n",
        "    \" &nbsp;|&nbsp; Last updated: <span id=\\\"last-updated\\\"></span>\"\n",
        "    \" &nbsp;|&nbsp; Autosomal matches: <span id=\\\"auto-count\\\" class=\\\"js-count\\\"></span>\"\n",
        "    \" &nbsp;|&nbsp; Showing: <span id=\\\"showing-count\\\"></span>\"\n",
        "    \"</div>\"\n",
        ")\n",
        "\n",
        "# ---------- 8) Build final HTML via string.Template ----------\n",
        "page_tpl = Template(\"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>ONS Yates Study Autosomal DNA Register</title>\n",
        "$TABLE_CSS\n",
        "</head>\n",
        "<body id=\"top\">\n",
        "<div class=\"wrap\">\n",
        "  <h1>ONS Yates Study Autosomal DNA Register</h1>\n",
        "  $UPDATED_BLOCK\n",
        "  $DYNAMIC_BLOCK\n",
        "  <div class=\"table-scroll\">\n",
        "    $HTML_TABLE\n",
        "  </div>\n",
        "</div>\n",
        "<button id=\"back-to-top\" class=\"back-to-top\">&#9650; Top</button>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "(function(){\n",
        "  function textOf(cell){ return (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').trim().toLowerCase(); }\n",
        "  function sortTable(tbl, colIndex, dir){\n",
        "    var tb=tbl && tbl.tBodies ? tbl.tBodies[0] : null; if(!tb) return;\n",
        "    var rows=[].slice.call(tb.rows||[]);\n",
        "    var asc=(dir==='asc');\n",
        "    rows.sort(function(a,b){\n",
        "      var A=textOf(a.cells[colIndex]), B=textOf(b.cells[colIndex]);\n",
        "      if(A<B) return asc?-1:1; if(A>B) return asc?1:-1; return 0;\n",
        "    });\n",
        "    var frag=document.createDocumentFragment();\n",
        "    for(var i=0;i<rows.length;i++) frag.appendChild(rows[i]);\n",
        "    tb.appendChild(frag);\n",
        "    updateShowing();\n",
        "  }\n",
        "\n",
        "  function bindHeaderSort(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "    var ths=tbl.tHead.rows[0].cells; if(!ths) return;\n",
        "    for(var i=0;i<ths.length;i++)(function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dir='asc';\n",
        "      th.addEventListener('click',function(){\n",
        "        dir=(dir==='asc')?'desc':'asc';\n",
        "        for (var j = 0; j < ths.length; j++){ ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+[↑↓]/,''); }\n",
        "        th.innerHTML += (dir==='asc' ? ' &uarr;' : ' &darr;');\n",
        "        sortTable(tbl,idx,dir);\n",
        "      },false);\n",
        "    })(i);\n",
        "  }\n",
        "\n",
        "  function stampLastUpdated(){\n",
        "    var el=document.getElementById('last-updated'); if(!el) return;\n",
        "    var d=new Date(document.lastModified||new Date());\n",
        "    function z(n){return(n<10?'0':'')+n;}\n",
        "    el.innerHTML=d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes());\n",
        "  }\n",
        "\n",
        "  function formatWithCommas(n){\n",
        "    try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }catch(e){ return String(n||''); }\n",
        "  }\n",
        "  function loadAutoCount(){\n",
        "    var el=document.getElementById('auto-count'); if(!el) return;\n",
        "    var url='$JS_COUNT_URL';\n",
        "    try{\n",
        "      var xhr=new XMLHttpRequest(); xhr.open('GET', url+(url.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true);\n",
        "      xhr.onreadystatechange=function(){ if(xhr.readyState===4){ if(xhr.status>=200&&xhr.status<300){\n",
        "        var m=(xhr.responseText||'').match(/(\\\\d+)/); var num=m?m[1]:'';\n",
        "        el.textContent = formatWithCommas(num) || '(unavailable)';\n",
        "      } else { el.textContent='(unavailable)'; } } };\n",
        "      xhr.send(null);\n",
        "    }catch(e){ el.textContent='(unavailable)'; }\n",
        "  }\n",
        "\n",
        "  function visibleRowCount(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return 0;\n",
        "    var rows=tbl.tBodies[0].rows, n=0;\n",
        "    for(var i=0;i<rows.length;i++){ if(rows[i].style.display!=='none') n++; }\n",
        "    return n;\n",
        "  }\n",
        "  function updateShowing(){\n",
        "    var el=document.getElementById('showing-count'); if(!el) return;\n",
        "    el.textContent = formatWithCommas(visibleRowCount());\n",
        "  }\n",
        "\n",
        "  function getParam(name){ var m=location.search.match(new RegExp('[?&]'+name+'=([^&]+)')); return m?decodeURIComponent(m[1].replace(/\\\\+/g,' ')):''; }\n",
        "  function bindSearch(){\n",
        "    var box=document.getElementById('search-box'); var tbl=document.getElementById('refactor-table'); if(!(box && tbl && tbl.tBodies && tbl.tBodies[0])) return; var tb=tbl.tBodies[0];\n",
        "    function norm(s){ return String(s||'').replace(/\\\\s+/g,' ').toLowerCase(); }\n",
        "    function rowText(tr){ var t=''; for(var i=1;i<tr.cells.length;i++){ t+= ' ' + (tr.cells[i].textContent||tr.cells[i].innerText||''); } return norm(t); }\n",
        "    var cached=[]; (function seed(){ var rows=tb.rows; cached=[]; for(var i=0;i<rows.length;i++){ cached.push({el:rows[i], txt:rowText(rows[i])}); } })();\n",
        "    function apply(q){\n",
        "      q=norm(q);\n",
        "      for(var i=0;i<cached.length;i++){ var hit = !q || cached[i].txt.indexOf(q)>-1; cached[i].el.style.display = hit? '' : 'none'; }\n",
        "      updateShowing();\n",
        "    }\n",
        "    var to=null; function onInput(){ if(to) clearTimeout(to); to=setTimeout(function(){ apply(box.value); }, 60); }\n",
        "    box.addEventListener('input', onInput, false);\n",
        "    box.addEventListener('search', onInput, false);\n",
        "    var q0=getParam('q');\n",
        "    if(q0){ box.value=q0; apply(q0); try{ history.replaceState(null, '', location.pathname); }catch(e){} }\n",
        "    else { box.value=''; apply(''); setTimeout(function(){ if(!getParam('q')){ box.value=''; apply(''); } }, 0); }\n",
        "  }\n",
        "\n",
        "  // Collect all checkboxes\n",
        "  function allRowCheckboxes(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return [];\n",
        "    var tb=tbl.tBodies[0], out=[];\n",
        "    for(var i=0;i<tb.rows.length;i++){\n",
        "      var cb=tb.rows[i].querySelector('.selbox');\n",
        "      if(cb) out.push(cb);\n",
        "    }\n",
        "    return out;\n",
        "  }\n",
        "\n",
        "  // When you check any row for a given name, sync all rows with that same name.\n",
        "  function bindGroupSync(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!tbl) return;\n",
        "    tbl.addEventListener('click', function(e){\n",
        "      if(!(e.target && e.target.classList && e.target.classList.contains('selbox'))) return;\n",
        "      var nm = e.target.getAttribute('data-name') || '';\n",
        "      var checked = !!e.target.checked;\n",
        "      var cbs = allRowCheckboxes();\n",
        "      for(var i=0;i<cbs.length;i++){\n",
        "        if((cbs[i].getAttribute('data-name')||'') === nm){ cbs[i].checked = checked; }\n",
        "      }\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  // Show all matches for the selected name(s)\n",
        "  function bindShowSelected(){\n",
        "    var btn=document.getElementById('show-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(){\n",
        "      var cbs = allRowCheckboxes();\n",
        "      var names = {};\n",
        "      for(var i=0;i<cbs.length;i++){ if(cbs[i].checked){ names[cbs[i].getAttribute('data-name')||''] = true; } }\n",
        "      var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "      var tb=tbl.tBodies[0];\n",
        "      for(var r=0;r<tb.rows.length;r++){\n",
        "        var cb = tb.rows[r].querySelector('.selbox');\n",
        "        var nm = cb ? (cb.getAttribute('data-name')||'') : '';\n",
        "        tb.rows[r].style.display = names[nm] ? '' : 'none';\n",
        "      }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  function bindShowAll(){\n",
        "    var btn=document.getElementById('show-all'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(){\n",
        "      var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "      var tb=tbl.tBodies[0];\n",
        "      for(var i=0;i<tb.rows.length;i++){ tb.rows[i].style.display=''; }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  function bindClear(){\n",
        "    var btn=document.getElementById('clear-selected'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(){\n",
        "      var cbs=allRowCheckboxes(); for(var i=0;i<cbs.length;i++) cbs[i].checked=false;\n",
        "      var tbl=document.getElementById('refactor-table'); if(tbl && tbl.tBodies && tbl.tBodies[0]){\n",
        "        var tb=tbl.tBodies[0]; for(var j=0;j<tb.rows.length;j++){ tb.rows[j].style.display=''; }\n",
        "      }\n",
        "      updateShowing();\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  function bindBackToTop(){\n",
        "    var btn=document.getElementById('back-to-top'); if(!btn) return;\n",
        "    window.addEventListener('scroll', function(){ btn.style.display = (window.scrollY>200) ? 'block' : 'none'; }, {passive:true});\n",
        "    btn.addEventListener('click', function(){ window.scrollTo(0,0); }, false);\n",
        "  }\n",
        "\n",
        "  function addCheckboxes(){\n",
        "    var tbl=document.getElementById('refactor-table'); if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "    var tb=tbl.tBodies[0];\n",
        "    for(var i=0;i<tb.rows.length;i++){\n",
        "      var tr=tb.rows[i]; var cell=tr.cells[0]; var findBtn=cell ? cell.querySelector('.find-btn') : null;\n",
        "      var name = findBtn ? (findBtn.getAttribute('title')||'').replace('Open a filtered view for ','') : ('Row '+(i+1));\n",
        "      if(cell){\n",
        "        cell.classList.add('find-cell');\n",
        "        cell.innerHTML = '<input type=\"checkbox\" class=\"selbox\" title=\"Select this row\" data-name=\"'+name.replace(/\"/g,'&quot;')+'\" /> ' + cell.innerHTML.replace(/^\\s*/, '');\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Print uses the currently visible rows (works with Show Selected)\n",
        "  function bindPrintCousinList(){\n",
        "    var btn=document.getElementById('print-cousin-list'); if(!btn) return;\n",
        "    btn.addEventListener('click', function(){\n",
        "      var tbl=document.getElementById('refactor-table'); if(!tbl) return;\n",
        "      var headerHtml = '';\n",
        "      try{ headerHtml = tbl.tHead.innerHTML; }catch(e){}\n",
        "\n",
        "      var tb=tbl.tBodies[0];\n",
        "      var visibleRowsHtml = '';\n",
        "      var visibleCount = 0;\n",
        "      if(tb){\n",
        "        for (var i = 0; i < tb.rows.length; i++) {\n",
        "          if (tb.rows[i].style.display !== 'none') {\n",
        "            visibleRowsHtml += tb.rows[i].outerHTML;\n",
        "            visibleCount++;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "\n",
        "      var css = '<style type=\"text/css\">' +\n",
        "        \"body { font-family: 'Times New Roman', Georgia, serif; font-size:12px; margin: 20px; }\" +\n",
        "        \"h1 { font-size:20px; text-align:center; }\" +\n",
        "        \"table { border-collapse:collapse; width:100%; table-layout:fixed; }\" +\n",
        "        \"th, td { border:1px solid #999; padding: 5px 7px; vertical-align:top; text-align:left; word-wrap:break-word; }\" +\n",
        "        \"th { background:#f0f0f0; }\" +\n",
        "        \"a { color:#000; text-decoration:none; }\" +\n",
        "        \"th:first-child, td:first-child { display:none; }\" +\n",
        "        \"th:nth-child(2), td:nth-child(2) { width: 40% !important; }\" +\n",
        "        \"th:nth-child(3), td:nth-child(3) { width: 60% !important; }\" +\n",
        "        '</style>';\n",
        "\n",
        "      var tableHtml = '<table border=\"1\">' + '<thead>' + headerHtml + '</thead><tbody>' + visibleRowsHtml + '</tbody></table>';\n",
        "\n",
        "      var docHtml = '<html><head><title>Cousin List (Filtered)</title>' + css + '</head><body onload=\"window.print(); window.close();\">' +\n",
        "                    '<h1>Cousin List</h1>' +\n",
        "                    '<p>Showing ' + visibleCount + ' filtered records.</p>' +\n",
        "                    tableHtml +\n",
        "                    '</body></html>';\n",
        "\n",
        "      var win = window.open('', 'CousinPrint');\n",
        "      win.document.open();\n",
        "      win.document.write(docHtml);\n",
        "      win.document.close();\n",
        "      win.focus();\n",
        "    }, false);\n",
        "  }\n",
        "\n",
        "  function initShowingStatic(){ try{ document.getElementById('showing-count').textContent = document.getElementById('refactor-table').tBodies[0].rows.length; }catch(e){} }\n",
        "\n",
        "  document.addEventListener('DOMContentLoaded', function(){\n",
        "    addCheckboxes();\n",
        "    stampLastUpdated();\n",
        "    loadAutoCount();\n",
        "    bindHeaderSort();\n",
        "    bindSearch();\n",
        "    bindGroupSync();\n",
        "    bindShowSelected();\n",
        "    bindShowAll();\n",
        "    bindClear();\n",
        "    bindBackToTop();\n",
        "    bindPrintCousinList();\n",
        "    initShowingStatic();\n",
        "  });\n",
        "})();\n",
        " //]]>\n",
        "</script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\")\n",
        "\n",
        "final_html = page_tpl.safe_substitute(\n",
        "    TABLE_CSS=TABLE_CSS,\n",
        "    UPDATED_BLOCK=UPDATED_BLOCK,\n",
        "    DYNAMIC_BLOCK=DYNAMIC_BLOCK,\n",
        "    HTML_TABLE=html_table,\n",
        "    JS_COUNT_URL=JS_COUNT_URL\n",
        ")\n",
        "\n",
        "# ---------- 8.1) Partials builder (match_count.htm + lineage_count.htm) ----------\n",
        "def _norm_code_for_count(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\",\" \").strip()\n",
        "    t = re.sub(r\"\\s{2,}\",\" \",t)\n",
        "    return t.lower()\n",
        "\n",
        "def _partial_css_wrapper():\n",
        "    return (\n",
        "        \"<style type=\\\"text/css\\\">\\n\"\n",
        "        \"  html { scroll-behavior: smooth; }\\n\"\n",
        "        \"  body { font-family:'Times New Roman', Georgia, serif; background:#ffffff; color:#222; margin:0; padding:0; }\\n\"\n",
        "        f\"  .wrap {{ max-width:{TABLE_WIDTH_PX}px; margin:0 auto; background:#ffffff; padding:20px; padding-bottom:32px; }}\\n\"\n",
        "        \"  a { color:#154b8b; text-decoration:none; } a:hover { text-decoration:underline; }\\n\"\n",
        "        \"  h1 { margin:0 0 8px 0; font-size:24px; line-height:1.2; text-align:center; }\\n\"\n",
        "        \"  .updated { font-size:12px; color:#555; text-align:center; margin:4px 0 12px 0; }\\n\"\n",
        "        \"  .table-scroll { max-height:75vh; overflow:auto; border:1px solid #ddd; }\\n\"\n",
        "        \"  table.sortable { border-collapse:collapse; width:100%; min-width:720px; }\\n\"\n",
        "        \"  table.sortable th, table.sortable td { border:1px solid #ddd; padding:6px 8px; }\\n\"\n",
        "        \"  table.sortable th { background:#e3eaf8; position:sticky; top:0; text-align:left; z-index:2; }\\n\"\n",
        "        \"</style>\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_header_block(showing_placeholder_js_ready):\n",
        "    return (\n",
        "        \"<div class=\\\"updated\\\">\"\n",
        "        f\"<a href=\\\"{HOME_URL}\\\" target=\\\"_blank\\\" rel=\\\"noopener\\\">Home</a>\"\n",
        "        \" &nbsp;|&nbsp; Last updated: <span id=\\\"last-updated\\\"></span>\"\n",
        "        \" &nbsp;|&nbsp; Autosomal matches: <span id=\\\"auto-count\\\"></span>\"\n",
        "        f\" &nbsp;|&nbsp; Showing: <span id=\\\"showing-count\\\">{showing_placeholder_js_ready}</span>\"\n",
        "        \"</div>\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_js_block():\n",
        "    return (\n",
        "        \"<script type=\\\"text/javascript\\\">\\n//<![CDATA[\\n\"\n",
        "        \"(function(){\\n\"\n",
        "        \" function z(n){return (n<10?'0':'')+n;}\\n\"\n",
        "        \" function stamp(){var el=document.getElementById('last-updated'); if(!el) return; var d=new Date(document.lastModified||new Date()); el.innerHTML=d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes());}\\n\"\n",
        "        f\" var URL='{JS_COUNT_URL}';\\n\"\n",
        "        \" function fmt(n){try{var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US');}catch(e){return String(n||'')}}\\n\"\n",
        "        \" function load(){var el=document.getElementById('auto-count'); if(!el) return; try{var xhr=new XMLHttpRequest(); xhr.open('GET', URL+(URL.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true); xhr.onreadystatechange=function(){if(xhr.readyState===4){if(xhr.status>=200&&xhr.status<300){var m=(xhr.responseText||'').match(/(\\\\d+)/); el.textContent=fmt(m?m[1]:'');} else {el.textContent='(unavailable)';}}}; xhr.send(null);}catch(e){el.textContent='(unavailable)';}}\\n\"\n",
        "        \" function setShowing(){var el=document.getElementById('refactor-table'); var n=0; try{var rows=el.tBodies[0].rows; for(var i=0;i<rows.length;i++){ if(rows[i].style.display!=='none') n++; }}catch(e){}; var sc=document.getElementById('showing-count'); if(sc) sc.textContent = fmt(n); }\\n\"\n",
        "        \" document.addEventListener('DOMContentLoaded', function(){ stamp(); load(); setShowing(); }, false);\\n\"\n",
        "        \"})();\\n\"\n",
        "        \"//]]>\\n</script>\\n\"\n",
        "    )\n",
        "\n",
        "def _partial_shell(title, table_html, showing_for_initial):\n",
        "    css = _partial_css_wrapper()\n",
        "    header = _partial_header_block(showing_for_initial)\n",
        "    js = _partial_js_block()\n",
        "    return (\n",
        "        \"<!DOCTYPE html PUBLIC \\\"-//W3C//DTD XHTML 1.0 Transitional//EN\\\"\\n\"\n",
        "        \"  \\\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\\\">\\n\"\n",
        "        \"<html xmlns=\\\"http://www.w3.org/1999/xhtml\\\" lang=\\\"en\\\">\\n<head>\\n\"\n",
        "        \"<meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=iso-8859-15\\\" />\\n\"\n",
        "        f\"<title>{_html.escape(title)}</title>\\n\"\n",
        "        f\"{css}</head>\\n<body>\\n\"\n",
        "        \"<div class=\\\"wrap\\\">\\n\"\n",
        "        f\"  <h1>{_html.escape(title)}</h1>\\n\"\n",
        "        f\"  {header}\"\n",
        "        \"  <div class=\\\"table-scroll\\\">\\n\"\n",
        "        f\"    {table_html}\\n\"\n",
        "        \"  </div>\\n\"\n",
        "        \"</div>\\n\"\n",
        "        f\"{js}</body>\\n</html>\"\n",
        "    )\n",
        "\n",
        "def build_and_write_partials(main_df: pd.DataFrame):\n",
        "    os.makedirs(\"partials\", exist_ok=True)\n",
        "\n",
        "    # MATCH COUNT\n",
        "    codes_raw = main_df[match_to_col].astype(str).map(lambda x: x.strip())\n",
        "    keys_norm = codes_raw.map(_norm_code_for_count)\n",
        "\n",
        "    counts_series = keys_norm.value_counts(dropna=False)\n",
        "    counts = counts_series.reset_index()\n",
        "    if counts.shape[1] >= 2:\n",
        "        counts.columns = [\"norm_key\", \"Count\"]\n",
        "    else:\n",
        "        counts[\"norm_key\"] = counts.index.astype(str)\n",
        "        counts[\"Count\"] = counts_series.values\n",
        "        counts = counts[[\"norm_key\",\"Count\"]]\n",
        "\n",
        "    first_display = {}\n",
        "    for code_disp, k in zip(codes_raw.tolist(), keys_norm.tolist()):\n",
        "        if k not in first_display and str(k) != \"\":\n",
        "            first_display[k] = code_disp\n",
        "\n",
        "    counts[\"Code\"] = counts[\"norm_key\"].map(lambda k: first_display.get(k, \"\"))\n",
        "    counts[\"Unmasked\"] = counts[\"norm_key\"].map(lambda k: MATCH_TO_UNMASKED.get(k, \"\"))\n",
        "\n",
        "    counts = counts.sort_values(by=[\"Code\",\"Count\"], ascending=[True, False], kind=\"mergesort\").reset_index(drop=True)\n",
        "    match_df = counts[[\"Code\",\"Unmasked\",\"Count\"]]\n",
        "\n",
        "    match_tbl = match_df.to_html(index=False, classes=\"sortable\", border=1, table_id=\"refactor-table\")\n",
        "    match_html = _partial_shell(\"Match Count\", match_tbl, \"0\")\n",
        "    mc_local = os.path.join(\"partials\", \"match_count.htm\")\n",
        "    with open(mc_local, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(match_html)\n",
        "    print(\"[OK] Wrote partial:\", mc_local)\n",
        "\n",
        "    # LINEAGE COUNT\n",
        "    first_series = main_df.get(\"First Ancestor\", pd.Series(dtype=str)).astype(str).map(lambda x: x.strip())\n",
        "    vc = first_series[first_series != \"\"].value_counts(dropna=False)\n",
        "    lin_df = vc.reset_index()\n",
        "    if lin_df.shape[1] >= 2:\n",
        "        lin_df.columns = [\"First Ancestor\",\"Count\"]\n",
        "    else:\n",
        "        lin_df[\"First Ancestor\"] = lin_df.index.astype(str)\n",
        "        lin_df[\"Count\"] = vc.values\n",
        "        lin_df = lin_df[[\"First Ancestor\",\"Count\"]]\n",
        "    lin_df = lin_df.sort_values([\"Count\",\"First Ancestor\"], ascending=[False, True], kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    lin_tbl = lin_df.to_html(index=False, classes=\"sortable\", border=1, table_id=\"refactor-table\")\n",
        "    lin_html = _partial_shell(\"Lineage Count\", lin_tbl, \"0\")\n",
        "    lc_local = os.path.join(\"partials\", \"lineage_count.htm\")\n",
        "    with open(lc_local, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(lin_html)\n",
        "    print(\"[OK] Wrote partial:\", lc_local)\n",
        "\n",
        "    # COUSIN LIST (PRINTABLE - FULL)\n",
        "    cousin_df = main_df[[\"Match Summary\"]].copy()\n",
        "    cousin_df = cousin_df.sort_values(by=\"Match Summary\", ascending=True, kind=\"mergesort\").reset_index(drop=True)\n",
        "\n",
        "    cousin_tbl = cousin_df.to_html(index=False, escape=False, classes=\"sortable\", border=1, table_id=\"refactor-table\")\n",
        "\n",
        "    cousin_html = _partial_shell(\"Cousin List (Printable)\", cousin_tbl, \"0\")\n",
        "    cl_local = os.path.join(\"partials\", \"cousin_list_print.htm\")\n",
        "    with open(cl_local, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(cousin_html)\n",
        "    print(\"[OK] Wrote partial:\", cl_local)\n",
        "\n",
        "    return mc_local, lc_local, cl_local\n",
        "\n",
        "# Build partials now (based on full df with helper columns)\n",
        "PARTIAL_MATCH_LOCAL, PARTIAL_LINEAGE_LOCAL, PARTIAL_COUSIN_LOCAL = build_and_write_partials(df)\n",
        "\n",
        "# ---------- 9) Save locally ----------\n",
        "with open(LOCAL_HTML, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "    f.write(final_html)\n",
        "print(\"Saved:\", os.path.abspath(LOCAL_HTML))\n",
        "\n",
        "# ---------- 10) Uploads (main page, Column A CSV, autosomal count if present, partials) ----------\n",
        "def save_and_upload_all():\n",
        "    if not all(os.environ.get(k) for k in ['FTP_HOST','FTP_USER','FTP_PASS']):\n",
        "        print(\"Missing FTP creds; skipping uploads.\")\n",
        "        return\n",
        "    try:\n",
        "        ftps = ftp_connect()\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, LOCAL_HTML, REMOTE_HTML)\n",
        "        except Exception as e:\n",
        "            print(\"Upload main HTML failed:\", e)\n",
        "        if UPLOAD_COLUMN_A and os.path.exists(MATCH_COUSINS_CSV):\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, MATCH_COUSINS_CSV, MATCH_COUSINS_CSV)\n",
        "            except Exception as e:\n",
        "                print(\"Upload Column A CSV failed:\", e)\n",
        "        if os.path.exists(LOCAL_COUNT_FILE):\n",
        "            try:\n",
        "                ftp_upload_overwrite(ftps, LOCAL_COUNT_FILE, REMOTE_COUNT_NAME)\n",
        "            except Exception as e:\n",
        "                print(\"Upload autosomal count failed:\", e)\n",
        "\n",
        "        # Upload partials to /partials/\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, PARTIAL_MATCH_LOCAL, posixpath.join(\"partials\",\"match_count.htm\"))\n",
        "        except Exception as e:\n",
        "            print(\"Upload match_count.htm failed:\", e)\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, PARTIAL_LINEAGE_LOCAL, posixpath.join(\"partials\",\"lineage_count.htm\"))\n",
        "        except Exception as e:\n",
        "            print(\"Upload lineage_count.htm failed:\", e)\n",
        "\n",
        "        # COUSIN LIST PARTIAL\n",
        "        try:\n",
        "            ftp_upload_overwrite(ftps, PARTIAL_COUSIN_LOCAL, posixpath.join(\"partials\",\"cousin_list_print.htm\"))\n",
        "        except Exception as e:\n",
        "            print(\"Upload cousin_list_print.htm failed:\", e)\n",
        "\n",
        "        try: ftps.quit()\n",
        "        except Exception: pass\n",
        "        print(\"Uploads complete.\")\n",
        "    except Exception as e:\n",
        "        print(\"FTP session failed:\", e)\n",
        "        traceback.print_exc()\n",
        "\n",
        "save_and_upload_all()\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "# ====== CUT STOP [1/1] INDEPENDENT CELL 2 ==\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0e7rlZGbiAS",
        "outputId": "e38edd5f-ef2b-4b42-d740-83376ca8e161"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:658: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:658: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-417621202.py:658: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  cell.innerHTML = '<input type=\"checkbox\" class=\"selbox\" title=\"Select this row\" data-name=\"'+name.replace(/\"/g,'&quot;')+'\" /> ' + cell.innerHTML.replace(/^\\s*/, '');\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pulled: partials/match_to_unmasked.csv -> /content/match_to_unmasked.server.csv\n",
            "Loaded resolver: 79 codes\n",
            "Loaded main CSV: 7 rows, 9 cols\n",
            "Wrote: /content/the_match_cousins.csv\n",
            "[OK] Wrote partial: partials/match_count.htm\n",
            "[OK] Wrote partial: partials/lineage_count.htm\n",
            "[OK] Wrote partial: partials/cousin_list_print.htm\n",
            "Saved: /content/ons_yates_dna_register.htm\n",
            "Uploaded: ons_yates_dna_register.htm -> ons_yates_dna_register.htm\n",
            "Uploaded: the_match_cousins.csv -> the_match_cousins.csv\n",
            "Uploaded: /content/autosomal_count.txt -> autosomal_count.txt\n",
            "Uploaded: partials/match_count.htm -> partials/match_count.htm\n",
            "Uploaded: partials/lineage_count.htm -> partials/lineage_count.htm\n",
            "Uploaded: partials/cousin_list_print.htm -> partials/cousin_list_print.htm\n",
            "Uploads complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3"
      ],
      "metadata": {
        "id": "INiJljOS1kRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gold 3 Ancestor Register (Dynamic header + live counts)\n",
        "# ====== CUT START [1/6] RULES + IMPORTS + SECRETS ==============================================\n",
        "# RON RULES — QUICK CODE CARD (v2025.10.27-Refined)\n",
        "# 1) EXECUTION: Complete & runnable in Colab; ISO-8859-15 (ASCII-only in source).\n",
        "# 2) PUNCTUATION IN STRINGS: Use HTML entities (&rsquo; &ldquo; &rdquo; &mdash; &rarr;).\n",
        "# 3) CONTENT: Deliver full runnable code (no snippets). No fabrication or inference.\n",
        "# 4) Python code (inline, executable, CUT-ready section)\n",
        "# 5) HTML: XHTML 1.0 Transitional style acceptable; avoid HTML5-only tags if not needed.\n",
        "# 6) INTEGRITY: Work in CUT-ready sections only; exactly five # lines after each section.\n",
        "\n",
        "import os, re, socket, traceback\n",
        "from ftplib import FTP_TLS\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "import html as _html\n",
        "import pandas as pd\n",
        "# ====== CUT STOP  [1/6] RULES + IMPORTS + SECRETS ===============================================\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "\n",
        "\n",
        "# ====== CUT START [2/6] SECRETS + LOAD DATA + COUNTS + PATHS ===================================\n",
        "# --- Securely load secrets (Colab or env) ---\n",
        "try:\n",
        "    from google.colab import userdata  # type: ignore\n",
        "    os.environ['FTP_HOST'] = userdata.get('FTP_HOST')\n",
        "    os.environ['FTP_USER'] = userdata.get('FTP_USER')\n",
        "    os.environ['FTP_PASS'] = userdata.get('FTP_PASS')\n",
        "    try:  os.environ['FTP_PORT'] = userdata.get('FTP_PORT')\n",
        "    except Exception: os.environ.setdefault('FTP_PORT', '21')\n",
        "    try:  os.environ['FTP_DIR']  = userdata.get('FTP_DIR')\n",
        "    except Exception: os.environ.setdefault('FTP_DIR', '')\n",
        "except Exception:\n",
        "    os.environ.setdefault('FTP_HOST', '')\n",
        "    os.environ.setdefault('FTP_USER', '')\n",
        "    os.environ.setdefault('FTP_PASS', '')\n",
        "    os.environ.setdefault('FTP_PORT', '21')\n",
        "    os.environ.setdefault('FTP_DIR', '')\n",
        "\n",
        "FTP_DIR  = os.environ.get('FTP_DIR', '').strip().strip('/')\n",
        "COUNT_PUBLIC_URL = (\"/%s/%s\" % (FTP_DIR, \"autosomal_count.txt\")) if FTP_DIR else \"/autosomal_count.txt\"\n",
        "\n",
        "# Inputs\n",
        "INPUT_CSV   = \"final_combined_df_with_value_labels.csv\"\n",
        "OUTPUT_NAME = \"yates_ancestor_register.htm\"\n",
        "HOME_URL    = \"https://yates.one-name.net/ons_yates_dna_register.htm\"\n",
        "\n",
        "# Load CSV\n",
        "try:\n",
        "    df = pd.read_csv(INPUT_CSV)\n",
        "    print(\"[OK] Loaded CSV:\", INPUT_CSV, f\"rows={len(df)}, cols={len(df.columns)}\")\n",
        "except FileNotFoundError:\n",
        "    raise SystemExit(\"[ERROR] Missing file: final_combined_df_with_value_labels.csv\")\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Failed to load CSV:\", e)\n",
        "    raise SystemExit(\"CSV loading failed.\")\n",
        "\n",
        "# Normalize haplogroup column presence\n",
        "if 'haplogroup' not in df.columns:\n",
        "    df['haplogroup'] = ''\n",
        "else:\n",
        "    df['haplogroup'] = df['haplogroup'].fillna('')\n",
        "\n",
        "# Read autosomal count locally if present (fallback display only)\n",
        "autosomal_count = None\n",
        "try:\n",
        "    with open(\"autosomal_count.txt\", \"r\") as f:\n",
        "        autosomal_count = int(f.read().strip())\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Optional delta vs previous run\n",
        "prev_count, additional_str = None, \"\"\n",
        "if os.path.exists(\"autosomal_count_prev.txt\"):\n",
        "    try:\n",
        "        with open(\"autosomal_count_prev.txt\", \"r\") as f:\n",
        "            prev_count = int(f.read().strip())\n",
        "        if autosomal_count is not None and prev_count is not None:\n",
        "            diff = autosomal_count - prev_count\n",
        "            if diff != 0:\n",
        "                additional_str = \" (+%d since last run)\" % diff\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Human-readable fallback timestamp (not shown; JS will stamp dynamically)\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "updated_fallback = now.strftime(\"%Y-%m-%d %H:%M\")\n",
        "# ====== CUT STOP  [2/6] SECRETS + LOAD DATA + COUNTS + PATHS ====================================\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "\n",
        "\n",
        "# ====== CUT START [3/6] MAP COLUMN B (masked code) -> COLUMN C (unmasked name) =================\n",
        "# Column letters in MAIN df:\n",
        "#   A = ID#\n",
        "#   B = match to (masked)\n",
        "#   C = Unmasked Name (output)\n",
        "\n",
        "A_IDX = 0\n",
        "B_IDX = 1\n",
        "C_IDX = 2\n",
        "\n",
        "def _norm_code(s):\n",
        "    t = str(s or \"\").strip()\n",
        "    if (t.startswith('\"') and t.endswith('\"')) or (t.startswith(\"'\") and t.endswith(\"'\")):\n",
        "        t = t[1:-1]\n",
        "    t = t.replace(\"\\u00a0\", \" \").strip()\n",
        "    t = re.sub(r\"\\s{2,}\", \" \", t)\n",
        "    return t.lower()\n",
        "\n",
        "REMOTE_PATH = \"partials/match_to_unmasked.csv\"\n",
        "LOCAL_PATH  = \"match_to_unmasked.csv\"\n",
        "\n",
        "# Pull resolver if not present\n",
        "if not os.path.exists(LOCAL_PATH):\n",
        "    print(\"Pulling resolver CSV from server...\")\n",
        "    with FTP_TLS(timeout=30) as ftps:\n",
        "        ftps.connect(os.environ.get(\"FTP_HOST\",\"\"), int(os.environ.get(\"FTP_PORT\",\"21\")))\n",
        "        ftps.login(os.environ.get(\"FTP_USER\",\"\"), os.environ.get(\"FTP_PASS\",\"\"))\n",
        "        try: ftps.prot_p()\n",
        "        except Exception: pass\n",
        "        try: ftps.set_pasv(True)\n",
        "        except Exception: pass\n",
        "        if FTP_DIR:\n",
        "            for p in FTP_DIR.split(\"/\"):\n",
        "                if not p: continue\n",
        "                try: ftps.cwd(p)\n",
        "                except Exception:\n",
        "                    try: ftps.mkd(p)\n",
        "                    except Exception: pass\n",
        "                    ftps.cwd(p)\n",
        "        try: ftps.cwd(\"partials\")\n",
        "        except Exception: pass\n",
        "        with open(LOCAL_PATH, \"wb\") as f:\n",
        "            ftps.retrbinary(\"RETR match_to_unmasked.csv\", f.write)\n",
        "    print(\"Resolver saved:\", os.path.abspath(LOCAL_PATH))\n",
        "else:\n",
        "    print(\"Using cached resolver:\", os.path.abspath(LOCAL_PATH))\n",
        "\n",
        "def _load_resolver(path):\n",
        "    last_err = None\n",
        "    for enc in (\"utf-8-sig\",\"iso-8859-15\",\"utf-8\",\"cp1252\",\"latin1\"):\n",
        "        try:\n",
        "            m = pd.read_csv(path, dtype=str, keep_default_na=False, encoding=enc)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            m = None\n",
        "    if m is None:\n",
        "        raise RuntimeError(\"Unable to read resolver CSV: %s (%r)\" % (path, last_err))\n",
        "    cols = {c.lower(): c for c in m.columns}\n",
        "    if \"code\" not in cols or \"unmasked\" not in cols:\n",
        "        raise ValueError(\"Resolver CSV must have columns: code, unmasked\")\n",
        "    m = m[[cols[\"code\"], cols[\"unmasked\"]]].copy()\n",
        "    m[\"__key__\"] = m[cols[\"code\"]].map(_norm_code)\n",
        "    m[\"__val__\"] = m[cols[\"unmasked\"]].astype(str)\n",
        "    m = m.drop_duplicates(subset=\"__key__\", keep=\"first\")\n",
        "    return dict(zip(m[\"__key__\"], m[\"__val__\"]))\n",
        "\n",
        "resolver_map = _load_resolver(LOCAL_PATH)\n",
        "\n",
        "if df.shape[1] < 3:\n",
        "    raise ValueError(\"Main df must have at least 3 columns: A(ID#), B(match to), C(unmasked).\")\n",
        "\n",
        "masked_raw = df.iloc[:, B_IDX].astype(str)\n",
        "masked_key = masked_raw.map(_norm_code)\n",
        "resolved   = masked_key.map(resolver_map)\n",
        "\n",
        "df.iloc[:, C_IDX] = resolved.fillna(\"\")\n",
        "\n",
        "mapped = int(resolved.notna().sum())\n",
        "total  = len(df)\n",
        "print(\"[OK] Column B -> C mapping:\", mapped, \"/\", total, \"unmatched:\", total - mapped)\n",
        "# ====== CUT STOP  [3/6] MAP COLUMN B (masked code) -> COLUMN C (unmasked name) =================\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "\n",
        "\n",
        "# ====== CUT START [4/6] XHTML TEMPLATE + TABLE (Dynamic header + counts + search) ===============\n",
        "_BTN_BG   = \"#5b79b8\"\n",
        "_BTN_BG_H = \"#4668aa\"\n",
        "_TH_BG    = \"#e3eaf8\"\n",
        "_LINK     = \"#154b8b\"\n",
        "\n",
        "# Fallback number text for initial render; JS will overwrite with live values\n",
        "auto_text = \"Unknown\" if autosomal_count is None else str(autosomal_count)\n",
        "\n",
        "full_html_template = \"\"\"<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n",
        " \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n",
        "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n",
        "<head>\n",
        "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-15\" />\n",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n",
        "<title>Ancestor Register</title>\n",
        "<style type=\"text/css\">\n",
        "  html { scroll-behavior: smooth; }\n",
        "  body { margin:0; padding:0; font-family: \"Times New Roman\", Georgia, serif; background:#ffffff; color:#222; font-size:14px; }\n",
        "  a { color:%(LINK)s; text-decoration:none; } a:hover { text-decoration:underline; }\n",
        "\n",
        "  .intro { padding:20px; text-align:center; }\n",
        "  .intro h2 { margin:0 0 6px 0; font-size:24px; line-height:1.2; }\n",
        "  .meta { font-size:12px; color:#555; margin:4px 0 10px 0; display:inline-block; }\n",
        "\n",
        "  .toolbar { margin:8px auto 12px auto; display:flex; flex-wrap:wrap; gap:6px; justify-content:center; }\n",
        "  .btn { display:inline-block; border:1px solid %(BTN_BG)s; background:%(BTN_BG)s; color:#fff;\n",
        "         padding:4px 9px; border-radius:6px; font-size:13px; line-height:1.2; text-decoration:none;\n",
        "         cursor:pointer; user-select:none; transition:background 0.2s, transform 0.1s; }\n",
        "  .btn:hover { background:%(BTN_BG_H)s; transform:translateY(-1px); }\n",
        "  .btn.light { background:#ffffff; color:#111; border-color:#bbb; }\n",
        "\n",
        "  .output-table { max-height:75vh; overflow:auto; border:1px solid #ddd; margin:0 20px 24px 20px; }\n",
        "\n",
        "  table.sortable { width:100%%; border-collapse:collapse; min-width:720px; table-layout:auto; }\n",
        "  table.sortable th, table.sortable td { border:1px solid #ddd; padding:6px 8px; background:#ffffff; white-space:nowrap; }\n",
        "  table.sortable th { position:sticky; top:0; z-index:2; text-align:left; cursor:pointer; background:%(TH_BG)s; box-shadow:0 1px 0 #ccc; }\n",
        "  table.sortable tr#first-row td { border-top:2px solid #999 !important; }\n",
        "\n",
        "  #searchBox { padding:4px 8px; font-size:13px; border:1px solid #bbb; border-radius:6px; outline:none; }\n",
        "\n",
        "  .back-to-top { position:fixed; right:16px; bottom:16px; padding:6px 10px;\n",
        "                 border:1px solid %(BTN_BG)s; background:%(BTN_BG)s; color:#fff;\n",
        "                 border-radius:6px; font-size:12px; display:none; z-index:9999; cursor:pointer; }\n",
        "  .back-to-top:hover { background:%(BTN_BG_H)s; }\n",
        "\n",
        "  @media screen and (max-width: 820px) {\n",
        "    .intro { padding:14px; }\n",
        "    .output-table { margin:0 12px 20px 12px; }\n",
        "    .intro h2 { font-size:20px; }\n",
        "    table.sortable { min-width:560px; }\n",
        "  }\n",
        "</style>\n",
        "<script type=\"text/javascript\">\n",
        "//<![CDATA[\n",
        "// Basic helpers\n",
        "function _cellText(cell){\n",
        "  var t = (cell && (cell.textContent || cell.innerText) || '').replace(/\\\\s+/g,' ').replace(/^\\\\s+|\\\\s+$/g,'').toLowerCase();\n",
        "  return t;\n",
        "}\n",
        "function _asNumber(s){\n",
        "  var m = (s||'').replace(/[^0-9.\\\\-]/g,'');\n",
        "  if(m.length===0) return NaN;\n",
        "  var v = parseFloat(m);\n",
        "  return isNaN(v) ? NaN : v;\n",
        "}\n",
        "\n",
        "// Sorting\n",
        "function sortTableByColumn(tbl, colIndex, dirAsc){\n",
        "  if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "  var tb = tbl.tBodies[0];\n",
        "  var rows = Array.prototype.slice.call(tb.rows || []);\n",
        "  rows.sort(function(a,b){\n",
        "    var A = _cellText(a.cells[colIndex] || null);\n",
        "    var B = _cellText(b.cells[colIndex] || null);\n",
        "    var nA = _asNumber(A), nB = _asNumber(B);\n",
        "    if(!isNaN(nA) && !isNaN(nB)){ return dirAsc ? (nA - nB) : (nB - nA); }\n",
        "    if(A < B) return dirAsc ? -1 : 1;\n",
        "    if(A > B) return dirAsc ?  1 : -1;\n",
        "    return 0;\n",
        "  });\n",
        "  var frag = document.createDocumentFragment();\n",
        "  for(var i=0;i<rows.length;i++){ frag.appendChild(rows[i]); }\n",
        "  tb.appendChild(frag);\n",
        "  updateShowingCount();\n",
        "}\n",
        "function bindHeaderSort(){\n",
        "  var tbl = document.getElementById('refactor-table');\n",
        "  if(!(tbl && tbl.tHead && tbl.tHead.rows.length)) return;\n",
        "  var ths = tbl.tHead.rows[0].cells || [];\n",
        "  for(var i=0;i<ths.length;i++){\n",
        "    (function(idx){\n",
        "      var th = ths[idx];\n",
        "      var dirAsc = true;\n",
        "      th.addEventListener('click', function(){\n",
        "        for(var j=0;j<ths.length;j++){ ths[j].innerHTML = ths[j].innerHTML.replace(/\\\\s+[\\\\u2191\\\\u2193]$/, ''); }\n",
        "        sortTableByColumn(tbl, idx, dirAsc);\n",
        "        th.innerHTML = th.innerHTML.replace(/\\\\s+[\\\\u2191\\\\u2193]$/, '') + (dirAsc ? ' \\\\u2191' : ' \\\\u2193');\n",
        "        dirAsc = !dirAsc;\n",
        "      }, false);\n",
        "    })(i);\n",
        "  }\n",
        "}\n",
        "\n",
        "// Filter + live showing count\n",
        "function filterTable(){\n",
        "  var q = (document.getElementById('searchBox').value || '').toLowerCase();\n",
        "  var tbl = document.getElementById('refactor-table');\n",
        "  if(!(tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "  var rows = tbl.tBodies[0].rows || [];\n",
        "  for(var i=0;i<rows.length;i++){\n",
        "    var cells = rows[i].cells, hit=false;\n",
        "    for(var j=0;j<cells.length;j++){\n",
        "      var txt = (cells[j].textContent || cells[j].innerText || '').toLowerCase();\n",
        "      if(txt.indexOf(q) > -1){ hit=true; break; }\n",
        "    }\n",
        "    rows[i].style.display = hit ? '' : 'none';\n",
        "  }\n",
        "  updateShowingCount();\n",
        "}\n",
        "function updateShowingCount(){\n",
        "  var el = document.getElementById('showing-count');\n",
        "  var tbl = document.getElementById('refactor-table');\n",
        "  if(!(el && tbl && tbl.tBodies && tbl.tBodies[0])) return;\n",
        "  var rows = tbl.tBodies[0].rows || [];\n",
        "  var vis = 0;\n",
        "  for(var i=0;i<rows.length;i++){ if(rows[i].style.display !== 'none') vis++; }\n",
        "  el.textContent = vis;\n",
        "}\n",
        "\n",
        "// Dynamic stamps\n",
        "function z(n){ return (n<10 ? '0' : '') + n; }\n",
        "function stampLastUpdated(){\n",
        "  var el=document.getElementById('last-updated'); if(!el) return;\n",
        "  var d=new Date(document.lastModified || new Date());\n",
        "  el.innerHTML=d.getFullYear()+'-'+z(d.getMonth()+1)+'-'+z(d.getDate())+' '+z(d.getHours())+':'+z(d.getMinutes());\n",
        "}\n",
        "function formatWithCommas(n){\n",
        "  try{ var x=parseInt(String(n||'').replace(/[^0-9\\\\-]/g,''),10); if(isNaN(x)) return ''; return x.toLocaleString('en-US'); }catch(e){ return String(n||''); }\n",
        "}\n",
        "function loadAutoCount(){\n",
        "  var el=document.getElementById('auto-count'); if(!el) return;\n",
        "  var url='{COUNT_URL}';\n",
        "  try{\n",
        "    var xhr=new XMLHttpRequest(); xhr.open('GET', url+(url.indexOf('?')>-1?'':'?v='+(new Date()).getTime()), true);\n",
        "    xhr.onreadystatechange=function(){ if(xhr.readyState===4){\n",
        "      if(xhr.status>=200&&xhr.status<300){\n",
        "        var m=(xhr.responseText||'').match(/(\\\\d+)/); var num=m?m[1]:'';\n",
        "        el.textContent = formatWithCommas(num) || '(unavailable)';\n",
        "      } else { el.textContent='(unavailable)'; }\n",
        "    }};\n",
        "    xhr.send(null);\n",
        "  }catch(e){ el.textContent='(unavailable)'; }\n",
        "}\n",
        "\n",
        "// Back-to-top\n",
        "function bindBackToTop(){\n",
        "  var btn = document.getElementById('back-to-top');\n",
        "  if(!btn) return;\n",
        "  function toggle(){ btn.style.display = (window.scrollY > 200) ? 'block' : 'none'; }\n",
        "  toggle(); window.addEventListener('scroll', toggle, {passive:true});\n",
        "  btn.addEventListener('click', function(){ window.scrollTo(0,0); }, false);\n",
        "}\n",
        "\n",
        "document.addEventListener('DOMContentLoaded', function(){\n",
        "  bindHeaderSort();\n",
        "  bindBackToTop();\n",
        "  stampLastUpdated();\n",
        "  loadAutoCount();\n",
        "  updateShowingCount();\n",
        "}, false);\n",
        "//]]>\n",
        "</script>\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"intro\">\n",
        "    <h2>Ancestor Register</h2>\n",
        "    <div class=\"meta\">\n",
        "      <a href=\"%(HOME)s\" target=\"_blank\" rel=\"noopener\">Home</a>\n",
        "      &nbsp;|&nbsp; Last updated: <span id=\"last-updated\">%(UPDATED)s</span>\n",
        "      &nbsp;|&nbsp; Autosomal matches: <span id=\"auto-count\">%(AUTO)s</span>\n",
        "      &nbsp;|&nbsp; Showing: <span id=\"showing-count\">0</span>\n",
        "    </div>\n",
        "    <div class=\"toolbar\">\n",
        "      <a class=\"btn\" href=\"%(HOME)s\" target=\"_blank\" rel=\"noopener\">&larr; Back to Main Register</a>\n",
        "      <input type=\"text\" id=\"searchBox\" class=\"btn light\" placeholder=\"Search this page&hellip;\" oninput=\"filterTable()\" />\n",
        "    </div>\n",
        "  </div>\n",
        "\n",
        "  <div class=\"output-table\" id=\"table-container\">\n",
        "    <!-- TABLE_PLACEHOLDER -->\n",
        "  </div>\n",
        "\n",
        "  <div class=\"back-to-top\" id=\"back-to-top\">&#9650; Top</div>\n",
        "</body>\n",
        "</html>\"\"\" % {\n",
        "    \"BTN_BG\": _BTN_BG, \"BTN_BG_H\": _BTN_BG_H, \"TH_BG\": _TH_BG, \"LINK\": _LINK,\n",
        "    \"HOME\": HOME_URL, \"UPDATED\": _html.escape(updated_fallback), \"AUTO\": _html.escape(auto_text)\n",
        "}\n",
        "\n",
        "# Build table HTML and mark first row\n",
        "table_html = df.to_html(index=False, border=1, classes=\"sortable\", table_id=\"refactor-table\")\n",
        "table_html = table_html.replace(\"<tbody>\\n<tr>\", \"<tbody>\\n<tr id=\\\"first-row\\\">\", 1)\n",
        "\n",
        "# Inject table and JS count URL\n",
        "final_html = full_html_template.replace(\"<!-- TABLE_PLACEHOLDER -->\", table_html)\n",
        "final_html = final_html.replace(\"{COUNT_URL}\", COUNT_PUBLIC_URL)\n",
        "# ====== CUT STOP  [4/6] XHTML TEMPLATE + TABLE (Dynamic header + counts + search) ===============\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "\n",
        "\n",
        "# ====== CUT START [5/6] SAVE LOCALLY + FTP UPLOAD ===============================================\n",
        "# Save locally (iso-8859-15 safe)\n",
        "try:\n",
        "    with open(OUTPUT_NAME, \"w\", encoding=\"iso-8859-15\", errors=\"xmlcharrefreplace\") as f:\n",
        "        f.write(final_html)\n",
        "    print(\"[OK] Saved locally:\", OUTPUT_NAME)\n",
        "except Exception as e:\n",
        "    print(\"[ERROR] Saving local file failed:\", e)\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Upload if credentials exist\n",
        "ftp_host = os.environ.get('FTP_HOST')\n",
        "ftp_user = os.environ.get('FTP_USER')\n",
        "ftp_pass = os.environ.get('FTP_PASS')\n",
        "ftp_port = os.environ.get('FTP_PORT', '21')\n",
        "ftp_dir  = os.environ.get('FTP_DIR', '')\n",
        "\n",
        "if all([ftp_host, ftp_user, ftp_pass]):\n",
        "    print(\"[INFO] Attempting FTP upload...\")\n",
        "    try:\n",
        "        socket.setdefaulttimeout(30)\n",
        "        with FTP_TLS(timeout=30) as ftps:\n",
        "            ftps.connect(ftp_host, int(ftp_port))\n",
        "            ftps.login(ftp_user, ftp_pass)\n",
        "            try: ftps.prot_p()\n",
        "            except Exception: pass\n",
        "            try: ftps.set_pasv(True)\n",
        "            except Exception: pass\n",
        "            # Navigate to target directory if provided\n",
        "            if ftp_dir.strip('/'):\n",
        "                for p in [p for p in ftp_dir.split('/') if p]:\n",
        "                    try: ftps.cwd(p)\n",
        "                    except Exception:\n",
        "                        try: ftps.mkd(p)\n",
        "                        except Exception: pass\n",
        "                        ftps.cwd(p)\n",
        "            # Replace remote file\n",
        "            try: ftps.delete(OUTPUT_NAME)\n",
        "            except Exception:\n",
        "                pass\n",
        "            with open(OUTPUT_NAME, \"rb\") as fh:\n",
        "                ftps.storbinary(f\"STOR {OUTPUT_NAME}\", fh)\n",
        "            print(\"[OK] Uploaded:\", OUTPUT_NAME, \"to\", ftps.pwd())\n",
        "    except Exception as e:\n",
        "        print(\"[ERROR] FTP upload failed:\", e)\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"[INFO] Skipping FTP upload: missing FTP credentials.\")\n",
        "# ====== CUT STOP  [5/6] SAVE LOCALLY + FTP UPLOAD ===============================================\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "\n",
        "\n",
        "# ====== CUT START [6/6] PERSIST COUNT + DONE ====================================================\n",
        "if autosomal_count is not None:\n",
        "    try:\n",
        "        with open(\"autosomal_count_prev.txt\", \"w\") as f:\n",
        "            f.write(str(autosomal_count))\n",
        "        print(\"[OK] Persisted autosomal_count_prev.txt\")\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not persist autosomal count:\", e)\n",
        "\n",
        "print(\"\\n--- Script Finished ---\")\n",
        "# ====== CUT STOP  [6/6] PERSIST COUNT + DONE ====================================================\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n",
        "#####\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySrJa6SLVEz0",
        "outputId": "523e0eb4-6914-40fa-b017-48205293a106"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Loaded CSV: final_combined_df_with_value_labels.csv rows=7, cols=9\n",
            "Pulling resolver CSV from server...\n",
            "Resolver saved: /content/match_to_unmasked.csv\n",
            "[OK] Column B -> C mapping: 7 / 7 unmatched: 0\n",
            "[OK] Saved locally: yates_ancestor_register.htm\n",
            "[INFO] Attempting FTP upload...\n",
            "[OK] Uploaded: yates_ancestor_register.htm to /\n",
            "[OK] Persisted autosomal_count_prev.txt\n",
            "\n",
            "--- Script Finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXP\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "from ftplib import FTP_TLS\n",
        "\n",
        "# ── CONFIG ───────────────────────────────────────────────────────────────\n",
        "info_csv   = \"/content/haplogroup_info.csv\"\n",
        "user_csv   = \"/content/y_dna_user_detail.csv\"\n",
        "output_csv = \"/content/y_dna_grid.csv\"\n",
        "output_htm = \"/content/y_dna_grid.htm\"\n",
        "\n",
        "# ── 1) Load & prepare haplogroup info ───────────────────────────────────\n",
        "df_info = pd.read_csv(info_csv)\n",
        "if \"Date\" in df_info.columns:\n",
        "    df_info.rename(columns={\"Date\": \"Era\"}, inplace=True)\n",
        "df_info = df_info.loc[df_info[\"Haplogroup\"].drop_duplicates().index]\n",
        "hap_order = df_info[\"Haplogroup\"].tolist()\n",
        "era_map   = dict(zip(df_info[\"Haplogroup\"], df_info.get(\"Era\", [\"\"] * len(df_info))))\n",
        "\n",
        "# ── 2) Load user detail table ───────────────────────────────────────────\n",
        "df_users = pd.read_csv(user_csv)\n",
        "if \"User_ID\" not in df_users.columns:\n",
        "    df_users.rename(columns={df_users.columns[0]: \"User_ID\"}, inplace=True)\n",
        "user_chains = [\n",
        "    [str(v) for v in row.drop(labels=[\"User_ID\"]).tolist() if pd.notna(v) and str(v).strip()]\n",
        "    for _, row in df_users.iterrows()\n",
        "]\n",
        "\n",
        "# ── 3) Insert new SNPs after parent ──────────────────────────────────────\n",
        "for chain in user_chains:\n",
        "    prev = None\n",
        "    for h in chain:\n",
        "        if prev and h not in hap_order:\n",
        "            idx = hap_order.index(prev)\n",
        "            hap_order.insert(idx + 1, h)\n",
        "        prev = h\n",
        "# Build final eras list\n",
        "eras = [era_map.get(h, \"\") for h in hap_order]\n",
        "\n",
        "# ── 4) Build horizontal grid DataFrame ───────────────────────────────────\n",
        "for h in hap_order:\n",
        "    if h not in df_users.columns:\n",
        "        df_users[h] = \"\"\n",
        "df_grid_h = df_users[[\"User_ID\"] + hap_order]\n",
        "\n",
        "# ── 5) Transform to vertical layout ─────────────────────────────────────\n",
        "df_vert = df_grid_h.set_index(\"User_ID\").T\n",
        "# Insert Era as first column\n",
        "df_vert.insert(0, 'Era', eras)\n",
        "df_vert.index.name = 'SNP'\n",
        "df_grid = df_vert.reset_index()\n",
        "\n",
        "# ── 6) Save vertical CSV ─────────────────────────────────────────────────\n",
        "df_grid.to_csv(output_csv, index=False)\n",
        "print(f\"✅ Vertical grid CSV saved to {output_csv}\")\n",
        "\n",
        "# ── 7) Generate XHTML (vertical) ────────────────────────────────────────\n",
        "now = datetime.now(ZoneInfo(\"America/New_York\"))\n",
        "ts  = now.strftime(\"%-m/%-d/%y, %-I:%M %p EDT\")\n",
        "\n",
        "template = '''<!DOCTYPE html>\n",
        "<html><head><meta charset=\"UTF-8\"><title>Yates Y-DNA Grid</title>\n",
        "<style>\n",
        "  body { background:#faf9d3; font-family:Arial,Helvetica,sans-serif; font-size:14px; }\n",
        "  table { width:100%; border:1px solid #333; border-collapse:collapse; table-layout:auto; }\n",
        "  th { background:#333; color:#fff; padding:6px; border:1px solid #999; }\n",
        "  .era { background:#666; color:#eee; padding:6px; border:1px solid #999; font-size:0.9em; }\n",
        "  td { padding:6px; border:1px solid #999; text-align:center; white-space:nowrap; }\n",
        "  .match { background:#fff; }\n",
        "  .blank { background:#ccc; color:#ccc; }\n",
        "</style>\n",
        "</head><body>\n",
        "  <h1 style=\"text-align:center;\">Yates Y-DNA Grid</h1>\n",
        "  <table>\n",
        "'''  # end template\n",
        "\n",
        "# Build header row\n",
        "cols = df_grid.columns.tolist()\n",
        "header_html = '<tr><th>SNP</th><th>Era</th>' + ''.join(f'<th>{u}</th>' for u in cols[2:]) + '</tr>'\n",
        "\n",
        "# Build data rows\n",
        "rows_html = []\n",
        "for _, row in df_grid.iterrows():\n",
        "    cells = []\n",
        "    for u in cols[2:]:\n",
        "        v = row[u]\n",
        "        if pd.isna(v) or not str(v).strip():\n",
        "            cells.append('<td class=\"blank\">–</td>')\n",
        "        else:\n",
        "            cells.append(f'<td class=\"match\">{v}</td>')\n",
        "    rows_html.append(f'<tr><td>{row[\"SNP\"]}</td><td class=\"era\">{row[\"Era\"]}</td>' + ''.join(cells) + '</tr>')\n",
        "\n",
        "# Combine and save HTML\n",
        "html = template + header_html + '\\n' + '\\n'.join(rows_html) + f'''\n",
        "  </table>\n",
        "  <p style=\"text-align:right;font-size:0.9em;\">Updated: {ts}</p>\n",
        "</body>\n",
        "</html>'''\n",
        "with open(output_htm, 'w', encoding='utf-8') as f:\n",
        "    f.write(html)\n",
        "print(f\"✅ Vertical XHTML Grid saved to {output_htm}\")\n",
        "\n",
        "# ── 8) FTP Upload ───────────────────────────────────────────────────────\n",
        "ftp = FTP_TLS()\n",
        "ftp.connect(os.environ['FTP_HOST'], int(os.environ.get('FTP_PORT',21)))\n",
        "ftp.login(os.environ['FTP_USER'], os.environ['FTP_PASS'])\n",
        "ftp.prot_p()\n",
        "for path in [output_csv, output_htm]:\n",
        "    name = os.path.basename(path)\n",
        "    try: ftp.delete(name)\n",
        "    except: pass\n",
        "    with open(path,'rb') as fp:\n",
        "        ftp.storbinary(f\"STOR {name}\", fp)\n",
        "ftp.quit()\n",
        "print(\"✅ Uploaded to server.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFqM0kUliAqX",
        "outputId": "d6d54e4c-5b1c-497b-f896-757477e43e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Vertical grid CSV saved to /content/y_dna_grid.csv\n",
            "✅ Vertical XHTML Grid saved to /content/y_dna_grid.htm\n",
            "✅ Uploaded to server.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 id=\"cell-end\">END - Runner</h2>\n",
        "<a href=\"#ons-top\">Back to top</a>\n"
      ],
      "metadata": {
        "id": "IivO1Bhyx_Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Y-DNA cell 1\n",
        "\n",
        "# === Cell 1: New user settings ===\n",
        "USER_ID       = 'I56217'  # the new column header\n",
        "PATH_STRING   = (      # the SNP chain for this user\n",
        "    \"R-M207 > R-M173 > R-M343 > R-M269 > R-FT266064 > R-FT266579 > R-FTF17042\"\n",
        ")\n",
        "INSERT_MISSING = True       # if True, adds any SNPs from PATH_STRING that aren't yet rows\n",
        "MASTER_CSV     = '/content/y_dna_user_detail_combo.csv'\n",
        "UPDATED_CSV    = '/content/y_dna_user_detail_combo_updated.csv'\n"
      ],
      "metadata": {
        "id": "0bO8B-Gnls49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Load → Append User → Save\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load the existing master CSV\n",
        "df = pd.read_csv(MASTER_CSV)\n",
        "\n",
        "# 2) Normalize the first column name to 'SNP' for easy matching\n",
        "first_col = df.columns[0]\n",
        "if first_col != 'SNP':\n",
        "    df.rename(columns={first_col: 'SNP'}, inplace=True)\n",
        "\n",
        "# 3) Parse the new user's SNP chain\n",
        "chain = PATH_STRING.split('>')\n",
        "\n",
        "# 4) Optionally insert any SNPs not yet present (appends at bottom)\n",
        "if INSERT_MISSING:\n",
        "    missing = [s for s in chain if s not in df['SNP'].values]\n",
        "    if missing:\n",
        "        df = pd.concat([df, pd.DataFrame([{'SNP': s} for s in missing])],\n",
        "                       ignore_index=True)\n",
        "\n",
        "# 5) Create the new user column in the next free position\n",
        "df[USER_ID] = ''\n",
        "\n",
        "# 6) Populate: copy the SNP value into that column where it matches the chain\n",
        "df.loc[df['SNP'].isin(chain), USER_ID] = df['SNP']\n",
        "\n",
        "# 7) Save the updated CSV back to /content\n",
        "df.to_csv(UPDATED_CSV, index=False)\n",
        "print(f\"✅ Updated CSV saved to {UPDATED_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjd2-uEdmJKR",
        "outputId": "b6746fb3-cc40-4e7e-d787-d1a5ec09ad00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated CSV saved to /content/y_dna_user_detail_combo_updated.csv\n"
          ]
        }
      ]
    }
  ]
}