{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNi45+JX2qEw+2pvPlRKElX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronyates47/Gedcom-Utils/blob/main/A_05_24_DNA_Library_(EXP_6_BW_chart).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rT5s4n9S5ZpZ",
        "outputId": "76d50c41-7d8e-4692-cee4-aa5c20cbf6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting python-gedcom\n",
            "  Downloading python_gedcom-1.0.0-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: python-gedcom\n",
            "Successfully installed python-gedcom-1.0.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install python-gedcom\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A_v_2024-05-22_produces CORRECT_DNA_Study_Library-(ok-chart)\n",
        "\n",
        "import csv\n",
        "import glob\n",
        "from gedcom.element.individual import IndividualElement\n",
        "from gedcom.parser import Parser\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "import os\n",
        "\n",
        "# Hard-coded interval scheme\n",
        "interval_scheme = [\n",
        "    (1000, 1), (1025, 2), (1050, 3), (1075, 4), (1100, 5),\n",
        "    (1125, 6), (1150, 7), (1175, 8), (1200, 9), (1225, 10),\n",
        "    (1250, 11), (1275, 12), (1300, 13), (1325, 14), (1350, 15),\n",
        "    (1375, 16), (1400, 17), (1425, 18), (1450, 19), (1475, 20),\n",
        "    (1500, 21), (1525, 22), (1550, 23), (1575, 24), (1600, 25),\n",
        "    (1625, 26), (1650, 27), (1675, 28), (1700, 29), (1725, 30),\n",
        "    (1750, 31), (1775, 32), (1800, 33), (1825, 34), (1850, 35),\n",
        "    (1875, 36), (1900, 37), (1925, 38), (1950, 39), (1975, 40),\n",
        "    (2000, 41), (2025, 42), (2050, 43), (2075, 44)\n",
        "]\n",
        "\n",
        "# Function to assign interval based on birthdate\n",
        "def assign_interval(birth_date, interval_scheme):\n",
        "    if pd.isnull(birth_date):\n",
        "        return 99\n",
        "    try:\n",
        "        birth_year = int(birth_date[-4:])\n",
        "    except ValueError:\n",
        "        return 99\n",
        "    for year, interval in interval_scheme:\n",
        "        if birth_year <= year:\n",
        "            return interval\n",
        "    return 99\n",
        "\n",
        "class GedcomDataset:\n",
        "    def __init__(self, gen_person):\n",
        "        self.gen_person = gen_person\n",
        "        self.extractable_detail = {}\n",
        "        self.anchor_gen1 = None  # Initialize anchor_gen1 here\n",
        "\n",
        "    def add_extractable_detail(self, key, value):\n",
        "        self.extractable_detail[key] = value\n",
        "\n",
        "    def get_gen_person(self):\n",
        "        name = self.extractable_detail.get('NAME', '')\n",
        "        parts = name.split('/', 1)\n",
        "        first_name = parts[0].split(' ')[0]\n",
        "        last_name = parts[1].rstrip('/') if len(parts) > 1 else \"\"\n",
        "        self.anchor_gen1 = last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "        global anchor_gen1  # Declare that we're using the global variable\n",
        "        anchor_gen1 = self.anchor_gen1  # Update the global variable\n",
        "        return self.gen_person.strip('@')\n",
        "\n",
        "    def get_anchor_gen1(self):\n",
        "        return self.anchor_gen1\n",
        "\n",
        "    def get_extractable_NPFX(self):\n",
        "        return self.extractable_detail.get('NPFX', '')\n",
        "\n",
        "    def get_extractable_cm(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            cm_value = npfx_value.split('&')[0].strip()\n",
        "        else:\n",
        "            cm_value = npfx_value.strip()\n",
        "        try:\n",
        "            int(cm_value)\n",
        "            return cm_value\n",
        "        except ValueError:\n",
        "            return 'error'\n",
        "\n",
        "    def get_extractable_sort(self):\n",
        "        npfx_value = self.extractable_detail.get('NPFX', '')\n",
        "        if '&' in npfx_value:\n",
        "            sort_value = npfx_value.split('&')[1].strip()\n",
        "            return sort_value\n",
        "        else:\n",
        "            return ''\n",
        "\n",
        "    def get_extractable_FAMC(self):\n",
        "        return self.extractable_detail.get('FAMC', '').strip('@')\n",
        "\n",
        "    def get_birth_date(self):\n",
        "        return self.extractable_detail.get('BIRTH_DATE', '')\n",
        "\n",
        "    def get_fams(self):\n",
        "        return self.extractable_detail.get('FAMS', '').strip('@')\n",
        "\n",
        "# Set of excluded record IDs\n",
        "excluded_ids = {\n",
        "    'I47640', 'I55585', 'I47666', 'I55586', 'I47570', 'I55587',\n",
        "    'I47569', 'I47641', 'I47571', 'I47572', 'I47549', 'I47550',\n",
        "    'I47573', 'I55588', 'I47548', 'I47551', 'I47553', 'I24502', 'I24503',\n",
        "    'I47659', 'I48070', 'I47660', 'I48129', 'I48130', 'I48126', 'I48127'\n",
        "}\n",
        "\n",
        "# Function definitions\n",
        "def extract_id(record):\n",
        "    id_start = record.find('@') + 1\n",
        "    id_end = record.find('@', id_start)\n",
        "    return record[id_start:id_end]\n",
        "\n",
        "def extract_name(record):\n",
        "    name_start = record.find('1 NAME ') + 6\n",
        "    name_end = record.find('\\n', name_start)\n",
        "    name = record[name_start:name_end]\n",
        "    first_name, last_name = name.split('/', 1)\n",
        "    first_name = first_name[:30]\n",
        "    last_name = last_name[:30].rstrip('/')\n",
        "    return last_name.replace(\" \", \"\") + first_name.replace(\" \", \"\")\n",
        "\n",
        "name_to_id = {}   # Global dictionary to hold name to ID mapping\n",
        "\n",
        "class Gedcom:\n",
        "    def __init__(self, file_name):\n",
        "        self.file_name = file_name\n",
        "        self.gedcom_datasets = []\n",
        "        self.filter_pool = []\n",
        "\n",
        "    @staticmethod\n",
        "    def get_standard_name(file_path):\n",
        "        file_name = file_path.split('/')[-1]\n",
        "        if '.' in file_name:\n",
        "            file_name = file_name.rsplit('.', 1)[0]\n",
        "        standard_name = file_name.replace(' ', '_').lower()\n",
        "        return standard_name\n",
        "\n",
        "    def parse_gedcom(self):\n",
        "        global name_to_id  # Declare name_to_id as global to modify it\n",
        "        with open(self.file_name, 'r', encoding='utf-8-sig') as f:\n",
        "            gedcom_lines = f.readlines()\n",
        "        current_dataset = None\n",
        "        npfx_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for line in gedcom_lines:\n",
        "            parts = line.strip().split(' ', 2)\n",
        "            level = int(parts[0])\n",
        "            tag = parts[1]\n",
        "            value = parts[2] if len(parts) > 2 else None\n",
        "\n",
        "            if level == 0 and tag.startswith('@') and tag.endswith('@') and value == 'INDI':\n",
        "                individual_id = tag.strip('@')\n",
        "                if individual_id in excluded_ids:\n",
        "                    continue  # Skip processing for the excluded IDs\n",
        "\n",
        "                total_count += 1\n",
        "                current_dataset = GedcomDataset(tag)\n",
        "                self.gedcom_datasets.append(current_dataset)\n",
        "\n",
        "                # Populate name_to_id\n",
        "                individual_name = current_dataset.get_anchor_gen1()\n",
        "                individual_id = current_dataset.get_gen_person()\n",
        "                name_to_id[individual_name] = individual_id\n",
        "\n",
        "            elif current_dataset is not None:\n",
        "                if level == 1 and tag in ['NAME', 'FAMC', 'FAMS', 'BIRT', 'SEX']:\n",
        "                    current_key = tag\n",
        "                    current_dataset.add_extractable_detail(current_key, value)\n",
        "\n",
        "                elif level == 2 and tag == 'DATE' and current_key == 'BIRT':\n",
        "                    current_dataset.add_extractable_detail('BIRTH_DATE', value)\n",
        "\n",
        "                elif level == 2 and tag == 'NPFX':\n",
        "                    npfx_count += 1\n",
        "                    current_dataset.add_extractable_detail(tag, value)\n",
        "\n",
        "        print(f'GEDCOM contained {total_count} total records')\n",
        "        print(f'Records tagged and filtered by NPFX: {npfx_count}')\n",
        "\n",
        "        # First level of filtering: Filter those with NPFX\n",
        "        for dataset in self.gedcom_datasets:\n",
        "            if dataset.get_extractable_NPFX():\n",
        "                self.filter_pool.append(dataset)\n",
        "\n",
        "        # Remove excluded IDs from filter_pool\n",
        "        self.filter_pool = [ds for ds in self.filter_pool if ds.get_gen_person() not in excluded_ids]\n",
        "\n",
        "        # Check if manual filtering should be applied\n",
        "        manual_filter_activated = True  # or False depending on your situation\n",
        "\n",
        "        # Second level of filtering: Apply manual filter from Excel sheet\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                manual_filtered_ids = set(df['ID'])\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids) - 1}\")\n",
        "\n",
        "                self.filter_pool = [dataset for dataset in self.filter_pool if dataset.get_gen_person() in manual_filtered_ids]\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def apply_manual_filter(self):\n",
        "        manual_filter_activated = True\n",
        "        if manual_filter_activated:\n",
        "            try:\n",
        "                df = pd.read_excel('filtered_ids.xlsx')\n",
        "                manual_filtered_ids = set(df['ID'].astype(str))  # Ensure IDs are strings\n",
        "                print(f\"Manual filter IDs loaded: {len(manual_filtered_ids)}\")\n",
        "            except FileNotFoundError:\n",
        "                print(\"filtered_ids.xlsx not found. Skipping second-level manual filter.\")\n",
        "            else:\n",
        "                # Debug output to verify IDs before filtering\n",
        "                print(\"IDs before manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) in manual_filtered_ids]\n",
        "                print(\"IDs after manual filter:\", [ds.get_gen_person() for ds in self.filter_pool][:10])\n",
        "                print(f\"After manual filter, total records: {len(self.filter_pool)}\")\n",
        "\n",
        "    def check_and_apply_exclusion_filter(self):\n",
        "        \"\"\"Apply exclusion filter if '/exclude_ids.xlsx' is present.\"\"\"\n",
        "        file_path = '/content/exclude_ids.xlsx'  # Updated path\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                df_exclude = pd.read_excel(file_path)\n",
        "                if 'ID' in df_exclude.columns:\n",
        "                    exclude_ids = set(df_exclude['ID'].astype(str))  # Ensure conversion to string\n",
        "                    print(f\"Exclusion filter IDs loaded: {len(exclude_ids)}\")\n",
        "                    print(f\"Sample of IDs to exclude: {list(exclude_ids)[:5]}\")  # Print some sample IDs\n",
        "                    # Apply the exclusion filter\n",
        "                    initial_count = len(self.filter_pool)\n",
        "                    self.filter_pool = [ds for ds in self.filter_pool if str(ds.get_gen_person()) not in exclude_ids]\n",
        "                    print(f\"Excluded {initial_count - len(self.filter_pool)} records based on exclusion IDs.\")\n",
        "                else:\n",
        "                    print(\"Column 'ID' not found in the Excel file.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to apply exclusion filter: {str(e)}\")\n",
        "        else:\n",
        "            print(f\"No exclusion filter applied, '{file_path}' not found. Check the path and ensure the file is uploaded to Colab.\")\n",
        "\n",
        "def input_prime_surname(last_prime_surname=None):\n",
        "    if last_prime_surname:\n",
        "        last_name = input(f\"Enter prime_surname (default: {last_prime_surname}): \")\n",
        "        if not last_name:\n",
        "            last_name = last_prime_surname\n",
        "    else:\n",
        "        last_name = input(\"Enter prime_surname: \")\n",
        "    return last_name\n",
        "\n",
        "def select_gedcom_file():\n",
        "    gedcom_files = glob.glob('*.ged')\n",
        "    if not gedcom_files:\n",
        "        print(\"No GEDCOM files found.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Automatically selecting the first GEDCOM file.\")\n",
        "    return gedcom_files[0]\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            selected_num = int(input(\"Enter the number of the GEDCOM file you want to use: \"))\n",
        "            if 1 <= selected_num <= len(gedcom_files):\n",
        "                return gedcom_files[selected_num - 1]\n",
        "            else:\n",
        "                print(\"Invalid number. Please enter a valid number from the list.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "gedcom_file_path = select_gedcom_file() # Call the function to let the user select the GEDCOM file\n",
        "if gedcom_file_path:\n",
        "    # Use the selected GEDCOM file path to create an instance of the Gedcom class\n",
        "    gedcom_instance = Gedcom(gedcom_file_path)\n",
        "    gedcom_instance.parse_gedcom()\n",
        "\n",
        "    individuals = []  # Initialize the list of individuals\n",
        "\n",
        "    for dataset in gedcom_instance.filter_pool:    # Iterate over the filter_pool list, add each last name and ID to list\n",
        "        individual_id = dataset.get_gen_person()\n",
        "        last_name = dataset.get_anchor_gen1()\n",
        "        individuals.append((last_name, individual_id))\n",
        "\n",
        "    print(f'Records tagged and filtered by NPFX: {len(individuals)}')\n",
        "\n",
        "    with open(gedcom_file_path, 'r') as file:    # Read the GEDCOM file and split it into individual and family records\n",
        "        data = file.read()\n",
        "    data = data.split('\\n0 ')\n",
        "    records = {extract_id(record): record for record in data}\n",
        "\n",
        "def has_both_parents(records, mother_id, father_id):\n",
        "    return mother_id in records and father_id in records\n",
        "\n",
        "visited_pairs = set()\n",
        "generation_table = []\n",
        "\n",
        "# Function to find parents, ensuring excluded IDs are not processed\n",
        "def find_parents(individual_id, generation, records):\n",
        "    if individual_id in excluded_ids or individual_id not in records:\n",
        "        return\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "    if famc_id not in records:\n",
        "        return\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if mother_id and mother_id in records and father_id and father_id in records:\n",
        "        parent_pair = (father_id, mother_id)\n",
        "        if parent_pair not in visited_pairs:\n",
        "            visited_pairs.add(parent_pair)\n",
        "            generation_table.append((generation, parent_pair))\n",
        "\n",
        "    if mother_id:\n",
        "        find_parents(mother_id, generation + 1, records)\n",
        "\n",
        "    if father_id:\n",
        "        find_parents(father_id, generation + 1, records)\n",
        "\n",
        "# Function to find distant ancestors, ensuring excluded IDs are not processed\n",
        "def find_distant_ancestors(individual_id, records, path=None):\n",
        "    if individual_id in excluded_ids:\n",
        "        return []\n",
        "    path = path if path is not None else []\n",
        "    if path is None:\n",
        "        path = [individual_id]\n",
        "    else:\n",
        "        path.append(individual_id)\n",
        "\n",
        "    if individual_id not in records:\n",
        "        return []\n",
        "\n",
        "    record = records[individual_id]\n",
        "    famc_start = record.find('1 FAMC @') + 8\n",
        "    famc_end = record.find('@', famc_start)\n",
        "    famc_id = record[famc_start:famc_end]\n",
        "\n",
        "    if famc_id not in records:\n",
        "        return [path]\n",
        "\n",
        "    fam_record = records[famc_id]\n",
        "    wife_start = fam_record.find('1 WIFE @') + 8\n",
        "    wife_end = fam_record.find('@', wife_start)\n",
        "    mother_id = fam_record[wife_start:wife_end]\n",
        "\n",
        "    husb_start = fam_record.find('1 HUSB @') + 8\n",
        "    husb_end = fam_record.find('@', husb_start)\n",
        "    father_id = fam_record[husb_start:husb_end]\n",
        "\n",
        "    if father_id is None and mother_id is None:\n",
        "        return [path]\n",
        "\n",
        "    paths = []\n",
        "    if father_id and father_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(father_id, records, new_path))\n",
        "\n",
        "    if mother_id and mother_id not in excluded_ids:\n",
        "        new_path = list(path)\n",
        "        paths.extend(find_distant_ancestors(mother_id, records, new_path))\n",
        "\n",
        "    return paths\n",
        "\n",
        "# Other parts of the script remain the same\n",
        "\n",
        "# Example usage after parsing and filtering\n",
        "gedcom_instance.parse_gedcom()\n",
        "\n",
        "# Save the filter_pool to an Excel file\n",
        "#print_filter_pool_to_excel(gedcom_instance.filter_pool)\n",
        "\n",
        "filtered_datasets = gedcom_instance.filter_pool\n",
        "\n",
        "def calculate_score(distant_ancestors_paths, records):\n",
        "    name_paths = []\n",
        "    for path in distant_ancestors_paths:\n",
        "        name_path = [extract_name(records.get(id, '')) for id in path]\n",
        "        name_paths.append(name_path)\n",
        "\n",
        "    path_scores = {}\n",
        "    for idx, name_path in enumerate(name_paths):\n",
        "        score = 0\n",
        "        for generation, name in enumerate(name_path):\n",
        "            if 'Yates' in name:\n",
        "                score += 1 * (generation + 1)\n",
        "        path_scores[idx] = score\n",
        "\n",
        "    if path_scores:\n",
        "        winning_path_index = max(path_scores, key=path_scores.get)\n",
        "        winning_path_score = path_scores[winning_path_index]\n",
        "        winning_path_names = name_paths[winning_path_index]\n",
        "        winning_path_ids = distant_ancestors_paths[winning_path_index]\n",
        "    else:\n",
        "        winning_path_index = None\n",
        "        winning_path_score = 0\n",
        "        winning_path_names = []\n",
        "        winning_path_ids = []\n",
        "\n",
        "    return winning_path_score, winning_path_names, winning_path_ids\n",
        "\n",
        "def filter_ancestral_line(winning_path_ids, generation_table):\n",
        "    matching_table = []\n",
        "    for generation, pair in generation_table:\n",
        "        id1, id2 = pair\n",
        "        if id1 in winning_path_ids or id2 in winning_path_ids:\n",
        "            matching_table.append((generation, pair))\n",
        "    return matching_table\n",
        "\n",
        "def process_individual(individual_id, gedcom_instance, records, interval_scheme):\n",
        "    global generation_table\n",
        "    global visited_pairs\n",
        "    global anchor_gen1  # Declare that we're using the global variable\n",
        "\n",
        "    generation_table = []\n",
        "    visited_pairs = set()\n",
        "\n",
        "    find_parents(individual_id, 1, records)\n",
        "    distant_ancestors_paths = find_distant_ancestors(individual_id, records)\n",
        "    winning_path_score, winning_path_names, winning_path_ids = calculate_score(distant_ancestors_paths, records)\n",
        "    filtered_ancestral_line = filter_ancestral_line(winning_path_ids, generation_table)\n",
        "    filtered_ancestral_line.sort(key=lambda x: x[0])\n",
        "    filtered_ancestral_line_names = []\n",
        "    birth_date = None  # Initialize birth_date\n",
        "\n",
        "    # Extract cm_value, sort_value, anchor_gen1, and birth_date\n",
        "    for dataset in gedcom_instance.filter_pool:\n",
        "        if dataset.get_gen_person() == individual_id:\n",
        "            cm_value = dataset.get_extractable_cm()\n",
        "            sort_value = dataset.get_extractable_sort()\n",
        "            anchor_gen1 = dataset.get_anchor_gen1()  # Update anchor_gen1 locally here\n",
        "            birth_date = dataset.extractable_detail.get('BIRTH_DATE')  # Get birth date\n",
        "            break\n",
        "    else:\n",
        "        cm_value = 'N/A'\n",
        "        sort_value = 'N/A'\n",
        "        birth_date = 'N/A'  # Set default if not found\n",
        "\n",
        "    if anchor_gen1 is not None:\n",
        "        filtered_ancestral_line_names.insert(0, anchor_gen1)\n",
        "\n",
        "    ancestors_data = []\n",
        "\n",
        "    # Extract ancestor information and concatenate name with Date Interval\n",
        "    for generation, pair in filtered_ancestral_line:\n",
        "        ancestor_details = []\n",
        "        for ancestor_id in pair:\n",
        "            if ancestor_id in records:\n",
        "                ancestor_record = records[ancestor_id]\n",
        "                ancestor_name = extract_name(ancestor_record)\n",
        "                ancestor_sex = 'M' if '1 SEX M' in ancestor_record else 'F' if '1 SEX F' in ancestor_record else ''\n",
        "                birth_date_start = ancestor_record.find('2 DATE ') + 7\n",
        "                birth_date_end = ancestor_record.find('\\n', birth_date_start)\n",
        "                ancestor_birth_date = ancestor_record[birth_date_start:birth_date_end].strip() if birth_date_start != -1 else 'N/A'\n",
        "                ancestor_date_interval = assign_interval(ancestor_birth_date, interval_scheme)\n",
        "                ancestor_details.append({\n",
        "                    'ID': ancestor_id,\n",
        "                    'Name': ancestor_name,\n",
        "                    'Sex': ancestor_sex,\n",
        "                    'Birth Date': ancestor_birth_date,\n",
        "                    'Date Interval': ancestor_date_interval\n",
        "                })\n",
        "        if ancestor_details:\n",
        "            name_with_interval_1 = f\"{ancestor_details[0]['Date Interval']}{ancestor_details[0]['Name']}\"\n",
        "            name_with_interval_2 = f\"{ancestor_details[1]['Date Interval']}{ancestor_details[1]['Name']}\" if len(ancestor_details) > 1 else '99Unknown'\n",
        "            combined_name_with_interval = f\"{name_with_interval_1}&{name_with_interval_2}\"\n",
        "            ancestors_data.append(combined_name_with_interval)\n",
        "\n",
        "    # Reverse the order to start with the oldest ancestor\n",
        "    ancestors_data.reverse()\n",
        "    filtered_ancestral_line_str = \"~~~\".join(ancestors_data)\n",
        "\n",
        "    # Assign date interval\n",
        "    date_interval = assign_interval(birth_date, interval_scheme)\n",
        "\n",
        "    individual_data = {\n",
        "        'ID': individual_id,\n",
        "        'Name': extract_name(records[individual_id]),\n",
        "        'Sex': dataset.extractable_detail.get('SEX', ''),\n",
        "        'Birth Date': birth_date,\n",
        "        'Date Interval': date_interval,\n",
        "        'FAMS': dataset.get_fams(),\n",
        "        'FAMC': dataset.get_extractable_FAMC(),\n",
        "        'cM': cm_value,\n",
        "        'Sort': sort_value,\n",
        "        'Ancestors': filtered_ancestral_line_str  # Add ancestors data to individual data\n",
        "    }\n",
        "\n",
        "    return individual_data, filtered_ancestral_line_str\n",
        "\n",
        "# Initialize the dictionary to store individual data\n",
        "DNA_Study_Library = {}\n",
        "\n",
        "# Process individuals and populate the dictionary\n",
        "for dataset in gedcom_instance.filter_pool:  # Assuming filter_pool is iterable\n",
        "    individual_id = dataset.get_gen_person()\n",
        "\n",
        "    # Reset global variables for each new individual\n",
        "    visited_pairs.clear()\n",
        "    generation_table = []\n",
        "\n",
        "    # Process Individual and Get Data\n",
        "    individual_data, filtered_ancestral_line_str = process_individual(individual_id, gedcom_instance, records, interval_scheme)\n",
        "\n",
        "    # Store the data in the dictionary\n",
        "    DNA_Study_Library[individual_id] = individual_data\n",
        "\n",
        "# Output the first 5 records in a columnar format for validation\n",
        "print(f\"{'ID':<6}{'Name':<20}{'Sort':<20}{'cM':<5}{'Ancestors':<30}\")\n",
        "for idx, (individual_id, data) in enumerate(DNA_Study_Library.items()):\n",
        "    if idx >= 1:\n",
        "        break\n",
        "    # Replace None values with empty strings\n",
        "    name = data['Name'] or ''\n",
        "    sex = data['Sex'] or ''\n",
        "    date_interval = data['Date Interval'] or ''\n",
        "    fams = data['FAMS'] or ''\n",
        "    famc = data['FAMC'] or ''\n",
        "    cm = data['cM'] or ''\n",
        "    sort_value = data['Sort'] or ''\n",
        "    ancestors = data['Ancestors']\n",
        "    print(f\"{individual_id:<7}{name:<25}{sort_value:<15}{cm:<5}{ancestors:<30}\")\n",
        "\n",
        "# Function to save the data to an Excel file\n",
        "def save_to_excel(data_dict, filename):\n",
        "    # Convert the dictionary to a pandas DataFrame\n",
        "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
        "\n",
        "    # Save the DataFrame to an Excel file using openpyxl\n",
        "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, sheet_name='DNA_Study_Library', index=False)  # index=False to prevent writing the index\n",
        "\n",
        "        # Apply some basic formatting\n",
        "        workbook = writer.book\n",
        "        worksheet = writer.sheets['DNA_Study_Library']\n",
        "\n",
        "        # Adjust column width\n",
        "        for column in worksheet.columns:\n",
        "            max_length = 0\n",
        "            column = [cell for cell in column]\n",
        "            for cell in column:\n",
        "                try:\n",
        "                    if len(str(cell.value)) > max_length:\n",
        "                        max_length = len(cell.value)\n",
        "                except:\n",
        "                    pass\n",
        "            adjusted_width = (max_length + 2)\n",
        "            worksheet.column_dimensions[column[0].column_letter].width = adjusted_width\n",
        "\n",
        "# Save the DNA Study Library to an Excel file\n",
        "save_to_excel(DNA_Study_Library, 'DNA_Study_Library.xlsx')\n",
        "\n",
        "print(\"Data has been saved to DNA_Study_Library.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA4p2nFZ9I6D",
        "outputId": "eec59c62-4212-4f2e-eaa4-b36d147a6dc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically selecting the first GEDCOM file.\n",
            "GEDCOM contained 55704 total records\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "filtered_ids.xlsx not found. Skipping second-level manual filter.\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "GEDCOM contained 55704 total records\n",
            "Records tagged and filtered by NPFX: 1045\n",
            "filtered_ids.xlsx not found. Skipping second-level manual filter.\n",
            "ID    Name                Sort                cM   Ancestors                     \n",
            "I73    YatesJamesRobert         yates,ronald   1047 30YatesBenjamin&42SearchingStill~~~31YatesBenjamin&42SearchingStill~~~32YatesJohn&42SearchingStill~~~33YatesJohn&33SwiftMary~~~34YatesJohnE&34RobersonElizabeth~~~36YatesJamesWilson&36OttMaryEllen~~~37YatesHarryElmer&38TingleyOrliaGeorgeann\n",
            "Data has been saved to DNA_Study_Library.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pySyyebczNQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/EXP#4_chart_black_separators.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook, drawing\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(filepath):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Fill NaN values in 'Ancestors' column with empty strings and ensure all values are strings\n",
        "    df['Ancestors'] = df['Ancestors'].fillna('').astype(str)\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in first_parents else '', axis=1)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to 8\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= 8].copy()\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    final_results_df['Prefix'] = final_results_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    final_results_df['Prefix'] = final_results_df['Prefix'].astype(str)\n",
        "\n",
        "    return final_results_df\n",
        "\n",
        "def sort_within_segment(group):\n",
        "    \"\"\"Sort the group by the FQ in descending order within each segment.\"\"\"\n",
        "    return group.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(sort_within_segment).reset_index(drop=True)\n",
        "\n",
        "    # Drop the Prefix column as it is no longer needed\n",
        "    sorted_df = sorted_df.drop(columns=['Prefix'])\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "def visualize_data(sorted_df):\n",
        "    \"\"\"Visualize the sorted data in an Excel file with styled segments.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "\n",
        "    # Initialize variables to track last seen parent initial and generation\n",
        "    last_parent_initial = None\n",
        "    parent_seen = set()\n",
        "\n",
        "    # Adding the rows and assigning interval offset for staggered node-parents\n",
        "    interval_offset = 0\n",
        "    for r in dataframe_to_rows(sorted_df, index=False, header=False):\n",
        "        current_parent = r[1]\n",
        "\n",
        "        # Calculate interval offset\n",
        "        if current_parent not in parent_seen:\n",
        "            parent_seen.add(current_parent)\n",
        "            current_parent_prefix = current_parent[:2] if current_parent else ''\n",
        "            if last_parent_initial is not None and current_parent_prefix != last_parent_initial:\n",
        "                # Insert a black row at the end of the previous segment\n",
        "                ws.append([''] * len(headers))\n",
        "                black_row_index = ws.max_row\n",
        "                ws.row_dimensions[black_row_index].height = 5\n",
        "                for cell in ws[black_row_index]:\n",
        "                    cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "            last_parent_initial = current_parent_prefix\n",
        "            ws.append([*r])\n",
        "        else:\n",
        "            r[1] = ''  # Clear parent name for repeated entries within the same generation\n",
        "            ws.append([r[0], '', *r[2:]])\n",
        "\n",
        "    # Add a black row at the end of the last segment\n",
        "    ws.append([''] * len(headers))\n",
        "    black_row_index = ws.max_row\n",
        "    ws.row_dimensions[black_row_index].height = 5\n",
        "    for cell in ws[black_row_index]:\n",
        "        cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    center_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    right_alignment = Alignment(horizontal=\"right\", vertical=\"center\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "\n",
        "        row[1].alignment = right_alignment  # Right-align Parents column\n",
        "        row[4].alignment = left_alignment  # Left-align Offspring & Spouse column\n",
        "\n",
        "    # Replace 'Yes' with an arrow image\n",
        "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=1):\n",
        "        for cell in row:\n",
        "            if cell.value == 'Yes':\n",
        "                if ws.cell(row=cell.row, column=2).value:  # Check if the Parents column is not blank\n",
        "                    img = drawing.image.Image('arrow.png')\n",
        "                    img.width, img.height = (25, 25)  # Increase size to 25x25\n",
        "                    img.anchor = cell.coordinate\n",
        "                    ws.add_image(img)\n",
        "                cell.value = ''  # Clear the cell value\n",
        "\n",
        "    # Save the workbook\n",
        "    output_file = '/content/EXP#4_chart_black_separators.xlsx'\n",
        "    wb.save(output_file)\n",
        "    print(f\"Results successfully saved with styled colors to {output_file}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Replace with the correct path\n",
        "\n",
        "# Process the data and save it in a dictionary\n",
        "processed_data_dict = process_data(input_file_path).to_dict(orient='list')\n",
        "\n",
        "# Sort all segments within the data\n",
        "sorted_data = sort_all_segments(processed_data_dict)\n",
        "\n",
        "# Visualize the sorted data\n",
        "visualize_data(sorted_data)\n",
        "\n",
        "\n",
        "\n",
        "#*********************************************************************************GOOD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d53_ertV20rB",
        "outputId": "794d76d1-61b0-4a2d-abab-d19ed5d01ef0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results successfully saved with styled colors to /content/EXP#4_chart_black_separators.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#*****************************************  Good Trailing Descendants\n",
        "\n",
        "import pandas as pd\n",
        "from openpyxl import Workbook, drawing\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "from openpyxl.styles import Border, Side, Alignment, PatternFill\n",
        "\n",
        "def parse_ancestors(df):\n",
        "    \"\"\"Parse the Ancestors column to determine the first parent pair.\"\"\"\n",
        "    first_parents = []\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        if nodes:\n",
        "            first_parent_pair = nodes[0]\n",
        "            first_parents.append(first_parent_pair)\n",
        "    return set(first_parents)\n",
        "\n",
        "def process_data(filepath):\n",
        "    \"\"\"Load data, process it to expand 'Ancestors', calculate FQ and QI, and return a dictionary of results.\"\"\"\n",
        "    # Load the DataFrame\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    # Fill NaN values in 'Ancestors' column with empty strings and ensure all values are strings\n",
        "    df['Ancestors'] = df['Ancestors'].fillna('').astype(str)\n",
        "\n",
        "    # Parse the Ancestors column to get the first parent pairs\n",
        "    first_parents = parse_ancestors(df)\n",
        "\n",
        "    # Prepare to expand and process\n",
        "    expanded_data = []\n",
        "\n",
        "    # Process each row to expand and mark root nodes\n",
        "    for index, row in df.iterrows():\n",
        "        nodes = row['Ancestors'].split('~~~')\n",
        "        for i, node in enumerate(nodes):\n",
        "            parent = nodes[i-1] if i > 0 else None\n",
        "            expanded_data.append({\n",
        "                'Parents': parent,\n",
        "                'Offspring & Spouse': node,\n",
        "                'cM': row['cM'],\n",
        "                'ID': row.get('ID', '')  # Ensure ID is optionally included\n",
        "            })\n",
        "\n",
        "    # Convert the list to a DataFrame\n",
        "    expanded_df = pd.DataFrame(expanded_data)\n",
        "\n",
        "    # Group by 'Parents' and 'Offspring & Spouse' and calculate FQ and QI\n",
        "    grouped_df = expanded_df.groupby(['Parents', 'Offspring & Spouse']).agg(\n",
        "        FQ=('Offspring & Spouse', 'size'),\n",
        "        cM_sum=('cM', 'sum')\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calculate QI as the sum of all cM values divided by FQ, rounded to an integer\n",
        "    grouped_df['QI'] = (grouped_df['cM_sum'] / grouped_df['FQ']).round().astype(int)\n",
        "\n",
        "    # Mark root nodes based on first parent pairs\n",
        "    grouped_df['Root'] = grouped_df.apply(lambda x: 'Yes' if x['Parents'] in first_parents else '', axis=1)\n",
        "\n",
        "    # Select relevant columns and filter results where FQ is greater than or equal to 8\n",
        "    final_results_df = grouped_df[['Root', 'Parents', 'FQ', 'QI', 'Offspring & Spouse']]\n",
        "    final_results_df = final_results_df[final_results_df['FQ'] >= 8].copy()\n",
        "\n",
        "    # Extract the prefix for segmenting the parents\n",
        "    final_results_df['Prefix'] = final_results_df['Parents'].str.extract(r'(\\d+)')\n",
        "\n",
        "    # Ensure the Prefix column is treated as a string\n",
        "    final_results_df['Prefix'] = final_results_df['Prefix'].astype(str)\n",
        "\n",
        "    return final_results_df, len(df)\n",
        "\n",
        "def sort_within_segment(group):\n",
        "    \"\"\"Sort the group by the FQ in descending order within each segment.\"\"\"\n",
        "    return group.sort_values(by='FQ', ascending=False)\n",
        "\n",
        "def sort_all_segments(data_dict):\n",
        "    \"\"\"Sort all segments within the data based on the FQ in descending order.\"\"\"\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    # Apply sorting within each segment\n",
        "    sorted_df = df.groupby('Prefix', group_keys=False).apply(sort_within_segment).reset_index(drop=True)\n",
        "\n",
        "    # Drop the Prefix column as it is no longer needed\n",
        "    sorted_df = sorted_df.drop(columns=['Prefix'])\n",
        "\n",
        "    return sorted_df\n",
        "\n",
        "def assemble_descendants(df, total_records):\n",
        "    \"\"\"Assemble descendants starting from root parents and include FQ, QI, and % Total.\"\"\"\n",
        "    root_parents = df[df['Root'] == 'Yes']['Parents'].unique()\n",
        "    assembled_data = []\n",
        "\n",
        "    for root in root_parents:\n",
        "        current_generation = [(root, 0)]\n",
        "        assembled_data.append({\n",
        "            'Root': 'Yes',\n",
        "            'Parents and Trailing Descendants': root,\n",
        "            'FQ': df[df['Parents'] == root]['FQ'].values[0],\n",
        "            'QI': df[df['Parents'] == root]['QI'].values[0],\n",
        "            '% Total': f\"{round(df[df['Parents'] == root]['FQ'].values[0] / total_records * 100)}%\"\n",
        "        })\n",
        "        while current_generation:\n",
        "            next_generation = []\n",
        "            for parent, depth in current_generation:\n",
        "                children = df[df['Parents'] == parent]['Offspring & Spouse'].tolist()\n",
        "                for i, child in enumerate(children):\n",
        "                    fq = df[(df['Parents'] == parent) & (df['Offspring & Spouse'] == child)]['FQ'].values[0]\n",
        "                    qi = df[(df['Parents'] == parent) & (df['Offspring & Spouse'] == child)]['QI'].values[0]\n",
        "                    percent_total = f\"{round(fq / total_records * 100)}%\"\n",
        "                    descendant = f\"{'  ' * (depth + 1)}â†’ {child}\"\n",
        "                    assembled_data.append({\n",
        "                        'Root': '',\n",
        "                        'Parents and Trailing Descendants': descendant,\n",
        "                        'FQ': fq,\n",
        "                        'QI': qi,\n",
        "                        '% Total': percent_total\n",
        "                    })\n",
        "                    next_generation.append((child, depth + 1))\n",
        "            current_generation = next_generation\n",
        "\n",
        "    return pd.DataFrame(assembled_data)\n",
        "\n",
        "def save_to_excel_with_styles(df, output_path):\n",
        "    \"\"\"Save the assembled data to an Excel file with the desired format and styles.\"\"\"\n",
        "    # Create an Excel file with openpyxl\n",
        "    wb = Workbook()\n",
        "    ws = wb.active\n",
        "\n",
        "    # Adding rows to worksheet\n",
        "    headers = ['Root', 'Parents and Trailing Descendants', 'FQ', 'QI', '% Total']\n",
        "    ws.append(headers)\n",
        "\n",
        "    # Center-align the header labels\n",
        "    header_alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
        "    for cell in ws[1]:\n",
        "        cell.alignment = header_alignment\n",
        "\n",
        "    # Initialize variables to track last seen parent initial and generation\n",
        "    parent_seen = set()\n",
        "    previous_root = None\n",
        "\n",
        "    for r in dataframe_to_rows(df, index=False, header=False):\n",
        "        current_parent = r[1].strip().split('â†’')[0]\n",
        "\n",
        "        # Insert a black row at the end of each root parent's segment\n",
        "        if r[0] == 'Yes' and previous_root is not None:\n",
        "            ws.append([''] * len(headers))\n",
        "            black_row_index = ws.max_row\n",
        "            ws.row_dimensions[black_row_index].height = 5\n",
        "            for cell in ws[black_row_index]:\n",
        "                cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "        # Add the row to the worksheet\n",
        "        ws.append(r)\n",
        "        if r[0] == 'Yes':\n",
        "            parent_seen.add(current_parent)\n",
        "            previous_root = current_parent\n",
        "\n",
        "    # Add a black row at the end of the last segment\n",
        "    ws.append([''] * len(headers))\n",
        "    black_row_index = ws.max_row\n",
        "    ws.row_dimensions[black_row_index].height = 5\n",
        "    for cell in ws[black_row_index]:\n",
        "        cell.fill = PatternFill(start_color='000000', end_color='000000', fill_type='solid')\n",
        "\n",
        "    # Adding borders and alignment for readability\n",
        "    thin = Side(border_style=\"thin\", color=\"000000\")\n",
        "    left_alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
        "\n",
        "    for row in ws.iter_rows(min_row=2, max_col=len(headers), max_row=ws.max_row):\n",
        "        for cell in row:\n",
        "            cell.border = Border(top=thin, left=thin, right=thin, bottom=thin)\n",
        "        row[1].alignment = left_alignment\n",
        "\n",
        "    # Add red arrow images for root nodes\n",
        "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=1):\n",
        "        for cell in row:\n",
        "            if cell.value == 'Yes':\n",
        "                img = drawing.image.Image('/content/arrow.png')\n",
        "                img.width, img.height = (25, 25)\n",
        "                img.anchor = cell.coordinate\n",
        "                ws.add_image(img)\n",
        "                cell.value = ''  # Clear the cell value\n",
        "\n",
        "    # Save the workbook\n",
        "    wb.save(output_path)\n",
        "    print(f\"Results successfully saved to {output_path}\")\n",
        "\n",
        "# Specify the file path for the data\n",
        "input_file_path = '/content/DNA_Study_Library.xlsx'  # Replace with the correct path\n",
        "\n",
        "# Process the data and save it in a dictionary\n",
        "processed_data, total_records = process_data(input_file_path)\n",
        "\n",
        "# Sort all segments within the data\n",
        "sorted_data = sort_all_segments(processed_data.to_dict(orient='list'))\n",
        "\n",
        "# Assemble descendants into a DataFrame\n",
        "descendants_df = assemble_descendants(pd.DataFrame(sorted_data), total_records)\n",
        "\n",
        "# Save the assembled data to an Excel file with styles\n",
        "output_file_path = '/content/trailing_descendants.xlsx'\n",
        "save_to_excel_with_styles(descendants_df, output_file_path)\n",
        "\n",
        "# Print records tagged and filtered by NPFX\n",
        "print(f\"Records tagged and filtered by NPFX: {total_records}\")\n",
        "\n",
        "\n",
        "#*****************************************  Good Trailing Descendants"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArQA8vqyLHxi",
        "outputId": "ce8a1d9a-4953-4460-8e5d-1f58ee7108e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results successfully saved to /content/trailing_descendants.xlsx\n"
          ]
        }
      ]
    }
  ]
}